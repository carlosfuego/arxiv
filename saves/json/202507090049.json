[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Kühn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Kühn"
                },
                "author": "Martin J. Kühn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Søren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Søren Stobbe"
                },
                "author": "Søren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01040v1",
                "updated": "2025-06-22T20:43:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T20:43:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Fast Clifford Neural Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Clifford Neural Layers"
                },
                "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"
                },
                "authors": [
                    {
                        "name": "Tianxiang Xia"
                    },
                    {
                        "name": "Max Neuwinger"
                    },
                    {
                        "name": "Lin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Xiao"
                },
                "author": "Lin Xiao",
                "arxiv_comment": "7 pages content-wise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05258v1",
                "updated": "2025-07-07T17:59:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    55,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:55Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    55,
                    0,
                    188,
                    0
                ],
                "title": "Spatio-Temporal LLM: Reasoning about Environments and Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-Temporal LLM: Reasoning about Environments and Actions"
                },
                "summary": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/."
                },
                "authors": [
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Zhenggang Tang"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    },
                    {
                        "name": "Alex Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alex Schwing"
                },
                "author": "Alex Schwing",
                "arxiv_comment": "Code and data are available at\n  https://zoezheng126.github.io/STLLM-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05257v1",
                "updated": "2025-07-07T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions"
                },
                "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents."
                },
                "authors": [
                    {
                        "name": "Yuanzhe Hu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "23 Pages, Y. Hu and Y. Wang contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05255v1",
                "updated": "2025-07-07T17:59:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    3,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:03Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    3,
                    0,
                    188,
                    0
                ],
                "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning"
                },
                "summary": "The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners."
                },
                "authors": [
                    {
                        "name": "Yana Wei"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05254v1",
                "updated": "2025-07-07T17:58:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    58,
                    53,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:58:53Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    58,
                    53,
                    0,
                    188,
                    0
                ],
                "title": "From Marginal to Joint Predictions: Evaluating Scene-Consistent\n  Trajectory Prediction Approaches for Automated Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Marginal to Joint Predictions: Evaluating Scene-Consistent\n  Trajectory Prediction Approaches for Automated Driving"
                },
                "summary": "Accurate motion prediction of surrounding traffic participants is crucial for\nthe safe and efficient operation of automated vehicles in dynamic environments.\nMarginal prediction models commonly forecast each agent's future trajectories\nindependently, often leading to sub-optimal planning decisions for an automated\nvehicle. In contrast, joint prediction models explicitly account for the\ninteractions between agents, yielding socially and physically consistent\npredictions on a scene level. However, existing approaches differ not only in\ntheir problem formulation but also in the model architectures and\nimplementation details used, making it difficult to compare them. In this work,\nwe systematically investigate different approaches to joint motion prediction,\nincluding post-processing of the marginal predictions, explicitly training the\nmodel for joint predictions, and framing the problem as a generative task. We\nevaluate each approach in terms of prediction accuracy, multi-modality, and\ninference efficiency, offering a comprehensive analysis of the strengths and\nlimitations of each approach. Several prediction examples are available at\nhttps://frommarginaltojointpred.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion prediction of surrounding traffic participants is crucial for\nthe safe and efficient operation of automated vehicles in dynamic environments.\nMarginal prediction models commonly forecast each agent's future trajectories\nindependently, often leading to sub-optimal planning decisions for an automated\nvehicle. In contrast, joint prediction models explicitly account for the\ninteractions between agents, yielding socially and physically consistent\npredictions on a scene level. However, existing approaches differ not only in\ntheir problem formulation but also in the model architectures and\nimplementation details used, making it difficult to compare them. In this work,\nwe systematically investigate different approaches to joint motion prediction,\nincluding post-processing of the marginal predictions, explicitly training the\nmodel for joint predictions, and framing the problem as a generative task. We\nevaluate each approach in terms of prediction accuracy, multi-modality, and\ninference efficiency, offering a comprehensive analysis of the strengths and\nlimitations of each approach. Several prediction examples are available at\nhttps://frommarginaltojointpred.github.io/."
                },
                "authors": [
                    {
                        "name": "Fabian Konstantinidis"
                    },
                    {
                        "name": "Ariel Dallari Guerreiro"
                    },
                    {
                        "name": "Raphael Trumpp"
                    },
                    {
                        "name": "Moritz Sackmann"
                    },
                    {
                        "name": "Ulrich Hofmann"
                    },
                    {
                        "name": "Marco Caccamo"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "arxiv_comment": "Accepted at International Conference on Intelligent Transportation\n  Systems 2025 (ITSC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05248v1",
                "updated": "2025-07-07T17:56:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    56,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:56:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    56,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models"
                },
                "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack."
                },
                "authors": [
                    {
                        "name": "Ziqi Miao"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Yuan Xiong"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Pengyu Zhu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "21 pages, 9 figures. Code and data available at\n  https://github.com/Dtc7w3PQ/Response-Attack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05244v1",
                "updated": "2025-07-07T17:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    53,
                    13,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    53,
                    13,
                    0,
                    188,
                    0
                ],
                "title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent\n  Collaboration"
                },
                "summary": "In collaborative tasks, being able to adapt to your teammates is a necessary\nrequirement for success. When teammates are heterogeneous, such as in\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\ntheir human partners in real time. This becomes particularly challenging in\ntasks with time pressure and complex strategic spaces where the dynamics can\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\ncooperator framework that learns to represent, categorize, and adapt to a range\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\nvariational autoencoder to learn a latent strategy space from trajectory data.\nThis latent space represents the underlying strategies that agents employ.\nSubsequently, the system identifies different types of strategy by clustering\nthe data. Finally, a cooperator agent is trained to generate partners for each\ntype of strategy, conditioned on these clusters. In order to adapt to\npreviously unseen partners, we leverage a fixed-share regret minimization\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\nWe assess our approach in a customized version of the Overcooked environment,\nposing a challenging cooperative cooking task that demands strong coordination\nacross a wide range of possible strategies. Using an online user study, we show\nthat our agent outperforms current baselines when working with unfamiliar human\npartners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In collaborative tasks, being able to adapt to your teammates is a necessary\nrequirement for success. When teammates are heterogeneous, such as in\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\ntheir human partners in real time. This becomes particularly challenging in\ntasks with time pressure and complex strategic spaces where the dynamics can\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\ncooperator framework that learns to represent, categorize, and adapt to a range\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\nvariational autoencoder to learn a latent strategy space from trajectory data.\nThis latent space represents the underlying strategies that agents employ.\nSubsequently, the system identifies different types of strategy by clustering\nthe data. Finally, a cooperator agent is trained to generate partners for each\ntype of strategy, conditioned on these clusters. In order to adapt to\npreviously unseen partners, we leverage a fixed-share regret minimization\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\nWe assess our approach in a customized version of the Overcooked environment,\nposing a challenging cooperative cooking task that demands strong coordination\nacross a wide range of possible strategies. Using an online user study, we show\nthat our agent outperforms current baselines when working with unfamiliar human\npartners."
                },
                "authors": [
                    {
                        "name": "Benjamin Li"
                    },
                    {
                        "name": "Shuyang Shi"
                    },
                    {
                        "name": "Lucia Romero"
                    },
                    {
                        "name": "Huao Li"
                    },
                    {
                        "name": "Yaqi Xie"
                    },
                    {
                        "name": "Woojun Kim"
                    },
                    {
                        "name": "Stefanos Nikolaidis"
                    },
                    {
                        "name": "Michael Lewis"
                    },
                    {
                        "name": "Katia Sycara"
                    },
                    {
                        "name": "Simon Stepputtis"
                    }
                ],
                "author_detail": {
                    "name": "Simon Stepputtis"
                },
                "author": "Simon Stepputtis",
                "arxiv_comment": "Best Paper Award at the RSS 2025 Generative Models x HRI (GenAI-HRI)\n  Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10645v3",
                "updated": "2025-07-07T17:51:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    51,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2024-10-14T15:54:33Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    54,
                    33,
                    0,
                    288,
                    0
                ],
                "title": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State"
                },
                "summary": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain is prepared in a disordered phase, and the other in a\nsymmetry-breaking ground state. When the couplings are switched back on, time\nevolution ensues. We argue that in integrable systems the front separating the\nordered region recedes at the maximal velocity of quasiparticle excitations\nover the ground state. We infer that, generically, the order parameters should\nvary on a subdiffusive scale of order $t^{1/3}$, where $t$ is time, and their\nfluctuations should exhibit the same scaling. This interfacial region exhibits\nfull range correlations, indicating that it cannot be decomposed into nearly\nuncorrelated subsystems. Using the transverse-field Ising chain as a case\nstudy, we demonstrate that all order parameters follow the same universal\nscaling functions. Through an analysis of the skew information, we uncover that\nthe breakdown of cluster decomposition has a quantum contribution: each\nsubsystem within the interfacial region, with extent comparable to the region,\nexists in a macroscopic quantum state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain is prepared in a disordered phase, and the other in a\nsymmetry-breaking ground state. When the couplings are switched back on, time\nevolution ensues. We argue that in integrable systems the front separating the\nordered region recedes at the maximal velocity of quasiparticle excitations\nover the ground state. We infer that, generically, the order parameters should\nvary on a subdiffusive scale of order $t^{1/3}$, where $t$ is time, and their\nfluctuations should exhibit the same scaling. This interfacial region exhibits\nfull range correlations, indicating that it cannot be decomposed into nearly\nuncorrelated subsystems. Using the transverse-field Ising chain as a case\nstudy, we demonstrate that all order parameters follow the same universal\nscaling functions. Through an analysis of the skew information, we uncover that\nthe breakdown of cluster decomposition has a quantum contribution: each\nsubsystem within the interfacial region, with extent comparable to the region,\nexists in a macroscopic quantum state."
                },
                "authors": [
                    {
                        "name": "Vanja Marić"
                    },
                    {
                        "name": "Florent Ferro"
                    },
                    {
                        "name": "Maurizio Fagotti"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Fagotti"
                },
                "author": "Maurizio Fagotti",
                "arxiv_doi": "10.1103/982k-5jmn",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/982k-5jmn",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.10645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. Lett. 134, 236302 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05606v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05606v3",
                "updated": "2025-07-07T17:44:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    44,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-05T21:37:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    37,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation"
                },
                "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Amirali Amini"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Yakov Bart"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Upol Ehsan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Lydia Chilton"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05606v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05606v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07191v2",
                "updated": "2025-07-07T17:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    42,
                    19,
                    0,
                    188,
                    0
                ],
                "published": "2024-11-11T18:05:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "The Super Weight in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Weight in Large Language Models"
                },
                "summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengxia Yu"
                    },
                    {
                        "name": "De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Colorado J Reed"
                    },
                    {
                        "name": "Alvin Wan"
                    }
                ],
                "author_detail": {
                    "name": "Alvin Wan"
                },
                "author": "Alvin Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01931v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01931v4",
                "updated": "2025-07-08T03:19:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    3,
                    19,
                    40,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-02T17:40:47Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    40,
                    47,
                    2,
                    92,
                    0
                ],
                "title": "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows"
                },
                "summary": "Agentic AI workflows (systems that autonomously plan and act) are becoming\nwidespread, yet their task success rate on complex tasks remains low. A\npromising solution is inference-time alignment, which uses extra compute at\ntest time to improve performance. Inference-time alignment relies on three\ncomponents: sampling, evaluation, and feedback. While most prior work studies\nsampling and automatic evaluation, feedback remains underexplored. To study the\nrole of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that\nrepeatedly inserts feedback extracted from different forms of critiques (reward\nmodels or AI-generated textual feedback) between decoding steps. Through IAD,\nwe analyze feedback along four dimensions: (1) its role in the accuracy-compute\ntrade-offs with limited inference budget, (2) quantifying the gains over\ndiversity-only baselines such as best-of-N sampling, (3) effectiveness of\ncomposing feedback from reward models versus textual critique, and (4)\nrobustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL,\nIntercode, and WebShop, we show that IAD with proper integration of high\nfidelity feedback leads to consistent gains up to 10 percent absolute\nperformance improvement over various baselines such as best-of-N. Our findings\nunderscore feedback as a crucial knob for inference-time alignment of agentic\nAI workflows with limited inference budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI workflows (systems that autonomously plan and act) are becoming\nwidespread, yet their task success rate on complex tasks remains low. A\npromising solution is inference-time alignment, which uses extra compute at\ntest time to improve performance. Inference-time alignment relies on three\ncomponents: sampling, evaluation, and feedback. While most prior work studies\nsampling and automatic evaluation, feedback remains underexplored. To study the\nrole of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that\nrepeatedly inserts feedback extracted from different forms of critiques (reward\nmodels or AI-generated textual feedback) between decoding steps. Through IAD,\nwe analyze feedback along four dimensions: (1) its role in the accuracy-compute\ntrade-offs with limited inference budget, (2) quantifying the gains over\ndiversity-only baselines such as best-of-N sampling, (3) effectiveness of\ncomposing feedback from reward models versus textual critique, and (4)\nrobustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL,\nIntercode, and WebShop, we show that IAD with proper integration of high\nfidelity feedback leads to consistent gains up to 10 percent absolute\nperformance improvement over various baselines such as best-of-N. Our findings\nunderscore feedback as a crucial knob for inference-time alignment of agentic\nAI workflows with limited inference budget."
                },
                "authors": [
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Nino Scherrer"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01931v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01931v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05229v1",
                "updated": "2025-07-07T17:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    39,
                    11,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    39,
                    11,
                    0,
                    188,
                    0
                ],
                "title": "Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV\n  Footage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV\n  Footage"
                },
                "summary": "Multi-object tracking (MOT) aims to maintain consistent identities of objects\nacross video frames. Associating objects in low-frame-rate videos captured by\nmoving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex\ndue to rapid changes in object appearance and position within the frame. The\ntask becomes even more challenging due to image degradation caused by cloud\nvideo streaming and compression algorithms. We present how instance association\nlearning from single-frame annotations can overcome these challenges. We show\nthat global features of the scene provide crucial context for low-FPS instance\nassociation, allowing our solution to be robust to distractors and gaps in\ndetections. We also demonstrate that such a tracking approach maintains high\nassociation quality even when reducing the input image resolution and latent\nrepresentation size for faster inference. Finally, we present a benchmark\ndataset of annotated military vehicles collected from publicly available data\nsources. This paper was initially presented at the NATO Science and Technology\nOrganization Symposium (ICMCIS) organized by the Information Systems Technology\n(IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in\nOeiras, Portugal, 13-14 May 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-object tracking (MOT) aims to maintain consistent identities of objects\nacross video frames. Associating objects in low-frame-rate videos captured by\nmoving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex\ndue to rapid changes in object appearance and position within the frame. The\ntask becomes even more challenging due to image degradation caused by cloud\nvideo streaming and compression algorithms. We present how instance association\nlearning from single-frame annotations can overcome these challenges. We show\nthat global features of the scene provide crucial context for low-FPS instance\nassociation, allowing our solution to be robust to distractors and gaps in\ndetections. We also demonstrate that such a tracking approach maintains high\nassociation quality even when reducing the input image resolution and latent\nrepresentation size for faster inference. Finally, we present a benchmark\ndataset of annotated military vehicles collected from publicly available data\nsources. This paper was initially presented at the NATO Science and Technology\nOrganization Symposium (ICMCIS) organized by the Information Systems Technology\n(IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in\nOeiras, Portugal, 13-14 May 2025."
                },
                "authors": [
                    {
                        "name": "Markiyan Kostiv"
                    },
                    {
                        "name": "Anatolii Adamovskyi"
                    },
                    {
                        "name": "Yevhen Cherniavskyi"
                    },
                    {
                        "name": "Mykyta Varenyk"
                    },
                    {
                        "name": "Ostap Viniavskyi"
                    },
                    {
                        "name": "Igor Krashenyi"
                    },
                    {
                        "name": "Oles Dobosevych"
                    }
                ],
                "author_detail": {
                    "name": "Oles Dobosevych"
                },
                "author": "Oles Dobosevych",
                "arxiv_doi": "10.1109/ICMCIS64378.2025.11047873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICMCIS64378.2025.11047873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.05229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18071v2",
                "updated": "2025-07-07T17:38:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    38,
                    20,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-23T16:16:46Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    16,
                    46,
                    4,
                    143,
                    0
                ],
                "title": "Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals"
                },
                "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning-the ability to derive\ngeneral rules from incomplete evidence, remains underexplored. This paper\ninvestigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose AlignXplore, a model that leverages extended reasoning chains to enable\nsystematic preference inference from behavioral signals in users' interaction\nhistories. Such explicit preference articulation enables efficient streaming\ninference: when new behavioral signals emerge, the model can directly build\nupon previously inferred preference descriptions rather than reprocessing\nhistorical signals from scratch, while also supporting iterative refinement to\nthe inferred preferences. We develop AlignXplore by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that AlignXplore achieves\nsubstantial improvements over the backbone model by an average of 15.49\\% on\nin-domain and out-of-domain benchmarks, while maintaining strong generalization\nability across different input formats and downstream models. Further analyses\nestablish best practices for preference inference learning through systematic\ncomparison of reward modeling strategies, while revealing the emergence of\nhuman-like inductive reasoning patterns during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning-the ability to derive\ngeneral rules from incomplete evidence, remains underexplored. This paper\ninvestigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose AlignXplore, a model that leverages extended reasoning chains to enable\nsystematic preference inference from behavioral signals in users' interaction\nhistories. Such explicit preference articulation enables efficient streaming\ninference: when new behavioral signals emerge, the model can directly build\nupon previously inferred preference descriptions rather than reprocessing\nhistorical signals from scratch, while also supporting iterative refinement to\nthe inferred preferences. We develop AlignXplore by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that AlignXplore achieves\nsubstantial improvements over the backbone model by an average of 15.49\\% on\nin-domain and out-of-domain benchmarks, while maintaining strong generalization\nability across different input formats and downstream models. Further analyses\nestablish best practices for preference inference learning through systematic\ncomparison of reward modeling strategies, while revealing the emergence of\nhuman-like inductive reasoning patterns during training."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05228v1",
                "updated": "2025-07-07T17:37:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    37,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:37:16Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    37,
                    16,
                    0,
                    188,
                    0
                ],
                "title": "Cascade: Token-Sharded Private LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade: Token-Sharded Private LLM Inference"
                },
                "summary": "As LLMs continue to increase in parameter size, the computational resources\nrequired to run them are available to fewer parties. Therefore, third-party\ninference services -- where LLMs are hosted by third parties with significant\ncomputational resources -- are becoming increasingly popular. However, third\nparty inference raises critical concerns about user data privacy. To mitigate\nthese risks, privacy researchers have developed provably secure schemes for\nthird-party inference, such as Secure Multi-Party Computation (SMPC). However,\nSMPC protocols have significant computational and communication overhead, and\ndo not scale to large models. In this work, we propose a new multi-party\ninference protocol, Cascade, that avoids these punitive costs by leveraging\nsharding in the sequence dimension to maintain privacy, trading off\ncryptographic privacy guarantees for increased performance and scalability. We\ndemonstrate that Cascade is resistant to a generalization of a recent attack\nthat is highly effective against other statistical privacy schemes, and that it\nis further resistant to learning-based attacks. As Cascade is orders of\nmagnitude faster than existing schemes, our findings offer practical solutions\nfor secure deployment of modern state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs continue to increase in parameter size, the computational resources\nrequired to run them are available to fewer parties. Therefore, third-party\ninference services -- where LLMs are hosted by third parties with significant\ncomputational resources -- are becoming increasingly popular. However, third\nparty inference raises critical concerns about user data privacy. To mitigate\nthese risks, privacy researchers have developed provably secure schemes for\nthird-party inference, such as Secure Multi-Party Computation (SMPC). However,\nSMPC protocols have significant computational and communication overhead, and\ndo not scale to large models. In this work, we propose a new multi-party\ninference protocol, Cascade, that avoids these punitive costs by leveraging\nsharding in the sequence dimension to maintain privacy, trading off\ncryptographic privacy guarantees for increased performance and scalability. We\ndemonstrate that Cascade is resistant to a generalization of a recent attack\nthat is highly effective against other statistical privacy schemes, and that it\nis further resistant to learning-based attacks. As Cascade is orders of\nmagnitude faster than existing schemes, our findings offer practical solutions\nfor secure deployment of modern state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Rahul Thomas"
                    },
                    {
                        "name": "Louai Zahran"
                    },
                    {
                        "name": "Erica Choi"
                    },
                    {
                        "name": "Akilesh Potti"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Arka Pal"
                    }
                ],
                "author_detail": {
                    "name": "Arka Pal"
                },
                "author": "Arka Pal",
                "arxiv_comment": "To be published in ICML 2025 Main Proceedings as \"Hidden No More:\n  Attacking and Defending Private Third-Party LLM Inference\", together with\n  arXiv:2505.18332",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05220v1",
                "updated": "2025-07-07T17:33:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    33,
                    18,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:33:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    33,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "QuEst: Enhancing Estimates of Quantile-Based Distributional Measures\n  Using Model Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuEst: Enhancing Estimates of Quantile-Based Distributional Measures\n  Using Model Predictions"
                },
                "summary": "As machine learning models grow increasingly competent, their predictions can\nsupplement scarce or expensive data in various important domains. In support of\nthis paradigm, algorithms have emerged to combine a small amount of\nhigh-fidelity observed data with a much larger set of imputed model outputs to\nestimate some quantity of interest. Yet current hybrid-inference tools target\nonly means or single quantiles, limiting their applicability for many critical\ndomains and use cases. We present QuEst, a principled framework to merge\nobserved and imputed data to deliver point estimates and rigorous confidence\nintervals for a wide family of quantile-based distributional measures. QuEst\ncovers a range of measures, from tail risk (CVaR) to population segments such\nas quartiles, that are central to fields such as economics, sociology,\neducation, medicine, and more. We extend QuEst to multidimensional metrics, and\nintroduce an additional optimization technique to further reduce variance in\nthis and other hybrid estimators. We demonstrate the utility of our framework\nthrough experiments in economic modeling, opinion polling, and language model\nauto-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models grow increasingly competent, their predictions can\nsupplement scarce or expensive data in various important domains. In support of\nthis paradigm, algorithms have emerged to combine a small amount of\nhigh-fidelity observed data with a much larger set of imputed model outputs to\nestimate some quantity of interest. Yet current hybrid-inference tools target\nonly means or single quantiles, limiting their applicability for many critical\ndomains and use cases. We present QuEst, a principled framework to merge\nobserved and imputed data to deliver point estimates and rigorous confidence\nintervals for a wide family of quantile-based distributional measures. QuEst\ncovers a range of measures, from tail risk (CVaR) to population segments such\nas quartiles, that are central to fields such as economics, sociology,\neducation, medicine, and more. We extend QuEst to multidimensional metrics, and\nintroduce an additional optimization technique to further reduce variance in\nthis and other hybrid estimators. We demonstrate the utility of our framework\nthrough experiments in economic modeling, opinion polling, and language model\nauto-evaluation."
                },
                "authors": [
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Thomas P Zollo"
                    },
                    {
                        "name": "Benjamin Eyre"
                    },
                    {
                        "name": "Amogh Inamdar"
                    },
                    {
                        "name": "David Madras"
                    },
                    {
                        "name": "Richard Zemel"
                    }
                ],
                "author_detail": {
                    "name": "Richard Zemel"
                },
                "author": "Richard Zemel",
                "arxiv_comment": "Published as a conference paper at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v2",
                "updated": "2025-07-07T17:32:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    32,
                    51,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05215v1",
                "updated": "2025-07-07T17:29:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    29,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:29:08Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    29,
                    8,
                    0,
                    188,
                    0
                ],
                "title": "Testing the ubiquitous presence of very high energy emission in\n  gamma-ray bursts with the MAGIC telescopes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing the ubiquitous presence of very high energy emission in\n  gamma-ray bursts with the MAGIC telescopes"
                },
                "summary": "Gamma-ray bursts (GRBs) are the most powerful transient objects in the\nUniverse, and they are a primary target for the MAGIC Collaboration.\nRecognizing the challenges of observing these elusive objects with Imaging\nAtmospheric Cherenkov Telescopes (IACTs), we implemented a dedicated\nobservational strategy that included an automated procedure for rapid\nre-pointing to transient sources. Since 2013, this automated procedure has\nenabled MAGIC to observe GRBs at a rate of approximately ten per year, which\nled to the successful detection of two GRBs at very high energies (VHE; E > 100\nGeV). We present a comprehensive analysis of 42 non-detected GRBs (4 short\nGRBs) observed by MAGIC from 2013 to 2019. We derived upper limits (ULs) on the\nobserved energy flux as well as on the intrinsic energy flux corrected for\nabsorption by the extragalactic background light (EBL) from the MAGIC\nobservations in selected energy and time intervals. We conducted a\ncomprehensive study of their properties to investigate the reasons for these\nnon-detections, including the possible peculiar properties of TeV-detected\nGRBs. We find that strong EBL absorption significantly hinders TeV detection\nfor the majority of GRBs in our sample. For a subset of 6 GRBs with redshift z\n< 2, we compared the UL on the intrinsic flux in the VHE domain with the\nsimultaneous X-ray flux, which is observed to be at the same level in the\ncurrent population of TeV-detected GRBs. Based on these inferred MAGIC ULs, we\nconclude that a VHE component with a luminosity comparable to the\nsimultaneously observed X-ray luminosity cannot be ruled out for this sample.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts (GRBs) are the most powerful transient objects in the\nUniverse, and they are a primary target for the MAGIC Collaboration.\nRecognizing the challenges of observing these elusive objects with Imaging\nAtmospheric Cherenkov Telescopes (IACTs), we implemented a dedicated\nobservational strategy that included an automated procedure for rapid\nre-pointing to transient sources. Since 2013, this automated procedure has\nenabled MAGIC to observe GRBs at a rate of approximately ten per year, which\nled to the successful detection of two GRBs at very high energies (VHE; E > 100\nGeV). We present a comprehensive analysis of 42 non-detected GRBs (4 short\nGRBs) observed by MAGIC from 2013 to 2019. We derived upper limits (ULs) on the\nobserved energy flux as well as on the intrinsic energy flux corrected for\nabsorption by the extragalactic background light (EBL) from the MAGIC\nobservations in selected energy and time intervals. We conducted a\ncomprehensive study of their properties to investigate the reasons for these\nnon-detections, including the possible peculiar properties of TeV-detected\nGRBs. We find that strong EBL absorption significantly hinders TeV detection\nfor the majority of GRBs in our sample. For a subset of 6 GRBs with redshift z\n< 2, we compared the UL on the intrinsic flux in the VHE domain with the\nsimultaneous X-ray flux, which is observed to be at the same level in the\ncurrent population of TeV-detected GRBs. Based on these inferred MAGIC ULs, we\nconclude that a VHE component with a luminosity comparable to the\nsimultaneously observed X-ray luminosity cannot be ruled out for this sample."
                },
                "authors": [
                    {
                        "name": "S. Abe"
                    },
                    {
                        "name": "J. Abhir"
                    },
                    {
                        "name": "A. Abhishek"
                    },
                    {
                        "name": "V. A. Acciari"
                    },
                    {
                        "name": "A. Aguasca-Cabot"
                    },
                    {
                        "name": "I. Agudo"
                    },
                    {
                        "name": "T. Aniello"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "L. A. Antonelli"
                    },
                    {
                        "name": "A. Arbet Engels"
                    },
                    {
                        "name": "C. Arcaro"
                    },
                    {
                        "name": "T. T. H. Arnesen"
                    },
                    {
                        "name": "K. Asano"
                    },
                    {
                        "name": "A. Babic"
                    },
                    {
                        "name": "C. Bakshi"
                    },
                    {
                        "name": "U. Barres de Almeida"
                    },
                    {
                        "name": "J. A. Barrio"
                    },
                    {
                        "name": "L. Barrios-Jimenez"
                    },
                    {
                        "name": "I. Batkovic"
                    },
                    {
                        "name": "J. Baxter"
                    },
                    {
                        "name": "J. Becerra Gonzalez"
                    },
                    {
                        "name": "W. Bednarek"
                    },
                    {
                        "name": "E. Bernardini"
                    },
                    {
                        "name": "J. Bernete"
                    },
                    {
                        "name": "A. Berti"
                    },
                    {
                        "name": "J. Besenrieder"
                    },
                    {
                        "name": "C. Bigongiari"
                    },
                    {
                        "name": "A. Biland"
                    },
                    {
                        "name": "O. Blanch"
                    },
                    {
                        "name": "G. Bonnoli"
                    },
                    {
                        "name": "Ž. Bošnjak"
                    },
                    {
                        "name": "E. Bronzini"
                    },
                    {
                        "name": "I. Burelli"
                    },
                    {
                        "name": "A. Campoy-Ordaz"
                    },
                    {
                        "name": "A. Carosi"
                    },
                    {
                        "name": "R. Carosi"
                    },
                    {
                        "name": "M. Carretero-Castrillo"
                    },
                    {
                        "name": "A. J. Castro-Tirado"
                    },
                    {
                        "name": "D. Cerasole"
                    },
                    {
                        "name": "G. Ceribella"
                    },
                    {
                        "name": "Y. Chai"
                    },
                    {
                        "name": "A. Cifuentes"
                    },
                    {
                        "name": "J. L. Contreras"
                    },
                    {
                        "name": "J. Cortina"
                    },
                    {
                        "name": "S. Covino"
                    },
                    {
                        "name": "G. D'Amico"
                    },
                    {
                        "name": "P. Da Vela"
                    },
                    {
                        "name": "F. Dazzi"
                    },
                    {
                        "name": "A. De Angelis"
                    },
                    {
                        "name": "B. De Lotto"
                    },
                    {
                        "name": "R. de Menezes"
                    },
                    {
                        "name": "M. Delfino"
                    },
                    {
                        "name": "J. Delgado"
                    },
                    {
                        "name": "C. Delgado Mendez"
                    },
                    {
                        "name": "F. Di Pierro"
                    },
                    {
                        "name": "R. Di Tria"
                    },
                    {
                        "name": "L. Di Venere"
                    },
                    {
                        "name": "A. Dinesh"
                    },
                    {
                        "name": "D. Dominis Prester"
                    },
                    {
                        "name": "A. Donini"
                    },
                    {
                        "name": "D. Dorner"
                    },
                    {
                        "name": "M. Doro"
                    },
                    {
                        "name": "L. Eisenberger"
                    },
                    {
                        "name": "D. Elsaesser"
                    },
                    {
                        "name": "J. Escudero"
                    },
                    {
                        "name": "L. Fariña"
                    },
                    {
                        "name": "A. Fattorini"
                    },
                    {
                        "name": "L. Foffano"
                    },
                    {
                        "name": "L. Font"
                    },
                    {
                        "name": "S. Fröse"
                    },
                    {
                        "name": "S. Fukami"
                    },
                    {
                        "name": "Y. Fukazawa"
                    },
                    {
                        "name": "R. J. García López"
                    },
                    {
                        "name": "S. García Soto"
                    },
                    {
                        "name": "M. Garczarczyk"
                    },
                    {
                        "name": "S. Gasparyan"
                    },
                    {
                        "name": "M. Gaug"
                    },
                    {
                        "name": "J. G. Giesbrecht Paiva"
                    },
                    {
                        "name": "N. Giglietto"
                    },
                    {
                        "name": "F. Giordano"
                    },
                    {
                        "name": "P. Gliwny"
                    },
                    {
                        "name": "N. Godinovic"
                    },
                    {
                        "name": "T. Gradetzke"
                    },
                    {
                        "name": "R. Grau"
                    },
                    {
                        "name": "D. Green"
                    },
                    {
                        "name": "J. G. Green"
                    },
                    {
                        "name": "P. Günther"
                    },
                    {
                        "name": "D. Hadasch"
                    },
                    {
                        "name": "A. Hahn"
                    },
                    {
                        "name": "T. Hassan"
                    },
                    {
                        "name": "L. Heckmann"
                    },
                    {
                        "name": "J. Herrera Llorente"
                    },
                    {
                        "name": "D. Hrupec"
                    },
                    {
                        "name": "R. Imazawa"
                    },
                    {
                        "name": "S. Inoue"
                    },
                    {
                        "name": "D. Israyelyan"
                    },
                    {
                        "name": "J. Jahanvi"
                    },
                    {
                        "name": "I. Jiménez Martínez"
                    },
                    {
                        "name": "J. Jiménez Quiles"
                    },
                    {
                        "name": "J. Jormanainen"
                    },
                    {
                        "name": "S. Kankkunen"
                    },
                    {
                        "name": "T. Kayanoki"
                    },
                    {
                        "name": "J. Konrad"
                    },
                    {
                        "name": "P. M. Kouch"
                    },
                    {
                        "name": "H. Kubo"
                    },
                    {
                        "name": "J. Kushida"
                    },
                    {
                        "name": "M. Láinez"
                    },
                    {
                        "name": "A. Lamastra"
                    },
                    {
                        "name": "E. Lindfors"
                    },
                    {
                        "name": "S. Lombardi"
                    },
                    {
                        "name": "F. Longo"
                    },
                    {
                        "name": "R. López-Coto"
                    },
                    {
                        "name": "M. López-Moya"
                    },
                    {
                        "name": "A. López-Oramas"
                    },
                    {
                        "name": "S. Loporchio"
                    },
                    {
                        "name": "L. Lulic"
                    },
                    {
                        "name": "E. Lyard"
                    },
                    {
                        "name": "P. Majumdar"
                    },
                    {
                        "name": "M. Makariev"
                    },
                    {
                        "name": "M. Mallamaci"
                    },
                    {
                        "name": "G. Maneva"
                    },
                    {
                        "name": "M. Manganaro"
                    },
                    {
                        "name": "S. Mangano"
                    },
                    {
                        "name": "K. Mannheim"
                    },
                    {
                        "name": "S. Marchesi"
                    },
                    {
                        "name": "M. Mariotti"
                    },
                    {
                        "name": "M. Martínez"
                    },
                    {
                        "name": "P. Maruševec"
                    },
                    {
                        "name": "A. Mas-Aguilar"
                    },
                    {
                        "name": "D. Mazin"
                    },
                    {
                        "name": "S. Menchiari"
                    },
                    {
                        "name": "J. Méndez Gallego"
                    },
                    {
                        "name": "S. Menon"
                    },
                    {
                        "name": "D. Miceli"
                    },
                    {
                        "name": "J. M. Miranda"
                    },
                    {
                        "name": "R. Mirzoyan"
                    },
                    {
                        "name": "M. Molero González"
                    },
                    {
                        "name": "E. Molina"
                    },
                    {
                        "name": "H. A. Mondal"
                    },
                    {
                        "name": "A. Moralejo"
                    },
                    {
                        "name": "E. Moretti"
                    },
                    {
                        "name": "T. Nakamori"
                    },
                    {
                        "name": "C. Nanci"
                    },
                    {
                        "name": "L. Nava"
                    },
                    {
                        "name": "V. Neustroev"
                    },
                    {
                        "name": "L. Nickel"
                    },
                    {
                        "name": "M. Nievas Rosillo"
                    },
                    {
                        "name": "C. Nigro"
                    },
                    {
                        "name": "L. Nikolic"
                    },
                    {
                        "name": "K. Nilsson"
                    },
                    {
                        "name": "K. Nishijima"
                    },
                    {
                        "name": "K. Noda"
                    },
                    {
                        "name": "S. Nozaki"
                    },
                    {
                        "name": "A. Okumura"
                    },
                    {
                        "name": "J. Otero-Santos"
                    },
                    {
                        "name": "S. Paiano"
                    },
                    {
                        "name": "D. Paneque"
                    },
                    {
                        "name": "R. Paoletti"
                    },
                    {
                        "name": "J. M. Paredes"
                    },
                    {
                        "name": "M. Peresano"
                    },
                    {
                        "name": "M. Persic"
                    },
                    {
                        "name": "M. Pihet"
                    },
                    {
                        "name": "G. Pirola"
                    },
                    {
                        "name": "F. Podobnik"
                    },
                    {
                        "name": "P. G. Prada Moroni"
                    },
                    {
                        "name": "E. Prandini"
                    },
                    {
                        "name": "M. Ribó"
                    },
                    {
                        "name": "J. Rico"
                    },
                    {
                        "name": "C. Righi"
                    },
                    {
                        "name": "N. Sahakyan"
                    },
                    {
                        "name": "T. Saito"
                    },
                    {
                        "name": "F. G. Saturni"
                    },
                    {
                        "name": "K. Schmitz"
                    },
                    {
                        "name": "F. Schmuckermaier"
                    },
                    {
                        "name": "A. Sciaccaluga"
                    },
                    {
                        "name": "G. Silvestri"
                    },
                    {
                        "name": "A. Simongini"
                    },
                    {
                        "name": "J. Sitarek"
                    },
                    {
                        "name": "V. Sliusar"
                    },
                    {
                        "name": "D. Sobczynska"
                    },
                    {
                        "name": "A. Stamerra"
                    },
                    {
                        "name": "J. Striškovic"
                    },
                    {
                        "name": "D. Strom"
                    },
                    {
                        "name": "M. Strzys"
                    },
                    {
                        "name": "Y. Suda"
                    },
                    {
                        "name": "H. Tajima"
                    },
                    {
                        "name": "M. Takahashi"
                    },
                    {
                        "name": "R. Takeishi"
                    },
                    {
                        "name": "P. Temnikov"
                    },
                    {
                        "name": "K. Terauchi"
                    },
                    {
                        "name": "T. Terzic"
                    },
                    {
                        "name": "M. Teshima"
                    },
                    {
                        "name": "A. Tutone"
                    },
                    {
                        "name": "S. Ubach"
                    },
                    {
                        "name": "J. van Scherpenberg"
                    },
                    {
                        "name": "M. Vazquez Acosta"
                    },
                    {
                        "name": "S. Ventura"
                    },
                    {
                        "name": "G. Verna"
                    },
                    {
                        "name": "I. Viale"
                    },
                    {
                        "name": "A. Vigliano"
                    },
                    {
                        "name": "C. F. Vigorito"
                    },
                    {
                        "name": "E. Visentin"
                    },
                    {
                        "name": "V. Vitale"
                    },
                    {
                        "name": "I. Vovk"
                    },
                    {
                        "name": "R. Walter"
                    },
                    {
                        "name": "F. Wersig"
                    },
                    {
                        "name": "M. Will"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "P. K. H. Yeung"
                    }
                ],
                "author_detail": {
                    "name": "P. K. H. Yeung"
                },
                "author": "P. K. H. Yeung",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23824v2",
                "updated": "2025-07-07T17:28:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    28,
                    31,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-28T06:14:30Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    6,
                    14,
                    30,
                    2,
                    148,
                    0
                ],
                "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:\n  Baseline Approaches and Automatic Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:\n  Baseline Approaches and Automatic Evaluation"
                },
                "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to aid the peer review process of scientific publication amid\nthe peer review crisis. However, having AI models generate full reviews in the\nsame way as human reviewers risks exacerbating the irresponsible use of\nLLM-generated reviews. As an alternative, we propose adopting LLMs as\nmanuscript quality checkers. We introduce several baseline approaches and an\nextendable automatic evaluation framework using top reasoning LLMs as judges to\ntackle the difficulty of recruiting domain experts for manual evaluation.\nUtilizing papers withdrawn from arXiv, we validated our proposed methods with\nseveral leading reasoning LLMs from multiple vendors and assessed their\nperformance and API costs for identifying critical errors and unsoundness\nproblems in scientific papers. o3 exhibited the best problem identification\nperformance among all models at a modest cost. This paper provides insights\ninto document-based scientific understanding/reasoning and lays a foundation\nfor future applications. Our dataset, code, and model outputs are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have sparked interest in\nutilizing them to aid the peer review process of scientific publication amid\nthe peer review crisis. However, having AI models generate full reviews in the\nsame way as human reviewers risks exacerbating the irresponsible use of\nLLM-generated reviews. As an alternative, we propose adopting LLMs as\nmanuscript quality checkers. We introduce several baseline approaches and an\nextendable automatic evaluation framework using top reasoning LLMs as judges to\ntackle the difficulty of recruiting domain experts for manual evaluation.\nUtilizing papers withdrawn from arXiv, we validated our proposed methods with\nseveral leading reasoning LLMs from multiple vendors and assessed their\nperformance and API costs for identifying critical errors and unsoundness\nproblems in scientific papers. o3 exhibited the best problem identification\nperformance among all models at a modest cost. This paper provides insights\ninto document-based scientific understanding/reasoning and lays a foundation\nfor future applications. Our dataset, code, and model outputs are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Tianmai M. Zhang"
                    },
                    {
                        "name": "Neil F. Abernethy"
                    }
                ],
                "author_detail": {
                    "name": "Neil F. Abernethy"
                },
                "author": "Neil F. Abernethy",
                "arxiv_comment": "Add results from new experiments; update discussion and GitHub link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05211v1",
                "updated": "2025-07-07T17:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation"
                },
                "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg."
                },
                "authors": [
                    {
                        "name": "Zongyan Han"
                    },
                    {
                        "name": "Mohamed El Amine Boudjoghra"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05210v1",
                "updated": "2025-07-07T17:20:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    20,
                    34,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:20:34Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    20,
                    34,
                    0,
                    188,
                    0
                ],
                "title": "Identification of Causal Effects with a Bunching Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of Causal Effects with a Bunching Design"
                },
                "summary": "We show that causal effects can be identified when there is bunching in the\ndistribution of a continuous treatment variable, without imposing any\nparametric assumptions. This yields a new nonparametric method for overcoming\nselection bias in the absence of instrumental variables, panel data, or other\npopular research designs for causal inference. The method leverages the change\nof variables theorem from integration theory, relating the selection bias to\nthe ratio of the density of the treatment and the density of the part of the\noutcome that varies with confounders. At the bunching point, the treatment\nlevel is constant, so the variation in the outcomes is due entirely to\nunobservables, allowing us to identify the denominator. Our main result\nidentifies the average causal response to the treatment among individuals who\nmarginally select into the bunching point. We further show that under\nadditional smoothness assumptions on the selection bias, treatment effects away\nfrom the bunching point may also be identified. We propose estimators based on\nstandard software packages and apply the method to estimate the effect of\nmaternal smoking during pregnancy on birth weight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that causal effects can be identified when there is bunching in the\ndistribution of a continuous treatment variable, without imposing any\nparametric assumptions. This yields a new nonparametric method for overcoming\nselection bias in the absence of instrumental variables, panel data, or other\npopular research designs for causal inference. The method leverages the change\nof variables theorem from integration theory, relating the selection bias to\nthe ratio of the density of the treatment and the density of the part of the\noutcome that varies with confounders. At the bunching point, the treatment\nlevel is constant, so the variation in the outcomes is due entirely to\nunobservables, allowing us to identify the denominator. Our main result\nidentifies the average causal response to the treatment among individuals who\nmarginally select into the bunching point. We further show that under\nadditional smoothness assumptions on the selection bias, treatment effects away\nfrom the bunching point may also be identified. We propose estimators based on\nstandard software packages and apply the method to estimate the effect of\nmaternal smoking during pregnancy on birth weight."
                },
                "authors": [
                    {
                        "name": "Carolina Caetano"
                    },
                    {
                        "name": "Gregorio Caetano"
                    },
                    {
                        "name": "Leonard Goff"
                    },
                    {
                        "name": "Eric Nielsen"
                    }
                ],
                "author_detail": {
                    "name": "Eric Nielsen"
                },
                "author": "Eric Nielsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05209v1",
                "updated": "2025-07-07T17:19:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    19,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:19:21Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    19,
                    21,
                    0,
                    188,
                    0
                ],
                "title": "Hierarchical Subtraction with Neural Density Estimators as a General\n  Solution to Overlapping Gravitational Wave Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Subtraction with Neural Density Estimators as a General\n  Solution to Overlapping Gravitational Wave Signals"
                },
                "summary": "Overlapping gravitational wave (GW) signals are expected in the\nthird-generation (3G) GW detectors, leading to one of the major challenges in\nGW data analysis. Inference of overlapping GW sources is complicated - it has\nbeen reported that hierarchical inference with signal subtraction may amplify\nerrors, while joint estimation, though more accurate, is computationally\nexpensive. However, in this work, we show that hierarchical subtraction can\nachieve accurate results with a sufficient number of iterations, and on the\nother hand, neural density estimators, being able to generate posterior samples\nrapidly, make it possible to perform signal subtraction and inference\nrepeatedly. We further develop likelihood-based resampling to accelerate the\nconvergence of the iterative subtraction. Our method provides fast and accurate\ninference for overlapping GW signals and is highly adaptable to various source\ntypes and time separations, offering a potential general solution for\noverlapping GW signal analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlapping gravitational wave (GW) signals are expected in the\nthird-generation (3G) GW detectors, leading to one of the major challenges in\nGW data analysis. Inference of overlapping GW sources is complicated - it has\nbeen reported that hierarchical inference with signal subtraction may amplify\nerrors, while joint estimation, though more accurate, is computationally\nexpensive. However, in this work, we show that hierarchical subtraction can\nachieve accurate results with a sufficient number of iterations, and on the\nother hand, neural density estimators, being able to generate posterior samples\nrapidly, make it possible to perform signal subtraction and inference\nrepeatedly. We further develop likelihood-based resampling to accelerate the\nconvergence of the iterative subtraction. Our method provides fast and accurate\ninference for overlapping GW signals and is highly adaptable to various source\ntypes and time separations, offering a potential general solution for\noverlapping GW signal analysis."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Hu"
                },
                "author": "Qian Hu",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05200v1",
                "updated": "2025-07-07T17:01:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    17,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:01:17Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    17,
                    0,
                    188,
                    0
                ],
                "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code"
                },
                "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation."
                },
                "authors": [
                    {
                        "name": "Susmita Das"
                    },
                    {
                        "name": "Madhusudan Ghosh"
                    },
                    {
                        "name": "Priyanka Swami"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Gul Calikli"
                    }
                ],
                "author_detail": {
                    "name": "Gul Calikli"
                },
                "author": "Gul Calikli",
                "arxiv_doi": "10.1145/3726302.3730212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05195v1",
                "updated": "2025-07-07T16:54:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    54,
                    18,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:54:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    54,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "Train-before-Test Harmonizes Language Model Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-before-Test Harmonizes Language Model Rankings"
                },
                "summary": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking."
                },
                "authors": [
                    {
                        "name": "Guanhua Zhang"
                    },
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Moritz Hardt"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Hardt"
                },
                "author": "Moritz Hardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05995v2",
                "updated": "2025-07-07T16:43:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    43,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-08T13:01:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose the\nNativQA framework, which can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages -- ranging from extremely\nlow-resource to high-resource languages -- resulting in over 300K\nQuestion-Answer (QA) pairs. The developed resources can be used for LLM\nbenchmarking and further fine-tuning. The framework has been made publicly\navailable for the community (https://gitlab.com/nativqa/nativqa-framework).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose the\nNativQA framework, which can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages -- ranging from extremely\nlow-resource to high-resource languages -- resulting in over 300K\nQuestion-Answer (QA) pairs. The developed resources can be used for LLM\nbenchmarking and further fine-tuning. The framework has been made publicly\navailable for the community (https://gitlab.com/nativqa/nativqa-framework)."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Sahinur Rahman Laskar"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shammur Absar Chowdhury"
                },
                "author": "Shammur Absar Chowdhury",
                "arxiv_comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05179v1",
                "updated": "2025-07-07T16:34:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    34,
                    28,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:34:28Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    34,
                    28,
                    0,
                    188,
                    0
                ],
                "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations"
                },
                "summary": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages."
                },
                "authors": [
                    {
                        "name": "Pulkit Bansal"
                    },
                    {
                        "name": "Raghvendra Kumar"
                    },
                    {
                        "name": "Shakti Singh"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05178v1",
                "updated": "2025-07-07T16:33:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    33,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:33:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    33,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale"
                },
                "summary": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain."
                },
                "authors": [
                    {
                        "name": "Jonathan Hyun"
                    },
                    {
                        "name": "Nicholas R Waytowich"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at:\n  http://generalroboticslab.com/CREW-Wildfire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05176v1",
                "updated": "2025-07-07T16:31:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    31,
                    7,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:31:07Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    31,
                    7,
                    0,
                    188,
                    0
                ],
                "title": "Mass Proxy Quality of Massive Halo Properties in the IllustrisTNG and\n  FLAMINGO Simulations: I. Hot Gas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass Proxy Quality of Massive Halo Properties in the IllustrisTNG and\n  FLAMINGO Simulations: I. Hot Gas"
                },
                "summary": "We examine scale and redshift dependence of mass-property relations (MPRs)\nfor five hot gas properties of two large group- and cluster-scale halo samples\nrealized by the IllustrisTNG, TNG-Cluster and FLAMINGO cosmological\nhydrodynamical simulations. For intrinsic properties of i) hot gas mass\n($M_{\\rm gas}$), ii) spectroscopic-like temperature ($T_{\\rm sl}$), iii)\nsoft-band X-ray luminosity ($L_{\\rm X}$), and iv) X-ray ($Y_{\\rm X}$) and v)\nSunyaev-Zel'dovich ($Y_{\\rm SZ}$) thermal energies, we use MPR parameters to\ninfer mass proxy quality (MPQ) -- the implied scatter in total halo mass\nconditioned on a property -- for halos with $M_{\\rm 500c} \\geq 10^{13}{\\, {\\rm\nM}_\\odot}$ at redshifts, $z \\in \\{0, 0.5, 1, 2\\}$. We find: (1) in general,\nscaling relation slopes and covariance display moderate to strong dependence on\nhalo mass, with redshift dependence secondary, (2) for halos with $M_{\\rm 500c}\n> 10^{14}{\\, {\\rm M}_\\odot}$, scalings of $M_{\\rm gas}$ and $Y_{\\rm SZ}$\nsimplify toward self-similar slope and constant intrinsic scatter (5 and 10%,\nrespectively) nearly independent of scale, making both measures ideal for\ncluster finding and characterization to $z=2$, (3) halo mass-conditioned\nlikelihoods of hot gas mass and thermal energy at fixed halo mass closely\nfollow a log-normal form, (4) despite normalization differences ranging up to\n$0.4$ dex, there is good qualitative, and often quantitative, agreement between\nthe scale-dependent slopes and property covariance of the two simulations.\nSlopes show appreciable redshift dependence at the group scale, while redshift\ndependence of the scatter is exhibited by low mass FLAMINGO halos only, (5)\nproperty correlations are largely consistent between the simulations, with\nvalues that mainly agree with existing empirical measurements. We close with a\nliterature survey placing our MPR slopes and intrinsic scatter estimates into\ncontext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine scale and redshift dependence of mass-property relations (MPRs)\nfor five hot gas properties of two large group- and cluster-scale halo samples\nrealized by the IllustrisTNG, TNG-Cluster and FLAMINGO cosmological\nhydrodynamical simulations. For intrinsic properties of i) hot gas mass\n($M_{\\rm gas}$), ii) spectroscopic-like temperature ($T_{\\rm sl}$), iii)\nsoft-band X-ray luminosity ($L_{\\rm X}$), and iv) X-ray ($Y_{\\rm X}$) and v)\nSunyaev-Zel'dovich ($Y_{\\rm SZ}$) thermal energies, we use MPR parameters to\ninfer mass proxy quality (MPQ) -- the implied scatter in total halo mass\nconditioned on a property -- for halos with $M_{\\rm 500c} \\geq 10^{13}{\\, {\\rm\nM}_\\odot}$ at redshifts, $z \\in \\{0, 0.5, 1, 2\\}$. We find: (1) in general,\nscaling relation slopes and covariance display moderate to strong dependence on\nhalo mass, with redshift dependence secondary, (2) for halos with $M_{\\rm 500c}\n> 10^{14}{\\, {\\rm M}_\\odot}$, scalings of $M_{\\rm gas}$ and $Y_{\\rm SZ}$\nsimplify toward self-similar slope and constant intrinsic scatter (5 and 10%,\nrespectively) nearly independent of scale, making both measures ideal for\ncluster finding and characterization to $z=2$, (3) halo mass-conditioned\nlikelihoods of hot gas mass and thermal energy at fixed halo mass closely\nfollow a log-normal form, (4) despite normalization differences ranging up to\n$0.4$ dex, there is good qualitative, and often quantitative, agreement between\nthe scale-dependent slopes and property covariance of the two simulations.\nSlopes show appreciable redshift dependence at the group scale, while redshift\ndependence of the scatter is exhibited by low mass FLAMINGO halos only, (5)\nproperty correlations are largely consistent between the simulations, with\nvalues that mainly agree with existing empirical measurements. We close with a\nliterature survey placing our MPR slopes and intrinsic scatter estimates into\ncontext."
                },
                "authors": [
                    {
                        "name": "Eddie Aljamal"
                    },
                    {
                        "name": "August E. Evrard"
                    },
                    {
                        "name": "Arya Farahi"
                    },
                    {
                        "name": "Annalisa Pillepich"
                    },
                    {
                        "name": "Dylan Nelson"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Joey Braspenning"
                    }
                ],
                "author_detail": {
                    "name": "Joey Braspenning"
                },
                "author": "Joey Braspenning",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20090v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20090v4",
                "updated": "2025-07-07T16:28:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    28,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2024-05-30T14:27:20Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    14,
                    27,
                    20,
                    3,
                    151,
                    0
                ],
                "title": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection."
                },
                "authors": [
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Jiayan Yang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Yichi Wang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20090v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20090v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18689v2",
                "updated": "2025-07-07T16:28:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    28,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-23T14:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments"
                },
                "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions."
                },
                "authors": [
                    {
                        "name": "Alessandro Saviolo"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05173v1",
                "updated": "2025-07-07T16:25:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    25,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:25:47Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    25,
                    47,
                    0,
                    188,
                    0
                ],
                "title": "Semantic Frame Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Frame Interpolation"
                },
                "summary": "Generating intermediate video content of varying lengths based on given first\nand last frames, along with text prompt information, offers significant\nresearch and application potential. However, traditional frame interpolation\ntasks primarily focus on scenarios with a small number of frames, no text\ncontrol, and minimal differences between the first and last frames. Recent\ncommunity developers have utilized large video models represented by Wan to\nendow frame-to-frame capabilities. However, these models can only generate a\nfixed number of frames and often fail to produce satisfactory results for\ncertain frame lengths, while this setting lacks a clear official definition and\na well-established benchmark. In this paper, we first propose a new practical\nSemantic Frame Interpolation (SFI) task from the perspective of academic\ndefinition, which covers the above two settings and supports inference at\nmultiple frame rates. To achieve this goal, we propose a novel SemFi model\nbuilding upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the\ngeneration of high-consistency content that aligns with control conditions\nacross various frame length limitations. Furthermore, we propose SFI-300K, the\nfirst general-purpose dataset and benchmark specifically designed for SFI. To\nsupport this, we collect and process data from the perspective of SFI,\ncarefully designing evaluation metrics and methods to assess the model's\nperformance across multiple dimensions, encompassing image and video, and\nvarious aspects, including consistency and diversity. Through extensive\nexperiments on SFI-300K, we demonstrate that our method is particularly\nwell-suited to meet the requirements of the SFI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating intermediate video content of varying lengths based on given first\nand last frames, along with text prompt information, offers significant\nresearch and application potential. However, traditional frame interpolation\ntasks primarily focus on scenarios with a small number of frames, no text\ncontrol, and minimal differences between the first and last frames. Recent\ncommunity developers have utilized large video models represented by Wan to\nendow frame-to-frame capabilities. However, these models can only generate a\nfixed number of frames and often fail to produce satisfactory results for\ncertain frame lengths, while this setting lacks a clear official definition and\na well-established benchmark. In this paper, we first propose a new practical\nSemantic Frame Interpolation (SFI) task from the perspective of academic\ndefinition, which covers the above two settings and supports inference at\nmultiple frame rates. To achieve this goal, we propose a novel SemFi model\nbuilding upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the\ngeneration of high-consistency content that aligns with control conditions\nacross various frame length limitations. Furthermore, we propose SFI-300K, the\nfirst general-purpose dataset and benchmark specifically designed for SFI. To\nsupport this, we collect and process data from the perspective of SFI,\ncarefully designing evaluation metrics and methods to assess the model's\nperformance across multiple dimensions, encompassing image and video, and\nvarious aspects, including consistency and diversity. Through extensive\nexperiments on SFI-300K, we demonstrate that our method is particularly\nwell-suited to meet the requirements of the SFI task."
                },
                "authors": [
                    {
                        "name": "Yijia Hong"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yuji Wang"
                    },
                    {
                        "name": "Weijian Cao"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "arxiv_comment": "https://github.com/hyj542682306/Semantic-Frame-Interpolation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23347v2",
                "updated": "2025-07-07T16:20:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    20,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-29T17:43:04Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    17,
                    43,
                    4,
                    6,
                    180,
                    0
                ],
                "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step\n  Image Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step\n  Image Translation"
                },
                "summary": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Shengqian Li"
                    },
                    {
                        "name": "Zuzeng Lin"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "Accepted to ICCV 2025. Code available at:\n  https://github.com/IamCreateAI/CycleVAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05157v1",
                "updated": "2025-07-07T16:13:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    13,
                    13,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:13:13Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    13,
                    13,
                    0,
                    188,
                    0
                ],
                "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models"
                },
                "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B."
                },
                "authors": [
                    {
                        "name": "Chinnappa Guggilla"
                    },
                    {
                        "name": "Budhaditya Roy"
                    },
                    {
                        "name": "Trupti Ramdas Chavan"
                    },
                    {
                        "name": "Abdul Rahman"
                    },
                    {
                        "name": "Edward Bowen"
                    }
                ],
                "author_detail": {
                    "name": "Edward Bowen"
                },
                "author": "Edward Bowen",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21844v2",
                "updated": "2025-07-07T16:11:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    11,
                    25,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-30T17:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks"
                },
                "summary": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme."
                },
                "authors": [
                    {
                        "name": "William Sutcliffe"
                    },
                    {
                        "name": "Marta Calvi"
                    },
                    {
                        "name": "Simone Capelli"
                    },
                    {
                        "name": "Jonas Eschle"
                    },
                    {
                        "name": "Julián García Pardiñas"
                    },
                    {
                        "name": "Abhijit Mathad"
                    },
                    {
                        "name": "Azusa Uzuki"
                    },
                    {
                        "name": "Nicola Serra"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Serra"
                },
                "author": "Nicola Serra",
                "arxiv_comment": "21 pages, 10 figures, 4 tables (planned submission to Machine\n  Learning Science and Technology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02483v3",
                "updated": "2025-07-07T16:03:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    3,
                    14,
                    0,
                    188,
                    0
                ],
                "published": "2025-02-04T16:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    59,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Distributional Diffusion Models with Scoring Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Diffusion Models with Scoring Rules"
                },
                "summary": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps."
                },
                "authors": [
                    {
                        "name": "Valentin De Bortoli"
                    },
                    {
                        "name": "Alexandre Galashov"
                    },
                    {
                        "name": "J. Swaroop Guntupalli"
                    },
                    {
                        "name": "Guangyao Zhou"
                    },
                    {
                        "name": "Kevin Murphy"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Arnaud Doucet"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Doucet"
                },
                "author": "Arnaud Doucet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05147v1",
                "updated": "2025-07-07T15:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    57,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    57,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "Pseudo-likelihood produces associative memories able to generalize, even\n  for asymmetric couplings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-likelihood produces associative memories able to generalize, even\n  for asymmetric couplings"
                },
                "summary": "Energy-based probabilistic models learned by maximizing the likelihood of the\ndata are limited by the intractability of the partition function. A widely used\nworkaround is to maximize the pseudo-likelihood, which replaces the global\nnormalization with tractable local normalizations. Here we show that, in the\nzero-temperature limit, a network trained to maximize pseudo-likelihood\nnaturally implements an associative memory: if the training set is small,\npatterns become fixed-point attractors whose basins of attraction exceed those\nof any classical Hopfield rule. We explain quantitatively this effect on\nuncorrelated random patterns. Moreover, we show that, for different structured\ndatasets coming from computer science (random feature model, MNIST), physics\n(spin glasses) and biology (proteins), as the number of training examples\nincreases the learned network goes beyond memorization, developing meaningful\nattractors with non-trivial correlations with test examples, thus showing the\nability to generalize. Our results therefore reveal pseudo-likelihood works\nboth as an efficient inference tool and as a principled mechanism for memory\nand generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-based probabilistic models learned by maximizing the likelihood of the\ndata are limited by the intractability of the partition function. A widely used\nworkaround is to maximize the pseudo-likelihood, which replaces the global\nnormalization with tractable local normalizations. Here we show that, in the\nzero-temperature limit, a network trained to maximize pseudo-likelihood\nnaturally implements an associative memory: if the training set is small,\npatterns become fixed-point attractors whose basins of attraction exceed those\nof any classical Hopfield rule. We explain quantitatively this effect on\nuncorrelated random patterns. Moreover, we show that, for different structured\ndatasets coming from computer science (random feature model, MNIST), physics\n(spin glasses) and biology (proteins), as the number of training examples\nincreases the learned network goes beyond memorization, developing meaningful\nattractors with non-trivial correlations with test examples, thus showing the\nability to generalize. Our results therefore reveal pseudo-likelihood works\nboth as an efficient inference tool and as a principled mechanism for memory\nand generalization."
                },
                "authors": [
                    {
                        "name": "Francesco D'Amico"
                    },
                    {
                        "name": "Dario Bocchi"
                    },
                    {
                        "name": "Luca Maria Del Bono"
                    },
                    {
                        "name": "Saverio Rossi"
                    },
                    {
                        "name": "Matteo Negri"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Negri"
                },
                "author": "Matteo Negri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21469v3",
                "updated": "2025-07-07T15:56:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    56,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-27T17:42:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    42,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete\n  Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete\n  Flow Matching"
                },
                "summary": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow achieves competitive performance against\nprevious SOTA diffusion models in conditional molecule generation across\nvarious properties while preserving the stability and validity of the generated\nmolecules, consistent with its unconditional counterpart. Additionally, it\nenables faster inference with significantly fewer time steps compared to\nbaseline models. We highlight the importance of validating the properties of\ngenerated molecules through DFT calculations performed at the same level of\ntheory as the training data. Specifically, our analysis identifies properties\nthat require DFT validation and others where a pretrained SE(3) geometric\nvector perceptron regressors provide sufficiently accurate predictions on\ngenerated molecules. Furthermore, we introduce a new property metric to assess\nthe model's ability to propose molecules with underrepresented property values,\nassessing its capacity for out-of-distribution generalization. Our findings\nreveal shortcomings in existing structural metrics, which mistakenly validate\nopen-shell molecules or molecules with invalid valence-charge configurations,\nunderscoring the need for improved evaluation frameworks. Overall, this work\npaves the way for developing targeted property-guided generation methods,\nenhancing the design of molecular generative models for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow achieves competitive performance against\nprevious SOTA diffusion models in conditional molecule generation across\nvarious properties while preserving the stability and validity of the generated\nmolecules, consistent with its unconditional counterpart. Additionally, it\nenables faster inference with significantly fewer time steps compared to\nbaseline models. We highlight the importance of validating the properties of\ngenerated molecules through DFT calculations performed at the same level of\ntheory as the training data. Specifically, our analysis identifies properties\nthat require DFT validation and others where a pretrained SE(3) geometric\nvector perceptron regressors provide sufficiently accurate predictions on\ngenerated molecules. Furthermore, we introduce a new property metric to assess\nthe model's ability to propose molecules with underrepresented property values,\nassessing its capacity for out-of-distribution generalization. Our findings\nreveal shortcomings in existing structural metrics, which mistakenly validate\nopen-shell molecules or molecules with invalid valence-charge configurations,\nunderscoring the need for improved evaluation frameworks. Overall, this work\npaves the way for developing targeted property-guided generation methods,\nenhancing the design of molecular generative models for diverse applications."
                },
                "authors": [
                    {
                        "name": "Cheng Zeng"
                    },
                    {
                        "name": "Jirui Jin"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Mark Transtrum"
                    },
                    {
                        "name": "Ellad B. Tadmor"
                    },
                    {
                        "name": "Richard G. Hennig"
                    },
                    {
                        "name": "Adrian Roitberg"
                    },
                    {
                        "name": "Stefano Martiniani"
                    },
                    {
                        "name": "Mingjie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Liu"
                },
                "author": "Mingjie Liu",
                "arxiv_comment": "code: https://github.com/Liu-Group-UF/PropMolFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05141v1",
                "updated": "2025-07-07T15:51:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    51,
                    18,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:51:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    51,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "Hardware-efficient tractable probabilistic inference for TinyML\n  Neurosymbolic AI applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-efficient tractable probabilistic inference for TinyML\n  Neurosymbolic AI applications"
                },
                "summary": "Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations\nassociated with deep learning (DL) models, e.g. quantifying their uncertainty\nor reason with explicit rules. Hence, TinyML hardware will need to support\nthese symbolic models to bring NSAI to embedded scenarios. Yet, although\nsymbolic models are typically compact, their sparsity and computation\nresolution contrasts with low-resolution and dense neuro models, which is a\nchallenge on resource-constrained TinyML hardware severely limiting the size of\nsymbolic models that can be computed. In this work, we remove this bottleneck\nleveraging a tight hardware/software integration to present a complete\nframework to compute NSAI with TinyML hardware. We focus on symbolic models\nrealized with tractable probabilistic circuits (PCs), a popular subclass of\nprobabilistic models for hardware integration. This framework: (1) trains a\nspecific class of hardware-efficient \\emph{deterministic} PCs, chosen for the\nsymbolic task; (2) \\emph{compresses} this PC until it can be computed on TinyML\nhardware with minimal accuracy degradation, using our $n^{th}$-root compression\ntechnique, and (3) \\emph{deploys} the complete NSAI model on TinyML hardware.\nCompared to a 64b precision baseline necessary for the PC without compression,\nour workflow leads to significant hardware reduction on FPGA (up to 82.3\\% in\nFF, 52.6\\% in LUTs, and 18.0\\% in Flash usage) and an average inference speedup\nof 4.67x on ESP32 microcontroller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations\nassociated with deep learning (DL) models, e.g. quantifying their uncertainty\nor reason with explicit rules. Hence, TinyML hardware will need to support\nthese symbolic models to bring NSAI to embedded scenarios. Yet, although\nsymbolic models are typically compact, their sparsity and computation\nresolution contrasts with low-resolution and dense neuro models, which is a\nchallenge on resource-constrained TinyML hardware severely limiting the size of\nsymbolic models that can be computed. In this work, we remove this bottleneck\nleveraging a tight hardware/software integration to present a complete\nframework to compute NSAI with TinyML hardware. We focus on symbolic models\nrealized with tractable probabilistic circuits (PCs), a popular subclass of\nprobabilistic models for hardware integration. This framework: (1) trains a\nspecific class of hardware-efficient \\emph{deterministic} PCs, chosen for the\nsymbolic task; (2) \\emph{compresses} this PC until it can be computed on TinyML\nhardware with minimal accuracy degradation, using our $n^{th}$-root compression\ntechnique, and (3) \\emph{deploys} the complete NSAI model on TinyML hardware.\nCompared to a 64b precision baseline necessary for the PC without compression,\nour workflow leads to significant hardware reduction on FPGA (up to 82.3\\% in\nFF, 52.6\\% in LUTs, and 18.0\\% in Flash usage) and an average inference speedup\nof 4.67x on ESP32 microcontroller."
                },
                "authors": [
                    {
                        "name": "Jelin Leslin"
                    },
                    {
                        "name": "Martin Trapp"
                    },
                    {
                        "name": "Martin Andraud"
                    }
                ],
                "author_detail": {
                    "name": "Martin Andraud"
                },
                "author": "Martin Andraud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05137v1",
                "updated": "2025-07-07T15:49:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:49:23Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization"
                },
                "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05129v1",
                "updated": "2025-07-07T15:41:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "title": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction"
                },
                "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Nigel Fernandez"
                    },
                    {
                        "name": "Christopher Ormerod"
                    },
                    {
                        "name": "Susan Lottridge"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05128v1",
                "updated": "2025-07-07T15:40:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    40,
                    11,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:40:11Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    40,
                    11,
                    0,
                    188,
                    0
                ],
                "title": "Practical considerations for Gaussian Process modeling for causal\n  inference quasi-experimental studies with panel data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical considerations for Gaussian Process modeling for causal\n  inference quasi-experimental studies with panel data"
                },
                "summary": "Estimating causal effects in quasi-experiments with spatio-temporal panel\ndata often requires adjusting for unmeasured confounding that varies across\nspace and time. Gaussian Processes (GPs) offer a flexible, nonparametric\nmodeling approach that can account for such complex dependencies through\ncarefully chosen covariance kernels. In this paper, we provide a practical and\ninterpretable framework for applying GPs to causal inference in panel data\nsettings. We demonstrate how GPs generalize popular methods such as synthetic\ncontrol and vertical regression, and we show that the GP posterior mean can be\nrepresented as a weighted average of observed outcomes, where the weights\nreflect spatial and temporal similarity. To support applied use, we explore how\ndifferent kernel choices impact both estimation performance and\ninterpretability, offering guidance for selecting between separable and\nnonseparable kernels. Through simulations and application to Hurricane Katrina\nmortality data, we illustrate how GP models can be used to estimate\ncounterfactual outcomes and quantify treatment effects. All code and materials\nare made publicly available to support reproducibility and encourage adoption.\nOur results suggest that GPs are a promising and interpretable tool for\naddressing unmeasured spatio-temporal confounding in quasi-experimental\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating causal effects in quasi-experiments with spatio-temporal panel\ndata often requires adjusting for unmeasured confounding that varies across\nspace and time. Gaussian Processes (GPs) offer a flexible, nonparametric\nmodeling approach that can account for such complex dependencies through\ncarefully chosen covariance kernels. In this paper, we provide a practical and\ninterpretable framework for applying GPs to causal inference in panel data\nsettings. We demonstrate how GPs generalize popular methods such as synthetic\ncontrol and vertical regression, and we show that the GP posterior mean can be\nrepresented as a weighted average of observed outcomes, where the weights\nreflect spatial and temporal similarity. To support applied use, we explore how\ndifferent kernel choices impact both estimation performance and\ninterpretability, offering guidance for selecting between separable and\nnonseparable kernels. Through simulations and application to Hurricane Katrina\nmortality data, we illustrate how GP models can be used to estimate\ncounterfactual outcomes and quantify treatment effects. All code and materials\nare made publicly available to support reproducibility and encourage adoption.\nOur results suggest that GPs are a promising and interpretable tool for\naddressing unmeasured spatio-temporal confounding in quasi-experimental\nstudies."
                },
                "authors": [
                    {
                        "name": "Sofia L. Vega"
                    },
                    {
                        "name": "Rachel C. Nethery"
                    }
                ],
                "author_detail": {
                    "name": "Rachel C. Nethery"
                },
                "author": "Rachel C. Nethery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05123v1",
                "updated": "2025-07-07T15:34:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    34,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:34:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    34,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques"
                },
                "summary": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems."
                },
                "authors": [
                    {
                        "name": "Walid Mohamed Aly"
                    },
                    {
                        "name": "Taysir Hassan A. Soliman"
                    },
                    {
                        "name": "Amr Mohamed AbdelAziz"
                    }
                ],
                "author_detail": {
                    "name": "Amr Mohamed AbdelAziz"
                },
                "author": "Amr Mohamed AbdelAziz",
                "arxiv_comment": "This manuscript is an extended version of the work accepted for\n  publication in the International Journal of Advanced Computer Science and\n  Applications (IJACSA), Volume 16, Issue 6, June 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05118v1",
                "updated": "2025-07-07T15:31:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    31,
                    36,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:31:36Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    31,
                    36,
                    0,
                    188,
                    0
                ],
                "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots"
                },
                "summary": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io."
                },
                "authors": [
                    {
                        "name": "Danil S. Grigorev"
                    },
                    {
                        "name": "Alexey K. Kovalev"
                    },
                    {
                        "name": "Aleksandr I. Panov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr I. Panov"
                },
                "author": "Aleksandr I. Panov",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05116v1",
                "updated": "2025-07-07T15:30:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    30,
                    55,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:30:55Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    30,
                    55,
                    0,
                    188,
                    0
                ],
                "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble\n  Voting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble\n  Voting"
                },
                "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Juyi Lin"
                    },
                    {
                        "name": "Amir Taherin"
                    },
                    {
                        "name": "Arash Akbari"
                    },
                    {
                        "name": "Arman Akbari"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Guangyu Chen"
                    },
                    {
                        "name": "Taskin Padir"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Weiwei Chen"
                    },
                    {
                        "name": "Yiqian Li"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "David Kaeli"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05114v1",
                "updated": "2025-07-07T15:30:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    30,
                    36,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:30:36Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    30,
                    36,
                    0,
                    188,
                    0
                ],
                "title": "Sequential multiple importance sampling for high-dimensional Bayesian\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential multiple importance sampling for high-dimensional Bayesian\n  inference"
                },
                "summary": "This paper introduces a sequential multiple importance sampling (SeMIS)\nalgorithm for high-dimensional Bayesian inference. The method estimates\nBayesian evidence using all generated samples from each proposal distribution\nwhile obtaining posterior samples through an importance-resampling scheme. A\nkey innovation of SeMIS is the use of a softly truncated prior distribution as\nthe intermediate proposal, providing a new way bridging prior and posterior\ndistributions. By enabling samples from high-likelihood regions to traverse\nlow-probability zones, SeMIS enhances mode mixing in challenging inference\nproblems. Comparative evaluations against subset simulation (SuS) and adaptive\nBayesian updating with structural reliability methods (aBUS) demonstrate that\nSeMIS achieves superior performance in evidence estimation (lower bias and\nvariance) and posterior sampling (higher effective sample sizes and closer\napproximation to the true posterior), particularly for multimodal\ndistributions. The efficacy of SeMIS is further validated in a high-dimensional\nfinite element model updating application, where it successfully localizes\nstructural damages by quantifying stiffness loss. The proposed algorithm not\nonly advances Bayesian computation for complex posterior distributions but also\nprovides a robust tool for uncertainty quantification in civil engineering\nsystems, offering new possibilities for probabilistic structural health\nmonitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a sequential multiple importance sampling (SeMIS)\nalgorithm for high-dimensional Bayesian inference. The method estimates\nBayesian evidence using all generated samples from each proposal distribution\nwhile obtaining posterior samples through an importance-resampling scheme. A\nkey innovation of SeMIS is the use of a softly truncated prior distribution as\nthe intermediate proposal, providing a new way bridging prior and posterior\ndistributions. By enabling samples from high-likelihood regions to traverse\nlow-probability zones, SeMIS enhances mode mixing in challenging inference\nproblems. Comparative evaluations against subset simulation (SuS) and adaptive\nBayesian updating with structural reliability methods (aBUS) demonstrate that\nSeMIS achieves superior performance in evidence estimation (lower bias and\nvariance) and posterior sampling (higher effective sample sizes and closer\napproximation to the true posterior), particularly for multimodal\ndistributions. The efficacy of SeMIS is further validated in a high-dimensional\nfinite element model updating application, where it successfully localizes\nstructural damages by quantifying stiffness loss. The proposed algorithm not\nonly advances Bayesian computation for complex posterior distributions but also\nprovides a robust tool for uncertainty quantification in civil engineering\nsystems, offering new possibilities for probabilistic structural health\nmonitoring."
                },
                "authors": [
                    {
                        "name": "Li Binbin"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Liao Zihan"
                    }
                ],
                "author_detail": {
                    "name": "Liao Zihan"
                },
                "author": "Liao Zihan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05110v2",
                "updated": "2025-07-08T03:40:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    3,
                    40,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T15:27:48Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    27,
                    48,
                    0,
                    188,
                    0
                ],
                "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution\n  Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution\n  Shift"
                },
                "summary": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Yue He"
                    },
                    {
                        "name": "Yunfei Wang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Haoxiang Cheng"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Peng Cui"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03004v2",
                "updated": "2025-07-07T15:21:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    21,
                    25,
                    0,
                    188,
                    0
                ],
                "published": "2024-02-05T13:45:37Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    13,
                    45,
                    37,
                    0,
                    36,
                    0
                ],
                "title": "Transformation Discriminant Analysis for Constructing Optimal Biomarker\n  Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformation Discriminant Analysis for Constructing Optimal Biomarker\n  Combinations"
                },
                "summary": "Accurate diagnostic tests are essential for effective screening and\ntreatment. However, individual biomarkers often fail to provide sufficient\ndiagnostic accuracy, as they typically capture only one aspect of the complex\ndisease process. Combining multiple biomarkers, each capturing a distinct\nmechanism, can help constructing more informative diagnostic tests. In\npractice, logistic regression is used as the default to combine biomarkers, but\nit can perform poorly when biomarker distributions exhibit skewness or differ\nacross disease groups. Nonparametric methods provide more flexibility but\ngenerally require large sample sizes that are infrequently available in\nbiomedical research.\n  We propose a novel framework called transformation discriminant analysis\nwhich combines biomarkers through the likelihood ratio function to construct\ntheoretically optimal diagnostic scores. Transformation discriminant analysis\nbalances between flexibility and efficiency. It can accommodate a wide range of\ndistributional shapes and disease-specific dependence structures while\nremaining fully parametric. This allows for likelihood inference and strong\nperformance even in small-sample settings.\n  We evaluate TDA through simulations and benchmark its performance against\ncommonly used methods. Finally, we illustrate its utility in constructing an\noptimal diagnostic test for hepatocellular carcinoma, a disease with no single\nideal biomarker. An open-source R implementation is provided for\nreproducibility and broader application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnostic tests are essential for effective screening and\ntreatment. However, individual biomarkers often fail to provide sufficient\ndiagnostic accuracy, as they typically capture only one aspect of the complex\ndisease process. Combining multiple biomarkers, each capturing a distinct\nmechanism, can help constructing more informative diagnostic tests. In\npractice, logistic regression is used as the default to combine biomarkers, but\nit can perform poorly when biomarker distributions exhibit skewness or differ\nacross disease groups. Nonparametric methods provide more flexibility but\ngenerally require large sample sizes that are infrequently available in\nbiomedical research.\n  We propose a novel framework called transformation discriminant analysis\nwhich combines biomarkers through the likelihood ratio function to construct\ntheoretically optimal diagnostic scores. Transformation discriminant analysis\nbalances between flexibility and efficiency. It can accommodate a wide range of\ndistributional shapes and disease-specific dependence structures while\nremaining fully parametric. This allows for likelihood inference and strong\nperformance even in small-sample settings.\n  We evaluate TDA through simulations and benchmark its performance against\ncommonly used methods. Finally, we illustrate its utility in constructing an\noptimal diagnostic test for hepatocellular carcinoma, a disease with no single\nideal biomarker. An open-source R implementation is provided for\nreproducibility and broader application."
                },
                "authors": [
                    {
                        "name": "Ainesh Sewak"
                    },
                    {
                        "name": "Sandra Siegfried"
                    },
                    {
                        "name": "Torsten Hothorn"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hothorn"
                },
                "author": "Torsten Hothorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05093v1",
                "updated": "2025-07-07T15:13:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    13,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:13:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    13,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders"
                },
                "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations."
                },
                "authors": [
                    {
                        "name": "Alberto Castagnaro"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Simeone Pizzi"
                    }
                ],
                "author_detail": {
                    "name": "Simeone Pizzi"
                },
                "author": "Simeone Pizzi",
                "arxiv_comment": "currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05040v3",
                "updated": "2025-07-07T15:05:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    5,
                    33,
                    0,
                    188,
                    0
                ],
                "published": "2025-02-07T16:07:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    7,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussRender: Learning 3D Occupancy with Gaussian Rendering"
                },
                "summary": "Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from visual inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce visible geometric\ncoherence. In this paper, we propose GaussRender, a module that improves 3D\noccupancy learning by enforcing projective consistency. Our key idea is to\nproject both predicted and ground-truth 3D occupancy into 2D camera views,\nwhere we apply supervision. Our method penalizes 3D configurations that produce\ninconsistent 2D projections, thereby enforcing a more coherent 3D structure. To\nachieve this efficiently, we leverage differentiable rendering with Gaussian\nsplatting. GaussRender seamlessly integrates with existing architectures while\nmaintaining efficiency and requiring no inference-time modifications. Extensive\nevaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics such as RayIoU. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from visual inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce visible geometric\ncoherence. In this paper, we propose GaussRender, a module that improves 3D\noccupancy learning by enforcing projective consistency. Our key idea is to\nproject both predicted and ground-truth 3D occupancy into 2D camera views,\nwhere we apply supervision. Our method penalizes 3D configurations that produce\ninconsistent 2D projections, thereby enforcing a more coherent 3D structure. To\nachieve this efficiently, we leverage differentiable rendering with Gaussian\nsplatting. GaussRender seamlessly integrates with existing architectures while\nmaintaining efficiency and requiring no inference-time modifications. Extensive\nevaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics such as RayIoU. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender."
                },
                "authors": [
                    {
                        "name": "Loïck Chambon"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Mickaël Chen"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01036v2",
                "updated": "2025-07-07T14:51:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    51,
                    15,
                    0,
                    188,
                    0
                ],
                "published": "2024-11-01T21:11:48Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    21,
                    11,
                    48,
                    4,
                    306,
                    0
                ],
                "title": "Computation-Aware Gaussian Processes: Model Selection And Linear-Time\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation-Aware Gaussian Processes: Model Selection And Linear-Time\n  Inference"
                },
                "summary": "Model selection in Gaussian processes scales prohibitively with the size of\nthe training dataset, both in time and memory. While many approximations exist,\nall incur inevitable approximation error. Recent work accounts for this error\nin the form of computational uncertainty, which enables -- at the cost of\nquadratic complexity -- an explicit tradeoff between computation and precision.\nHere we extend this development to model selection, which requires significant\nenhancements to the existing approach, including linear-time scaling in the\nsize of the dataset. We propose a novel training loss for hyperparameter\noptimization and demonstrate empirically that the resulting method can\noutperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model\nselection, on medium to large-scale datasets. Our experiments show that model\nselection for computation-aware GPs trained on 1.8 million data points can be\ndone within a few hours on a single GPU. As a result of this work, Gaussian\nprocesses can be trained on large-scale datasets without significantly\ncompromising their ability to quantify uncertainty -- a fundamental\nprerequisite for optimal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model selection in Gaussian processes scales prohibitively with the size of\nthe training dataset, both in time and memory. While many approximations exist,\nall incur inevitable approximation error. Recent work accounts for this error\nin the form of computational uncertainty, which enables -- at the cost of\nquadratic complexity -- an explicit tradeoff between computation and precision.\nHere we extend this development to model selection, which requires significant\nenhancements to the existing approach, including linear-time scaling in the\nsize of the dataset. We propose a novel training loss for hyperparameter\noptimization and demonstrate empirically that the resulting method can\noutperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model\nselection, on medium to large-scale datasets. Our experiments show that model\nselection for computation-aware GPs trained on 1.8 million data points can be\ndone within a few hours on a single GPU. As a result of this work, Gaussian\nprocesses can be trained on large-scale datasets without significantly\ncompromising their ability to quantify uncertainty -- a fundamental\nprerequisite for optimal decision-making."
                },
                "authors": [
                    {
                        "name": "Jonathan Wenger"
                    },
                    {
                        "name": "Kaiwen Wu"
                    },
                    {
                        "name": "Philipp Hennig"
                    },
                    {
                        "name": "Jacob R. Gardner"
                    },
                    {
                        "name": "Geoff Pleiss"
                    },
                    {
                        "name": "John P. Cunningham"
                    }
                ],
                "author_detail": {
                    "name": "John P. Cunningham"
                },
                "author": "John P. Cunningham",
                "arxiv_comment": "Advances in Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05068v1",
                "updated": "2025-07-07T14:50:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    50,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:50:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    50,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models"
                },
                "summary": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA."
                },
                "authors": [
                    {
                        "name": "Hongyao Yu"
                    },
                    {
                        "name": "Yixiang Qiu"
                    },
                    {
                        "name": "Yiheng Yang"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Tianqu Zhuang"
                    },
                    {
                        "name": "Jiaxin Hong"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Tao Xia"
                },
                "author": "Shu-Tao Xia",
                "arxiv_comment": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05067v1",
                "updated": "2025-07-07T14:49:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    49,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:49:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    49,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "Quantifying Resolution Limits in Pedestal Profile Measurements with\n  Gaussian Process Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Resolution Limits in Pedestal Profile Measurements with\n  Gaussian Process Regression"
                },
                "summary": "Edge transport barriers (ETBs) in magnetically confined fusion plasmas,\ncommonly known as pedestals, play a crucial role in achieving high confinement\nplasmas. However, their defining characteristic, a steep rise in plasma\npressure over short length scales, makes them challenging to diagnose\nexperimentally. In this work, we use Gaussian Process Regression (GPR) to\ndevelop first-principles metrics for quantifying the spatiotemporal resolution\nlimits of inferring differentiable profiles of temperature, pressure, or other\nquantities from experimental measurements. Although we focus on pedestals, the\nmethods are fully general and can be applied to any setting involving the\ninference of profiles from discrete measurements. First, we establish a\ncorrespondence between GPR and low-pass filtering, giving an explicit\nexpression for the effective `cutoff frequency' associated with smoothing\nincurred by GPR. Second, we introduce a novel information-theoretic metric,\n\\(N_{eff}\\), which measures the effective number of data points contributing to\nthe inferred value of a profile or its derivative. These metrics enable a\nquantitative assessment of the trade-off between `over-fitting' and\n`over-regularization', providing both practitioners and consumers of GPR with a\nsystematic way to evaluate the credibility of inferred profiles. We apply these\ntools to develop practical advice for using GPR in both time-independent and\ntime-dependent settings, and demonstrate their usage on inferring pedestal\nprofiles using measurements from the DIII-D tokamak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge transport barriers (ETBs) in magnetically confined fusion plasmas,\ncommonly known as pedestals, play a crucial role in achieving high confinement\nplasmas. However, their defining characteristic, a steep rise in plasma\npressure over short length scales, makes them challenging to diagnose\nexperimentally. In this work, we use Gaussian Process Regression (GPR) to\ndevelop first-principles metrics for quantifying the spatiotemporal resolution\nlimits of inferring differentiable profiles of temperature, pressure, or other\nquantities from experimental measurements. Although we focus on pedestals, the\nmethods are fully general and can be applied to any setting involving the\ninference of profiles from discrete measurements. First, we establish a\ncorrespondence between GPR and low-pass filtering, giving an explicit\nexpression for the effective `cutoff frequency' associated with smoothing\nincurred by GPR. Second, we introduce a novel information-theoretic metric,\n\\(N_{eff}\\), which measures the effective number of data points contributing to\nthe inferred value of a profile or its derivative. These metrics enable a\nquantitative assessment of the trade-off between `over-fitting' and\n`over-regularization', providing both practitioners and consumers of GPR with a\nsystematic way to evaluate the credibility of inferred profiles. We apply these\ntools to develop practical advice for using GPR in both time-independent and\ntime-dependent settings, and demonstrate their usage on inferring pedestal\nprofiles using measurements from the DIII-D tokamak."
                },
                "authors": [
                    {
                        "name": "Norman M. Cao"
                    },
                    {
                        "name": "David R. Hatch"
                    },
                    {
                        "name": "Craig Michoski"
                    },
                    {
                        "name": "Todd A. Oliver"
                    },
                    {
                        "name": "David Eldon"
                    },
                    {
                        "name": "Andrew Oakleigh Nelson"
                    },
                    {
                        "name": "Matthew Waller"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Waller"
                },
                "author": "Matthew Waller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05065v1",
                "updated": "2025-07-07T14:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    49,
                    18,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    49,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replacing thinking with tool usage enables reasoning in small language\n  models"
                },
                "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task."
                },
                "authors": [
                    {
                        "name": "Corrado Rainone"
                    },
                    {
                        "name": "Tim Bakker"
                    },
                    {
                        "name": "Roland Memisevic"
                    }
                ],
                "author_detail": {
                    "name": "Roland Memisevic"
                },
                "author": "Roland Memisevic",
                "arxiv_comment": "23 pages, includes appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05060v2",
                "updated": "2025-07-08T06:11:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    11,
                    39,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T14:45:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    45,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "A COMPASS to Model Comparison and Simulation-Based Inference in Galactic\n  Chemical Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A COMPASS to Model Comparison and Simulation-Based Inference in Galactic\n  Chemical Evolution"
                },
                "summary": "We present COMPASS, a novel simulation-based inference framework that\ncombines score-based diffusion models with transformer architectures to jointly\nperform parameter estimation and Bayesian model comparison across competing\nGalactic Chemical Evolution (GCE) models. COMPASS handles high-dimensional,\nincomplete, and variable-size stellar abundance datasets. Applied to\nhigh-precision elemental abundance measurements, COMPASS evaluates 40\ncombinations of nucleosynthetic yield tables. The model strongly favours\nAsymptotic Giant Branch yields from NuGrid and core-collapse SN yields used in\nthe IllustrisTNG simulation, achieving near-unity cumulative posterior\nprobability. Using the preferred model, we infer a steep high-mass IMF slope\nand an elevated Supernova Ia normalization, consistent with prior solar\nneighbourhood studies but now derived from fully amortized Bayesian inference.\nOur results demonstrate that modern SBI methods can robustly constrain\nuncertain physics in astrophysical simulators and enable principled model\nselection when analysing complex, simulation-based data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present COMPASS, a novel simulation-based inference framework that\ncombines score-based diffusion models with transformer architectures to jointly\nperform parameter estimation and Bayesian model comparison across competing\nGalactic Chemical Evolution (GCE) models. COMPASS handles high-dimensional,\nincomplete, and variable-size stellar abundance datasets. Applied to\nhigh-precision elemental abundance measurements, COMPASS evaluates 40\ncombinations of nucleosynthetic yield tables. The model strongly favours\nAsymptotic Giant Branch yields from NuGrid and core-collapse SN yields used in\nthe IllustrisTNG simulation, achieving near-unity cumulative posterior\nprobability. Using the preferred model, we infer a steep high-mass IMF slope\nand an elevated Supernova Ia normalization, consistent with prior solar\nneighbourhood studies but now derived from fully amortized Bayesian inference.\nOur results demonstrate that modern SBI methods can robustly constrain\nuncertain physics in astrophysical simulators and enable principled model\nselection when analysing complex, simulation-based data."
                },
                "authors": [
                    {
                        "name": "Berkay Gunes"
                    },
                    {
                        "name": "Sven Buder"
                    },
                    {
                        "name": "Tobias Buck"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Buck"
                },
                "author": "Tobias Buck",
                "arxiv_comment": "Accepted at the 2025 Workshop on Machine Learning for Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05046v1",
                "updated": "2025-07-07T14:29:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    29,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:29:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    29,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students"
                },
                "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings."
                },
                "authors": [
                    {
                        "name": "Kadija Bouyzourn"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "25 pages, 11 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05043v1",
                "updated": "2025-07-07T14:27:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:27:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLink: Distributed and Efficient Serving Framework for Large Models"
                },
                "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models."
                },
                "authors": [
                    {
                        "name": "Lewei Jin"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Kui Zhang"
                    },
                    {
                        "name": "Yifan Zhuo"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Bowei Yang"
                    },
                    {
                        "name": "Zhengong Cai"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19794v2",
                "updated": "2025-07-07T14:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study"
                },
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14409v3",
                "updated": "2025-07-07T14:16:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    16,
                    50,
                    0,
                    188,
                    0
                ],
                "published": "2025-03-18T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    49,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Inference and Learning of Nonlinear LFR State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and Learning of Nonlinear LFR State-Space Models"
                },
                "summary": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, which is prone to poor local minima and slow convergence. This paper\npresents a computationally efficient initialization method for nonlinear linear\nfractional representation (NL-LFR) models using periodic data. By first\ninferring the latent signals and subsequently estimating the model parameters,\nthe approach generates initial estimates for use in a later nonlinear\noptimization step. The proposed method shows robustness against poor local\nminima, and achieves a twofold error reduction compared to the state-of-the-art\non a challenging benchmark dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, which is prone to poor local minima and slow convergence. This paper\npresents a computationally efficient initialization method for nonlinear linear\nfractional representation (NL-LFR) models using periodic data. By first\ninferring the latent signals and subsequently estimating the model parameters,\nthe approach generates initial estimates for use in a later nonlinear\noptimization step. The proposed method shows robustness against poor local\nminima, and achieves a twofold error reduction compared to the state-of-the-art\non a challenging benchmark dataset."
                },
                "authors": [
                    {
                        "name": "Merijn Floren"
                    },
                    {
                        "name": "Jean-Philippe Noël"
                    },
                    {
                        "name": "Jan Swevers"
                    }
                ],
                "author_detail": {
                    "name": "Jan Swevers"
                },
                "author": "Jan Swevers",
                "arxiv_doi": "10.1109/LCSYS.2025.3580354",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LCSYS.2025.3580354",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Code is available at: https://github.com/merijnfloren/freq-statespace\n  ; final, published paper in IEEE Xplore:\n  https://ieeexplore.ieee.org/abstract/document/11037476/",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00946v3",
                "updated": "2025-07-07T13:57:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    57,
                    37,
                    0,
                    188,
                    0
                ],
                "published": "2024-09-02T05:09:46Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    9,
                    46,
                    0,
                    246,
                    0
                ],
                "title": "A Framework for Synthetic Audio Conversations Generation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Synthetic Audio Conversations Generation using Large\n  Language Models"
                },
                "summary": "In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems."
                },
                "authors": [
                    {
                        "name": "Kaung Myat Kyaw"
                    },
                    {
                        "name": "Jonathan Hoyin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Hoyin Chan"
                },
                "author": "Jonathan Hoyin Chan",
                "arxiv_doi": "10.1109/WI-IAT62293.2024.00056",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WI-IAT62293.2024.00056",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.00946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted at the WI-IAT'24. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media",
                "arxiv_journal_ref": "IEEE/WIC International Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT), 2024, pp. 355--359",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07783v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07783v4",
                "updated": "2025-07-07T13:56:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    56,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-12T17:36:14Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    36,
                    14,
                    0,
                    132,
                    0
                ],
                "title": "Relative Overfitting and Accept-Reject Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Overfitting and Accept-Reject Framework"
                },
                "summary": "The scaling of Large Language Models (LLMs) currently faces significant\nchallenges. Model assembly is widely considered a promising solution to break\nthrough these performance bottlenecks. However, current ensembling methods are\nprimarily guided by the statistical expectation that combining multiple models\nover large samples will lead to performance gains. We propose an ensemble\nframework that transitions from such stochastic, sample-dependent methods to a\nregular, controllable approach based on fine-grained model segmentation. This\nregularity governs how models are segmented to ensure performance improvement,\nhow the magnitude of this improvement varies with model selection, and what\nfactors determine its theoretical maximum. To formalize this pattern, we\nintroduce the concept of'relative overfitting,' which is derived from the\nperformance discrepancies between constituent models and builds a bridge\nbetween ensemble outcomes and the inherent attributes of these models. We\ndetail the patterns of this framework within the domain of NLP and briefly\ndescribe its extensibility to other fields, such as computer vision (CV) and AI\nfor science. Our approach was validated using both custom-built and pre-trained\nmainstream models across diverse benchmarks, including language modeling,\nlong-context tasks, and question-answering (QA). The results indicate that the\nensemble rules we proposed are generally effective and that we provide a\nrigorous proof of these rules in certain experimental scenarios. The proposed\nframework offers a new perspective for understanding ensemble theory and\nprovides a systematic approach to addressing the performance bottlenecks of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of Large Language Models (LLMs) currently faces significant\nchallenges. Model assembly is widely considered a promising solution to break\nthrough these performance bottlenecks. However, current ensembling methods are\nprimarily guided by the statistical expectation that combining multiple models\nover large samples will lead to performance gains. We propose an ensemble\nframework that transitions from such stochastic, sample-dependent methods to a\nregular, controllable approach based on fine-grained model segmentation. This\nregularity governs how models are segmented to ensure performance improvement,\nhow the magnitude of this improvement varies with model selection, and what\nfactors determine its theoretical maximum. To formalize this pattern, we\nintroduce the concept of'relative overfitting,' which is derived from the\nperformance discrepancies between constituent models and builds a bridge\nbetween ensemble outcomes and the inherent attributes of these models. We\ndetail the patterns of this framework within the domain of NLP and briefly\ndescribe its extensibility to other fields, such as computer vision (CV) and AI\nfor science. Our approach was validated using both custom-built and pre-trained\nmainstream models across diverse benchmarks, including language modeling,\nlong-context tasks, and question-answering (QA). The results indicate that the\nensemble rules we proposed are generally effective and that we provide a\nrigorous proof of these rules in certain experimental scenarios. The proposed\nframework offers a new perspective for understanding ensemble theory and\nprovides a systematic approach to addressing the performance bottlenecks of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yanxin Liu"
                    },
                    {
                        "name": "Yunqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunqi Zhang"
                },
                "author": "Yunqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07783v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07783v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05010v1",
                "updated": "2025-07-07T13:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    48,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:48:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    48,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification"
                },
                "summary": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT."
                },
                "authors": [
                    {
                        "name": "Chenfei Xiong"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Lorena Calvo-Bartolomé"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05007v1",
                "updated": "2025-07-07T13:44:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    44,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:44:58Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    44,
                    58,
                    0,
                    188,
                    0
                ],
                "title": "Multi-modal Representations for Fine-grained Multi-label Critical View\n  of Safety Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Representations for Fine-grained Multi-label Critical View\n  of Safety Recognition"
                },
                "summary": "The Critical View of Safety (CVS) is crucial for safe laparoscopic\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\ntask, even for experts. Traditional models for CVS recognition depend on\nvision-only models learning with costly, labor-intensive spatial annotations.\nThis study investigates how text can be harnessed as a powerful tool for both\ntraining and inference in multi-modal surgical foundation models to automate\nCVS recognition. Unlike many existing multi-modal models, which are primarily\nadapted for multi-class classification, CVS recognition requires a multi-label\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\nsignificant performance gap for this task. To address this, we propose\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\nbinary classification across multiple labels by aligning image embeddings with\ntextual descriptions of each CVS criterion using positive and negative prompts.\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\nboosts CVS recognition over image-only methods. We also propose text-specific\ninference methods, that helps in analysing the image-text alignment. While\nfurther work is needed to match state-of-the-art spatial annotation-based\nmethods, this approach highlights the potential of adapting generalist models\nto specialized surgical tasks. Code:\nhttps://github.com/CAMMA-public/CVS-AdaptNet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Critical View of Safety (CVS) is crucial for safe laparoscopic\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\ntask, even for experts. Traditional models for CVS recognition depend on\nvision-only models learning with costly, labor-intensive spatial annotations.\nThis study investigates how text can be harnessed as a powerful tool for both\ntraining and inference in multi-modal surgical foundation models to automate\nCVS recognition. Unlike many existing multi-modal models, which are primarily\nadapted for multi-class classification, CVS recognition requires a multi-label\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\nsignificant performance gap for this task. To address this, we propose\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\nbinary classification across multiple labels by aligning image embeddings with\ntextual descriptions of each CVS criterion using positive and negative prompts.\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\nboosts CVS recognition over image-only methods. We also propose text-specific\ninference methods, that helps in analysing the image-text alignment. While\nfurther work is needed to match state-of-the-art spatial annotation-based\nmethods, this approach highlights the potential of adapting generalist models\nto specialized surgical tasks. Code:\nhttps://github.com/CAMMA-public/CVS-AdaptNet"
                },
                "authors": [
                    {
                        "name": "Britty Baby"
                    },
                    {
                        "name": "Vinkle Srivastav"
                    },
                    {
                        "name": "Pooja P. Jain"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Pietro Mascagni"
                    },
                    {
                        "name": "Nicolas Padoy"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Padoy"
                },
                "author": "Nicolas Padoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v1",
                "updated": "2025-07-07T13:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04984v1",
                "updated": "2025-07-07T13:25:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    25,
                    32,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:25:32Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    25,
                    32,
                    0,
                    188,
                    0
                ],
                "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation"
                },
                "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$\n(we use n to denote time in videos to avoid notation overload with the timestep\n$t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and\n$I_1$. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$\n(we use n to denote time in videos to avoid notation overload with the timestep\n$t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and\n$I_1$. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page."
                },
                "authors": [
                    {
                        "name": "Zonglin Lyu"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20147v2",
                "updated": "2025-07-07T13:23:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    23,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-26T15:46:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    46,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities"
                },
                "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Aoxue Li"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Ning Kang"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "37 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10385v2",
                "updated": "2025-07-07T13:21:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    21,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2024-12-18T09:35:28Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    35,
                    28,
                    2,
                    353,
                    0
                ],
                "title": "Autonomous Microscopy Experiments through Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Microscopy Experiments through Large Language Model Agents"
                },
                "summary": "Large language models (LLMs) are revolutionizing self driving laboratories\n(SDLs) for materials research, promising unprecedented acceleration of\nscientific discovery. However, current SDL implementations rely on rigid\nprotocols that fail to capture the adaptability and intuition of expert\nscientists in dynamic experimental settings. We introduce Artificially\nIntelligent Lab Assistant (AILA), a framework automating atomic force\nmicroscopy through LLM driven agents. Further, we develop AFMBench a\ncomprehensive evaluation suite challenging AI agents across the complete\nscientific workflow from experimental design to results analysis. We find that\nstate of the art models struggle with basic tasks and coordination scenarios.\nNotably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in\nmaterials domain question answering (QA) benchmarks, revealing that domain\nspecific QA proficiency does not necessarily translate to effective agentic\ncapabilities. Additionally, we observe that LLMs can deviate from instructions,\nraising safety alignment concerns for SDL applications. Our ablations reveal\nthat multi agent frameworks outperform single-agent architectures. We also\nobserve significant prompt fragility, where slight modifications in prompt\nstructure cause substantial performance variations in capable models like GPT\n4o. Finally, we evaluate AILA's effectiveness in increasingly advanced\nexperiments AFM calibration, feature detection, mechanical property\nmeasurement, graphene layer counting, and indenter detection. Our findings\nunderscore the necessity for rigorous benchmarking protocols and prompt\nengineering strategies before deploying AI laboratory assistants in scientific\nresearch environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing self driving laboratories\n(SDLs) for materials research, promising unprecedented acceleration of\nscientific discovery. However, current SDL implementations rely on rigid\nprotocols that fail to capture the adaptability and intuition of expert\nscientists in dynamic experimental settings. We introduce Artificially\nIntelligent Lab Assistant (AILA), a framework automating atomic force\nmicroscopy through LLM driven agents. Further, we develop AFMBench a\ncomprehensive evaluation suite challenging AI agents across the complete\nscientific workflow from experimental design to results analysis. We find that\nstate of the art models struggle with basic tasks and coordination scenarios.\nNotably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in\nmaterials domain question answering (QA) benchmarks, revealing that domain\nspecific QA proficiency does not necessarily translate to effective agentic\ncapabilities. Additionally, we observe that LLMs can deviate from instructions,\nraising safety alignment concerns for SDL applications. Our ablations reveal\nthat multi agent frameworks outperform single-agent architectures. We also\nobserve significant prompt fragility, where slight modifications in prompt\nstructure cause substantial performance variations in capable models like GPT\n4o. Finally, we evaluate AILA's effectiveness in increasingly advanced\nexperiments AFM calibration, feature detection, mechanical property\nmeasurement, graphene layer counting, and indenter detection. Our findings\nunderscore the necessity for rigorous benchmarking protocols and prompt\nengineering strategies before deploying AI laboratory assistants in scientific\nresearch environments."
                },
                "authors": [
                    {
                        "name": "Indrajeet Mandal"
                    },
                    {
                        "name": "Jitendra Soni"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Morten M. Smedskjaer"
                    },
                    {
                        "name": "Katrin Wondraczek"
                    },
                    {
                        "name": "Lothar Wondraczek"
                    },
                    {
                        "name": "Nitya Nand Gosvami"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04976v1",
                "updated": "2025-07-07T13:19:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    19,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:19:43Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    19,
                    43,
                    0,
                    188,
                    0
                ],
                "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models"
                },
                "summary": "In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets."
                },
                "authors": [
                    {
                        "name": "Eunseop Yoon"
                    },
                    {
                        "name": "Hee Suk Yoon"
                    },
                    {
                        "name": "Mark A. Hasegawa-Johnson"
                    },
                    {
                        "name": "Chang D. Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang D. Yoo"
                },
                "author": "Chang D. Yoo",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04961v1",
                "updated": "2025-07-07T13:04:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    4,
                    26,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:04:26Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    4,
                    26,
                    0,
                    188,
                    0
                ],
                "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D\n  Geometry-Consistent Attention Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D\n  Geometry-Consistent Attention Prior"
                },
                "summary": "3D Gaussian Splatting based 3D editing has demonstrated impressive\nperformance in recent years. However, the multi-view editing often exhibits\nsignificant local inconsistency, especially in areas of non-rigid deformation,\nwhich lead to local artifacts, texture blurring, or semantic variations in\nedited 3D scenes. We also found that the existing editing methods, which rely\nentirely on text prompts make the editing process a \"one-shot deal\", making it\ndifficult for users to control the editing degree flexibly. In response to\nthese challenges, we present InterGSEdit, a novel framework for high-quality\n3DGS editing via interactively selecting key views with users' preferences. We\npropose a CLIP-based Semantic Consistency Selection (CSCS) strategy to\nadaptively screen a group of semantically consistent reference views for each\nuser-selected key view. Then, the cross-attention maps derived from the\nreference views are used in a weighted Gaussian Splatting unprojection to\nconstruct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project\n$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D\ncross-attention via Attention Fusion Network (AFN). AFN employs an adaptive\nattention strategy that prioritizes 3D-constrained attention for geometric\nconsistency during early inference, and gradually prioritizes 2D\ncross-attention maps in diffusion for fine-grained features during the later\ninference. Extensive experiments demonstrate that InterGSEdit achieves\nstate-of-the-art performance, delivering consistent, high-fidelity 3DGS editing\nwith improved user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting based 3D editing has demonstrated impressive\nperformance in recent years. However, the multi-view editing often exhibits\nsignificant local inconsistency, especially in areas of non-rigid deformation,\nwhich lead to local artifacts, texture blurring, or semantic variations in\nedited 3D scenes. We also found that the existing editing methods, which rely\nentirely on text prompts make the editing process a \"one-shot deal\", making it\ndifficult for users to control the editing degree flexibly. In response to\nthese challenges, we present InterGSEdit, a novel framework for high-quality\n3DGS editing via interactively selecting key views with users' preferences. We\npropose a CLIP-based Semantic Consistency Selection (CSCS) strategy to\nadaptively screen a group of semantically consistent reference views for each\nuser-selected key view. Then, the cross-attention maps derived from the\nreference views are used in a weighted Gaussian Splatting unprojection to\nconstruct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project\n$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D\ncross-attention via Attention Fusion Network (AFN). AFN employs an adaptive\nattention strategy that prioritizes 3D-constrained attention for geometric\nconsistency during early inference, and gradually prioritizes 2D\ncross-attention maps in diffusion for fine-grained features during the later\ninference. Extensive experiments demonstrate that InterGSEdit achieves\nstate-of-the-art performance, delivering consistent, high-fidelity 3DGS editing\nwith improved user experience."
                },
                "authors": [
                    {
                        "name": "Minghao Wen"
                    },
                    {
                        "name": "Shengjie Wu"
                    },
                    {
                        "name": "Kangkan Wang"
                    },
                    {
                        "name": "Dong Liang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liang"
                },
                "author": "Dong Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07435v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07435v4",
                "updated": "2025-07-07T13:04:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    4,
                    2,
                    0,
                    188,
                    0
                ],
                "published": "2025-03-10T15:18:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    18,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds"
                },
                "summary": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods adapted for\npoint clouds, on average, and across multiple openness levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods adapted for\npoint clouds, on average, and across multiple openness levels."
                },
                "authors": [
                    {
                        "name": "Riccardo Mazzieri"
                    },
                    {
                        "name": "Jacopo Pegoraro"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07435v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07435v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04958v1",
                "updated": "2025-07-07T13:01:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    1,
                    6,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:01:06Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    1,
                    6,
                    0,
                    188,
                    0
                ],
                "title": "Boosting Temporal Sentence Grounding via Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Temporal Sentence Grounding via Causal Inference"
                },
                "summary": "Temporal Sentence Grounding (TSG) aims to identify relevant moments in an\nuntrimmed video that semantically correspond to a given textual query. Despite\nexisting studies having made substantial progress, they often overlook the\nissue of spurious correlations between video and textual queries. These\nspurious correlations arise from two primary factors: (1) inherent biases in\nthe textual data, such as frequent co-occurrences of specific verbs or phrases,\nand (2) the model's tendency to overfit to salient or repetitive patterns in\nvideo content. Such biases mislead the model into associating textual cues with\nincorrect visual moments, resulting in unreliable predictions and poor\ngeneralization to out-of-distribution examples. To overcome these limitations,\nwe propose a novel TSG framework, causal intervention and counterfactual\nreasoning that utilizes causal inference to eliminate spurious correlations and\nenhance the model's robustness. Specifically, we first formulate the TSG task\nfrom a causal perspective with a structural causal model. Then, to address\nunobserved confounders reflecting textual biases toward specific verbs or\nphrases, a textual causal intervention is proposed, utilizing do-calculus to\nestimate the causal effects. Furthermore, visual counterfactual reasoning is\nperformed by constructing a counterfactual scenario that focuses solely on\nvideo features, excluding the query and fused multi-modal features. This allows\nus to debias the model by isolating and removing the influence of the video\nfrom the overall effect. Experiments on public datasets demonstrate the\nsuperiority of the proposed method. The code is available at\nhttps://github.com/Tangkfan/CICR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Sentence Grounding (TSG) aims to identify relevant moments in an\nuntrimmed video that semantically correspond to a given textual query. Despite\nexisting studies having made substantial progress, they often overlook the\nissue of spurious correlations between video and textual queries. These\nspurious correlations arise from two primary factors: (1) inherent biases in\nthe textual data, such as frequent co-occurrences of specific verbs or phrases,\nand (2) the model's tendency to overfit to salient or repetitive patterns in\nvideo content. Such biases mislead the model into associating textual cues with\nincorrect visual moments, resulting in unreliable predictions and poor\ngeneralization to out-of-distribution examples. To overcome these limitations,\nwe propose a novel TSG framework, causal intervention and counterfactual\nreasoning that utilizes causal inference to eliminate spurious correlations and\nenhance the model's robustness. Specifically, we first formulate the TSG task\nfrom a causal perspective with a structural causal model. Then, to address\nunobserved confounders reflecting textual biases toward specific verbs or\nphrases, a textual causal intervention is proposed, utilizing do-calculus to\nestimate the causal effects. Furthermore, visual counterfactual reasoning is\nperformed by constructing a counterfactual scenario that focuses solely on\nvideo features, excluding the query and fused multi-modal features. This allows\nus to debias the model by isolating and removing the influence of the video\nfrom the overall effect. Experiments on public datasets demonstrate the\nsuperiority of the proposed method. The code is available at\nhttps://github.com/Tangkfan/CICR."
                },
                "authors": [
                    {
                        "name": "Kefan Tang"
                    },
                    {
                        "name": "Lihuo He"
                    },
                    {
                        "name": "Jisheng Dang"
                    },
                    {
                        "name": "Xinbo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xinbo Gao"
                },
                "author": "Xinbo Gao",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04952v1",
                "updated": "2025-07-07T12:53:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:53:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation"
                },
                "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Bohui Zhai"
                    },
                    {
                        "name": "Guoxiang He"
                    },
                    {
                        "name": "Hebin Li"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Lingyun Tan"
                    },
                    {
                        "name": "Pengyu Guo"
                    },
                    {
                        "name": "Xianshu Pang"
                    },
                    {
                        "name": "Yang Ruan"
                    },
                    {
                        "name": "Zhifeng Zhang"
                    },
                    {
                        "name": "Zhonghu Wang"
                    },
                    {
                        "name": "Ziyan Xu"
                    },
                    {
                        "name": "Zuopu Yin"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04951v1",
                "updated": "2025-07-07T12:52:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    52,
                    57,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:52:57Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    52,
                    57,
                    0,
                    188,
                    0
                ],
                "title": "What is emergence, after all?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is emergence, after all?"
                },
                "summary": "The term emergence is increasingly used across scientific disciplines to\ndescribe phenomena that arise from interactions among a system's components but\ncannot be readily inferred by examining those components in isolation. While\noften invoked to explain higher-level behaviors, such as flocking,\nsynchronization, or collective intelligence, the term is frequently used\nwithout precision, sometimes giving rise to ambiguity or even mystique. In this\nperspective paper, we clarify the scientific meaning of emergence as a\nmeasurable, physically grounded phenomenon. Through concrete examples, such as\ntemperature, magnetism, and herd immunity in social networks, we review how\ncollective behavior can arise from local interactions that are constrained by\nglobal boundaries. By disentangling emergence from vague overuse, we emphasize\nits role as a rigorous tool for understanding complex systems. Our goal is to\nshow that emergence, when properly framed, offers not mysticism but insight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The term emergence is increasingly used across scientific disciplines to\ndescribe phenomena that arise from interactions among a system's components but\ncannot be readily inferred by examining those components in isolation. While\noften invoked to explain higher-level behaviors, such as flocking,\nsynchronization, or collective intelligence, the term is frequently used\nwithout precision, sometimes giving rise to ambiguity or even mystique. In this\nperspective paper, we clarify the scientific meaning of emergence as a\nmeasurable, physically grounded phenomenon. Through concrete examples, such as\ntemperature, magnetism, and herd immunity in social networks, we review how\ncollective behavior can arise from local interactions that are constrained by\nglobal boundaries. By disentangling emergence from vague overuse, we emphasize\nits role as a rigorous tool for understanding complex systems. Our goal is to\nshow that emergence, when properly framed, offers not mysticism but insight."
                },
                "authors": [
                    {
                        "name": "Abbas K. Rizi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas K. Rizi"
                },
                "author": "Abbas K. Rizi",
                "arxiv_comment": "6 pages, 2 figures, 92 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04942v2",
                "updated": "2025-07-08T06:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    37,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T12:38:53Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    38,
                    53,
                    0,
                    188,
                    0
                ],
                "title": "SIGIR 2025 -- LiveRAG Challenge Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGIR 2025 -- LiveRAG Challenge Report"
                },
                "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy."
                },
                "authors": [
                    {
                        "name": "David Carmel"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Oren Somekh"
                    },
                    {
                        "name": "Ran Tavory"
                    },
                    {
                        "name": "Mehdi Ghissassi"
                    },
                    {
                        "name": "Edo Liberty"
                    },
                    {
                        "name": "Roy Miara"
                    }
                ],
                "author_detail": {
                    "name": "Roy Miara"
                },
                "author": "Roy Miara",
                "arxiv_comment": "9 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04931v1",
                "updated": "2025-07-07T12:26:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    26,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:26:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    26,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "LIFT: Automating Symbolic Execution Optimization with Large Language\n  Models for AI Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Automating Symbolic Execution Optimization with Large Language\n  Models for AI Networks"
                },
                "summary": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems."
                },
                "authors": [
                    {
                        "name": "Ruoxi Wang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Chunchi Liu"
                    },
                    {
                        "name": "Yinhao Xiao"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI\n  Computing (NAIC). 7 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04920v1",
                "updated": "2025-07-07T12:06:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    6,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:06:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    6,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Object-centric Denoising Diffusion Models for Physical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-centric Denoising Diffusion Models for Physical Reasoning"
                },
                "summary": "Reasoning about the trajectories of multiple, interacting objects is integral\nto physical reasoning tasks in machine learning. This involves conditions\nimposed on the objects at different time steps, for instance initial states or\ndesired goal states. Existing approaches in physical reasoning generally rely\non autoregressive modeling, which can only be conditioned on initial states,\nbut not on later states. In fields such as planning for reinforcement learning,\nsimilar challenges are being addressed with denoising diffusion models. In this\nwork, we propose an object-centric denoising diffusion model architecture for\nphysical reasoning that is translation equivariant over time, permutation\nequivariant over objects, and can be conditioned on arbitrary time steps for\narbitrary objects. We demonstrate how this model can solve tasks with multiple\nconditions and examine its performance when changing object numbers and\ntrajectory lengths during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about the trajectories of multiple, interacting objects is integral\nto physical reasoning tasks in machine learning. This involves conditions\nimposed on the objects at different time steps, for instance initial states or\ndesired goal states. Existing approaches in physical reasoning generally rely\non autoregressive modeling, which can only be conditioned on initial states,\nbut not on later states. In fields such as planning for reinforcement learning,\nsimilar challenges are being addressed with denoising diffusion models. In this\nwork, we propose an object-centric denoising diffusion model architecture for\nphysical reasoning that is translation equivariant over time, permutation\nequivariant over objects, and can be conditioned on arbitrary time steps for\narbitrary objects. We demonstrate how this model can solve tasks with multiple\nconditions and examine its performance when changing object numbers and\ntrajectory lengths during inference."
                },
                "authors": [
                    {
                        "name": "Moritz Lange"
                    },
                    {
                        "name": "Raphael C. Engelhardt"
                    },
                    {
                        "name": "Wolfgang Konen"
                    },
                    {
                        "name": "Andrew Melnik"
                    },
                    {
                        "name": "Laurenz Wiskott"
                    }
                ],
                "author_detail": {
                    "name": "Laurenz Wiskott"
                },
                "author": "Laurenz Wiskott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04917v1",
                "updated": "2025-07-07T12:04:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    4,
                    10,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:04:10Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    4,
                    10,
                    0,
                    188,
                    0
                ],
                "title": "Leadership Detection via Time-Lagged Correlation-Based Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leadership Detection via Time-Lagged Correlation-Based Network Inference"
                },
                "summary": "Understanding leadership dynamics in collective behavior is a key challenge\nin animal ecology, swarm robotics, and intelligent transportation. Traditional\ninformation-theoretic approaches, including Transfer Entropy (TE) and\nTime-Lagged Mutual Information (TLMI), have been widely used to infer\nleader-follower relationships but face critical limitations in noisy or\nshort-duration datasets due to their reliance on robust probability\nestimations. This study proposes a method based on dynamic network inference\nusing time-lagged correlations across multiple kinematic variables: velocity,\nacceleration, and direction. Our approach constructs directed influence graphs\nover time, enabling the identification of leadership patterns without the need\nfor large volumes of data or parameter-sensitive discretization. We validate\nour method through two multi-agent simulations in NetLogo: a modified Vicsek\nmodel with informed leaders and a predator-prey model featuring coordinated and\nindependent wolf groups. Experimental results demonstrate that the\nnetwork-based method outperforms TE and TLMI in scenarios with limited\nspatiotemporal observations, ranking true leaders at the top of influence\nmetrics more consistently than TE and TLMI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding leadership dynamics in collective behavior is a key challenge\nin animal ecology, swarm robotics, and intelligent transportation. Traditional\ninformation-theoretic approaches, including Transfer Entropy (TE) and\nTime-Lagged Mutual Information (TLMI), have been widely used to infer\nleader-follower relationships but face critical limitations in noisy or\nshort-duration datasets due to their reliance on robust probability\nestimations. This study proposes a method based on dynamic network inference\nusing time-lagged correlations across multiple kinematic variables: velocity,\nacceleration, and direction. Our approach constructs directed influence graphs\nover time, enabling the identification of leadership patterns without the need\nfor large volumes of data or parameter-sensitive discretization. We validate\nour method through two multi-agent simulations in NetLogo: a modified Vicsek\nmodel with informed leaders and a predator-prey model featuring coordinated and\nindependent wolf groups. Experimental results demonstrate that the\nnetwork-based method outperforms TE and TLMI in scenarios with limited\nspatiotemporal observations, ranking true leaders at the top of influence\nmetrics more consistently than TE and TLMI."
                },
                "authors": [
                    {
                        "name": "Thayanne França da Silva"
                    },
                    {
                        "name": "José Everardo Bessa Maia"
                    }
                ],
                "author_detail": {
                    "name": "José Everardo Bessa Maia"
                },
                "author": "José Everardo Bessa Maia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00047v2",
                "updated": "2025-07-07T12:01:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    1,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2025-03-30T23:16:23Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    23,
                    16,
                    23,
                    6,
                    89,
                    0
                ],
                "title": "EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic\n  Single-Cell Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic\n  Single-Cell Analysis"
                },
                "summary": "Microfluidic Live-Cell Imaging (MLCI) yields data on microbial cell\nfactories. However, continuous acquisition is challenging as high-throughput\nexperiments often lack real-time insights, delaying responses to stochastic\nevents. We introduce three components in the Experiment Automation Pipeline for\nEvent-Driven Microscopy to Smart Microfluidic Single-Cell Analysis (EAP4EMSIG):\na fast, accurate Multi-Layer Perceptron (MLP)-based autofocusing method\npredicting the focus offset, an evaluation of real-time segmentation methods\nand a real-time data analysis dashboard. Our MLP-based autofocusing achieves a\nMean Absolute Error (MAE) of 0.105 $\\mu$m with inference times from 87 ms.\nAmong eleven evaluated Deep Learning (DL) segmentation methods, Cellpose\nreached a Panoptic Quality (PQ) of 93.36 %, while a distance-based method was\nfastest (121 ms, Panoptic Quality 93.02 %).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microfluidic Live-Cell Imaging (MLCI) yields data on microbial cell\nfactories. However, continuous acquisition is challenging as high-throughput\nexperiments often lack real-time insights, delaying responses to stochastic\nevents. We introduce three components in the Experiment Automation Pipeline for\nEvent-Driven Microscopy to Smart Microfluidic Single-Cell Analysis (EAP4EMSIG):\na fast, accurate Multi-Layer Perceptron (MLP)-based autofocusing method\npredicting the focus offset, an evaluation of real-time segmentation methods\nand a real-time data analysis dashboard. Our MLP-based autofocusing achieves a\nMean Absolute Error (MAE) of 0.105 $\\mu$m with inference times from 87 ms.\nAmong eleven evaluated Deep Learning (DL) segmentation methods, Cellpose\nreached a Panoptic Quality (PQ) of 93.36 %, while a distance-based method was\nfastest (121 ms, Panoptic Quality 93.02 %)."
                },
                "authors": [
                    {
                        "name": "Nils Friederich"
                    },
                    {
                        "name": "Angelo Jovin Yamachui Sitcheu"
                    },
                    {
                        "name": "Annika Nassal"
                    },
                    {
                        "name": "Erenus Yildiz"
                    },
                    {
                        "name": "Matthias Pesch"
                    },
                    {
                        "name": "Maximilian Beichter"
                    },
                    {
                        "name": "Lukas Scholtes"
                    },
                    {
                        "name": "Bahar Akbaba"
                    },
                    {
                        "name": "Thomas Lautenschlager"
                    },
                    {
                        "name": "Oliver Neumann"
                    },
                    {
                        "name": "Dietrich Kohlheyer"
                    },
                    {
                        "name": "Hanno Scharr"
                    },
                    {
                        "name": "Johannes Seiffarth"
                    },
                    {
                        "name": "Katharina Nöh"
                    },
                    {
                        "name": "Ralf Mikut"
                    }
                ],
                "author_detail": {
                    "name": "Ralf Mikut"
                },
                "author": "Ralf Mikut",
                "arxiv_comment": "Submitted to: at - Automatisierungstechnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16174v2",
                "updated": "2025-07-07T11:43:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    43,
                    34,
                    0,
                    188,
                    0
                ],
                "published": "2025-02-22T10:31:50Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    50,
                    5,
                    53,
                    0
                ],
                "title": "Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation\n  via Latent Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation\n  via Latent Prototypes"
                },
                "summary": "With the rise of LLMs, ensuring model safety and alignment has become a\ncritical concern. While modern instruction-finetuned LLMs incorporate alignment\nduring training, they still frequently require moderation tools to prevent\nunsafe behavior. The most common approach to moderation are guard models that\nflag unsafe inputs. However, guards require costly training and are typically\nlimited to fixed-size, pre-trained options, making them difficult to adapt to\nevolving risks and resource constraints. We hypothesize that\ninstruction-finetuned LLMs already encode safety-relevant information\ninternally and explore training-free safety assessment methods that work with\noff-the-shelf models. We show that simple prompting allows models to recognize\nharmful inputs they would otherwise mishandle. We also demonstrate that safe\nand unsafe prompts are distinctly separable in the models' latent space.\nBuilding on this, we introduce the Latent Prototype Moderator (LPM), a\ntraining-free moderation method that uses Mahalanobis distance in latent space\nto assess input safety. LPM is a lightweight, customizable add-on that\ngeneralizes across model families and sizes. Our method matches or exceeds\nstate-of-the-art guard models across multiple safety benchmarks, offering a\npractical and flexible solution for scalable LLM moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, ensuring model safety and alignment has become a\ncritical concern. While modern instruction-finetuned LLMs incorporate alignment\nduring training, they still frequently require moderation tools to prevent\nunsafe behavior. The most common approach to moderation are guard models that\nflag unsafe inputs. However, guards require costly training and are typically\nlimited to fixed-size, pre-trained options, making them difficult to adapt to\nevolving risks and resource constraints. We hypothesize that\ninstruction-finetuned LLMs already encode safety-relevant information\ninternally and explore training-free safety assessment methods that work with\noff-the-shelf models. We show that simple prompting allows models to recognize\nharmful inputs they would otherwise mishandle. We also demonstrate that safe\nand unsafe prompts are distinctly separable in the models' latent space.\nBuilding on this, we introduce the Latent Prototype Moderator (LPM), a\ntraining-free moderation method that uses Mahalanobis distance in latent space\nto assess input safety. LPM is a lightweight, customizable add-on that\ngeneralizes across model families and sizes. Our method matches or exceeds\nstate-of-the-art guard models across multiple safety benchmarks, offering a\npractical and flexible solution for scalable LLM moderation."
                },
                "authors": [
                    {
                        "name": "Maciej Chrabąszcz"
                    },
                    {
                        "name": "Filip Szatkowski"
                    },
                    {
                        "name": "Bartosz Wójcik"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Tomasz Trzciński"
                    },
                    {
                        "name": "Sebastian Cygert"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Cygert"
                },
                "author": "Sebastian Cygert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04893v1",
                "updated": "2025-07-07T11:27:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    49,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:27:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident\n  Severity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident\n  Severity Prediction"
                },
                "summary": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications."
                },
                "authors": [
                    {
                        "name": "Kaleem Ullah Qasim"
                    },
                    {
                        "name": "Jiashu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiashu Zhang"
                },
                "author": "Jiashu Zhang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v3",
                "updated": "2025-07-07T11:27:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    2,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhi Zhang"
                },
                "author": "Yuzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04886v1",
                "updated": "2025-07-07T11:17:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:17:32Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations"
                },
                "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04884v1",
                "updated": "2025-07-07T11:16:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    16,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:16:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    16,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "Building Open-Retrieval Conversational Question Answering Systems by\n  Generating Synthetic Data and Decontextualizing User Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Open-Retrieval Conversational Question Answering Systems by\n  Generating Synthetic Data and Decontextualizing User Questions"
                },
                "summary": "We consider open-retrieval conversational question answering (OR-CONVQA), an\nextension of question answering where system responses need to be (i) aware of\ndialog history and (ii) grounded in documents (or document fragments) retrieved\nper question. Domain-specific OR-CONVQA training datasets are crucial for\nreal-world applications, but hard to obtain. We propose a pipeline that\ncapitalizes on the abundance of plain text documents in organizations (e.g.,\nproduct documentation) to automatically produce realistic OR-CONVQA dialogs\nwith annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we\ngenerate in-dialog question-answer pairs, self-contained (decontextualized,\ne.g., no referring expressions) versions of user questions, and propositions\n(sentences expressing prominent information from the documents) the system\nresponses are grounded in. We show how the synthetic dialogs can be used to\ntrain efficient question rewriters that decontextualize user questions,\nallowing existing dialog-unaware retrievers to be utilized. The retrieved\ninformation and the decontextualized question are then passed on to an LLM that\ngenerates the system's response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider open-retrieval conversational question answering (OR-CONVQA), an\nextension of question answering where system responses need to be (i) aware of\ndialog history and (ii) grounded in documents (or document fragments) retrieved\nper question. Domain-specific OR-CONVQA training datasets are crucial for\nreal-world applications, but hard to obtain. We propose a pipeline that\ncapitalizes on the abundance of plain text documents in organizations (e.g.,\nproduct documentation) to automatically produce realistic OR-CONVQA dialogs\nwith annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we\ngenerate in-dialog question-answer pairs, self-contained (decontextualized,\ne.g., no referring expressions) versions of user questions, and propositions\n(sentences expressing prominent information from the documents) the system\nresponses are grounded in. We show how the synthetic dialogs can be used to\ntrain efficient question rewriters that decontextualize user questions,\nallowing existing dialog-unaware retrievers to be utilized. The retrieved\ninformation and the decontextualized question are then passed on to an LLM that\ngenerates the system's response."
                },
                "authors": [
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Nikolaos Stylianou"
                    },
                    {
                        "name": "Alexandra Fiotaki"
                    },
                    {
                        "name": "Spiros Methenitis"
                    },
                    {
                        "name": "Elisavet Palogiannidi"
                    },
                    {
                        "name": "Themos Stafylakis"
                    },
                    {
                        "name": "Ion Androutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Ion Androutsopoulos"
                },
                "author": "Ion Androutsopoulos",
                "arxiv_comment": "Accepted at SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04877v1",
                "updated": "2025-07-07T11:04:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    4,
                    3,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:04:03Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    4,
                    3,
                    0,
                    188,
                    0
                ],
                "title": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese\n  Medicine"
                },
                "summary": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise."
                },
                "authors": [
                    {
                        "name": "Zewen Sun"
                    },
                    {
                        "name": "Ruoxiang Huang"
                    },
                    {
                        "name": "Jiahe Feng"
                    },
                    {
                        "name": "Rundong Kong"
                    },
                    {
                        "name": "Yuqian Wang"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Ziqi Gong"
                    },
                    {
                        "name": "Yuyuan Qin"
                    },
                    {
                        "name": "Yingxue Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04870v1",
                "updated": "2025-07-07T10:56:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    56,
                    12,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:56:12Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    56,
                    12,
                    0,
                    188,
                    0
                ],
                "title": "NTSFormer: A Self-Teaching Graph Transformer for Multimodal Cold-Start\n  Node Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NTSFormer: A Self-Teaching Graph Transformer for Multimodal Cold-Start\n  Node Classification"
                },
                "summary": "Cold-start node classification on multimodal graphs is challenging because\ncold-start nodes are isolated (i.e., no edges) and often have missing\nmodalities (e.g., absent text or image features). Existing methods address\nstructural isolation by degrading graph learning models to MLPs for cold-start\ninference, using a teacher model (with graph access) to guide the MLP. However,\nthis results in limited model capacity in the student, which is further\nchallenged when modalities are missing. In this paper, we propose\nNeighbor-to-Self Graph Transformer (NTSFormer), a unified Graph Transformer\nframework that jointly tackles the isolation and missing-modality issues via a\nself-teaching paradigm. Specifically, NTSFormer uses a cold-start attention\nmask to simultaneously make two predictions for each node: a \"student\"\nprediction based only on self-information (i.e., the node's own features), and\na \"teacher\" prediction incorporating both self and neighbor information. This\nenables the model to supervise itself without degrading to an MLP, thereby\nfully leveraging the Transformer's capacity to handle missing modalities. To\nhandle diverse graph information and missing modalities, NTSFormer performs a\none-time multimodal graph pre-computation that converts structural and feature\ndata into token sequences, which are then processed by a Mixture-of-Experts\n(MoE) Input Projection and Transformer layers for effective fusion.\nExperimental results on public datasets show that NTSFormer achieves superior\nperformance on multimodal cold-start node classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-start node classification on multimodal graphs is challenging because\ncold-start nodes are isolated (i.e., no edges) and often have missing\nmodalities (e.g., absent text or image features). Existing methods address\nstructural isolation by degrading graph learning models to MLPs for cold-start\ninference, using a teacher model (with graph access) to guide the MLP. However,\nthis results in limited model capacity in the student, which is further\nchallenged when modalities are missing. In this paper, we propose\nNeighbor-to-Self Graph Transformer (NTSFormer), a unified Graph Transformer\nframework that jointly tackles the isolation and missing-modality issues via a\nself-teaching paradigm. Specifically, NTSFormer uses a cold-start attention\nmask to simultaneously make two predictions for each node: a \"student\"\nprediction based only on self-information (i.e., the node's own features), and\na \"teacher\" prediction incorporating both self and neighbor information. This\nenables the model to supervise itself without degrading to an MLP, thereby\nfully leveraging the Transformer's capacity to handle missing modalities. To\nhandle diverse graph information and missing modalities, NTSFormer performs a\none-time multimodal graph pre-computation that converts structural and feature\ndata into token sequences, which are then processed by a Mixture-of-Experts\n(MoE) Input Projection and Transformer layers for effective fusion.\nExperimental results on public datasets show that NTSFormer achieves superior\nperformance on multimodal cold-start node classification tasks."
                },
                "authors": [
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04857v1",
                "updated": "2025-07-07T10:30:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    30,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:30:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    30,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "Supporting Software Formal Verification with Large Language Models: An\n  Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Software Formal Verification with Large Language Models: An\n  Experimental Study"
                },
                "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Marie Farrell"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Liping Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhao"
                },
                "author": "Liping Zhao",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09720v2",
                "updated": "2025-07-07T10:27:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    27,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-13T21:01:21Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    21,
                    1,
                    21,
                    6,
                    103,
                    0
                ],
                "title": "NotebookLM: An LLM with RAG for active learning and collaborative\n  tutoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotebookLM: An LLM with RAG for active learning and collaborative\n  tutoring"
                },
                "summary": "This study explores NotebookLM, a Google Gemini powered AI platform that\nintegrates Retrieval Augmented Generation (RAG), as a collaborative physics\ntutor, an area of research that is developing quickly. In our implementation,\nNotebookLM was configured as an AI physics collaborative tutor to support\nstudents in solving conceptually oriented physics problems using a\ncollaborative, Socratic approach. When deployed as a collaborative tutor, the\nsystem restricts student interaction to a chat only interface, promoting\ncontrolled and guided engagement. By grounding its responses in teacher\nprovided source documents, NotebookLM helps mitigate one of the major\nshortcomings of standard large language models--hallucinations--thereby\nensuring more traceable and reliable answers. Our experiments demonstrate\nNotebookLM's potential as a low cost, easily implemented RAG based tool for\npersonalized and traceable AI assisted physics learning in diverse educational\nsettings. Furthermore, NotebookLM also functions as a valuable study tool for\nboth teachers and students by generating targeted questions, study guides, and\nsupplementary materials that enhance both classroom instruction and independent\nresearch. While limitations remain, particularly regarding legal restrictions,\nthe current text only mode of interaction, and the intrinsic reliability\nchallenges of statistical models, this work presents a promising example of a\ngrounded AI application in physics education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores NotebookLM, a Google Gemini powered AI platform that\nintegrates Retrieval Augmented Generation (RAG), as a collaborative physics\ntutor, an area of research that is developing quickly. In our implementation,\nNotebookLM was configured as an AI physics collaborative tutor to support\nstudents in solving conceptually oriented physics problems using a\ncollaborative, Socratic approach. When deployed as a collaborative tutor, the\nsystem restricts student interaction to a chat only interface, promoting\ncontrolled and guided engagement. By grounding its responses in teacher\nprovided source documents, NotebookLM helps mitigate one of the major\nshortcomings of standard large language models--hallucinations--thereby\nensuring more traceable and reliable answers. Our experiments demonstrate\nNotebookLM's potential as a low cost, easily implemented RAG based tool for\npersonalized and traceable AI assisted physics learning in diverse educational\nsettings. Furthermore, NotebookLM also functions as a valuable study tool for\nboth teachers and students by generating targeted questions, study guides, and\nsupplementary materials that enhance both classroom instruction and independent\nresearch. While limitations remain, particularly regarding legal restrictions,\nthe current text only mode of interaction, and the intrinsic reliability\nchallenges of statistical models, this work presents a promising example of a\ngrounded AI application in physics education."
                },
                "authors": [
                    {
                        "name": "Eugenio Tufino"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Tufino"
                },
                "author": "Eugenio Tufino",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04854v1",
                "updated": "2025-07-07T10:26:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    26,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:26:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    26,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large\n  Language Models"
                },
                "summary": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released."
                },
                "authors": [
                    {
                        "name": "Shrey Ganatra"
                    },
                    {
                        "name": "Swapnil Bhattacharyya"
                    },
                    {
                        "name": "Harshvivek Kashid"
                    },
                    {
                        "name": "Spandan Anaokar"
                    },
                    {
                        "name": "Shruti Nair"
                    },
                    {
                        "name": "Reshma Sekhar"
                    },
                    {
                        "name": "Siddharth Manohar"
                    },
                    {
                        "name": "Rahul Hemrajani"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04852v1",
                "updated": "2025-07-07T10:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:20:16Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "title": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels"
                },
                "summary": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Hanjie Zhao"
                    },
                    {
                        "name": "Senbin Zhu"
                    },
                    {
                        "name": "Hongde Liu"
                    },
                    {
                        "name": "Zhihong Zhang"
                    },
                    {
                        "name": "Yuxiang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Jia"
                },
                "author": "Yuxiang Jia",
                "arxiv_comment": "The paper has been accepted by NLPCC2025. 12 pages, 5 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04841v1",
                "updated": "2025-07-07T10:03:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    20,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:03:20Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    20,
                    0,
                    188,
                    0
                ],
                "title": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient\n  Task-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient\n  Task-Oriented Dialogue Systems"
                },
                "summary": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Quang-Vinh Nguyen"
                    },
                    {
                        "name": "Quang-Chieu Nguyen"
                    },
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted at SIGdial 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03637v2",
                "updated": "2025-07-07T09:53:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    53,
                    22,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-04T07:30:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    30,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "RewardAnything: Generalizable Principle-Following Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RewardAnything: Generalizable Principle-Following Reward Models"
                },
                "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles."
                },
                "authors": [
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Weizheng Gu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "25 pages, 9 figures, Code & model weights available at:\n  https://zhuohaoyu.github.io/RewardAnything",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04820v1",
                "updated": "2025-07-07T09:38:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    38,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:38:43Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    38,
                    43,
                    0,
                    188,
                    0
                ],
                "title": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation"
                },
                "summary": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving."
                },
                "authors": [
                    {
                        "name": "Junru Wu"
                    },
                    {
                        "name": "Le Yan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Honglei Zhuang"
                    },
                    {
                        "name": "Paul Suganthan G. C."
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhe Dong"
                    },
                    {
                        "name": "Xuanhui Wang"
                    },
                    {
                        "name": "Harrie Oosterhuis"
                    }
                ],
                "author_detail": {
                    "name": "Harrie Oosterhuis"
                },
                "author": "Harrie Oosterhuis",
                "arxiv_comment": "ReNeuIR 2025 (at SIGIR 2025) - 4th Workshop on Reaching Efficiency in\n  Neural Information Retrieval, July 17, 2025, Padua, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04817v1",
                "updated": "2025-07-07T09:36:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    36,
                    0,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:36:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    36,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and\n  Duration Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and\n  Duration Parameters"
                },
                "summary": "Precise control over speech characteristics, such as pitch, duration, and\nspeech rate, remains a significant challenge in the field of voice conversion.\nThe ability to manipulate parameters like pitch and syllable rate is an\nimportant element for effective identity conversion, but can also be used\nindependently for voice transformation, achieving goals that were historically\naddressed by vocoder-based methods.\n  In this work, we explore a convolutional neural network-based approach that\naims to provide means for modifying fundamental frequency (F0), phoneme\nsequences, intensity, and speaker identity. Rather than relying on\ndisentanglement techniques, our model is explicitly conditioned on these\nfactors to generate mel spectrograms, which are then converted into waveforms\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\nintuitively controlled voice transformations.\n  We evaluate our approach on speaker conversion and expressive speech tasks\nusing both perceptual and objective metrics. The results suggest that the\nproposed method offers substantial flexibility, while maintaining high\nintelligibility and speaker similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise control over speech characteristics, such as pitch, duration, and\nspeech rate, remains a significant challenge in the field of voice conversion.\nThe ability to manipulate parameters like pitch and syllable rate is an\nimportant element for effective identity conversion, but can also be used\nindependently for voice transformation, achieving goals that were historically\naddressed by vocoder-based methods.\n  In this work, we explore a convolutional neural network-based approach that\naims to provide means for modifying fundamental frequency (F0), phoneme\nsequences, intensity, and speaker identity. Rather than relying on\ndisentanglement techniques, our model is explicitly conditioned on these\nfactors to generate mel spectrograms, which are then converted into waveforms\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\nintuitively controlled voice transformations.\n  We evaluate our approach on speaker conversion and expressive speech tasks\nusing both perceptual and objective metrics. The results suggest that the\nproposed method offers substantial flexibility, while maintaining high\nintelligibility and speaker similarity."
                },
                "authors": [
                    {
                        "name": "Mathilde Abrassart"
                    },
                    {
                        "name": "Nicolas Obin"
                    },
                    {
                        "name": "Axel Roebel"
                    }
                ],
                "author_detail": {
                    "name": "Axel Roebel"
                },
                "author": "Axel Roebel",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23323v2",
                "updated": "2025-07-07T09:26:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    26,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-29T16:41:41Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    16,
                    41,
                    41,
                    6,
                    180,
                    0
                ],
                "title": "SwiftSeg: Efficient Training-Free Open-Vocabulary Segmentation via\n  Hierarchical Attention Refinement Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSeg: Efficient Training-Free Open-Vocabulary Segmentation via\n  Hierarchical Attention Refinement Method"
                },
                "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Quang-Huy Che"
                    },
                    {
                        "name": "Vinh-Tiep Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Vinh-Tiep Nguyen"
                },
                "author": "Vinh-Tiep Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04803v1",
                "updated": "2025-07-07T09:22:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    22,
                    6,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:22:06Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    22,
                    6,
                    0,
                    188,
                    0
                ],
                "title": "Application and Evaluation of Large Language Models for Forecasting the\n  Impact of Traffic Incidents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application and Evaluation of Large Language Models for Forecasting the\n  Impact of Traffic Incidents"
                },
                "summary": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction."
                },
                "authors": [
                    {
                        "name": "George Jagadeesh"
                    },
                    {
                        "name": "Srikrishna Iyer"
                    },
                    {
                        "name": "Michal Polanowski"
                    },
                    {
                        "name": "Kai Xin Thia"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xin Thia"
                },
                "author": "Kai Xin Thia",
                "arxiv_comment": "This paper has been accepted for publication at the 2025 IEEE 28th\n  International Conference on Intelligent Transportation Systems (ITSC), Gold\n  Coast, Australia, 2025. Copyright IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05853v2",
                "updated": "2025-07-07T09:14:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    14,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-06T08:16:07Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    8,
                    16,
                    7,
                    4,
                    157,
                    0
                ],
                "title": "Training-Free Query Optimization via LLM-Based Plan Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Query Optimization via LLM-Based Plan Similarity"
                },
                "summary": "Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Vladimir Boorlakov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Boorlakov"
                },
                "author": "Vladimir Boorlakov",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13596v2",
                "updated": "2025-07-07T09:09:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    9,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-16T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems"
                },
                "summary": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Long-Vu Hoang"
                    },
                    {
                        "name": "Huy-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Huy-Dat Tran"
                },
                "author": "Huy-Dat Tran",
                "arxiv_comment": "Accepted to Interspeech MLCSLM-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10246v2",
                "updated": "2025-07-07T09:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    8,
                    31,
                    0,
                    188,
                    0
                ],
                "published": "2025-03-13T10:45:54Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    45,
                    54,
                    3,
                    72,
                    0
                ],
                "title": "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation"
                },
                "summary": "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude the individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and effect\nestimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude the individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and effect\nestimation."
                },
                "authors": [
                    {
                        "name": "Samuel Pawel"
                    },
                    {
                        "name": "Małgorzata Roos"
                    },
                    {
                        "name": "Leonhard Held"
                    }
                ],
                "author_detail": {
                    "name": "Leonhard Held"
                },
                "author": "Leonhard Held",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04780v1",
                "updated": "2025-07-07T08:57:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    57,
                    57,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:57:57Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    57,
                    57,
                    0,
                    188,
                    0
                ],
                "title": "Retrodicting Chaotic Systems: An Algorithmic Information Theory Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrodicting Chaotic Systems: An Algorithmic Information Theory Approach"
                },
                "summary": "Making accurate inferences about data is a key task in science and\nmathematics. Here we study the problem of \\emph{retrodiction}, inferring past\nvalues of a series, in the context of chaotic dynamical systems. Specifically,\nwe are interested in inferring the starting value $x_0$ in the series\n$x_0,x_1,x_2,\\dots,x_n$ given the value of $x_n$, and the associated function\n$f$ which determines the series as $f(x_i)=x_{i+1}$. Even in the deterministic\ncase this is a challenging problem, due to mixing and the typically\nexponentially many candidate past values in the pre-image of any given value\n$x_n$ (e.g., a current observation). We study this task from the perspective of\nalgorithmic information theory, which motivates two approaches: One to search\nfor the `simplest' value in the set of candidates, and one to look for the\nvalue in the lowest density region of the candidates. We test these methods\nnumerically on the logistic map, Tent map, Bernoulli map, and Julia/Mandelbrot\nmap, which are well-studied maps in chaos theory. The methods aid in\nretrodiction by assigning low ranks to candidates which are more likely to be\nthe true starting value, which works well in some parameter and map cases, but\nthe methods are not effective in all cases, and several open problems remain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making accurate inferences about data is a key task in science and\nmathematics. Here we study the problem of \\emph{retrodiction}, inferring past\nvalues of a series, in the context of chaotic dynamical systems. Specifically,\nwe are interested in inferring the starting value $x_0$ in the series\n$x_0,x_1,x_2,\\dots,x_n$ given the value of $x_n$, and the associated function\n$f$ which determines the series as $f(x_i)=x_{i+1}$. Even in the deterministic\ncase this is a challenging problem, due to mixing and the typically\nexponentially many candidate past values in the pre-image of any given value\n$x_n$ (e.g., a current observation). We study this task from the perspective of\nalgorithmic information theory, which motivates two approaches: One to search\nfor the `simplest' value in the set of candidates, and one to look for the\nvalue in the lowest density region of the candidates. We test these methods\nnumerically on the logistic map, Tent map, Bernoulli map, and Julia/Mandelbrot\nmap, which are well-studied maps in chaos theory. The methods aid in\nretrodiction by assigning low ranks to candidates which are more likely to be\nthe true starting value, which works well in some parameter and map cases, but\nthe methods are not effective in all cases, and several open problems remain."
                },
                "authors": [
                    {
                        "name": "Kamal Dingle"
                    },
                    {
                        "name": "Boumediene Hamzi"
                    },
                    {
                        "name": "Marcus Hutter"
                    },
                    {
                        "name": "Houman Owhadi"
                    }
                ],
                "author_detail": {
                    "name": "Houman Owhadi"
                },
                "author": "Houman Owhadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04770v1",
                "updated": "2025-07-07T08:45:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    45,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:45:08Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    45,
                    8,
                    0,
                    188,
                    0
                ],
                "title": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System"
                },
                "summary": "Furniture decoration is an important task in various industrial applications.\nHowever, achieving a high-quality decorative result is often time-consuming and\nrequires specialized artistic expertise. To tackle these challenges, we explore\nhow multi-agent systems can assist in automating the decoration process. We\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\nSpecifically, given a human prompt and a household furniture item such as a\nworking desk or a TV stand, our system suggests relevant assets with\nappropriate styles and materials, and arranges them on the item, ensuring the\ndecorative result meets functionality, aesthetic, and ambiance preferences.\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\nfulfilling distinct roles in a typical decoration project. These agents\ncollaborate through communication, logical reasoning, and validation to\ntransform the requirements into the final outcome. Extensive experiments\ndemonstrate that our FurniMAS significantly outperforms other baselines in\ngenerating high-quality 3D decor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Furniture decoration is an important task in various industrial applications.\nHowever, achieving a high-quality decorative result is often time-consuming and\nrequires specialized artistic expertise. To tackle these challenges, we explore\nhow multi-agent systems can assist in automating the decoration process. We\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\nSpecifically, given a human prompt and a household furniture item such as a\nworking desk or a TV stand, our system suggests relevant assets with\nappropriate styles and materials, and arranges them on the item, ensuring the\ndecorative result meets functionality, aesthetic, and ambiance preferences.\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\nfulfilling distinct roles in a typical decoration project. These agents\ncollaborate through communication, logical reasoning, and validation to\ntransform the requirements into the final outcome. Extensive experiments\ndemonstrate that our FurniMAS significantly outperforms other baselines in\ngenerating high-quality 3D decor."
                },
                "authors": [
                    {
                        "name": "Toan Nguyen"
                    },
                    {
                        "name": "Tri Le"
                    },
                    {
                        "name": "Quang Nguyen"
                    },
                    {
                        "name": "Anh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen"
                },
                "author": "Anh Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04766v1",
                "updated": "2025-07-07T08:43:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    43,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:43:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    43,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via\n  High-Difficulty and Dynamic Physics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via\n  High-Difficulty and Dynamic Physics Problems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in domains\nsuch as mathematics and programming, yet their capabilities in physics remain\nunderexplored and poorly understood. Physics poses unique challenges that\ndemand not only precise computation but also deep conceptual understanding and\nphysical modeling skills. Existing benchmarks often fall short due to limited\ndifficulty, multiple-choice formats, and static evaluation settings that fail\nto capture physical modeling ability. In this paper, we introduce\nABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'\nphysical reasoning and generalization capabilities. ABench-Physics consists of\ntwo components: Phy_A, a static set of 400 graduate- or Olympiad-level\nproblems; and Phy_B, a dynamic subset of 100 problems equipped with an\nautomatic variation engine to test model robustness across changing conditions.\nAll questions require precise numerical answers, with strict formatting and\ntolerance constraints. Our evaluation of several state-of-the-art LLMs reveals\nsubstantial performance gaps, highlighting persistent limitations in physical\nreasoning, especially in generalization to dynamic variants. ABench-Physics\nprovides a challenging and diagnostic framework for advancing scientific\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in domains\nsuch as mathematics and programming, yet their capabilities in physics remain\nunderexplored and poorly understood. Physics poses unique challenges that\ndemand not only precise computation but also deep conceptual understanding and\nphysical modeling skills. Existing benchmarks often fall short due to limited\ndifficulty, multiple-choice formats, and static evaluation settings that fail\nto capture physical modeling ability. In this paper, we introduce\nABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'\nphysical reasoning and generalization capabilities. ABench-Physics consists of\ntwo components: Phy_A, a static set of 400 graduate- or Olympiad-level\nproblems; and Phy_B, a dynamic subset of 100 problems equipped with an\nautomatic variation engine to test model robustness across changing conditions.\nAll questions require precise numerical answers, with strict formatting and\ntolerance constraints. Our evaluation of several state-of-the-art LLMs reveals\nsubstantial performance gaps, highlighting persistent limitations in physical\nreasoning, especially in generalization to dynamic variants. ABench-Physics\nprovides a challenging and diagnostic framework for advancing scientific\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yingfan Ma"
                    },
                    {
                        "name": "Yanmei Gu"
                    },
                    {
                        "name": "Zhengkai Yang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05258v1",
                "updated": "2025-07-07T17:59:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    55,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:55Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    55,
                    0,
                    188,
                    0
                ],
                "title": "Spatio-Temporal LLM: Reasoning about Environments and Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-Temporal LLM: Reasoning about Environments and Actions"
                },
                "summary": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/."
                },
                "authors": [
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Zhenggang Tang"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    },
                    {
                        "name": "Alex Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alex Schwing"
                },
                "author": "Alex Schwing",
                "arxiv_comment": "Code and data are available at\n  https://zoezheng126.github.io/STLLM-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05257v1",
                "updated": "2025-07-07T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions"
                },
                "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents."
                },
                "authors": [
                    {
                        "name": "Yuanzhe Hu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "23 Pages, Y. Hu and Y. Wang contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05255v1",
                "updated": "2025-07-07T17:59:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    3,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:59:03Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    59,
                    3,
                    0,
                    188,
                    0
                ],
                "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning"
                },
                "summary": "The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners."
                },
                "authors": [
                    {
                        "name": "Yana Wei"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08468v2",
                "updated": "2025-07-07T17:58:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    58,
                    15,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-13T11:50:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    50,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?"
                },
                "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist."
                },
                "authors": [
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Mohammed Saidul Islam"
                    },
                    {
                        "name": "Ridwan Mahbub"
                    },
                    {
                        "name": "Ahmed Masry"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Amran Bhuiyan"
                    },
                    {
                        "name": "Mir Tafseer Nayeem"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Enamul Hoque"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Accepted at ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05248v1",
                "updated": "2025-07-07T17:56:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    56,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:56:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    56,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models"
                },
                "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack."
                },
                "authors": [
                    {
                        "name": "Ziqi Miao"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Yuan Xiong"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Pengyu Zhu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "21 pages, 9 figures. Code and data available at\n  https://github.com/Dtc7w3PQ/Response-Attack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05606v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05606v3",
                "updated": "2025-07-07T17:44:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    44,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-05T21:37:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    37,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation"
                },
                "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Amirali Amini"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Yakov Bart"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Upol Ehsan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Lydia Chilton"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05606v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05606v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07191v2",
                "updated": "2025-07-07T17:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    42,
                    19,
                    0,
                    188,
                    0
                ],
                "published": "2024-11-11T18:05:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "The Super Weight in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Weight in Large Language Models"
                },
                "summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengxia Yu"
                    },
                    {
                        "name": "De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Colorado J Reed"
                    },
                    {
                        "name": "Alvin Wan"
                    }
                ],
                "author_detail": {
                    "name": "Alvin Wan"
                },
                "author": "Alvin Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18071v2",
                "updated": "2025-07-07T17:38:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    38,
                    20,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-23T16:16:46Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    16,
                    46,
                    4,
                    143,
                    0
                ],
                "title": "Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals"
                },
                "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning-the ability to derive\ngeneral rules from incomplete evidence, remains underexplored. This paper\ninvestigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose AlignXplore, a model that leverages extended reasoning chains to enable\nsystematic preference inference from behavioral signals in users' interaction\nhistories. Such explicit preference articulation enables efficient streaming\ninference: when new behavioral signals emerge, the model can directly build\nupon previously inferred preference descriptions rather than reprocessing\nhistorical signals from scratch, while also supporting iterative refinement to\nthe inferred preferences. We develop AlignXplore by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that AlignXplore achieves\nsubstantial improvements over the backbone model by an average of 15.49\\% on\nin-domain and out-of-domain benchmarks, while maintaining strong generalization\nability across different input formats and downstream models. Further analyses\nestablish best practices for preference inference learning through systematic\ncomparison of reward modeling strategies, while revealing the emergence of\nhuman-like inductive reasoning patterns during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning-the ability to derive\ngeneral rules from incomplete evidence, remains underexplored. This paper\ninvestigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose AlignXplore, a model that leverages extended reasoning chains to enable\nsystematic preference inference from behavioral signals in users' interaction\nhistories. Such explicit preference articulation enables efficient streaming\ninference: when new behavioral signals emerge, the model can directly build\nupon previously inferred preference descriptions rather than reprocessing\nhistorical signals from scratch, while also supporting iterative refinement to\nthe inferred preferences. We develop AlignXplore by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that AlignXplore achieves\nsubstantial improvements over the backbone model by an average of 15.49\\% on\nin-domain and out-of-domain benchmarks, while maintaining strong generalization\nability across different input formats and downstream models. Further analyses\nestablish best practices for preference inference learning through systematic\ncomparison of reward modeling strategies, while revealing the emergence of\nhuman-like inductive reasoning patterns during training."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05228v1",
                "updated": "2025-07-07T17:37:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    37,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:37:16Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    37,
                    16,
                    0,
                    188,
                    0
                ],
                "title": "Cascade: Token-Sharded Private LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade: Token-Sharded Private LLM Inference"
                },
                "summary": "As LLMs continue to increase in parameter size, the computational resources\nrequired to run them are available to fewer parties. Therefore, third-party\ninference services -- where LLMs are hosted by third parties with significant\ncomputational resources -- are becoming increasingly popular. However, third\nparty inference raises critical concerns about user data privacy. To mitigate\nthese risks, privacy researchers have developed provably secure schemes for\nthird-party inference, such as Secure Multi-Party Computation (SMPC). However,\nSMPC protocols have significant computational and communication overhead, and\ndo not scale to large models. In this work, we propose a new multi-party\ninference protocol, Cascade, that avoids these punitive costs by leveraging\nsharding in the sequence dimension to maintain privacy, trading off\ncryptographic privacy guarantees for increased performance and scalability. We\ndemonstrate that Cascade is resistant to a generalization of a recent attack\nthat is highly effective against other statistical privacy schemes, and that it\nis further resistant to learning-based attacks. As Cascade is orders of\nmagnitude faster than existing schemes, our findings offer practical solutions\nfor secure deployment of modern state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs continue to increase in parameter size, the computational resources\nrequired to run them are available to fewer parties. Therefore, third-party\ninference services -- where LLMs are hosted by third parties with significant\ncomputational resources -- are becoming increasingly popular. However, third\nparty inference raises critical concerns about user data privacy. To mitigate\nthese risks, privacy researchers have developed provably secure schemes for\nthird-party inference, such as Secure Multi-Party Computation (SMPC). However,\nSMPC protocols have significant computational and communication overhead, and\ndo not scale to large models. In this work, we propose a new multi-party\ninference protocol, Cascade, that avoids these punitive costs by leveraging\nsharding in the sequence dimension to maintain privacy, trading off\ncryptographic privacy guarantees for increased performance and scalability. We\ndemonstrate that Cascade is resistant to a generalization of a recent attack\nthat is highly effective against other statistical privacy schemes, and that it\nis further resistant to learning-based attacks. As Cascade is orders of\nmagnitude faster than existing schemes, our findings offer practical solutions\nfor secure deployment of modern state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Rahul Thomas"
                    },
                    {
                        "name": "Louai Zahran"
                    },
                    {
                        "name": "Erica Choi"
                    },
                    {
                        "name": "Akilesh Potti"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Arka Pal"
                    }
                ],
                "author_detail": {
                    "name": "Arka Pal"
                },
                "author": "Arka Pal",
                "arxiv_comment": "To be published in ICML 2025 Main Proceedings as \"Hidden No More:\n  Attacking and Defending Private Third-Party LLM Inference\", together with\n  arXiv:2505.18332",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v2",
                "updated": "2025-07-07T17:32:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    32,
                    51,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05216v1",
                "updated": "2025-07-07T17:29:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    29,
                    13,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:29:13Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    29,
                    13,
                    0,
                    188,
                    0
                ],
                "title": "Bridging Prediction and Intervention Problems in Social Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Prediction and Intervention Problems in Social Systems"
                },
                "summary": "Many automated decision systems (ADS) are designed to solve prediction\nproblems -- where the goal is to learn patterns from a sample of the population\nand apply them to individuals from the same population. In reality, these\nprediction systems operationalize holistic policy interventions in deployment.\nOnce deployed, ADS can shape impacted population outcomes through an effective\npolicy change in how decision-makers operate, while also being defined by past\nand present interactions between stakeholders and the limitations of existing\norganizational, as well as societal, infrastructure and context. In this work,\nwe consider the ways in which we must shift from a prediction-focused paradigm\nto an interventionist paradigm when considering the impact of ADS within social\nsystems. We argue this requires a new default problem setup for ADS beyond\nprediction, to instead consider predictions as decision support, final\ndecisions, and outcomes. We highlight how this perspective unifies modern\nstatistical frameworks and other tools to study the design, implementation, and\nevaluation of ADS systems, and point to the research directions necessary to\noperationalize this paradigm shift. Using these tools, we characterize the\nlimitations of focusing on isolated prediction tasks, and lay the foundation\nfor a more intervention-oriented approach to developing and deploying ADS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many automated decision systems (ADS) are designed to solve prediction\nproblems -- where the goal is to learn patterns from a sample of the population\nand apply them to individuals from the same population. In reality, these\nprediction systems operationalize holistic policy interventions in deployment.\nOnce deployed, ADS can shape impacted population outcomes through an effective\npolicy change in how decision-makers operate, while also being defined by past\nand present interactions between stakeholders and the limitations of existing\norganizational, as well as societal, infrastructure and context. In this work,\nwe consider the ways in which we must shift from a prediction-focused paradigm\nto an interventionist paradigm when considering the impact of ADS within social\nsystems. We argue this requires a new default problem setup for ADS beyond\nprediction, to instead consider predictions as decision support, final\ndecisions, and outcomes. We highlight how this perspective unifies modern\nstatistical frameworks and other tools to study the design, implementation, and\nevaluation of ADS systems, and point to the research directions necessary to\noperationalize this paradigm shift. Using these tools, we characterize the\nlimitations of focusing on isolated prediction tasks, and lay the foundation\nfor a more intervention-oriented approach to developing and deploying ADS."
                },
                "authors": [
                    {
                        "name": "Lydia T. Liu"
                    },
                    {
                        "name": "Inioluwa Deborah Raji"
                    },
                    {
                        "name": "Angela Zhou"
                    },
                    {
                        "name": "Luke Guerdan"
                    },
                    {
                        "name": "Jessica Hullman"
                    },
                    {
                        "name": "Daniel Malinsky"
                    },
                    {
                        "name": "Bryan Wilder"
                    },
                    {
                        "name": "Simone Zhang"
                    },
                    {
                        "name": "Hammaad Adam"
                    },
                    {
                        "name": "Amanda Coston"
                    },
                    {
                        "name": "Ben Laufer"
                    },
                    {
                        "name": "Ezinne Nwankwo"
                    },
                    {
                        "name": "Michael Zanger-Tishler"
                    },
                    {
                        "name": "Eli Ben-Michael"
                    },
                    {
                        "name": "Solon Barocas"
                    },
                    {
                        "name": "Avi Feller"
                    },
                    {
                        "name": "Marissa Gerchick"
                    },
                    {
                        "name": "Talia Gillis"
                    },
                    {
                        "name": "Shion Guha"
                    },
                    {
                        "name": "Daniel Ho"
                    },
                    {
                        "name": "Lily Hu"
                    },
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Joshua Loftus"
                    },
                    {
                        "name": "Razieh Nabi"
                    },
                    {
                        "name": "Arvind Narayanan"
                    },
                    {
                        "name": "Ben Recht"
                    },
                    {
                        "name": "Juan Carlos Perdomo"
                    },
                    {
                        "name": "Matthew Salganik"
                    },
                    {
                        "name": "Mark Sendak"
                    },
                    {
                        "name": "Alexander Tolbert"
                    },
                    {
                        "name": "Berk Ustun"
                    },
                    {
                        "name": "Suresh Venkatasubramanian"
                    },
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Ashia Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Ashia Wilson"
                },
                "author": "Ashia Wilson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23824v2",
                "updated": "2025-07-07T17:28:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    28,
                    31,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-28T06:14:30Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    6,
                    14,
                    30,
                    2,
                    148,
                    0
                ],
                "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:\n  Baseline Approaches and Automatic Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs:\n  Baseline Approaches and Automatic Evaluation"
                },
                "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to aid the peer review process of scientific publication amid\nthe peer review crisis. However, having AI models generate full reviews in the\nsame way as human reviewers risks exacerbating the irresponsible use of\nLLM-generated reviews. As an alternative, we propose adopting LLMs as\nmanuscript quality checkers. We introduce several baseline approaches and an\nextendable automatic evaluation framework using top reasoning LLMs as judges to\ntackle the difficulty of recruiting domain experts for manual evaluation.\nUtilizing papers withdrawn from arXiv, we validated our proposed methods with\nseveral leading reasoning LLMs from multiple vendors and assessed their\nperformance and API costs for identifying critical errors and unsoundness\nproblems in scientific papers. o3 exhibited the best problem identification\nperformance among all models at a modest cost. This paper provides insights\ninto document-based scientific understanding/reasoning and lays a foundation\nfor future applications. Our dataset, code, and model outputs are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have sparked interest in\nutilizing them to aid the peer review process of scientific publication amid\nthe peer review crisis. However, having AI models generate full reviews in the\nsame way as human reviewers risks exacerbating the irresponsible use of\nLLM-generated reviews. As an alternative, we propose adopting LLMs as\nmanuscript quality checkers. We introduce several baseline approaches and an\nextendable automatic evaluation framework using top reasoning LLMs as judges to\ntackle the difficulty of recruiting domain experts for manual evaluation.\nUtilizing papers withdrawn from arXiv, we validated our proposed methods with\nseveral leading reasoning LLMs from multiple vendors and assessed their\nperformance and API costs for identifying critical errors and unsoundness\nproblems in scientific papers. o3 exhibited the best problem identification\nperformance among all models at a modest cost. This paper provides insights\ninto document-based scientific understanding/reasoning and lays a foundation\nfor future applications. Our dataset, code, and model outputs are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Tianmai M. Zhang"
                    },
                    {
                        "name": "Neil F. Abernethy"
                    }
                ],
                "author_detail": {
                    "name": "Neil F. Abernethy"
                },
                "author": "Neil F. Abernethy",
                "arxiv_comment": "Add results from new experiments; update discussion and GitHub link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05211v1",
                "updated": "2025-07-07T17:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    22,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation"
                },
                "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg."
                },
                "authors": [
                    {
                        "name": "Zongyan Han"
                    },
                    {
                        "name": "Mohamed El Amine Boudjoghra"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05201v1",
                "updated": "2025-07-07T17:01:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:01:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "MedGemma Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedGemma Technical Report"
                },
                "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma."
                },
                "authors": [
                    {
                        "name": "Andrew Sellergren"
                    },
                    {
                        "name": "Sahar Kazemzadeh"
                    },
                    {
                        "name": "Tiam Jaroensri"
                    },
                    {
                        "name": "Atilla Kiraly"
                    },
                    {
                        "name": "Madeleine Traverse"
                    },
                    {
                        "name": "Timo Kohlberger"
                    },
                    {
                        "name": "Shawn Xu"
                    },
                    {
                        "name": "Fayaz Jamil"
                    },
                    {
                        "name": "Cían Hughes"
                    },
                    {
                        "name": "Charles Lau"
                    },
                    {
                        "name": "Justin Chen"
                    },
                    {
                        "name": "Fereshteh Mahvar"
                    },
                    {
                        "name": "Liron Yatziv"
                    },
                    {
                        "name": "Tiffany Chen"
                    },
                    {
                        "name": "Bram Sterling"
                    },
                    {
                        "name": "Stefanie Anna Baby"
                    },
                    {
                        "name": "Susanna Maria Baby"
                    },
                    {
                        "name": "Jeremy Lai"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Lu Yang"
                    },
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Per Bjornsson"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Ryan Brush"
                    },
                    {
                        "name": "Kenneth Philbrick"
                    },
                    {
                        "name": "Howard Hu"
                    },
                    {
                        "name": "Howard Yang"
                    },
                    {
                        "name": "Richa Tiwari"
                    },
                    {
                        "name": "Sunny Jansen"
                    },
                    {
                        "name": "Preeti Singh"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Shekoofeh Azizi"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Riviere"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Katherine Chou"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Kavi Goel"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Jonathon Shlens"
                    },
                    {
                        "name": "David Fleet"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Gus Martins"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Shravya Shetty"
                    },
                    {
                        "name": "David F. Steiner"
                    },
                    {
                        "name": "Can Kirmizibayrak"
                    },
                    {
                        "name": "Rory Pilgrim"
                    },
                    {
                        "name": "Daniel Golden"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "arxiv_affiliation": "Dima",
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05200v1",
                "updated": "2025-07-07T17:01:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    17,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:01:17Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    17,
                    0,
                    188,
                    0
                ],
                "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code"
                },
                "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation."
                },
                "authors": [
                    {
                        "name": "Susmita Das"
                    },
                    {
                        "name": "Madhusudan Ghosh"
                    },
                    {
                        "name": "Priyanka Swami"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Gul Calikli"
                    }
                ],
                "author_detail": {
                    "name": "Gul Calikli"
                },
                "author": "Gul Calikli",
                "arxiv_doi": "10.1145/3726302.3730212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05195v1",
                "updated": "2025-07-07T16:54:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    54,
                    18,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:54:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    54,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "Train-before-Test Harmonizes Language Model Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-before-Test Harmonizes Language Model Rankings"
                },
                "summary": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking."
                },
                "authors": [
                    {
                        "name": "Guanhua Zhang"
                    },
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Moritz Hardt"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Hardt"
                },
                "author": "Moritz Hardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05995v2",
                "updated": "2025-07-07T16:43:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    43,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-08T13:01:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose the\nNativQA framework, which can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages -- ranging from extremely\nlow-resource to high-resource languages -- resulting in over 300K\nQuestion-Answer (QA) pairs. The developed resources can be used for LLM\nbenchmarking and further fine-tuning. The framework has been made publicly\navailable for the community (https://gitlab.com/nativqa/nativqa-framework).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose the\nNativQA framework, which can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages -- ranging from extremely\nlow-resource to high-resource languages -- resulting in over 300K\nQuestion-Answer (QA) pairs. The developed resources can be used for LLM\nbenchmarking and further fine-tuning. The framework has been made publicly\navailable for the community (https://gitlab.com/nativqa/nativqa-framework)."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Sahinur Rahman Laskar"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shammur Absar Chowdhury"
                },
                "author": "Shammur Absar Chowdhury",
                "arxiv_comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05179v1",
                "updated": "2025-07-07T16:34:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    34,
                    28,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:34:28Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    34,
                    28,
                    0,
                    188,
                    0
                ],
                "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations"
                },
                "summary": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages."
                },
                "authors": [
                    {
                        "name": "Pulkit Bansal"
                    },
                    {
                        "name": "Raghvendra Kumar"
                    },
                    {
                        "name": "Shakti Singh"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05178v1",
                "updated": "2025-07-07T16:33:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    33,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:33:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    33,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale"
                },
                "summary": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain."
                },
                "authors": [
                    {
                        "name": "Jonathan Hyun"
                    },
                    {
                        "name": "Nicholas R Waytowich"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at:\n  http://generalroboticslab.com/CREW-Wildfire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20090v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20090v4",
                "updated": "2025-07-07T16:28:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    28,
                    58,
                    0,
                    188,
                    0
                ],
                "published": "2024-05-30T14:27:20Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    14,
                    27,
                    20,
                    3,
                    151,
                    0
                ],
                "title": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection."
                },
                "authors": [
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Jiayan Yang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Yichi Wang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20090v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20090v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18689v2",
                "updated": "2025-07-07T16:28:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    28,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-23T14:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments"
                },
                "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions."
                },
                "authors": [
                    {
                        "name": "Alessandro Saviolo"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05162v1",
                "updated": "2025-07-07T16:18:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    18,
                    19,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:18:19Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    18,
                    19,
                    0,
                    188,
                    0
                ],
                "title": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral\n  Domains"
                },
                "summary": "The recent proliferation of photorealistic AI-generated images (AIGI) has\nraised urgent concerns about their potential misuse, particularly on social\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\non large, deep neural architectures, creating significant computational\nbarriers to real-time, large-scale deployment on platforms like social media.\nTo challenge this reliance on computationally intensive models, we introduce\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\nthe detection performance and efficiency of off-the-shelf lightweight neural\nnetworks. In this framework, we comprehensively train and evaluate selected\nmodels on a representative subset of the GenImage dataset across spatial,\nspectral, and fusion image domains. Our results demonstrate that lightweight\nmodels can achieve competitive accuracy, even under adversarial conditions,\nwhile incurring substantially lower memory and computation costs compared to\ncurrent state-of-the-art methods. This study offers valuable insight into the\ntrade-off between efficiency and performance in AIGI detection and lays a\nfoundation for the development of practical, scalable, and trustworthy\ndetection systems. The source code of LAID can be found at:\nhttps://github.com/nchivar/LAID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent proliferation of photorealistic AI-generated images (AIGI) has\nraised urgent concerns about their potential misuse, particularly on social\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\non large, deep neural architectures, creating significant computational\nbarriers to real-time, large-scale deployment on platforms like social media.\nTo challenge this reliance on computationally intensive models, we introduce\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\nthe detection performance and efficiency of off-the-shelf lightweight neural\nnetworks. In this framework, we comprehensively train and evaluate selected\nmodels on a representative subset of the GenImage dataset across spatial,\nspectral, and fusion image domains. Our results demonstrate that lightweight\nmodels can achieve competitive accuracy, even under adversarial conditions,\nwhile incurring substantially lower memory and computation costs compared to\ncurrent state-of-the-art methods. This study offers valuable insight into the\ntrade-off between efficiency and performance in AIGI detection and lays a\nfoundation for the development of practical, scalable, and trustworthy\ndetection systems. The source code of LAID can be found at:\nhttps://github.com/nchivar/LAID."
                },
                "authors": [
                    {
                        "name": "Nicholas Chivaran"
                    },
                    {
                        "name": "Jianbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Ni"
                },
                "author": "Jianbing Ni",
                "arxiv_comment": "To appear in the proceedings of PST2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05157v1",
                "updated": "2025-07-07T16:13:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    13,
                    13,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T16:13:13Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    13,
                    13,
                    0,
                    188,
                    0
                ],
                "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models"
                },
                "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B."
                },
                "authors": [
                    {
                        "name": "Chinnappa Guggilla"
                    },
                    {
                        "name": "Budhaditya Roy"
                    },
                    {
                        "name": "Trupti Ramdas Chavan"
                    },
                    {
                        "name": "Abdul Rahman"
                    },
                    {
                        "name": "Edward Bowen"
                    }
                ],
                "author_detail": {
                    "name": "Edward Bowen"
                },
                "author": "Edward Bowen",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05137v1",
                "updated": "2025-07-07T15:49:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:49:23Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization"
                },
                "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05129v1",
                "updated": "2025-07-07T15:41:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:41:38Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    41,
                    38,
                    0,
                    188,
                    0
                ],
                "title": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction"
                },
                "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment."
                },
                "authors": [
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Nigel Fernandez"
                    },
                    {
                        "name": "Christopher Ormerod"
                    },
                    {
                        "name": "Susan Lottridge"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13764v2",
                "updated": "2025-07-07T15:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    37,
                    19,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-19T22:34:05Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    22,
                    34,
                    5,
                    0,
                    139,
                    0
                ],
                "title": "Incremental Firmware Update Over-the-Air for Low-Power IoT Devices over\n  LoRaWAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental Firmware Update Over-the-Air for Low-Power IoT Devices over\n  LoRaWAN"
                },
                "summary": "Efficiently supporting remote firmware updates in Internet of Things (IoT)\ndevices remains a significant challenge due to the limitations of many IoT\ncommunication protocols, which often make it impractical to transmit full\nfirmware images. Techniques such as firmware partitioning have been introduced\nto mitigate this issue, but they frequently fall short, especially in\nbattery-powered systems where time and energy constraints are critical. As a\nresult, physical maintenance interventions are still commonly required, which\nis costly and inconvenient in large-scale deployments. In this work, we present\na lightweight and innovative method that addresses this challenge by generating\nhighly compact delta patches, enabling firmware reconstruction directly on the\ndevice. Our algorithm is specifically optimized for low-power devices,\nminimizing both memory usage and computational overhead. Compared to existing\nsolutions, our approach significantly reduces the data volume needed for\nupdates while maintaining performance comparable to more complex alternatives.\nExperimental evaluations confirm that our method yields substantial time and\nenergy savings, making it particularly well-suited for battery-powered IoT\nnodes. Although our implementation targets the LoRaWAN protocol, the approach\nis flexible and can be adapted to other IoT communication technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently supporting remote firmware updates in Internet of Things (IoT)\ndevices remains a significant challenge due to the limitations of many IoT\ncommunication protocols, which often make it impractical to transmit full\nfirmware images. Techniques such as firmware partitioning have been introduced\nto mitigate this issue, but they frequently fall short, especially in\nbattery-powered systems where time and energy constraints are critical. As a\nresult, physical maintenance interventions are still commonly required, which\nis costly and inconvenient in large-scale deployments. In this work, we present\na lightweight and innovative method that addresses this challenge by generating\nhighly compact delta patches, enabling firmware reconstruction directly on the\ndevice. Our algorithm is specifically optimized for low-power devices,\nminimizing both memory usage and computational overhead. Compared to existing\nsolutions, our approach significantly reduces the data volume needed for\nupdates while maintaining performance comparable to more complex alternatives.\nExperimental evaluations confirm that our method yields substantial time and\nenergy savings, making it particularly well-suited for battery-powered IoT\nnodes. Although our implementation targets the LoRaWAN protocol, the approach\nis flexible and can be adapted to other IoT communication technologies."
                },
                "authors": [
                    {
                        "name": "Andrea De Simone"
                    },
                    {
                        "name": "Giovanna Turvani"
                    },
                    {
                        "name": "Fabrizio Riente"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Riente"
                },
                "author": "Fabrizio Riente",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05123v1",
                "updated": "2025-07-07T15:34:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    34,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:34:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    34,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques"
                },
                "summary": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems."
                },
                "authors": [
                    {
                        "name": "Walid Mohamed Aly"
                    },
                    {
                        "name": "Taysir Hassan A. Soliman"
                    },
                    {
                        "name": "Amr Mohamed AbdelAziz"
                    }
                ],
                "author_detail": {
                    "name": "Amr Mohamed AbdelAziz"
                },
                "author": "Amr Mohamed AbdelAziz",
                "arxiv_comment": "This manuscript is an extended version of the work accepted for\n  publication in the International Journal of Advanced Computer Science and\n  Applications (IJACSA), Volume 16, Issue 6, June 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05118v1",
                "updated": "2025-07-07T15:31:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    31,
                    36,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:31:36Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    31,
                    36,
                    0,
                    188,
                    0
                ],
                "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots"
                },
                "summary": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io."
                },
                "authors": [
                    {
                        "name": "Danil S. Grigorev"
                    },
                    {
                        "name": "Alexey K. Kovalev"
                    },
                    {
                        "name": "Aleksandr I. Panov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr I. Panov"
                },
                "author": "Aleksandr I. Panov",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05110v2",
                "updated": "2025-07-08T03:40:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    3,
                    40,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T15:27:48Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    27,
                    48,
                    0,
                    188,
                    0
                ],
                "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution\n  Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution\n  Shift"
                },
                "summary": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Yue He"
                    },
                    {
                        "name": "Yunfei Wang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Haoxiang Cheng"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Peng Cui"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05093v1",
                "updated": "2025-07-07T15:13:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    13,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:13:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    13,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders"
                },
                "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations."
                },
                "authors": [
                    {
                        "name": "Alberto Castagnaro"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Simeone Pizzi"
                    }
                ],
                "author_detail": {
                    "name": "Simeone Pizzi"
                },
                "author": "Simeone Pizzi",
                "arxiv_comment": "currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05081v1",
                "updated": "2025-07-07T15:06:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    6,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T15:06:16Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    6,
                    16,
                    0,
                    188,
                    0
                ],
                "title": "ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration\n  Energy Harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration\n  Energy Harvesting"
                },
                "summary": "Vibration energy harvesting is a promising solution for powering battery-free\nIoT systems; however, the instability of ambient vibrations presents\nsignificant challenges, such as limited harvested energy, intermittent power\nsupply, and poor adaptability to various applications. To address these\nchallenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT\nplatform that supports multiple vibration energy harvesters (piezoelectric,\nelectromagnetic, and triboelectric) and accommodates sensing tasks with varying\napplication requirements through standardized hot-swappable interfaces.\nViPSN~2.0 incorporates an energy-indication power management framework tailored\nto various application demands, including light-duty discrete sampling,\nheavy-duty high-power sensing, and complex-duty streaming tasks, thereby\neffectively managing fluctuating energy availability. The platform's\nversatility and robustness are validated through three representative\napplications: ViPSN-Beacon, enabling ultra-low-power wireless beacon\ntransmission from a single transient fingertip press; ViPSN-LoRa, supporting\nhigh-power, long-range wireless communication powered by wave vibrations in\nactual marine environments; and ViPSN-Cam, enabling intermittent image capture\nand wireless transfer. Experimental results demonstrate that ViPSN~2.0 can\nreliably meet a wide range of requirements in practical battery-free IoT\ndeployments under energy-constrained conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibration energy harvesting is a promising solution for powering battery-free\nIoT systems; however, the instability of ambient vibrations presents\nsignificant challenges, such as limited harvested energy, intermittent power\nsupply, and poor adaptability to various applications. To address these\nchallenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT\nplatform that supports multiple vibration energy harvesters (piezoelectric,\nelectromagnetic, and triboelectric) and accommodates sensing tasks with varying\napplication requirements through standardized hot-swappable interfaces.\nViPSN~2.0 incorporates an energy-indication power management framework tailored\nto various application demands, including light-duty discrete sampling,\nheavy-duty high-power sensing, and complex-duty streaming tasks, thereby\neffectively managing fluctuating energy availability. The platform's\nversatility and robustness are validated through three representative\napplications: ViPSN-Beacon, enabling ultra-low-power wireless beacon\ntransmission from a single transient fingertip press; ViPSN-LoRa, supporting\nhigh-power, long-range wireless communication powered by wave vibrations in\nactual marine environments; and ViPSN-Cam, enabling intermittent image capture\nand wireless transfer. Experimental results demonstrate that ViPSN~2.0 can\nreliably meet a wide range of requirements in practical battery-free IoT\ndeployments under energy-constrained conditions."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Mianxin Xiao"
                    },
                    {
                        "name": "Xi Shen"
                    },
                    {
                        "name": "Jiaqing Chu"
                    },
                    {
                        "name": "Weifeng Huang"
                    },
                    {
                        "name": "Jiashun Li"
                    },
                    {
                        "name": "Yaoyi Li"
                    },
                    {
                        "name": "Mingjing Cai"
                    },
                    {
                        "name": "Jiaming Chen"
                    },
                    {
                        "name": "Xinming Zhang"
                    },
                    {
                        "name": "Daxing Zhang"
                    },
                    {
                        "name": "Congsi Wang"
                    },
                    {
                        "name": "Hong Tang"
                    },
                    {
                        "name": "Bao Zhao"
                    },
                    {
                        "name": "Qitao Lu"
                    },
                    {
                        "name": "Yilong Wang"
                    },
                    {
                        "name": "Jianjun Wang"
                    },
                    {
                        "name": "Minyi Xu"
                    },
                    {
                        "name": "Shitong Fang"
                    },
                    {
                        "name": "Xuanyu Huang. Chaoyang Zhao"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Yaowen Yang"
                    },
                    {
                        "name": "Guobiao Hu"
                    },
                    {
                        "name": "Junrui Liang"
                    },
                    {
                        "name": "Wei-Hsin Liao"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Hsin Liao"
                },
                "author": "Wei-Hsin Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05068v1",
                "updated": "2025-07-07T14:50:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    50,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:50:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    50,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models"
                },
                "summary": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA."
                },
                "authors": [
                    {
                        "name": "Hongyao Yu"
                    },
                    {
                        "name": "Yixiang Qiu"
                    },
                    {
                        "name": "Yiheng Yang"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Tianqu Zhuang"
                    },
                    {
                        "name": "Jiaxin Hong"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Tao Xia"
                },
                "author": "Shu-Tao Xia",
                "arxiv_comment": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05046v1",
                "updated": "2025-07-07T14:29:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    29,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:29:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    29,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students"
                },
                "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings."
                },
                "authors": [
                    {
                        "name": "Kadija Bouyzourn"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "25 pages, 11 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05043v1",
                "updated": "2025-07-07T14:27:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T14:27:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLink: Distributed and Efficient Serving Framework for Large Models"
                },
                "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models."
                },
                "authors": [
                    {
                        "name": "Lewei Jin"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Kui Zhang"
                    },
                    {
                        "name": "Yifan Zhuo"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Bowei Yang"
                    },
                    {
                        "name": "Zhengong Cai"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19794v2",
                "updated": "2025-07-07T14:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study"
                },
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00946v3",
                "updated": "2025-07-07T13:57:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    57,
                    37,
                    0,
                    188,
                    0
                ],
                "published": "2024-09-02T05:09:46Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    9,
                    46,
                    0,
                    246,
                    0
                ],
                "title": "A Framework for Synthetic Audio Conversations Generation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Synthetic Audio Conversations Generation using Large\n  Language Models"
                },
                "summary": "In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems."
                },
                "authors": [
                    {
                        "name": "Kaung Myat Kyaw"
                    },
                    {
                        "name": "Jonathan Hoyin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Hoyin Chan"
                },
                "author": "Jonathan Hoyin Chan",
                "arxiv_doi": "10.1109/WI-IAT62293.2024.00056",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WI-IAT62293.2024.00056",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.00946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted at the WI-IAT'24. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media",
                "arxiv_journal_ref": "IEEE/WIC International Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT), 2024, pp. 355--359",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07783v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07783v4",
                "updated": "2025-07-07T13:56:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    56,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-12T17:36:14Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    36,
                    14,
                    0,
                    132,
                    0
                ],
                "title": "Relative Overfitting and Accept-Reject Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Overfitting and Accept-Reject Framework"
                },
                "summary": "The scaling of Large Language Models (LLMs) currently faces significant\nchallenges. Model assembly is widely considered a promising solution to break\nthrough these performance bottlenecks. However, current ensembling methods are\nprimarily guided by the statistical expectation that combining multiple models\nover large samples will lead to performance gains. We propose an ensemble\nframework that transitions from such stochastic, sample-dependent methods to a\nregular, controllable approach based on fine-grained model segmentation. This\nregularity governs how models are segmented to ensure performance improvement,\nhow the magnitude of this improvement varies with model selection, and what\nfactors determine its theoretical maximum. To formalize this pattern, we\nintroduce the concept of'relative overfitting,' which is derived from the\nperformance discrepancies between constituent models and builds a bridge\nbetween ensemble outcomes and the inherent attributes of these models. We\ndetail the patterns of this framework within the domain of NLP and briefly\ndescribe its extensibility to other fields, such as computer vision (CV) and AI\nfor science. Our approach was validated using both custom-built and pre-trained\nmainstream models across diverse benchmarks, including language modeling,\nlong-context tasks, and question-answering (QA). The results indicate that the\nensemble rules we proposed are generally effective and that we provide a\nrigorous proof of these rules in certain experimental scenarios. The proposed\nframework offers a new perspective for understanding ensemble theory and\nprovides a systematic approach to addressing the performance bottlenecks of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of Large Language Models (LLMs) currently faces significant\nchallenges. Model assembly is widely considered a promising solution to break\nthrough these performance bottlenecks. However, current ensembling methods are\nprimarily guided by the statistical expectation that combining multiple models\nover large samples will lead to performance gains. We propose an ensemble\nframework that transitions from such stochastic, sample-dependent methods to a\nregular, controllable approach based on fine-grained model segmentation. This\nregularity governs how models are segmented to ensure performance improvement,\nhow the magnitude of this improvement varies with model selection, and what\nfactors determine its theoretical maximum. To formalize this pattern, we\nintroduce the concept of'relative overfitting,' which is derived from the\nperformance discrepancies between constituent models and builds a bridge\nbetween ensemble outcomes and the inherent attributes of these models. We\ndetail the patterns of this framework within the domain of NLP and briefly\ndescribe its extensibility to other fields, such as computer vision (CV) and AI\nfor science. Our approach was validated using both custom-built and pre-trained\nmainstream models across diverse benchmarks, including language modeling,\nlong-context tasks, and question-answering (QA). The results indicate that the\nensemble rules we proposed are generally effective and that we provide a\nrigorous proof of these rules in certain experimental scenarios. The proposed\nframework offers a new perspective for understanding ensemble theory and\nprovides a systematic approach to addressing the performance bottlenecks of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yanxin Liu"
                    },
                    {
                        "name": "Yunqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunqi Zhang"
                },
                "author": "Yunqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07783v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07783v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05010v1",
                "updated": "2025-07-07T13:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    48,
                    54,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:48:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    48,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification"
                },
                "summary": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT."
                },
                "authors": [
                    {
                        "name": "Chenfei Xiong"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Lorena Calvo-Bartolomé"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04997v1",
                "updated": "2025-07-07T13:36:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    36,
                    10,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:36:10Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    36,
                    10,
                    0,
                    188,
                    0
                ],
                "title": "Exploring O-RAN Compression Techniques in Decentralized Distributed MIMO\n  Systems: Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring O-RAN Compression Techniques in Decentralized Distributed MIMO\n  Systems: Reducing Fronthaul Load"
                },
                "summary": "This paper explores the application of uplink fronthaul compression\ntechniques within Open RAN (O-RAN) to mitigate fronthaul load in decentralized\ndistributed MIMO (DD-MIMO) systems. With the ever-increasing demand for high\ndata rates and system scalability, the fronthaul load becomes a critical\nbottleneck. Our method uses O-RAN compression techniques to efficiently\ncompress the fronthaul signals. The goal is to greatly lower the fronthaul load\nwhile having little effect on the overall system performance, as shown by Block\nError Rate (BLER) curves. Through rigorous link-level simulations, we compare\nour quantization strategies against a benchmark scenario with no quantization,\nproviding insights into the trade-offs between fronthaul data rate reduction\nand link performance integrity. The results demonstrate that our proposed\nquantization techniques not only lower the fronthaul load but also maintain a\ncompetitive link quality, making them a viable solution for enhancing the\nefficiency of next-generation wireless networks. This study underscores the\npotential of quantization in O-RAN contexts to achieve optimal balance between\nsystem capacity and performance, paving the way for more scalable and robust\nDD-MIMO deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of uplink fronthaul compression\ntechniques within Open RAN (O-RAN) to mitigate fronthaul load in decentralized\ndistributed MIMO (DD-MIMO) systems. With the ever-increasing demand for high\ndata rates and system scalability, the fronthaul load becomes a critical\nbottleneck. Our method uses O-RAN compression techniques to efficiently\ncompress the fronthaul signals. The goal is to greatly lower the fronthaul load\nwhile having little effect on the overall system performance, as shown by Block\nError Rate (BLER) curves. Through rigorous link-level simulations, we compare\nour quantization strategies against a benchmark scenario with no quantization,\nproviding insights into the trade-offs between fronthaul data rate reduction\nand link performance integrity. The results demonstrate that our proposed\nquantization techniques not only lower the fronthaul load but also maintain a\ncompetitive link quality, making them a viable solution for enhancing the\nefficiency of next-generation wireless networks. This study underscores the\npotential of quantization in O-RAN contexts to achieve optimal balance between\nsystem capacity and performance, paving the way for more scalable and robust\nDD-MIMO deployments."
                },
                "authors": [
                    {
                        "name": "Mostafa Rahmani"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Vida Ranjbar"
                    },
                    {
                        "name": "Ahmed Al-Tahmeesschi"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    },
                    {
                        "name": "Sofie Pollin"
                    },
                    {
                        "name": "Alister G. Burr"
                    }
                ],
                "author_detail": {
                    "name": "Alister G. Burr"
                },
                "author": "Alister G. Burr",
                "arxiv_comment": "Accepted in IEEE PIMRC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v1",
                "updated": "2025-07-07T13:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20147v2",
                "updated": "2025-07-07T13:23:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    23,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-26T15:46:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    46,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities"
                },
                "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Aoxue Li"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Ning Kang"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "37 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10385v2",
                "updated": "2025-07-07T13:21:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    21,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2024-12-18T09:35:28Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    35,
                    28,
                    2,
                    353,
                    0
                ],
                "title": "Autonomous Microscopy Experiments through Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Microscopy Experiments through Large Language Model Agents"
                },
                "summary": "Large language models (LLMs) are revolutionizing self driving laboratories\n(SDLs) for materials research, promising unprecedented acceleration of\nscientific discovery. However, current SDL implementations rely on rigid\nprotocols that fail to capture the adaptability and intuition of expert\nscientists in dynamic experimental settings. We introduce Artificially\nIntelligent Lab Assistant (AILA), a framework automating atomic force\nmicroscopy through LLM driven agents. Further, we develop AFMBench a\ncomprehensive evaluation suite challenging AI agents across the complete\nscientific workflow from experimental design to results analysis. We find that\nstate of the art models struggle with basic tasks and coordination scenarios.\nNotably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in\nmaterials domain question answering (QA) benchmarks, revealing that domain\nspecific QA proficiency does not necessarily translate to effective agentic\ncapabilities. Additionally, we observe that LLMs can deviate from instructions,\nraising safety alignment concerns for SDL applications. Our ablations reveal\nthat multi agent frameworks outperform single-agent architectures. We also\nobserve significant prompt fragility, where slight modifications in prompt\nstructure cause substantial performance variations in capable models like GPT\n4o. Finally, we evaluate AILA's effectiveness in increasingly advanced\nexperiments AFM calibration, feature detection, mechanical property\nmeasurement, graphene layer counting, and indenter detection. Our findings\nunderscore the necessity for rigorous benchmarking protocols and prompt\nengineering strategies before deploying AI laboratory assistants in scientific\nresearch environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing self driving laboratories\n(SDLs) for materials research, promising unprecedented acceleration of\nscientific discovery. However, current SDL implementations rely on rigid\nprotocols that fail to capture the adaptability and intuition of expert\nscientists in dynamic experimental settings. We introduce Artificially\nIntelligent Lab Assistant (AILA), a framework automating atomic force\nmicroscopy through LLM driven agents. Further, we develop AFMBench a\ncomprehensive evaluation suite challenging AI agents across the complete\nscientific workflow from experimental design to results analysis. We find that\nstate of the art models struggle with basic tasks and coordination scenarios.\nNotably, Claude 3.5 sonnet performs unexpectedly poorly despite excelling in\nmaterials domain question answering (QA) benchmarks, revealing that domain\nspecific QA proficiency does not necessarily translate to effective agentic\ncapabilities. Additionally, we observe that LLMs can deviate from instructions,\nraising safety alignment concerns for SDL applications. Our ablations reveal\nthat multi agent frameworks outperform single-agent architectures. We also\nobserve significant prompt fragility, where slight modifications in prompt\nstructure cause substantial performance variations in capable models like GPT\n4o. Finally, we evaluate AILA's effectiveness in increasingly advanced\nexperiments AFM calibration, feature detection, mechanical property\nmeasurement, graphene layer counting, and indenter detection. Our findings\nunderscore the necessity for rigorous benchmarking protocols and prompt\nengineering strategies before deploying AI laboratory assistants in scientific\nresearch environments."
                },
                "authors": [
                    {
                        "name": "Indrajeet Mandal"
                    },
                    {
                        "name": "Jitendra Soni"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Morten M. Smedskjaer"
                    },
                    {
                        "name": "Katrin Wondraczek"
                    },
                    {
                        "name": "Lothar Wondraczek"
                    },
                    {
                        "name": "Nitya Nand Gosvami"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "author": "N. M. Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04976v1",
                "updated": "2025-07-07T13:19:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    19,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:19:43Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    19,
                    43,
                    0,
                    188,
                    0
                ],
                "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models"
                },
                "summary": "In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets."
                },
                "authors": [
                    {
                        "name": "Eunseop Yoon"
                    },
                    {
                        "name": "Hee Suk Yoon"
                    },
                    {
                        "name": "Mark A. Hasegawa-Johnson"
                    },
                    {
                        "name": "Chang D. Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang D. Yoo"
                },
                "author": "Chang D. Yoo",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04969v1",
                "updated": "2025-07-07T13:12:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    12,
                    28,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:12:28Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    12,
                    28,
                    0,
                    188,
                    0
                ],
                "title": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing"
                },
                "summary": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems."
                },
                "authors": [
                    {
                        "name": "Chanh Nguyen"
                    },
                    {
                        "name": "Erik Elmroth"
                    },
                    {
                        "name": "Monowar Bhuyan"
                    }
                ],
                "author_detail": {
                    "name": "Monowar Bhuyan"
                },
                "author": "Monowar Bhuyan",
                "arxiv_comment": "12 pages, 6 figures, IEEE CISOSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04345v2",
                "updated": "2025-07-07T13:07:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    7,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2024-06-06T17:59:58Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    59,
                    58,
                    3,
                    158,
                    0
                ],
                "title": "Active Stereo in the Wild through Virtual Pattern Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Stereo in the Wild through Virtual Pattern Projection"
                },
                "summary": "This paper presents a novel general-purpose guided stereo paradigm that\nmimics the active stereo principle by replacing the unreliable physical pattern\nprojector with a depth sensor. It works by projecting virtual patterns\nconsistent with the scene geometry onto the left and right images acquired by a\nconventional stereo camera, using the sparse hints obtained from a depth\nsensor, to facilitate the visual correspondence. Purposely, any depth sensing\ndevice can be seamlessly plugged into our framework, enabling the deployment of\na virtual active stereo setup in any possible environment and overcoming the\nsevere limitations of physical pattern projection, such as the limited working\nrange and environmental conditions. Exhaustive experiments on indoor and\noutdoor datasets featuring both long and close range, including those providing\nraw, unfiltered depth hints from off-the-shelf depth sensors, highlight the\neffectiveness of our approach in notably boosting the robustness and accuracy\nof algorithms and deep stereo without any code modification and even without\nre-training. Additionally, we assess the performance of our strategy on active\nstereo evaluation datasets with conventional pattern projection. Indeed, in all\nthese scenarios, our virtual pattern projection paradigm achieves\nstate-of-the-art performance. The source code is available at:\nhttps://github.com/bartn8/vppstereo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel general-purpose guided stereo paradigm that\nmimics the active stereo principle by replacing the unreliable physical pattern\nprojector with a depth sensor. It works by projecting virtual patterns\nconsistent with the scene geometry onto the left and right images acquired by a\nconventional stereo camera, using the sparse hints obtained from a depth\nsensor, to facilitate the visual correspondence. Purposely, any depth sensing\ndevice can be seamlessly plugged into our framework, enabling the deployment of\na virtual active stereo setup in any possible environment and overcoming the\nsevere limitations of physical pattern projection, such as the limited working\nrange and environmental conditions. Exhaustive experiments on indoor and\noutdoor datasets featuring both long and close range, including those providing\nraw, unfiltered depth hints from off-the-shelf depth sensors, highlight the\neffectiveness of our approach in notably boosting the robustness and accuracy\nof algorithms and deep stereo without any code modification and even without\nre-training. Additionally, we assess the performance of our strategy on active\nstereo evaluation datasets with conventional pattern projection. Indeed, in all\nthese scenarios, our virtual pattern projection paradigm achieves\nstate-of-the-art performance. The source code is available at:\nhttps://github.com/bartn8/vppstereo."
                },
                "authors": [
                    {
                        "name": "Luca Bartolomei"
                    },
                    {
                        "name": "Matteo Poggi"
                    },
                    {
                        "name": "Fabio Tosi"
                    },
                    {
                        "name": "Andrea Conti"
                    },
                    {
                        "name": "Stefano Mattoccia"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Mattoccia"
                },
                "author": "Stefano Mattoccia",
                "arxiv_comment": "IJCV extended version of ICCV 2023 paper: \"Active Stereo Without\n  Pattern Projector\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04952v1",
                "updated": "2025-07-07T12:53:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:53:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation"
                },
                "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Bohui Zhai"
                    },
                    {
                        "name": "Guoxiang He"
                    },
                    {
                        "name": "Hebin Li"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Lingyun Tan"
                    },
                    {
                        "name": "Pengyu Guo"
                    },
                    {
                        "name": "Xianshu Pang"
                    },
                    {
                        "name": "Yang Ruan"
                    },
                    {
                        "name": "Zhifeng Zhang"
                    },
                    {
                        "name": "Zhonghu Wang"
                    },
                    {
                        "name": "Ziyan Xu"
                    },
                    {
                        "name": "Zuopu Yin"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04942v2",
                "updated": "2025-07-08T06:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    37,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T12:38:53Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    38,
                    53,
                    0,
                    188,
                    0
                ],
                "title": "SIGIR 2025 -- LiveRAG Challenge Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGIR 2025 -- LiveRAG Challenge Report"
                },
                "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy."
                },
                "authors": [
                    {
                        "name": "David Carmel"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Oren Somekh"
                    },
                    {
                        "name": "Ran Tavory"
                    },
                    {
                        "name": "Mehdi Ghissassi"
                    },
                    {
                        "name": "Edo Liberty"
                    },
                    {
                        "name": "Roy Miara"
                    }
                ],
                "author_detail": {
                    "name": "Roy Miara"
                },
                "author": "Roy Miara",
                "arxiv_comment": "9 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04931v1",
                "updated": "2025-07-07T12:26:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    26,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T12:26:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    26,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "LIFT: Automating Symbolic Execution Optimization with Large Language\n  Models for AI Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Automating Symbolic Execution Optimization with Large Language\n  Models for AI Networks"
                },
                "summary": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems."
                },
                "authors": [
                    {
                        "name": "Ruoxi Wang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Chunchi Liu"
                    },
                    {
                        "name": "Yinhao Xiao"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI\n  Computing (NAIC). 7 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08933v2",
                "updated": "2025-07-07T12:01:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    1,
                    32,
                    0,
                    188,
                    0
                ],
                "published": "2024-06-13T09:03:53Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    9,
                    3,
                    53,
                    3,
                    165,
                    0
                ],
                "title": "LaCoOT: Layer Collapse through Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCoOT: Layer Collapse through Optimal Transport"
                },
                "summary": "Although deep neural networks are well-known for their outstanding\nperformance in tackling complex tasks, their hunger for computational resources\nremains a significant hurdle, posing energy-consumption issues and restricting\ntheir deployment on resource-constrained devices, preventing their widespread\nadoption. In this paper, we present an optimal transport-based method to reduce\nthe depth of over-parametrized deep neural networks, alleviating their\ncomputational burden. More specifically, we propose a new regularization\nstrategy based on the Max-Sliced Wasserstein distance to minimize the distance\nbetween the intermediate feature distributions in the neural network. We show\nthat minimizing this distance enables the complete removal of intermediate\nlayers in the network, achieving better performance/depth trade-off compared to\nexisting techniques. We assess the effectiveness of our method on traditional\nimage classification setups and extend it to generative image models. Our code\nis available at https://github.com/VGCQ/LaCoOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although deep neural networks are well-known for their outstanding\nperformance in tackling complex tasks, their hunger for computational resources\nremains a significant hurdle, posing energy-consumption issues and restricting\ntheir deployment on resource-constrained devices, preventing their widespread\nadoption. In this paper, we present an optimal transport-based method to reduce\nthe depth of over-parametrized deep neural networks, alleviating their\ncomputational burden. More specifically, we propose a new regularization\nstrategy based on the Max-Sliced Wasserstein distance to minimize the distance\nbetween the intermediate feature distributions in the neural network. We show\nthat minimizing this distance enables the complete removal of intermediate\nlayers in the network, achieving better performance/depth trade-off compared to\nexisting techniques. We assess the effectiveness of our method on traditional\nimage classification setups and extend it to generative image models. Our code\nis available at https://github.com/VGCQ/LaCoOT."
                },
                "authors": [
                    {
                        "name": "Victor Quétu"
                    },
                    {
                        "name": "Nour Hezbri"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "arxiv_comment": "ICCV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16174v2",
                "updated": "2025-07-07T11:43:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    43,
                    34,
                    0,
                    188,
                    0
                ],
                "published": "2025-02-22T10:31:50Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    50,
                    5,
                    53,
                    0
                ],
                "title": "Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation\n  via Latent Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation\n  via Latent Prototypes"
                },
                "summary": "With the rise of LLMs, ensuring model safety and alignment has become a\ncritical concern. While modern instruction-finetuned LLMs incorporate alignment\nduring training, they still frequently require moderation tools to prevent\nunsafe behavior. The most common approach to moderation are guard models that\nflag unsafe inputs. However, guards require costly training and are typically\nlimited to fixed-size, pre-trained options, making them difficult to adapt to\nevolving risks and resource constraints. We hypothesize that\ninstruction-finetuned LLMs already encode safety-relevant information\ninternally and explore training-free safety assessment methods that work with\noff-the-shelf models. We show that simple prompting allows models to recognize\nharmful inputs they would otherwise mishandle. We also demonstrate that safe\nand unsafe prompts are distinctly separable in the models' latent space.\nBuilding on this, we introduce the Latent Prototype Moderator (LPM), a\ntraining-free moderation method that uses Mahalanobis distance in latent space\nto assess input safety. LPM is a lightweight, customizable add-on that\ngeneralizes across model families and sizes. Our method matches or exceeds\nstate-of-the-art guard models across multiple safety benchmarks, offering a\npractical and flexible solution for scalable LLM moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of LLMs, ensuring model safety and alignment has become a\ncritical concern. While modern instruction-finetuned LLMs incorporate alignment\nduring training, they still frequently require moderation tools to prevent\nunsafe behavior. The most common approach to moderation are guard models that\nflag unsafe inputs. However, guards require costly training and are typically\nlimited to fixed-size, pre-trained options, making them difficult to adapt to\nevolving risks and resource constraints. We hypothesize that\ninstruction-finetuned LLMs already encode safety-relevant information\ninternally and explore training-free safety assessment methods that work with\noff-the-shelf models. We show that simple prompting allows models to recognize\nharmful inputs they would otherwise mishandle. We also demonstrate that safe\nand unsafe prompts are distinctly separable in the models' latent space.\nBuilding on this, we introduce the Latent Prototype Moderator (LPM), a\ntraining-free moderation method that uses Mahalanobis distance in latent space\nto assess input safety. LPM is a lightweight, customizable add-on that\ngeneralizes across model families and sizes. Our method matches or exceeds\nstate-of-the-art guard models across multiple safety benchmarks, offering a\npractical and flexible solution for scalable LLM moderation."
                },
                "authors": [
                    {
                        "name": "Maciej Chrabąszcz"
                    },
                    {
                        "name": "Filip Szatkowski"
                    },
                    {
                        "name": "Bartosz Wójcik"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Tomasz Trzciński"
                    },
                    {
                        "name": "Sebastian Cygert"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Cygert"
                },
                "author": "Sebastian Cygert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04893v1",
                "updated": "2025-07-07T11:27:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    49,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:27:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident\n  Severity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident\n  Severity Prediction"
                },
                "summary": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications."
                },
                "authors": [
                    {
                        "name": "Kaleem Ullah Qasim"
                    },
                    {
                        "name": "Jiashu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiashu Zhang"
                },
                "author": "Jiashu Zhang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v3",
                "updated": "2025-07-07T11:27:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    27,
                    2,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhi Zhang"
                },
                "author": "Yuzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04886v1",
                "updated": "2025-07-07T11:17:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:17:32Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations"
                },
                "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04884v1",
                "updated": "2025-07-07T11:16:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    16,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:16:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    16,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "Building Open-Retrieval Conversational Question Answering Systems by\n  Generating Synthetic Data and Decontextualizing User Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Open-Retrieval Conversational Question Answering Systems by\n  Generating Synthetic Data and Decontextualizing User Questions"
                },
                "summary": "We consider open-retrieval conversational question answering (OR-CONVQA), an\nextension of question answering where system responses need to be (i) aware of\ndialog history and (ii) grounded in documents (or document fragments) retrieved\nper question. Domain-specific OR-CONVQA training datasets are crucial for\nreal-world applications, but hard to obtain. We propose a pipeline that\ncapitalizes on the abundance of plain text documents in organizations (e.g.,\nproduct documentation) to automatically produce realistic OR-CONVQA dialogs\nwith annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we\ngenerate in-dialog question-answer pairs, self-contained (decontextualized,\ne.g., no referring expressions) versions of user questions, and propositions\n(sentences expressing prominent information from the documents) the system\nresponses are grounded in. We show how the synthetic dialogs can be used to\ntrain efficient question rewriters that decontextualize user questions,\nallowing existing dialog-unaware retrievers to be utilized. The retrieved\ninformation and the decontextualized question are then passed on to an LLM that\ngenerates the system's response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider open-retrieval conversational question answering (OR-CONVQA), an\nextension of question answering where system responses need to be (i) aware of\ndialog history and (ii) grounded in documents (or document fragments) retrieved\nper question. Domain-specific OR-CONVQA training datasets are crucial for\nreal-world applications, but hard to obtain. We propose a pipeline that\ncapitalizes on the abundance of plain text documents in organizations (e.g.,\nproduct documentation) to automatically produce realistic OR-CONVQA dialogs\nwith annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we\ngenerate in-dialog question-answer pairs, self-contained (decontextualized,\ne.g., no referring expressions) versions of user questions, and propositions\n(sentences expressing prominent information from the documents) the system\nresponses are grounded in. We show how the synthetic dialogs can be used to\ntrain efficient question rewriters that decontextualize user questions,\nallowing existing dialog-unaware retrievers to be utilized. The retrieved\ninformation and the decontextualized question are then passed on to an LLM that\ngenerates the system's response."
                },
                "authors": [
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Nikolaos Stylianou"
                    },
                    {
                        "name": "Alexandra Fiotaki"
                    },
                    {
                        "name": "Spiros Methenitis"
                    },
                    {
                        "name": "Elisavet Palogiannidi"
                    },
                    {
                        "name": "Themos Stafylakis"
                    },
                    {
                        "name": "Ion Androutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Ion Androutsopoulos"
                },
                "author": "Ion Androutsopoulos",
                "arxiv_comment": "Accepted at SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04880v1",
                "updated": "2025-07-07T11:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    9,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:09:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    9,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context\n  Attention Network for Colorectal Polyp Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context\n  Attention Network for Colorectal Polyp Detection"
                },
                "summary": "Colorectal cancer (CRC) is closely linked to the malignant transformation of\ncolorectal polyps, making early detection essential. However, current models\nstruggle with detecting small lesions, accurately localizing boundaries, and\nproviding interpretable decisions. To address these issues, we propose HGNet,\nwhich integrates High-Order Spatial Awareness Hypergraph and Multi-Scale\nContext Attention. Key innovations include: (1) an Efficient Multi-Scale\nContext Attention (EMCA) module to enhance lesion feature representation and\nboundary modeling; (2) the deployment of a spatial hypergraph convolution\nmodule before the detection head to capture higher-order spatial relationships\nbetween nodes; (3) the application of transfer learning to address the scarcity\nof medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for\ndecision visualization. Experimental results show that HGNet achieves 94%\naccuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion\ndifferentiation and clinical interpretability. The source code will be made\npublicly available upon publication of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colorectal cancer (CRC) is closely linked to the malignant transformation of\ncolorectal polyps, making early detection essential. However, current models\nstruggle with detecting small lesions, accurately localizing boundaries, and\nproviding interpretable decisions. To address these issues, we propose HGNet,\nwhich integrates High-Order Spatial Awareness Hypergraph and Multi-Scale\nContext Attention. Key innovations include: (1) an Efficient Multi-Scale\nContext Attention (EMCA) module to enhance lesion feature representation and\nboundary modeling; (2) the deployment of a spatial hypergraph convolution\nmodule before the detection head to capture higher-order spatial relationships\nbetween nodes; (3) the application of transfer learning to address the scarcity\nof medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for\ndecision visualization. Experimental results show that HGNet achieves 94%\naccuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion\ndifferentiation and clinical interpretability. The source code will be made\npublicly available upon publication of this paper."
                },
                "authors": [
                    {
                        "name": "Xiaofang Liu"
                    },
                    {
                        "name": "Lingling Sun"
                    },
                    {
                        "name": "Xuqing Zhang"
                    },
                    {
                        "name": "Yuannong Ye"
                    },
                    {
                        "name": "Bin zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin zhao"
                },
                "author": "Bin zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04877v1",
                "updated": "2025-07-07T11:04:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    4,
                    3,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T11:04:03Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    4,
                    3,
                    0,
                    188,
                    0
                ],
                "title": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese\n  Medicine"
                },
                "summary": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise."
                },
                "authors": [
                    {
                        "name": "Zewen Sun"
                    },
                    {
                        "name": "Ruoxiang Huang"
                    },
                    {
                        "name": "Jiahe Feng"
                    },
                    {
                        "name": "Rundong Kong"
                    },
                    {
                        "name": "Yuqian Wang"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Ziqi Gong"
                    },
                    {
                        "name": "Yuyuan Qin"
                    },
                    {
                        "name": "Yingxue Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01461v2",
                "updated": "2025-07-07T10:55:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    55,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-02T08:19:58Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    19,
                    58,
                    2,
                    183,
                    0
                ],
                "title": "Handling out-of-order input arrival in CEP engines on the edge combining\n  optimistic, pessimistic and lazy evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling out-of-order input arrival in CEP engines on the edge combining\n  optimistic, pessimistic and lazy evaluation"
                },
                "summary": "In Complex Event Processing, handling out-of-order, late, and duplicate\nevents is critical for real-time analytics, especially on resource-constrained\ndevices that process heterogeneous data from multiple sources. We present\nLimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and\nspeculative processing to efficiently handle data inconsistencies while\nsupporting multi-pattern detection under relaxed semantics. LimeCEP integrates\nKafka for efficient message ordering, retention, and duplicate elimination, and\noffers configurable strategies to trade off between accuracy, latency, and\nresource consumption. Compared to state-of-the-art systems like SASE and\nFlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up\nto 10 times lower memory usage and 6 times lower CPU utilization, while\nmaintaining near-perfect precision and recall under high-disorder input\nstreams, making it well-suited for non-cloud deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Complex Event Processing, handling out-of-order, late, and duplicate\nevents is critical for real-time analytics, especially on resource-constrained\ndevices that process heterogeneous data from multiple sources. We present\nLimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and\nspeculative processing to efficiently handle data inconsistencies while\nsupporting multi-pattern detection under relaxed semantics. LimeCEP integrates\nKafka for efficient message ordering, retention, and duplicate elimination, and\noffers configurable strategies to trade off between accuracy, latency, and\nresource consumption. Compared to state-of-the-art systems like SASE and\nFlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up\nto 10 times lower memory usage and 6 times lower CPU utilization, while\nmaintaining near-perfect precision and recall under high-disorder input\nstreams, making it well-suited for non-cloud deployments."
                },
                "authors": [
                    {
                        "name": "Styliani Kyrama"
                    },
                    {
                        "name": "Anastasios Gounaris"
                    }
                ],
                "author_detail": {
                    "name": "Anastasios Gounaris"
                },
                "author": "Anastasios Gounaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04857v1",
                "updated": "2025-07-07T10:30:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    30,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:30:05Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    30,
                    5,
                    0,
                    188,
                    0
                ],
                "title": "Supporting Software Formal Verification with Large Language Models: An\n  Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Software Formal Verification with Large Language Models: An\n  Experimental Study"
                },
                "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Marie Farrell"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Liping Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhao"
                },
                "author": "Liping Zhao",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09720v2",
                "updated": "2025-07-07T10:27:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    27,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-13T21:01:21Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    21,
                    1,
                    21,
                    6,
                    103,
                    0
                ],
                "title": "NotebookLM: An LLM with RAG for active learning and collaborative\n  tutoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotebookLM: An LLM with RAG for active learning and collaborative\n  tutoring"
                },
                "summary": "This study explores NotebookLM, a Google Gemini powered AI platform that\nintegrates Retrieval Augmented Generation (RAG), as a collaborative physics\ntutor, an area of research that is developing quickly. In our implementation,\nNotebookLM was configured as an AI physics collaborative tutor to support\nstudents in solving conceptually oriented physics problems using a\ncollaborative, Socratic approach. When deployed as a collaborative tutor, the\nsystem restricts student interaction to a chat only interface, promoting\ncontrolled and guided engagement. By grounding its responses in teacher\nprovided source documents, NotebookLM helps mitigate one of the major\nshortcomings of standard large language models--hallucinations--thereby\nensuring more traceable and reliable answers. Our experiments demonstrate\nNotebookLM's potential as a low cost, easily implemented RAG based tool for\npersonalized and traceable AI assisted physics learning in diverse educational\nsettings. Furthermore, NotebookLM also functions as a valuable study tool for\nboth teachers and students by generating targeted questions, study guides, and\nsupplementary materials that enhance both classroom instruction and independent\nresearch. While limitations remain, particularly regarding legal restrictions,\nthe current text only mode of interaction, and the intrinsic reliability\nchallenges of statistical models, this work presents a promising example of a\ngrounded AI application in physics education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores NotebookLM, a Google Gemini powered AI platform that\nintegrates Retrieval Augmented Generation (RAG), as a collaborative physics\ntutor, an area of research that is developing quickly. In our implementation,\nNotebookLM was configured as an AI physics collaborative tutor to support\nstudents in solving conceptually oriented physics problems using a\ncollaborative, Socratic approach. When deployed as a collaborative tutor, the\nsystem restricts student interaction to a chat only interface, promoting\ncontrolled and guided engagement. By grounding its responses in teacher\nprovided source documents, NotebookLM helps mitigate one of the major\nshortcomings of standard large language models--hallucinations--thereby\nensuring more traceable and reliable answers. Our experiments demonstrate\nNotebookLM's potential as a low cost, easily implemented RAG based tool for\npersonalized and traceable AI assisted physics learning in diverse educational\nsettings. Furthermore, NotebookLM also functions as a valuable study tool for\nboth teachers and students by generating targeted questions, study guides, and\nsupplementary materials that enhance both classroom instruction and independent\nresearch. While limitations remain, particularly regarding legal restrictions,\nthe current text only mode of interaction, and the intrinsic reliability\nchallenges of statistical models, this work presents a promising example of a\ngrounded AI application in physics education."
                },
                "authors": [
                    {
                        "name": "Eugenio Tufino"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Tufino"
                },
                "author": "Eugenio Tufino",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04854v1",
                "updated": "2025-07-07T10:26:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    26,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:26:42Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    26,
                    42,
                    0,
                    188,
                    0
                ],
                "title": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large\n  Language Models"
                },
                "summary": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released."
                },
                "authors": [
                    {
                        "name": "Shrey Ganatra"
                    },
                    {
                        "name": "Swapnil Bhattacharyya"
                    },
                    {
                        "name": "Harshvivek Kashid"
                    },
                    {
                        "name": "Spandan Anaokar"
                    },
                    {
                        "name": "Shruti Nair"
                    },
                    {
                        "name": "Reshma Sekhar"
                    },
                    {
                        "name": "Siddharth Manohar"
                    },
                    {
                        "name": "Rahul Hemrajani"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04852v1",
                "updated": "2025-07-07T10:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:20:16Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    20,
                    16,
                    0,
                    188,
                    0
                ],
                "title": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels"
                },
                "summary": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Hanjie Zhao"
                    },
                    {
                        "name": "Senbin Zhu"
                    },
                    {
                        "name": "Hongde Liu"
                    },
                    {
                        "name": "Zhihong Zhang"
                    },
                    {
                        "name": "Yuxiang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Jia"
                },
                "author": "Yuxiang Jia",
                "arxiv_comment": "The paper has been accepted by NLPCC2025. 12 pages, 5 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04842v1",
                "updated": "2025-07-07T10:03:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    31,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:03:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    31,
                    0,
                    188,
                    0
                ],
                "title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing"
                },
                "summary": "Rapid analysis of satellite data is vital for many remote sensing\napplications, from disaster response to environmental monitoring, but is\nbecoming harder to achieve with the increasing volumes of data generated by\nmodern satellites. On-satellite machine learning (ML) offers a potential\nsolution, by reducing latency associated with transmission of these large data\nvolumes to ground stations, but state-of-the-art models are often too large or\npower-hungry for satellite deployment. Vessel detection using Synthetic\nAperture Radar (SAR) is a critical time-sensitive task for maritime security\nthat exemplifies this challenge. SAR vessel detection has previously been\ndemonstrated only by ML models that either are too large for satellite\ndeployment, have not been developed for sufficiently low-power hardware, or\nhave only been developed and tested on small SAR datasets that do not\nsufficiently represent the real-world task. Here we address this issue by\ndeveloping and deploying a new efficient and highly performant SAR vessel\ndetection model, using a customised YOLOv8 architecture specifically optimized\nfor FPGA-based processing within common satellite power constraints (<10W). We\ntrain and evaluate our model on the largest and most diverse open SAR vessel\ndataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our\nFPGA-based model has detection and classification performance only ~2% and 3%\nlower than values from state-of-the-art GPU-based models, despite being two to\nthree orders of magnitude smaller in size. This work demonstrates small yet\nhighly performant ML models for time-critical SAR analysis, paving the way for\nmore autonomous, responsive, and scalable Earth observation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid analysis of satellite data is vital for many remote sensing\napplications, from disaster response to environmental monitoring, but is\nbecoming harder to achieve with the increasing volumes of data generated by\nmodern satellites. On-satellite machine learning (ML) offers a potential\nsolution, by reducing latency associated with transmission of these large data\nvolumes to ground stations, but state-of-the-art models are often too large or\npower-hungry for satellite deployment. Vessel detection using Synthetic\nAperture Radar (SAR) is a critical time-sensitive task for maritime security\nthat exemplifies this challenge. SAR vessel detection has previously been\ndemonstrated only by ML models that either are too large for satellite\ndeployment, have not been developed for sufficiently low-power hardware, or\nhave only been developed and tested on small SAR datasets that do not\nsufficiently represent the real-world task. Here we address this issue by\ndeveloping and deploying a new efficient and highly performant SAR vessel\ndetection model, using a customised YOLOv8 architecture specifically optimized\nfor FPGA-based processing within common satellite power constraints (<10W). We\ntrain and evaluate our model on the largest and most diverse open SAR vessel\ndataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our\nFPGA-based model has detection and classification performance only ~2% and 3%\nlower than values from state-of-the-art GPU-based models, despite being two to\nthree orders of magnitude smaller in size. This work demonstrates small yet\nhighly performant ML models for time-critical SAR analysis, paving the way for\nmore autonomous, responsive, and scalable Earth observation systems."
                },
                "authors": [
                    {
                        "name": "Colin Laganier"
                    },
                    {
                        "name": "Liam Fletcher"
                    },
                    {
                        "name": "Elim Kwan"
                    },
                    {
                        "name": "Richard Walters"
                    },
                    {
                        "name": "Victoria Nockles"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Nockles"
                },
                "author": "Victoria Nockles",
                "arxiv_comment": "14 pages, 5 figures, 3 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04841v1",
                "updated": "2025-07-07T10:03:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    20,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T10:03:20Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    20,
                    0,
                    188,
                    0
                ],
                "title": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient\n  Task-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient\n  Task-Oriented Dialogue Systems"
                },
                "summary": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Quang-Vinh Nguyen"
                    },
                    {
                        "name": "Quang-Chieu Nguyen"
                    },
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted at SIGdial 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03637v2",
                "updated": "2025-07-07T09:53:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    53,
                    22,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-04T07:30:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    30,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "RewardAnything: Generalizable Principle-Following Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RewardAnything: Generalizable Principle-Following Reward Models"
                },
                "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles."
                },
                "authors": [
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Weizheng Gu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "25 pages, 9 figures, Code & model weights available at:\n  https://zhuohaoyu.github.io/RewardAnything",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04820v1",
                "updated": "2025-07-07T09:38:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    38,
                    43,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:38:43Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    38,
                    43,
                    0,
                    188,
                    0
                ],
                "title": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation"
                },
                "summary": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving."
                },
                "authors": [
                    {
                        "name": "Junru Wu"
                    },
                    {
                        "name": "Le Yan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Honglei Zhuang"
                    },
                    {
                        "name": "Paul Suganthan G. C."
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhe Dong"
                    },
                    {
                        "name": "Xuanhui Wang"
                    },
                    {
                        "name": "Harrie Oosterhuis"
                    }
                ],
                "author_detail": {
                    "name": "Harrie Oosterhuis"
                },
                "author": "Harrie Oosterhuis",
                "arxiv_comment": "ReNeuIR 2025 (at SIGIR 2025) - 4th Workshop on Reaching Efficiency in\n  Neural Information Retrieval, July 17, 2025, Padua, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04818v1",
                "updated": "2025-07-07T09:37:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    37,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:37:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    37,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Enabling Security on the Edge: A CHERI Compartmentalized Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Security on the Edge: A CHERI Compartmentalized Network Stack"
                },
                "summary": "The widespread deployment of embedded systems in critical infrastructures,\ninterconnected edge devices like autonomous drones, and smart industrial\nsystems requires robust security measures. Compromised systems increase the\nrisks of operational failures, data breaches, and -- in safety-critical\nenvironments -- potential physical harm to people. Despite these risks, current\nsecurity measures are often insufficient to fully address the attack surfaces\nof embedded devices. CHERI provides strong security from the hardware level by\nenabling fine-grained compartmentalization and memory protection, which can\nreduce the attack surface and improve the reliability of such devices. In this\nwork, we explore the potential of CHERI to compartmentalize one of the most\ncritical and targeted components of interconnected systems: their network\nstack. Our case study examines the trade-offs of isolating applications, TCP/IP\nlibraries, and network drivers on a CheriBSD system deployed on the Arm Morello\nplatform. Our results suggest that CHERI has the potential to enhance security\nwhile maintaining performance in embedded-like environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of embedded systems in critical infrastructures,\ninterconnected edge devices like autonomous drones, and smart industrial\nsystems requires robust security measures. Compromised systems increase the\nrisks of operational failures, data breaches, and -- in safety-critical\nenvironments -- potential physical harm to people. Despite these risks, current\nsecurity measures are often insufficient to fully address the attack surfaces\nof embedded devices. CHERI provides strong security from the hardware level by\nenabling fine-grained compartmentalization and memory protection, which can\nreduce the attack surface and improve the reliability of such devices. In this\nwork, we explore the potential of CHERI to compartmentalize one of the most\ncritical and targeted components of interconnected systems: their network\nstack. Our case study examines the trade-offs of isolating applications, TCP/IP\nlibraries, and network drivers on a CheriBSD system deployed on the Arm Morello\nplatform. Our results suggest that CHERI has the potential to enhance security\nwhile maintaining performance in embedded-like environments."
                },
                "authors": [
                    {
                        "name": "Donato Ferraro"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Marongiu"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Marongiu"
                },
                "author": "Andrea Marongiu",
                "arxiv_comment": "Accepted for publication at Design, Automation and Test in Europe\n  Conference | The European Event for Electronic System Design & Test 2025\n  (DATE25), 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04803v1",
                "updated": "2025-07-07T09:22:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    22,
                    6,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T09:22:06Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    22,
                    6,
                    0,
                    188,
                    0
                ],
                "title": "Application and Evaluation of Large Language Models for Forecasting the\n  Impact of Traffic Incidents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application and Evaluation of Large Language Models for Forecasting the\n  Impact of Traffic Incidents"
                },
                "summary": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction."
                },
                "authors": [
                    {
                        "name": "George Jagadeesh"
                    },
                    {
                        "name": "Srikrishna Iyer"
                    },
                    {
                        "name": "Michal Polanowski"
                    },
                    {
                        "name": "Kai Xin Thia"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xin Thia"
                },
                "author": "Kai Xin Thia",
                "arxiv_comment": "This paper has been accepted for publication at the 2025 IEEE 28th\n  International Conference on Intelligent Transportation Systems (ITSC), Gold\n  Coast, Australia, 2025. Copyright IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05853v2",
                "updated": "2025-07-07T09:14:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    14,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-06T08:16:07Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    8,
                    16,
                    7,
                    4,
                    157,
                    0
                ],
                "title": "Training-Free Query Optimization via LLM-Based Plan Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Query Optimization via LLM-Based Plan Similarity"
                },
                "summary": "Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Vladimir Boorlakov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Boorlakov"
                },
                "author": "Vladimir Boorlakov",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13596v2",
                "updated": "2025-07-07T09:09:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    9,
                    16,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-16T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems"
                },
                "summary": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Long-Vu Hoang"
                    },
                    {
                        "name": "Huy-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Huy-Dat Tran"
                },
                "author": "Huy-Dat Tran",
                "arxiv_comment": "Accepted to Interspeech MLCSLM-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04775v1",
                "updated": "2025-07-07T08:51:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    51,
                    14,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:51:14Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    51,
                    14,
                    0,
                    188,
                    0
                ],
                "title": "FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on\n  GPUs"
                },
                "summary": "Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are\ngaining significant traction due to their ability to provide\npost-quantum-resistant, privacy-preserving approximate computing; an especially\ndesirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing\nparadigms. OpenFHE is a leading CPU-based FHE library with robust CKKS\noperations, but its server-side performance is not yet sufficient for practical\ncloud deployment. As GPU computing becomes more common in data centers, many\nFHE libraries are adding GPU support. However, integrating an efficient GPU\nbackend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction\nLayer (HAL), its flexible architecture sacrifices performance due to the\nabstraction layers required for multi-scheme and multi-backend compatibility.\nIn this work, we introduce FIDESlib, the first open-source server-side CKKS GPU\nlibrary that is fully interoperable with well-established client-side OpenFHE\noperations. Unlike other existing open-source GPU libraries, FIDESlib provides\nthe first implementation featuring heavily optimized GPU kernels for all CKKS\nprimitives, including bootstrapping. Our library also integrates robust\nbenchmarking and testing, ensuring it remains adaptable to further\noptimization. Furthermore, its software architecture is designed to support\nextensions to a multi-GPU backend for enhanced acceleration. Our experiments\nacross various GPU systems and the leading open-source CKKS library to date,\nPhantom, show that FIDESlib offers superior performance and scalability. For\nbootstrapping, FIDESlib achieves no less than 70x speedup over the\nAVX-optimized OpenFHE implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are\ngaining significant traction due to their ability to provide\npost-quantum-resistant, privacy-preserving approximate computing; an especially\ndesirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing\nparadigms. OpenFHE is a leading CPU-based FHE library with robust CKKS\noperations, but its server-side performance is not yet sufficient for practical\ncloud deployment. As GPU computing becomes more common in data centers, many\nFHE libraries are adding GPU support. However, integrating an efficient GPU\nbackend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction\nLayer (HAL), its flexible architecture sacrifices performance due to the\nabstraction layers required for multi-scheme and multi-backend compatibility.\nIn this work, we introduce FIDESlib, the first open-source server-side CKKS GPU\nlibrary that is fully interoperable with well-established client-side OpenFHE\noperations. Unlike other existing open-source GPU libraries, FIDESlib provides\nthe first implementation featuring heavily optimized GPU kernels for all CKKS\nprimitives, including bootstrapping. Our library also integrates robust\nbenchmarking and testing, ensuring it remains adaptable to further\noptimization. Furthermore, its software architecture is designed to support\nextensions to a multi-GPU backend for enhanced acceleration. Our experiments\nacross various GPU systems and the leading open-source CKKS library to date,\nPhantom, show that FIDESlib offers superior performance and scalability. For\nbootstrapping, FIDESlib achieves no less than 70x speedup over the\nAVX-optimized OpenFHE implementation."
                },
                "authors": [
                    {
                        "name": "Carlos Agulló-Domingo"
                    },
                    {
                        "name": "Óscar Vera-López"
                    },
                    {
                        "name": "Seyda Guzelhan"
                    },
                    {
                        "name": "Lohit Daksha"
                    },
                    {
                        "name": "Aymane El Jerari"
                    },
                    {
                        "name": "Kaustubh Shivdikar"
                    },
                    {
                        "name": "Rashmi Agrawal"
                    },
                    {
                        "name": "David Kaeli"
                    },
                    {
                        "name": "Ajay Joshi"
                    },
                    {
                        "name": "José L. Abellán"
                    }
                ],
                "author_detail": {
                    "name": "José L. Abellán"
                },
                "arxiv_affiliation": "Universidad de Murcia",
                "author": "José L. Abellán",
                "arxiv_comment": "Presented as poster paper at 2025 IEEE International Symposium on\n  Performance Analysis of Systems and Software (ISPASS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09695v2",
                "updated": "2025-07-07T08:47:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    47,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-11T13:10:49Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    10,
                    49,
                    2,
                    162,
                    0
                ],
                "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and\n  Interpretable Spiking Neural Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and\n  Interpretable Spiking Neural Model"
                },
                "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN."
                },
                "authors": [
                    {
                        "name": "Changwei Wu"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Jinying Zong"
                    },
                    {
                        "name": "Jie Dong"
                    },
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Yong Peng"
                    },
                    {
                        "name": "Jin Fan"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Changmiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Changmiao Wang"
                },
                "author": "Changmiao Wang",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04770v1",
                "updated": "2025-07-07T08:45:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    45,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:45:08Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    45,
                    8,
                    0,
                    188,
                    0
                ],
                "title": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System"
                },
                "summary": "Furniture decoration is an important task in various industrial applications.\nHowever, achieving a high-quality decorative result is often time-consuming and\nrequires specialized artistic expertise. To tackle these challenges, we explore\nhow multi-agent systems can assist in automating the decoration process. We\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\nSpecifically, given a human prompt and a household furniture item such as a\nworking desk or a TV stand, our system suggests relevant assets with\nappropriate styles and materials, and arranges them on the item, ensuring the\ndecorative result meets functionality, aesthetic, and ambiance preferences.\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\nfulfilling distinct roles in a typical decoration project. These agents\ncollaborate through communication, logical reasoning, and validation to\ntransform the requirements into the final outcome. Extensive experiments\ndemonstrate that our FurniMAS significantly outperforms other baselines in\ngenerating high-quality 3D decor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Furniture decoration is an important task in various industrial applications.\nHowever, achieving a high-quality decorative result is often time-consuming and\nrequires specialized artistic expertise. To tackle these challenges, we explore\nhow multi-agent systems can assist in automating the decoration process. We\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\nSpecifically, given a human prompt and a household furniture item such as a\nworking desk or a TV stand, our system suggests relevant assets with\nappropriate styles and materials, and arranges them on the item, ensuring the\ndecorative result meets functionality, aesthetic, and ambiance preferences.\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\nfulfilling distinct roles in a typical decoration project. These agents\ncollaborate through communication, logical reasoning, and validation to\ntransform the requirements into the final outcome. Extensive experiments\ndemonstrate that our FurniMAS significantly outperforms other baselines in\ngenerating high-quality 3D decor."
                },
                "authors": [
                    {
                        "name": "Toan Nguyen"
                    },
                    {
                        "name": "Tri Le"
                    },
                    {
                        "name": "Quang Nguyen"
                    },
                    {
                        "name": "Anh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen"
                },
                "author": "Anh Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04766v1",
                "updated": "2025-07-07T08:43:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    43,
                    56,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:43:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    43,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via\n  High-Difficulty and Dynamic Physics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via\n  High-Difficulty and Dynamic Physics Problems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in domains\nsuch as mathematics and programming, yet their capabilities in physics remain\nunderexplored and poorly understood. Physics poses unique challenges that\ndemand not only precise computation but also deep conceptual understanding and\nphysical modeling skills. Existing benchmarks often fall short due to limited\ndifficulty, multiple-choice formats, and static evaluation settings that fail\nto capture physical modeling ability. In this paper, we introduce\nABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'\nphysical reasoning and generalization capabilities. ABench-Physics consists of\ntwo components: Phy_A, a static set of 400 graduate- or Olympiad-level\nproblems; and Phy_B, a dynamic subset of 100 problems equipped with an\nautomatic variation engine to test model robustness across changing conditions.\nAll questions require precise numerical answers, with strict formatting and\ntolerance constraints. Our evaluation of several state-of-the-art LLMs reveals\nsubstantial performance gaps, highlighting persistent limitations in physical\nreasoning, especially in generalization to dynamic variants. ABench-Physics\nprovides a challenging and diagnostic framework for advancing scientific\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in domains\nsuch as mathematics and programming, yet their capabilities in physics remain\nunderexplored and poorly understood. Physics poses unique challenges that\ndemand not only precise computation but also deep conceptual understanding and\nphysical modeling skills. Existing benchmarks often fall short due to limited\ndifficulty, multiple-choice formats, and static evaluation settings that fail\nto capture physical modeling ability. In this paper, we introduce\nABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'\nphysical reasoning and generalization capabilities. ABench-Physics consists of\ntwo components: Phy_A, a static set of 400 graduate- or Olympiad-level\nproblems; and Phy_B, a dynamic subset of 100 problems equipped with an\nautomatic variation engine to test model robustness across changing conditions.\nAll questions require precise numerical answers, with strict formatting and\ntolerance constraints. Our evaluation of several state-of-the-art LLMs reveals\nsubstantial performance gaps, highlighting persistent limitations in physical\nreasoning, especially in generalization to dynamic variants. ABench-Physics\nprovides a challenging and diagnostic framework for advancing scientific\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yingfan Ma"
                    },
                    {
                        "name": "Yanmei Gu"
                    },
                    {
                        "name": "Zhengkai Yang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04756v1",
                "updated": "2025-07-07T08:32:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    32,
                    29,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:32:29Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    32,
                    29,
                    0,
                    188,
                    0
                ],
                "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta\n  Steering"
                },
                "summary": "Personalized text generation has become crucial for adapting language models\nto diverse and evolving users' personal context across cultural, temporal, and\ncontextual dimensions. While existing methods often rely on centralized\nfine-tuning or static preference alignment, they struggle to achieve real-time\nadaptation under resource constraints inherent to personal devices. This\nlimitation creates a dilemma: large cloud-based models lack access to localized\nuser-specific information, while small on-device models cannot match the\ngeneration quality of their cloud counterparts. To address this dichotomy, we\npresent CoSteer, a novel collaborative framework that enables decoding-time\npersonalization through localized delta steering. Our key insight lies in\nleveraging the logits difference between personal context-aware and -agnostic\noutputs from local small models as steering signals for cloud-based LLMs.\nSpecifically, we formulate token-level optimization as an online learning\nproblem, where local delta vectors dynamically adjust the remote LLM's logits\nwithin the on-device environment. This approach preserves privacy by\ntransmitting only the final steered tokens rather than raw data or intermediate\nvectors, while maintaining cloud-based LLMs' general capabilities without\nfine-tuning. Through comprehensive experiments on various personalized\ngeneration tasks, we demonstrate that CoSteer effectively assists LLMs in\ngenerating personalized content by leveraging locally stored user profiles and\nhistories, ensuring privacy preservation through on-device data processing\nwhile maintaining acceptable computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text generation has become crucial for adapting language models\nto diverse and evolving users' personal context across cultural, temporal, and\ncontextual dimensions. While existing methods often rely on centralized\nfine-tuning or static preference alignment, they struggle to achieve real-time\nadaptation under resource constraints inherent to personal devices. This\nlimitation creates a dilemma: large cloud-based models lack access to localized\nuser-specific information, while small on-device models cannot match the\ngeneration quality of their cloud counterparts. To address this dichotomy, we\npresent CoSteer, a novel collaborative framework that enables decoding-time\npersonalization through localized delta steering. Our key insight lies in\nleveraging the logits difference between personal context-aware and -agnostic\noutputs from local small models as steering signals for cloud-based LLMs.\nSpecifically, we formulate token-level optimization as an online learning\nproblem, where local delta vectors dynamically adjust the remote LLM's logits\nwithin the on-device environment. This approach preserves privacy by\ntransmitting only the final steered tokens rather than raw data or intermediate\nvectors, while maintaining cloud-based LLMs' general capabilities without\nfine-tuning. Through comprehensive experiments on various personalized\ngeneration tasks, we demonstrate that CoSteer effectively assists LLMs in\ngenerating personalized content by leveraging locally stored user profiles and\nhistories, ensuring privacy preservation through on-device data processing\nwhile maintaining acceptable computational overhead."
                },
                "authors": [
                    {
                        "name": "Hang Lv"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hongchao Gu"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04752v1",
                "updated": "2025-07-07T08:28:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    28,
                    7,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:28:07Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    28,
                    7,
                    0,
                    188,
                    0
                ],
                "title": "Large Language Models for Network Intrusion Detection Systems:\n  Foundations, Implementations, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Network Intrusion Detection Systems:\n  Foundations, Implementations, and Future Directions"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Xinchen Zhang"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Donglin Xie"
                    },
                    {
                        "name": "Weicai Long"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. H. Ngai"
                },
                "author": "Edith C. H. Ngai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04751v1",
                "updated": "2025-07-07T08:27:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    27,
                    44,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:27:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    27,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "LLMs as Architects and Critics for Multi-Source Opinion Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Architects and Critics for Multi-Source Opinion Summarization"
                },
                "summary": "Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion\nsummarization by incorporating additional sources of product metadata such as\ndescriptions, key features, specifications, and ratings, alongside reviews.\nThis integration results in comprehensive summaries that capture both\nsubjective opinions and objective product attributes essential for informed\ndecision-making. While Large Language Models (LLMs) have shown significant\nsuccess in various Natural Language Processing (NLP) tasks, their potential in\nM-OS remains largely unexplored. Additionally, the lack of evaluation datasets\nfor this task has impeded further advancements. To bridge this gap, we\nintroduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion\nsummaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,\naspect coverage, sentiment consistency, specificity. Our results demonstrate\nthat M-OS significantly enhances user engagement, as evidenced by a user study\nin which, on average, 87% of participants preferred M-OS over opinion\nsummaries. Our experiments demonstrate that factually enriched summaries\nenhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with\nhuman judgment, achieving an average Spearman correlation of \\r{ho} = 0.74,\nwhich surpasses the performance of previous methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion\nsummarization by incorporating additional sources of product metadata such as\ndescriptions, key features, specifications, and ratings, alongside reviews.\nThis integration results in comprehensive summaries that capture both\nsubjective opinions and objective product attributes essential for informed\ndecision-making. While Large Language Models (LLMs) have shown significant\nsuccess in various Natural Language Processing (NLP) tasks, their potential in\nM-OS remains largely unexplored. Additionally, the lack of evaluation datasets\nfor this task has impeded further advancements. To bridge this gap, we\nintroduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion\nsummaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,\naspect coverage, sentiment consistency, specificity. Our results demonstrate\nthat M-OS significantly enhances user engagement, as evidenced by a user study\nin which, on average, 87% of participants preferred M-OS over opinion\nsummaries. Our experiments demonstrate that factually enriched summaries\nenhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with\nhuman judgment, achieving an average Spearman correlation of \\r{ho} = 0.74,\nwhich surpasses the performance of previous methodologies."
                },
                "authors": [
                    {
                        "name": "Anuj Attri"
                    },
                    {
                        "name": "Arnav Attri"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Amey Patil"
                    },
                    {
                        "name": "Muthusamy Chelliah"
                    },
                    {
                        "name": "Nikesh Garera"
                    }
                ],
                "author_detail": {
                    "name": "Nikesh Garera"
                },
                "author": "Nikesh Garera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04748v1",
                "updated": "2025-07-07T08:19:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    19,
                    17,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:19:17Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    19,
                    17,
                    0,
                    188,
                    0
                ],
                "title": "LLM-based Question-Answer Framework for Sensor-driven HVAC System\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Question-Answer Framework for Sensor-driven HVAC System\n  Interaction"
                },
                "summary": "Question-answering (QA) interfaces powered by large language models (LLMs)\npresent a promising direction for improving interactivity with HVAC system\ninsights, particularly for non-expert users. However, enabling accurate,\nreal-time, and context-aware interactions with HVAC systems introduces unique\nchallenges, including the integration of frequently updated sensor data,\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\ntranslate high-level user queries into structured execution instructions, and\nan Agent that performs SQL-based data retrieval, statistical processing, and\nfinal response generation. To address HVAC-specific challenges, JARVIS\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\ndeployment-specific information integration, (2) a parameterized SQL builder\nand executor to improve data access reliability, and (3) a bottom-up planning\nscheme to ensure consistency across multi-stage response generation. We\nevaluate JARVIS using real-world data collected from a commercial HVAC system\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\neffectiveness in delivering accurate and interpretable responses across diverse\nqueries. Results show that JARVIS consistently outperforms baseline and\nablation variants in both automated and user-centered assessments, achieving\nhigh response quality and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-answering (QA) interfaces powered by large language models (LLMs)\npresent a promising direction for improving interactivity with HVAC system\ninsights, particularly for non-expert users. However, enabling accurate,\nreal-time, and context-aware interactions with HVAC systems introduces unique\nchallenges, including the integration of frequently updated sensor data,\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\ntranslate high-level user queries into structured execution instructions, and\nan Agent that performs SQL-based data retrieval, statistical processing, and\nfinal response generation. To address HVAC-specific challenges, JARVIS\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\ndeployment-specific information integration, (2) a parameterized SQL builder\nand executor to improve data access reliability, and (3) a bottom-up planning\nscheme to ensure consistency across multi-stage response generation. We\nevaluate JARVIS using real-world data collected from a commercial HVAC system\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\neffectiveness in delivering accurate and interpretable responses across diverse\nqueries. Results show that JARVIS consistently outperforms baseline and\nablation variants in both automated and user-centered assessments, achieving\nhigh response quality and accuracy."
                },
                "authors": [
                    {
                        "name": "Sungmin Lee"
                    },
                    {
                        "name": "Minju Kang"
                    },
                    {
                        "name": "Joonhee Lee"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Dongju Kim"
                    },
                    {
                        "name": "Jingi Hong"
                    },
                    {
                        "name": "Jun Shin"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "JeongGil Ko"
                    }
                ],
                "author_detail": {
                    "name": "JeongGil Ko"
                },
                "author": "JeongGil Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04746v1",
                "updated": "2025-07-07T08:19:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    19,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:19:08Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    19,
                    8,
                    0,
                    188,
                    0
                ],
                "title": "A Tale of Two Scripts: Transliteration and Post-Correction for\n  Judeo-Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tale of Two Scripts: Transliteration and Post-Correction for\n  Judeo-Arabic"
                },
                "summary": "Judeo-Arabic refers to Arabic variants historically spoken by Jewish\ncommunities across the Arab world, primarily during the Middle Ages. Unlike\nstandard Arabic, it is written in Hebrew script by Jewish writers and for\nJewish audiences. Transliterating Judeo-Arabic into Arabic script is\nchallenging due to ambiguous letter mappings, inconsistent orthographic\nconventions, and frequent code-switching into Hebrew and Aramaic. In this\npaper, we introduce a two-step approach to automatically transliterate\nJudeo-Arabic into Arabic script: simple character-level mapping followed by\npost-correction to address grammatical and orthographic errors. We also present\nthe first benchmark evaluation of LLMs on this task. Finally, we show that\ntransliteration enables Arabic NLP tools to perform morphosyntactic tagging and\nmachine translation, which would have not been feasible on the original texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judeo-Arabic refers to Arabic variants historically spoken by Jewish\ncommunities across the Arab world, primarily during the Middle Ages. Unlike\nstandard Arabic, it is written in Hebrew script by Jewish writers and for\nJewish audiences. Transliterating Judeo-Arabic into Arabic script is\nchallenging due to ambiguous letter mappings, inconsistent orthographic\nconventions, and frequent code-switching into Hebrew and Aramaic. In this\npaper, we introduce a two-step approach to automatically transliterate\nJudeo-Arabic into Arabic script: simple character-level mapping followed by\npost-correction to address grammatical and orthographic errors. We also present\nthe first benchmark evaluation of LLMs on this task. Finally, we show that\ntransliteration enables Arabic NLP tools to perform morphosyntactic tagging and\nmachine translation, which would have not been feasible on the original texts."
                },
                "authors": [
                    {
                        "name": "Juan Moreno Gonzalez"
                    },
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Nizar Habash"
                    }
                ],
                "author_detail": {
                    "name": "Nizar Habash"
                },
                "author": "Nizar Habash",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04742v2",
                "updated": "2025-07-08T02:54:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    54,
                    20,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T08:16:54Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    16,
                    54,
                    0,
                    188,
                    0
                ],
                "title": "Activation Steering for Chain-of-Thought Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering for Chain-of-Thought Compression"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning when they include\nintermediate steps, known as \"chains of thought\" (CoTs). However, these\nrationales are often overly verbose, even for simple problems, leading to\nwasted context, increased latency, and higher energy consumption. We observe\nthat verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct\nregions in the model's residual-stream activation space. By extracting and\ninjecting a \"steering vector\" to transition between these modes, we can\nreliably shift generation toward more concise reasoning, effectively\ncompressing CoTs without retraining. We formalize this approach as\nActivation-Steered Compression (ASC), an inference-time technique that shortens\nreasoning traces by directly modifying hidden representations. In addition, we\nprovide a theoretical analysis of the impact of ASC on the output distribution,\nderived from a closed-form KL-divergence-bounded constraint to regulate\nsteering strength. Using only 100 paired verbose and concise examples, ASC\nachieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,\nwhile maintaining accuracy across 7B, 8B, and 32B parameter models. As a\ntraining-free method, ASC introduces negligible runtime overhead and, on\nMATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock\ntime on an 8B model. This makes ASC a practical and efficient tool for\nstreamlining the deployment of reasoning-capable LLMs in latency- or\ncost-sensitive settings. The code is available at:\nhttps://github.com/ArminAzizi98/ASC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning when they include\nintermediate steps, known as \"chains of thought\" (CoTs). However, these\nrationales are often overly verbose, even for simple problems, leading to\nwasted context, increased latency, and higher energy consumption. We observe\nthat verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct\nregions in the model's residual-stream activation space. By extracting and\ninjecting a \"steering vector\" to transition between these modes, we can\nreliably shift generation toward more concise reasoning, effectively\ncompressing CoTs without retraining. We formalize this approach as\nActivation-Steered Compression (ASC), an inference-time technique that shortens\nreasoning traces by directly modifying hidden representations. In addition, we\nprovide a theoretical analysis of the impact of ASC on the output distribution,\nderived from a closed-form KL-divergence-bounded constraint to regulate\nsteering strength. Using only 100 paired verbose and concise examples, ASC\nachieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,\nwhile maintaining accuracy across 7B, 8B, and 32B parameter models. As a\ntraining-free method, ASC introduces negligible runtime overhead and, on\nMATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock\ntime on an 8B model. This makes ASC a practical and efficient tool for\nstreamlining the deployment of reasoning-capable LLMs in latency- or\ncost-sensitive settings. The code is available at:\nhttps://github.com/ArminAzizi98/ASC"
                },
                "authors": [
                    {
                        "name": "Seyedarmin Azizi"
                    },
                    {
                        "name": "Erfan Baghaei Potraghloo"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04736v1",
                "updated": "2025-07-07T08:08:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    8,
                    20,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T08:08:20Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    8,
                    8,
                    20,
                    0,
                    188,
                    0
                ],
                "title": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical\n  Reward-Driven Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical\n  Reward-Driven Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) show significant potential for automating\nRegister-Transfer Level (RTL) code generation. However, current approaches face\na critical challenge: they can not simultaneously optimize for functional\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\nbased on supervised fine-tuning often generate functionally correct but\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\ncontrast, post-processing techniques that attempt to improve PPA metrics after\ngeneration are often inefficient because they operate externally without\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\ndesign capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\nreinforcement learning framework to train LLMs to generate RTL code that\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\nemploys a hierarchical reward system, which incorporates direct feedback on\nsyntax, functional correctness (from simulators) and PPA metrics (from\nsynthesis tools) during reinforcement learning. This enables the model to learn\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\ngenerated 27 RTL designs surpassing the PPA metrics of the original\nhuman-written code. Our findings demonstrate the effectiveness of integrating\ntoolchain feedback into LLM training and highlight the potential for\nreinforcement learning to enable automated generation of human-surpassing RTL\ncode. We open-source our code in anonymous github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show significant potential for automating\nRegister-Transfer Level (RTL) code generation. However, current approaches face\na critical challenge: they can not simultaneously optimize for functional\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\nbased on supervised fine-tuning often generate functionally correct but\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\ncontrast, post-processing techniques that attempt to improve PPA metrics after\ngeneration are often inefficient because they operate externally without\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\ndesign capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\nreinforcement learning framework to train LLMs to generate RTL code that\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\nemploys a hierarchical reward system, which incorporates direct feedback on\nsyntax, functional correctness (from simulators) and PPA metrics (from\nsynthesis tools) during reinforcement learning. This enables the model to learn\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\ngenerated 27 RTL designs surpassing the PPA metrics of the original\nhuman-written code. Our findings demonstrate the effectiveness of integrating\ntoolchain feedback into LLM training and highlight the potential for\nreinforcement learning to enable automated generation of human-surpassing RTL\ncode. We open-source our code in anonymous github."
                },
                "authors": [
                    {
                        "name": "Zhirong Chen"
                    },
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Zhuolin Li"
                    },
                    {
                        "name": "Xinyang He"
                    },
                    {
                        "name": "Chujie Chen"
                    },
                    {
                        "name": "Cangyuan Li"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04733v1",
                "updated": "2025-07-07T07:58:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    58,
                    15,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T07:58:15Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    58,
                    15,
                    0,
                    188,
                    0
                ],
                "title": "\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization"
                },
                "summary": "Product recommendations inherently involve comparisons, yet traditional\nopinion summarization often fails to provide holistic comparative insights. We\npropose the novel task of generating Query-Focused Comparative Explainable\nSummaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address\nthe lack of query-focused recommendation datasets, we introduce MS-Q2P,\ncomprising 7,500 queries mapped to 22,500 recommended products with metadata.\nWe leverage Large Language Models (LLMs) to generate tabular comparative\nsummaries with query-specific explanations. Our approach is personalized,\nprivacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS\nas an intermediate step reduces inference latency approximately by 40% compared\nto the direct input approach (DIA), which processes raw data directly. We\nevaluate open-source and proprietary LLMs for generating and assessing QF-CES.\nExtensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,\nfaithfulness, informativeness, format adherence, and query relevance) showed an\naverage Spearman correlation of 0.74 with human judgments, indicating its\npotential for QF-CES evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product recommendations inherently involve comparisons, yet traditional\nopinion summarization often fails to provide holistic comparative insights. We\npropose the novel task of generating Query-Focused Comparative Explainable\nSummaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address\nthe lack of query-focused recommendation datasets, we introduce MS-Q2P,\ncomprising 7,500 queries mapped to 22,500 recommended products with metadata.\nWe leverage Large Language Models (LLMs) to generate tabular comparative\nsummaries with query-specific explanations. Our approach is personalized,\nprivacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS\nas an intermediate step reduces inference latency approximately by 40% compared\nto the direct input approach (DIA), which processes raw data directly. We\nevaluate open-source and proprietary LLMs for generating and assessing QF-CES.\nExtensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,\nfaithfulness, informativeness, format adherence, and query relevance) showed an\naverage Spearman correlation of 0.74 with human judgments, indicating its\npotential for QF-CES evaluation."
                },
                "authors": [
                    {
                        "name": "Arnav Attri"
                    },
                    {
                        "name": "Anuj Attri"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Amey Patil"
                    },
                    {
                        "name": "Muthusamy Chelliah"
                    },
                    {
                        "name": "Nikesh Garera"
                    }
                ],
                "author_detail": {
                    "name": "Nikesh Garera"
                },
                "author": "Nikesh Garera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; I.2.7; H.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22139v2",
                "updated": "2025-07-07T07:52:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    52,
                    11,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-27T11:30:51Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    30,
                    51,
                    4,
                    178,
                    0
                ],
                "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Shaojie Zhang"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Jianqin Yin"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01050v2",
                "updated": "2025-07-07T07:48:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    48,
                    5,
                    0,
                    188,
                    0
                ],
                "published": "2025-06-23T05:48:10Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    5,
                    48,
                    10,
                    0,
                    174,
                    0
                ],
                "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Detoxification: Data Efficiency, Semantic Preservation and Model\n  Generalization"
                },
                "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at: https://github.com/allacnobug/Detoxification-of-Text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at: https://github.com/allacnobug/Detoxification-of-Text."
                },
                "authors": [
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "name": "Wenming Shao"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04724v1",
                "updated": "2025-07-07T07:34:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    34,
                    34,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T07:34:34Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    34,
                    34,
                    0,
                    188,
                    0
                ],
                "title": "Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents\n  in LLM-Based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents\n  in LLM-Based Multi-Agent Systems"
                },
                "summary": "Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate\nremarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit\nstrong collaborative abilities, the security risks in their communication and\ncoordination remain underexplored. We bridge this gap by systematically\ninvestigating intention-hiding threats in LLM-MAS, and design four\nrepresentative attack paradigms that subtly disrupt task completion while\nmaintaining high concealment. These attacks are evaluated in centralized,\ndecentralized, and layered communication structures. Experiments conducted on\nsix benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,\nand biographies, demonstrate that they exhibit strong disruptive capabilities.\nTo identify these threats, we propose a psychology-based detection framework\nAgentXposed, which combines the HEXACO personality model with the Reid\nTechnique, using progressive questionnaire inquiries and behavior-based\nmonitoring. Experiments conducted on six types of attacks show that our\ndetection framework effectively identifies all types of malicious behaviors.\nThe detection rate for our intention-hiding attacks is slightly lower than that\nof the two baselines, Incorrect Fact Injection and Dark Traits Injection,\ndemonstrating the effectiveness of intention concealment. Our findings reveal\nthe structural and behavioral risks posed by intention-hiding attacks and offer\nvaluable insights into securing LLM-based multi-agent systems through\npsychological perspectives, which contributes to a deeper understanding of\nmulti-agent safety. The code and data are available at\nhttps://anonymous.4open.science/r/AgentXposed-F814.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate\nremarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit\nstrong collaborative abilities, the security risks in their communication and\ncoordination remain underexplored. We bridge this gap by systematically\ninvestigating intention-hiding threats in LLM-MAS, and design four\nrepresentative attack paradigms that subtly disrupt task completion while\nmaintaining high concealment. These attacks are evaluated in centralized,\ndecentralized, and layered communication structures. Experiments conducted on\nsix benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,\nand biographies, demonstrate that they exhibit strong disruptive capabilities.\nTo identify these threats, we propose a psychology-based detection framework\nAgentXposed, which combines the HEXACO personality model with the Reid\nTechnique, using progressive questionnaire inquiries and behavior-based\nmonitoring. Experiments conducted on six types of attacks show that our\ndetection framework effectively identifies all types of malicious behaviors.\nThe detection rate for our intention-hiding attacks is slightly lower than that\nof the two baselines, Incorrect Fact Injection and Dark Traits Injection,\ndemonstrating the effectiveness of intention concealment. Our findings reveal\nthe structural and behavioral risks posed by intention-hiding attacks and offer\nvaluable insights into securing LLM-based multi-agent systems through\npsychological perspectives, which contributes to a deeper understanding of\nmulti-agent safety. The code and data are available at\nhttps://anonymous.4open.science/r/AgentXposed-F814."
                },
                "authors": [
                    {
                        "name": "Yizhe Xie"
                    },
                    {
                        "name": "Congcong Zhu"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Minghao Wang"
                    },
                    {
                        "name": "Chi Liu"
                    },
                    {
                        "name": "Minglu Zhu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tianqing Zhu"
                },
                "author": "Tianqing Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04723v1",
                "updated": "2025-07-07T07:33:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    33,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T07:33:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    33,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework"
                },
                "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io"
                },
                "authors": [
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Baibei Ji"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Keyan Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12913v2",
                "updated": "2025-07-07T07:19:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    7,
                    19,
                    42,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-17T13:02:44Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    2,
                    44,
                    3,
                    107,
                    0
                ],
                "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAIN: Mutual Alignment Is Necessary for instruction tuning"
                },
                "summary": "Instruction tuning has empowered large language models (LLMs) to achieve\nremarkable performance, yet its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. To meet this demand,\nvarious methods have been developed to synthesize data at scale. However,\ncurrent methods for scaling up data generation often overlook a crucial aspect:\nthe alignment between instructions and responses. We hypothesize that the\nquality of instruction-response pairs is determined not by the individual\nquality of each component, but by the degree of mutual alignment. To address\nthis, we propose a Mutual Alignment Framework (MAIN) which enforces coherence\nbetween instructions and responses through mutual constraints. We demonstrate\nthat MAIN generalizes well across model architectures and sizes, achieving\nstate-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse\nbenchmarks. This work underscores the critical role of instruction-response\nalignment in enabling generalizable and high-quality instruction tuning for\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has empowered large language models (LLMs) to achieve\nremarkable performance, yet its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. To meet this demand,\nvarious methods have been developed to synthesize data at scale. However,\ncurrent methods for scaling up data generation often overlook a crucial aspect:\nthe alignment between instructions and responses. We hypothesize that the\nquality of instruction-response pairs is determined not by the individual\nquality of each component, but by the degree of mutual alignment. To address\nthis, we propose a Mutual Alignment Framework (MAIN) which enforces coherence\nbetween instructions and responses through mutual constraints. We demonstrate\nthat MAIN generalizes well across model architectures and sizes, achieving\nstate-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse\nbenchmarks. This work underscores the critical role of instruction-response\nalignment in enabling generalizable and high-quality instruction tuning for\nLLMs."
                },
                "authors": [
                    {
                        "name": "Fanyi Yang"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Xixin Cao"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Weiwei Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04708v1",
                "updated": "2025-07-07T06:59:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    59,
                    37,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:59:37Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    59,
                    37,
                    0,
                    188,
                    0
                ],
                "title": "Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion\n  Triggers in E-commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion\n  Triggers in E-commerce"
                },
                "summary": "Customer reviews on e-commerce platforms capture critical affective signals\nthat drive purchasing decisions. However, no existing research has explored the\njoint task of emotion detection and explanatory span identification in\ne-commerce reviews - a crucial gap in understanding what triggers customer\nemotional responses. To bridge this gap, we propose a novel joint task unifying\nEmotion detection and Opinion Trigger extraction (EOT), which explicitly models\nthe relationship between causal text spans (opinion triggers) and affective\ndimensions (emotion categories) grounded in Plutchik's theory of 8 primary\nemotions. In the absence of labeled data, we introduce EOT-X, a human-annotated\ncollection of 2,400 reviews with fine-grained emotions and opinion triggers. We\nevaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured\nprompting framework with systematic reasoning and self-reflection. Our\nframework surpasses zero-shot and chain-of-thought techniques, across\ne-commerce domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer reviews on e-commerce platforms capture critical affective signals\nthat drive purchasing decisions. However, no existing research has explored the\njoint task of emotion detection and explanatory span identification in\ne-commerce reviews - a crucial gap in understanding what triggers customer\nemotional responses. To bridge this gap, we propose a novel joint task unifying\nEmotion detection and Opinion Trigger extraction (EOT), which explicitly models\nthe relationship between causal text spans (opinion triggers) and affective\ndimensions (emotion categories) grounded in Plutchik's theory of 8 primary\nemotions. In the absence of labeled data, we introduce EOT-X, a human-annotated\ncollection of 2,400 reviews with fine-grained emotions and opinion triggers. We\nevaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured\nprompting framework with systematic reasoning and self-reflection. Our\nframework surpasses zero-shot and chain-of-thought techniques, across\ne-commerce domains."
                },
                "authors": [
                    {
                        "name": "Arnav Attri"
                    },
                    {
                        "name": "Anuj Attri"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Amey Patil"
                    },
                    {
                        "name": "Muthusamy Chelliah"
                    },
                    {
                        "name": "Nikesh Garera"
                    }
                ],
                "author_detail": {
                    "name": "Nikesh Garera"
                },
                "author": "Nikesh Garera",
                "arxiv_comment": "23 pages, 11 figures, 7 tables. Dataset and code will be made\n  publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04706v1",
                "updated": "2025-07-07T06:57:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    57,
                    34,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:57:34Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    57,
                    34,
                    0,
                    188,
                    0
                ],
                "title": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced\n  Retrieval-Augmented Generation and Multilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced\n  Retrieval-Augmented Generation and Multilevel Optimization"
                },
                "summary": "Urban general intelligence (UGI) refers to the capacity of AI systems to\nautonomously perceive, reason, and act within dynamic and complex urban\nenvironments. In this paper, we introduce UrbanMind, a tool-enhanced\nretrieval-augmented generation (RAG) framework designed to facilitate UGI.\nCentral to UrbanMind is a novel architecture based on Continual\nRetrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates\ndomain-specific knowledge and evolving urban data to support long-term\nadaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel\noptimization framework, where different layers are treated as interdependent\nsub-problems. Each layer has distinct objectives and can be optimized either\nindependently or jointly through a hierarchical learning process. The framework\nis highly flexible, supporting both end-to-end training and partial layer-wise\noptimization based on resource or deployment constraints. To remain adaptive\nunder data drift, it is further integrated with an incremental corpus updating\nmechanism. Evaluations on real-world urban tasks of a variety of complexity\nverify the effectiveness of the proposed framework. This work presents a\npromising step toward the realization of general-purpose LLM agents in future\nurban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban general intelligence (UGI) refers to the capacity of AI systems to\nautonomously perceive, reason, and act within dynamic and complex urban\nenvironments. In this paper, we introduce UrbanMind, a tool-enhanced\nretrieval-augmented generation (RAG) framework designed to facilitate UGI.\nCentral to UrbanMind is a novel architecture based on Continual\nRetrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates\ndomain-specific knowledge and evolving urban data to support long-term\nadaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel\noptimization framework, where different layers are treated as interdependent\nsub-problems. Each layer has distinct objectives and can be optimized either\nindependently or jointly through a hierarchical learning process. The framework\nis highly flexible, supporting both end-to-end training and partial layer-wise\noptimization based on resource or deployment constraints. To remain adaptive\nunder data drift, it is further integrated with an incremental corpus updating\nmechanism. Evaluations on real-world urban tasks of a variety of complexity\nverify the effectiveness of the proposed framework. This work presents a\npromising step toward the realization of general-purpose LLM agents in future\nurban environments."
                },
                "authors": [
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Zelin Zhu"
                    },
                    {
                        "name": "Chengtao Jian"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Shengjie Zhao"
                    },
                    {
                        "name": "Xiaozhou Ye"
                    },
                    {
                        "name": "Ye Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ouyang"
                },
                "author": "Ye Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04701v1",
                "updated": "2025-07-07T06:50:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    50,
                    46,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:50:46Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    50,
                    46,
                    0,
                    188,
                    0
                ],
                "title": "XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL"
                },
                "summary": "To leverage the advantages of LLM in addressing challenges in the Text-to-SQL\ntask, we present XiYan-SQL, an innovative framework effectively generating and\nutilizing multiple SQL candidates. It consists of three components: 1) a Schema\nFilter module filtering and obtaining multiple relevant schemas; 2) a\nmulti-generator ensemble approach generating multiple highquality and diverse\nSQL queries; 3) a selection model with a candidate reorganization strategy\nimplemented to obtain the optimal SQL query. Specifically, for the\nmulti-generator ensemble, we employ a multi-task fine-tuning strategy to\nenhance the capabilities of SQL generation models for the intrinsic alignment\nbetween SQL and text, and construct multiple generation models with distinct\ngeneration styles by fine-tuning across different SQL formats. The experimental\nresults and comprehensive analysis demonstrate the effectiveness and robustness\nof our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63%\non the notable BIRD benchmark, surpassing all previous methods. It also attains\nSOTA performance on the Spider test set with an accuracy of 89.65%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To leverage the advantages of LLM in addressing challenges in the Text-to-SQL\ntask, we present XiYan-SQL, an innovative framework effectively generating and\nutilizing multiple SQL candidates. It consists of three components: 1) a Schema\nFilter module filtering and obtaining multiple relevant schemas; 2) a\nmulti-generator ensemble approach generating multiple highquality and diverse\nSQL queries; 3) a selection model with a candidate reorganization strategy\nimplemented to obtain the optimal SQL query. Specifically, for the\nmulti-generator ensemble, we employ a multi-task fine-tuning strategy to\nenhance the capabilities of SQL generation models for the intrinsic alignment\nbetween SQL and text, and construct multiple generation models with distinct\ngeneration styles by fine-tuning across different SQL formats. The experimental\nresults and comprehensive analysis demonstrate the effectiveness and robustness\nof our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63%\non the notable BIRD benchmark, surpassing all previous methods. It also attains\nSOTA performance on the Spider test set with an accuracy of 89.65%."
                },
                "authors": [
                    {
                        "name": "Yifu Liu"
                    },
                    {
                        "name": "Yin Zhu"
                    },
                    {
                        "name": "Yingqi Gao"
                    },
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Xiaoxia Li"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Yuntao Hong"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12112v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12112v4",
                "updated": "2025-07-07T06:48:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    48,
                    3,
                    0,
                    188,
                    0
                ],
                "published": "2024-08-22T03:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    54,
                    8,
                    3,
                    235,
                    0
                ],
                "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards"
                },
                "summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Shresth Verma"
                    },
                    {
                        "name": "Niclas Boehmer"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12112v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12112v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13860v2",
                "updated": "2025-07-07T06:34:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    34,
                    7,
                    0,
                    188,
                    0
                ],
                "published": "2025-05-20T03:12:21Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    12,
                    21,
                    1,
                    140,
                    0
                ],
                "title": "Domain Adaptation of VLM for Soccer Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of VLM for Soccer Video Understanding"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask."
                },
                "authors": [
                    {
                        "name": "Tiancheng Jiang"
                    },
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Parmida Atighehchian"
                    },
                    {
                        "name": "Shinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shinan Zhang"
                },
                "author": "Shinan Zhang",
                "arxiv_comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04687v1",
                "updated": "2025-07-07T06:08:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    8,
                    45,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:08:45Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    8,
                    45,
                    0,
                    188,
                    0
                ],
                "title": "AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes"
                },
                "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships."
                },
                "authors": [
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Chuan Lei"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    },
                    {
                        "name": "Xiao Qin"
                    },
                    {
                        "name": "Christos Faloutsos"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    }
                ],
                "author_detail": {
                    "name": "Huzefa Rangwala"
                },
                "author": "Huzefa Rangwala",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01932v3",
                "updated": "2025-07-07T06:06:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    6,
                    46,
                    0,
                    188,
                    0
                ],
                "published": "2023-05-03T07:13:47Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    7,
                    13,
                    47,
                    2,
                    123,
                    0
                ],
                "title": "Fully Automatic Neural Network Reduction for Formal Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Automatic Neural Network Reduction for Formal Verification"
                },
                "summary": "Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems under strict time constraints. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. Our sound\nreduction approach is applicable to neural networks with any type of\nelement-wise activation function, such as ReLU, sigmoid, and tanh. The network\nreduction is computed on the fly while simultaneously verifying the original\nnetwork and its specification. All parameters are automatically tuned to\nminimize the network size without compromising verifiability. We further show\nthe applicability of our approach to convolutional neural networks by\nexplicitly exploiting similar neighboring pixels. Our evaluation shows that our\napproach reduces large neural networks to a fraction of the original number of\nneurons and thus shortens the verification time to a similar degree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems under strict time constraints. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. Our sound\nreduction approach is applicable to neural networks with any type of\nelement-wise activation function, such as ReLU, sigmoid, and tanh. The network\nreduction is computed on the fly while simultaneously verifying the original\nnetwork and its specification. All parameters are automatically tuned to\nminimize the network size without compromising verifiability. We further show\nthe applicability of our approach to convolutional neural networks by\nexplicitly exploiting similar neighboring pixels. Our evaluation shows that our\napproach reduces large neural networks to a fraction of the original number of\nneurons and thus shortens the verification time to a similar degree."
                },
                "authors": [
                    {
                        "name": "Tobias Ladner"
                    },
                    {
                        "name": "Matthias Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Althoff"
                },
                "author": "Matthias Althoff",
                "arxiv_comment": "published at Transactions on Machine Learning Research (TMLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04673v1",
                "updated": "2025-07-07T05:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    35,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T05:35:21Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    35,
                    21,
                    0,
                    188,
                    0
                ],
                "title": "Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by\n  Forging Assistant Message",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by\n  Forging Assistant Message"
                },
                "summary": "The rise of conversational interfaces has greatly enhanced LLM usability by\nleveraging dialogue history for sophisticated reasoning. However, this reliance\nintroduces an unexplored attack surface. This paper introduces Trojan Horse\nPrompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by\nforging the model's own past utterances within the conversational history\nprovided to its API. A malicious payload is injected into a model-attributed\nmessage, followed by a benign user prompt to trigger harmful content\ngeneration. This vulnerability stems from Asymmetric Safety Alignment: models\nare extensively trained to refuse harmful user requests but lack comparable\nskepticism towards their own purported conversational history. This implicit\ntrust in its \"past\" creates a high-impact vulnerability. Experimental\nvalidation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan\nHorse Prompting achieves a significantly higher Attack Success Rate (ASR) than\nestablished user-turn jailbreaking methods. These findings reveal a fundamental\nflaw in modern conversational AI security, necessitating a paradigm shift from\ninput-level filtering to robust, protocol-level validation of conversational\ncontext integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of conversational interfaces has greatly enhanced LLM usability by\nleveraging dialogue history for sophisticated reasoning. However, this reliance\nintroduces an unexplored attack surface. This paper introduces Trojan Horse\nPrompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by\nforging the model's own past utterances within the conversational history\nprovided to its API. A malicious payload is injected into a model-attributed\nmessage, followed by a benign user prompt to trigger harmful content\ngeneration. This vulnerability stems from Asymmetric Safety Alignment: models\nare extensively trained to refuse harmful user requests but lack comparable\nskepticism towards their own purported conversational history. This implicit\ntrust in its \"past\" creates a high-impact vulnerability. Experimental\nvalidation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan\nHorse Prompting achieves a significantly higher Attack Success Rate (ASR) than\nestablished user-turn jailbreaking methods. These findings reveal a fundamental\nflaw in modern conversational AI security, necessitating a paradigm shift from\ninput-level filtering to robust, protocol-level validation of conversational\ncontext integrity."
                },
                "authors": [
                    {
                        "name": "Wei Duan"
                    },
                    {
                        "name": "Li Qian"
                    }
                ],
                "author_detail": {
                    "name": "Li Qian"
                },
                "author": "Li Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15484v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15484v4",
                "updated": "2025-07-07T05:27:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    27,
                    8,
                    0,
                    188,
                    0
                ],
                "published": "2024-12-20T01:37:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    1,
                    37,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage"
                },
                "summary": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions. Our code and data are available at\nhttps://github.com/adobe-research/CapMAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions. Our code and data are available at\nhttps://github.com/adobe-research/CapMAS."
                },
                "authors": [
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Jing Shi"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15484v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15484v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04671v1",
                "updated": "2025-07-07T05:22:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    22,
                    55,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T05:22:55Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    22,
                    55,
                    0,
                    188,
                    0
                ],
                "title": "DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and\n  Continuous Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and\n  Continuous Adaptation"
                },
                "summary": "Neural Architecture Search (NAS) has emerged as a powerful approach for\nautomating neural network design. However, existing NAS methods face critical\nlimitations in real-world deployments: architectures lack adaptability across\nscenarios, each deployment context requires costly separate searches, and\nperformance consistency across diverse platforms remains challenging. We\npropose DANCE (Dynamic Architectures with Neural Continuous Evolution), which\nreformulates architecture search as a continuous evolution problem through\nlearning distributions over architectural components. DANCE introduces three\nkey innovations: a continuous architecture distribution enabling smooth\nadaptation, a unified architecture space with learned selection gates for\nefficient sampling, and a multi-stage training strategy for effective\ndeployment optimization. Extensive experiments across five datasets demonstrate\nDANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS\napproaches in terms of accuracy while significantly reducing search costs.\nUnder varying computational constraints, DANCE maintains robust performance\nwhile smoothly adapting architectures to different hardware requirements. The\ncode and appendix can be found at\nhttps://github.com/Applied-Machine-Learning-Lab/DANCE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) has emerged as a powerful approach for\nautomating neural network design. However, existing NAS methods face critical\nlimitations in real-world deployments: architectures lack adaptability across\nscenarios, each deployment context requires costly separate searches, and\nperformance consistency across diverse platforms remains challenging. We\npropose DANCE (Dynamic Architectures with Neural Continuous Evolution), which\nreformulates architecture search as a continuous evolution problem through\nlearning distributions over architectural components. DANCE introduces three\nkey innovations: a continuous architecture distribution enabling smooth\nadaptation, a unified architecture space with learned selection gates for\nefficient sampling, and a multi-stage training strategy for effective\ndeployment optimization. Extensive experiments across five datasets demonstrate\nDANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS\napproaches in terms of accuracy while significantly reducing search costs.\nUnder varying computational constraints, DANCE maintains robust performance\nwhile smoothly adapting architectures to different hardware requirements. The\ncode and appendix can be found at\nhttps://github.com/Applied-Machine-Learning-Lab/DANCE."
                },
                "authors": [
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Tianshuo Wei"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Shanshan Ye"
                    },
                    {
                        "name": "Lixin Zou"
                    },
                    {
                        "name": "Xuetao Wei"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "Accepted by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01431v2",
                "updated": "2025-07-07T05:10:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    10,
                    47,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-02T07:33:19Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    33,
                    19,
                    2,
                    183,
                    0
                ],
                "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading"
                },
                "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."
                },
                "authors": [
                    {
                        "name": "Yoonseok Yang"
                    },
                    {
                        "name": "Minjune Kim"
                    },
                    {
                        "name": "Marlon Rondinelli"
                    },
                    {
                        "name": "Keren Shao"
                    }
                ],
                "author_detail": {
                    "name": "Keren Shao"
                },
                "author": "Keren Shao",
                "arxiv_comment": "7 pages, 5 figues, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]