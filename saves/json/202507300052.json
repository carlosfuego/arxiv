[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v1",
                "updated": "2025-07-28T16:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v1",
                "updated": "2025-07-26T10:34:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v1",
                "updated": "2025-07-25T23:55:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v2",
                "updated": "2025-07-24T17:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    20,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pál András Papp"
                    },
                    {
                        "name": "Toni Böhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristóbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "José María Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muñoz"
                    },
                    {
                        "name": "Manuel Gil Pérez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anaïs Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.21053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21053v1",
                "updated": "2025-07-28T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Flow Matching Policy Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow Matching Policy Gradients"
                },
                "summary": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings."
                },
                "authors": [
                    {
                        "name": "David McAllister"
                    },
                    {
                        "name": "Songwei Ge"
                    },
                    {
                        "name": "Brent Yi"
                    },
                    {
                        "name": "Chung Min Kim"
                    },
                    {
                        "name": "Ethan Weber"
                    },
                    {
                        "name": "Hongsuk Choi"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "arxiv_comment": "See our blog post: https://flowreinforce.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v1",
                "updated": "2025-07-28T17:59:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenghailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21040v1",
                "updated": "2025-07-28T17:56:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    56,
                    34,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:56:34Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    56,
                    34,
                    0,
                    209,
                    0
                ],
                "title": "Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps:\n  An Interpretation and Potential Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps:\n  An Interpretation and Potential Improvements"
                },
                "summary": "We propose a probabilistic interpretation of transformers as unrolled\ninference steps assuming a probabilistic Laplacian Eigenmaps model from the\nProbDR framework. Our derivation shows that at initialisation, transformers\nperform \"linear\" dimensionality reduction. We also show that within the\ntransformer block, a graph Laplacian term arises from our arguments, rather\nthan an attention matrix (which we interpret as an adjacency matrix). We\ndemonstrate that simply subtracting the identity from the attention matrix (and\nthereby taking a graph diffusion step) improves validation performance on a\nlanguage model and a simple vision transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a probabilistic interpretation of transformers as unrolled\ninference steps assuming a probabilistic Laplacian Eigenmaps model from the\nProbDR framework. Our derivation shows that at initialisation, transformers\nperform \"linear\" dimensionality reduction. We also show that within the\ntransformer block, a graph Laplacian term arises from our arguments, rather\nthan an attention matrix (which we interpret as an adjacency matrix). We\ndemonstrate that simply subtracting the identity from the attention matrix (and\nthereby taking a graph diffusion step) improves validation performance on a\nlanguage model and a simple vision transformer."
                },
                "authors": [
                    {
                        "name": "Aditya Ravuri"
                    },
                    {
                        "name": "Neil D. Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil D. Lawrence"
                },
                "author": "Neil D. Lawrence",
                "arxiv_comment": "Initial version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21035v1",
                "updated": "2025-07-28T17:55:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis"
                },
                "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "51 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21028v1",
                "updated": "2025-07-28T17:48:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    48,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:48:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    48,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with\n  Multi-Dimensional Human Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with\n  Multi-Dimensional Human Evaluation"
                },
                "summary": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Huimin Zeng"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12854v3",
                "updated": "2025-07-28T17:46:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    46,
                    50,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-17T06:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    28,
                    25,
                    0,
                    76,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation"
                },
                "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations."
                },
                "authors": [
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Jiahao Lin"
                    },
                    {
                        "name": "Xiangyu Tian"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Dongmei Jiang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "arxiv_comment": "23pages",
                "arxiv_journal_ref": "COLM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21017v1",
                "updated": "2025-07-28T17:38:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    38,
                    29,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:38:29Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    38,
                    29,
                    0,
                    209,
                    0
                ],
                "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them"
                },
                "summary": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments."
                },
                "authors": [
                    {
                        "name": "Weichen Zhang"
                    },
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Pohao Huang"
                    },
                    {
                        "name": "Jiayue Pu"
                    },
                    {
                        "name": "Heyue Lin"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "Code and data: https://github.com/sunblaze-ucb/mirage-bench.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04873v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04873v4",
                "updated": "2025-07-29T01:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    1,
                    45,
                    34,
                    1,
                    210,
                    0
                ],
                "published": "2025-01-08T23:07:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    7,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Back Home: A Computer Vision Solution to Seashell Identification for\n  Ecological Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back Home: A Computer Vision Solution to Seashell Identification for\n  Ecological Restoration"
                },
                "summary": "Illegal souvenir collection strips an estimated five tonnes of seashells from\nCosta Rica's beaches each year. Yet, once these specimens are seized, their\ncoastal origin -- Pacific or Caribbean -- cannot be verified easily due to the\nlack of information, preventing their return when confiscated by local\nauthorities. To solve this issue, we introduce BackHome19K, the first\nlarge-scale image corpus (19,058 photographs, 516 species) annotated with\ncoast-level labels, and propose a lightweight pipeline that infers provenance\nin real time on a mobile-grade CPU. A trained anomaly filter pre-screens\nuploads, increasing robustness to user-generated noise. On a held-out test set,\nthe classifier attains 86.3% balanced accuracy, while the filter rejects 93% of\n180 out-of-domain objects with zero false negatives. Deployed as a web\napplication, the system has already processed 70,000 shells for wildlife\nofficers in under three seconds per image, enabling confiscated specimens to be\nsafely repatriated to their native ecosystems. The dataset is available at\nhttps://huggingface.co/datasets/FIFCO/BackHome19K",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illegal souvenir collection strips an estimated five tonnes of seashells from\nCosta Rica's beaches each year. Yet, once these specimens are seized, their\ncoastal origin -- Pacific or Caribbean -- cannot be verified easily due to the\nlack of information, preventing their return when confiscated by local\nauthorities. To solve this issue, we introduce BackHome19K, the first\nlarge-scale image corpus (19,058 photographs, 516 species) annotated with\ncoast-level labels, and propose a lightweight pipeline that infers provenance\nin real time on a mobile-grade CPU. A trained anomaly filter pre-screens\nuploads, increasing robustness to user-generated noise. On a held-out test set,\nthe classifier attains 86.3% balanced accuracy, while the filter rejects 93% of\n180 out-of-domain objects with zero false negatives. Deployed as a web\napplication, the system has already processed 70,000 shells for wildlife\nofficers in under three seconds per image, enabling confiscated specimens to be\nsafely repatriated to their native ecosystems. The dataset is available at\nhttps://huggingface.co/datasets/FIFCO/BackHome19K"
                },
                "authors": [
                    {
                        "name": "Alexander Valverde"
                    },
                    {
                        "name": "Luis Solano"
                    },
                    {
                        "name": "André Montoya"
                    }
                ],
                "author_detail": {
                    "name": "André Montoya"
                },
                "author": "André Montoya",
                "arxiv_comment": "ICCV 2025 (CV4E Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04873v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04873v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02087v2",
                "updated": "2025-07-28T17:26:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    26,
                    1,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-02T19:02:18Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    19,
                    2,
                    18,
                    2,
                    183,
                    0
                ],
                "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions"
                },
                "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes."
                },
                "authors": [
                    {
                        "name": "Eitan Anzenberg"
                    },
                    {
                        "name": "Arunava Samajpati"
                    },
                    {
                        "name": "Sivasankaran Chandrasekar"
                    },
                    {
                        "name": "Varun Kacholia"
                    }
                ],
                "author_detail": {
                    "name": "Varun Kacholia"
                },
                "author": "Varun Kacholia",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21012v1",
                "updated": "2025-07-28T17:23:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    23,
                    34,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:23:34Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    23,
                    34,
                    0,
                    209,
                    0
                ],
                "title": "User-Centered Design with AI in the Loop: A Case Study of Rapid User\n  Interface Prototyping with \"Vibe Coding\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-Centered Design with AI in the Loop: A Case Study of Rapid User\n  Interface Prototyping with \"Vibe Coding\""
                },
                "summary": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Tanay Maheshwari"
                    },
                    {
                        "name": "Alex Voelker"
                    }
                ],
                "author_detail": {
                    "name": "Alex Voelker"
                },
                "author": "Alex Voelker",
                "arxiv_journal_ref": "ACM Collective Intelligence 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21009v1",
                "updated": "2025-07-28T17:22:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    22,
                    10,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:22:10Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    22,
                    10,
                    0,
                    209,
                    0
                ],
                "title": "Memorization in Fine-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Fine-Tuned Large Language Models"
                },
                "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."
                },
                "authors": [
                    {
                        "name": "Danil Savine"
                    },
                    {
                        "name": "Muni Sreenivas Pydi"
                    },
                    {
                        "name": "Jamal Atif"
                    },
                    {
                        "name": "Olivier Cappé"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Cappé"
                },
                "author": "Olivier Cappé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20999v1",
                "updated": "2025-07-28T17:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."
                },
                "authors": [
                    {
                        "name": "Yining Huang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Keke Tang"
                    },
                    {
                        "name": "Meilian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Meilian Chen"
                },
                "author": "Meilian Chen",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20995v1",
                "updated": "2025-07-28T17:04:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    4,
                    28,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:04:28Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    4,
                    28,
                    0,
                    209,
                    0
                ],
                "title": "VArsity: Can Large Language Models Keep Power Engineering Students in\n  Phase?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VArsity: Can Large Language Models Keep Power Engineering Students in\n  Phase?"
                },
                "summary": "This paper provides an educational case study regarding our experience in\ndeploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023\nofferings of ECE 4320: Power System Analysis and Control at Georgia Tech. As\npart of course assessments, students were tasked with identifying, explaining,\nand correcting errors in the ChatGPT outputs corresponding to power factor\ncorrection problems. While most students successfully identified the errors in\nthe outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found\nthe errors from the ChatGPT o1 version much more difficult to identify in\nSpring 2025. As shown in this case study, the role of LLMs in pedagogy,\nassessment, and learning in power engineering classrooms is an important topic\ndeserving further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an educational case study regarding our experience in\ndeploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023\nofferings of ECE 4320: Power System Analysis and Control at Georgia Tech. As\npart of course assessments, students were tasked with identifying, explaining,\nand correcting errors in the ChatGPT outputs corresponding to power factor\ncorrection problems. While most students successfully identified the errors in\nthe outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found\nthe errors from the ChatGPT o1 version much more difficult to identify in\nSpring 2025. As shown in this case study, the role of LLMs in pedagogy,\nassessment, and learning in power engineering classrooms is an important topic\ndeserving further investigation."
                },
                "authors": [
                    {
                        "name": "Samuel Talkington"
                    },
                    {
                        "name": "Daniel K. Molzahn"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. Molzahn"
                },
                "author": "Daniel K. Molzahn",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14621v2",
                "updated": "2025-07-28T17:02:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    2,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-19T13:38:05Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    13,
                    38,
                    5,
                    5,
                    200,
                    0
                ],
                "title": "Testing Clustered Equal Predictive Ability with Unknown Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Clustered Equal Predictive Ability with Unknown Clusters"
                },
                "summary": "This paper proposes a selective inference procedure for testing equal\npredictive ability in panel data settings with unknown heterogeneity. The\nframework allows predictive performance to vary across unobserved clusters and\naccounts for the data-driven selection of these clusters using the Panel Kmeans\nAlgorithm. A post-selection Wald-type statistic is constructed, and valid\n$p$-values are derived under general forms of autocorrelation and\ncross-sectional dependence in forecast loss differentials. The method\naccommodates conditioning on covariates or common factors and permits both\nstrong and weak dependence across units. Simulations demonstrate the\nfinite-sample validity of the procedure and show that it has very high power.\nAn empirical application to exchange rate forecasting using machine learning\nmethods illustrates the practical relevance of accounting for unknown clusters\nin forecast evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a selective inference procedure for testing equal\npredictive ability in panel data settings with unknown heterogeneity. The\nframework allows predictive performance to vary across unobserved clusters and\naccounts for the data-driven selection of these clusters using the Panel Kmeans\nAlgorithm. A post-selection Wald-type statistic is constructed, and valid\n$p$-values are derived under general forms of autocorrelation and\ncross-sectional dependence in forecast loss differentials. The method\naccommodates conditioning on covariates or common factors and permits both\nstrong and weak dependence across units. Simulations demonstrate the\nfinite-sample validity of the procedure and show that it has very high power.\nAn empirical application to exchange rate forecasting using machine learning\nmethods illustrates the practical relevance of accounting for unknown clusters\nin forecast evaluation."
                },
                "authors": [
                    {
                        "name": "Oguzhan Akgun"
                    },
                    {
                        "name": "Alain Pirotte"
                    },
                    {
                        "name": "Giovanni Urga"
                    },
                    {
                        "name": "Zhenlin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenlin Yang"
                },
                "author": "Zhenlin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20994v1",
                "updated": "2025-07-28T16:59:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    59,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:59:53Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    59,
                    53,
                    0,
                    209,
                    0
                ],
                "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM"
                },
                "summary": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Wujia Niu"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "arxiv_comment": "Codes and data are available at\n  https://github.com/listen0425/Security-Tensors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20993v1",
                "updated": "2025-07-28T16:52:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    52,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:52:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    52,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Personalized Treatment Effect Estimation from Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Treatment Effect Estimation from Unstructured Data"
                },
                "summary": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity."
                },
                "authors": [
                    {
                        "name": "Henri Arno"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20990v1",
                "updated": "2025-07-28T16:50:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:50:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "PyBird-JAX: Accelerated inference in large-scale structure with\n  model-independent emulation of one-loop galaxy power spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyBird-JAX: Accelerated inference in large-scale structure with\n  model-independent emulation of one-loop galaxy power spectra"
                },
                "summary": "We present $\\texttt{PyBird-JAX}$, a differentiable, $\\texttt{JAX}$-based\nimplementation of $\\texttt{PyBird}$, using internal neural network emulators to\naccelerate computationally costly operations for rapid large-scale structure\n(LSS) analysis. $\\texttt{PyBird-JAX}$ computes one-loop EFTofLSS predictions\nfor redshift-space galaxy power spectrum multipoles in 1.2 ms on a CPU and 0.2\nms on a GPU, achieving 3-4 orders of magnitude speed-up over $\\texttt{PyBird}$.\nThe emulators take a compact spline-based representation of the input linear\npower spectrum $P(k)$ as feature vectors, making the approach applicable to a\nwide range of cosmological models. We rigorously validate its accuracy against\nlarge-volume simulations and on BOSS data, including cosmologies not explicitly\nrepresented in the training set. Leveraging automatic differentiation,\n$\\texttt{PyBird-JAX}$ supports Fisher forecasting, Taylor expansion of model\npredictions, gradient-based searches, and vectorised ensemble sampling.\nInterfaced with a variety of samplers and Boltzmann solvers,\n$\\texttt{PyBird-JAX}$ provides a high-performance, end-to-end inference\npipeline. Combined with a symbolic-$P(k)$ generator, a typical Stage-4 LSS MCMC\nconverges in minutes on a GPU. Our results demonstrate that\n$\\texttt{PyBird-JAX}$ delivers the precision and speed required for upcoming\nLSS surveys, opening the door to accelerated cosmological inference with\nminimal accuracy loss and no pretraining. In a companion paper [1], we put\n$\\texttt{PyBird-JAX}$ to use in achieving LSS marginalised constraints free\nfrom volume projection effects through non-flat measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present $\\texttt{PyBird-JAX}$, a differentiable, $\\texttt{JAX}$-based\nimplementation of $\\texttt{PyBird}$, using internal neural network emulators to\naccelerate computationally costly operations for rapid large-scale structure\n(LSS) analysis. $\\texttt{PyBird-JAX}$ computes one-loop EFTofLSS predictions\nfor redshift-space galaxy power spectrum multipoles in 1.2 ms on a CPU and 0.2\nms on a GPU, achieving 3-4 orders of magnitude speed-up over $\\texttt{PyBird}$.\nThe emulators take a compact spline-based representation of the input linear\npower spectrum $P(k)$ as feature vectors, making the approach applicable to a\nwide range of cosmological models. We rigorously validate its accuracy against\nlarge-volume simulations and on BOSS data, including cosmologies not explicitly\nrepresented in the training set. Leveraging automatic differentiation,\n$\\texttt{PyBird-JAX}$ supports Fisher forecasting, Taylor expansion of model\npredictions, gradient-based searches, and vectorised ensemble sampling.\nInterfaced with a variety of samplers and Boltzmann solvers,\n$\\texttt{PyBird-JAX}$ provides a high-performance, end-to-end inference\npipeline. Combined with a symbolic-$P(k)$ generator, a typical Stage-4 LSS MCMC\nconverges in minutes on a GPU. Our results demonstrate that\n$\\texttt{PyBird-JAX}$ delivers the precision and speed required for upcoming\nLSS surveys, opening the door to accelerated cosmological inference with\nminimal accuracy loss and no pretraining. In a companion paper [1], we put\n$\\texttt{PyBird-JAX}$ to use in achieving LSS marginalised constraints free\nfrom volume projection effects through non-flat measures."
                },
                "authors": [
                    {
                        "name": "Alexander Reeves"
                    },
                    {
                        "name": "Pierre Zhang"
                    },
                    {
                        "name": "Henry Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Henry Zheng"
                },
                "author": "Henry Zheng",
                "arxiv_comment": "29 + 14 pages, 9 figures, 4 tables. $\\texttt{PyBird-JAX}$ code is\n  available at https://github.com/pierrexyz/pybird",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20991v1",
                "updated": "2025-07-28T16:50:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:50:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "Debiasing inference in large-scale structure with non-flat volume\n  measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing inference in large-scale structure with non-flat volume\n  measures"
                },
                "summary": "Increasingly large parameter spaces, used to more accurately model precision\nobservables in physics, can paradoxically lead to large deviations in the\ninferred parameters of interest -- a bias known as volume projection effects --\nwhen marginalising over many nuisance parameters. For posterior distributions\nthat admit a Laplace expansion, we show that this artefact of Bayesian\ninference can be mitigated by defining expectation values with respect to a\nnon-flat volume measure, such that the posterior mean becomes unbiased on\naverage. We begin by finding a measure that ensures the mean is an unbiased\nestimator of the mode. Although the mode itself, as we rediscover, is biased\nunder sample averaging, this choice yields the least biased estimator due to a\ncancellation we clarify. We further explain why bias in marginal posteriors can\nappear relatively large, yet remains correctable, when the number of nuisances\nis large. To demonstrate our approach, we present mock analyses in large-scale\nstructure (LSS) wherein cosmological parameters are subject to large projection\neffects (at the 1-2$\\sigma$ level) under a flat measure, that are however\nrecovered at high fidelity ($<0.1\\sigma$) when estimated using non-flat\ncounterparts. Our cosmological analyses are enabled by $\\texttt{PyBird-JAX}$, a\nfast, differentiable pipeline for LSS developed in our companion paper [1].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasingly large parameter spaces, used to more accurately model precision\nobservables in physics, can paradoxically lead to large deviations in the\ninferred parameters of interest -- a bias known as volume projection effects --\nwhen marginalising over many nuisance parameters. For posterior distributions\nthat admit a Laplace expansion, we show that this artefact of Bayesian\ninference can be mitigated by defining expectation values with respect to a\nnon-flat volume measure, such that the posterior mean becomes unbiased on\naverage. We begin by finding a measure that ensures the mean is an unbiased\nestimator of the mode. Although the mode itself, as we rediscover, is biased\nunder sample averaging, this choice yields the least biased estimator due to a\ncancellation we clarify. We further explain why bias in marginal posteriors can\nappear relatively large, yet remains correctable, when the number of nuisances\nis large. To demonstrate our approach, we present mock analyses in large-scale\nstructure (LSS) wherein cosmological parameters are subject to large projection\neffects (at the 1-2$\\sigma$ level) under a flat measure, that are however\nrecovered at high fidelity ($<0.1\\sigma$) when estimated using non-flat\ncounterparts. Our cosmological analyses are enabled by $\\texttt{PyBird-JAX}$, a\nfast, differentiable pipeline for LSS developed in our companion paper [1]."
                },
                "authors": [
                    {
                        "name": "Alexander Reeves"
                    },
                    {
                        "name": "Pierre Zhang"
                    },
                    {
                        "name": "Henry Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Henry Zheng"
                },
                "author": "Henry Zheng",
                "arxiv_comment": "30 + 9 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00022v3",
                "updated": "2025-07-28T16:50:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    18,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-21T17:06:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    6,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "Scaling Physical Reasoning with the PHYSICS Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Physical Reasoning with the PHYSICS Dataset"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "arxiv_comment": "Work on physical datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16815v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16815v3",
                "updated": "2025-07-28T16:47:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    47,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-25T15:35:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREE-Merging: Fourier Transform for Efficient Model Merging"
                },
                "summary": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16815v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16815v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20989v1",
                "updated": "2025-07-28T16:47:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    47,
                    48,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:47:48Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    47,
                    48,
                    0,
                    209,
                    0
                ],
                "title": "Probing the Neutral Fraction of the Warm Ionized Medium via [NI] 5200",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Neutral Fraction of the Warm Ionized Medium via [NI] 5200"
                },
                "summary": "Most of the ionized mass in the Milky Way is in the Warm Ionized Medium (WIM)\nand not in the bright H~II regions. The WIM is traced by dispersion measure and\nhas been extensively studied in recombination lines (primarily, H$\\alpha$) and\noptical nebular lines (primarily, S+ and N+). The observations can be well\nexplained by a photo-ionized nebula with a low ionization parameter. It is\ngenerally thought that the source of ionization (and heating) of the WIM is due\nto Lyman continuum leaking from HII regions which are concentrated in the\nGalactic plane. The rays of the diffuse Galactic Lyman-continuum radiation\nfield incident on the Warm Neutral Medium (WNM) are absorbed, forming an\nionized skin. In nebulae with low-ionization parameter the transition from\nionized gas to neutral gas is gradual, unlike the case for HII regions with\ntheir sharp Stromgren spheres. The transition region is warm enough to excite\noxygen and nitrogen atoms to emit [OI] 6300,6363 and [NI] 5198,5200. Domgorgen\n& Mathis (1994) recognized the value of [OI] 6300 as a diagnostic of the\nfraction of the diffuse continuum that is absorbed by the WNM and therefore\nconstrains the fraction of the diffuse Lyman continuum that escapes to the\nhalo. Unfortunately, observations of Galactic [OI] 6300 have been stymied by\nbright [OI] 6300 airglow emission. [NI] 5200,5198 has been a historically less\npopular probe because this doublet is less luminous than the oxygen doublet.\nHowever, we point out that the [NI] airglow is two orders of magnitude smaller\nthan that of [OI]. Furthermore, even in the presence of comparable airglow, the\nWIM [NI] emission can be inferred using the doublet intensity ratio for which a\nmedium-resolution spectrometer such as the Local Volume Mapper will suffice.\nSeparately, we note, in extragalactic systems, that [OI]6300/[NI]5200 is a\nrobust measure of the O/N abundance ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most of the ionized mass in the Milky Way is in the Warm Ionized Medium (WIM)\nand not in the bright H~II regions. The WIM is traced by dispersion measure and\nhas been extensively studied in recombination lines (primarily, H$\\alpha$) and\noptical nebular lines (primarily, S+ and N+). The observations can be well\nexplained by a photo-ionized nebula with a low ionization parameter. It is\ngenerally thought that the source of ionization (and heating) of the WIM is due\nto Lyman continuum leaking from HII regions which are concentrated in the\nGalactic plane. The rays of the diffuse Galactic Lyman-continuum radiation\nfield incident on the Warm Neutral Medium (WNM) are absorbed, forming an\nionized skin. In nebulae with low-ionization parameter the transition from\nionized gas to neutral gas is gradual, unlike the case for HII regions with\ntheir sharp Stromgren spheres. The transition region is warm enough to excite\noxygen and nitrogen atoms to emit [OI] 6300,6363 and [NI] 5198,5200. Domgorgen\n& Mathis (1994) recognized the value of [OI] 6300 as a diagnostic of the\nfraction of the diffuse continuum that is absorbed by the WNM and therefore\nconstrains the fraction of the diffuse Lyman continuum that escapes to the\nhalo. Unfortunately, observations of Galactic [OI] 6300 have been stymied by\nbright [OI] 6300 airglow emission. [NI] 5200,5198 has been a historically less\npopular probe because this doublet is less luminous than the oxygen doublet.\nHowever, we point out that the [NI] airglow is two orders of magnitude smaller\nthan that of [OI]. Furthermore, even in the presence of comparable airglow, the\nWIM [NI] emission can be inferred using the doublet intensity ratio for which a\nmedium-resolution spectrometer such as the Local Volume Mapper will suffice.\nSeparately, we note, in extragalactic systems, that [OI]6300/[NI]5200 is a\nrobust measure of the O/N abundance ratio."
                },
                "authors": [
                    {
                        "name": "S. R. Kulkarni"
                    },
                    {
                        "name": "S. Noll"
                    },
                    {
                        "name": "W. Kausch"
                    },
                    {
                        "name": "Soumyadeep Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Soumyadeep Bhattacharjee"
                },
                "author": "Soumyadeep Bhattacharjee",
                "arxiv_comment": "20 pages; 18 figures; 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20985v1",
                "updated": "2025-07-28T16:45:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    32,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:45:32Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    32,
                    0,
                    209,
                    0
                ],
                "title": "Behavioral Study of Dashboard Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral Study of Dashboard Mechanisms"
                },
                "summary": "Visualization dashboards are increasingly used in strategic settings like\nauctions to enhance decision-making and reduce strategic confusion. This paper\npresents behavioral experiments evaluating how different dashboard designs\naffect bid optimization in reverse first-price auctions. Additionally, we\nassess how dashboard designs impact the auction designer's ability to\naccurately infer bidders' preferences within the dashboard mechanism framework.\nWe compare visualizations of the bid allocation rule, commonly deployed in\npractice, to alternatives that display expected utility. We find that\nutility-based visualizations significantly improve bidding by reducing\ncognitive demands on bidders. However, even with improved dashboards, bidders\nsystematically under-shade their bids, driven by an implicit preference for\ncertain wins in uncertain settings. As a result, dashboard-based mechanisms\nthat assume fully rational or risk-neutral bidder responses to dashboards can\nproduce significant estimation errors when inferring private preferences, which\nmay lead to suboptimal allocations in practice. Explicitly modeling agents'\nbehavioral responses to dashboards substantially improves inference accuracy,\nhighlighting the need to align visualization design and econometric inference\nassumptions in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization dashboards are increasingly used in strategic settings like\nauctions to enhance decision-making and reduce strategic confusion. This paper\npresents behavioral experiments evaluating how different dashboard designs\naffect bid optimization in reverse first-price auctions. Additionally, we\nassess how dashboard designs impact the auction designer's ability to\naccurately infer bidders' preferences within the dashboard mechanism framework.\nWe compare visualizations of the bid allocation rule, commonly deployed in\npractice, to alternatives that display expected utility. We find that\nutility-based visualizations significantly improve bidding by reducing\ncognitive demands on bidders. However, even with improved dashboards, bidders\nsystematically under-shade their bids, driven by an implicit preference for\ncertain wins in uncertain settings. As a result, dashboard-based mechanisms\nthat assume fully rational or risk-neutral bidder responses to dashboards can\nproduce significant estimation errors when inferring private preferences, which\nmay lead to suboptimal allocations in practice. Explicitly modeling agents'\nbehavioral responses to dashboards substantially improves inference accuracy,\nhighlighting the need to align visualization design and econometric inference\nassumptions in practice."
                },
                "authors": [
                    {
                        "name": "Paula Kayongo"
                    },
                    {
                        "name": "Jessica Hullman"
                    },
                    {
                        "name": "Jason Hartline"
                    }
                ],
                "author_detail": {
                    "name": "Jason Hartline"
                },
                "author": "Jason Hartline",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v1",
                "updated": "2025-07-28T16:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20977v1",
                "updated": "2025-07-28T16:39:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    39,
                    16,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:39:16Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    39,
                    16,
                    0,
                    209,
                    0
                ],
                "title": "Repairing vulnerabilities without invisible hands. A differentiated\n  replication study on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repairing vulnerabilities without invisible hands. A differentiated\n  replication study on LLMs"
                },
                "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method."
                },
                "authors": [
                    {
                        "name": "Maria Camporese"
                    },
                    {
                        "name": "Fabio Massacci"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massacci"
                },
                "author": "Fabio Massacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20975v1",
                "updated": "2025-07-28T16:37:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    37,
                    56,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:37:56Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    37,
                    56,
                    0,
                    209,
                    0
                ],
                "title": "Locally Adaptive Conformal Inference for Operator Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locally Adaptive Conformal Inference for Operator Models"
                },
                "summary": "Operator models are regression algorithms for functional data and have become\na key tool for emulating large-scale dynamical systems. Recent advances in deep\nneural operators have dramatically improved the accuracy and scalability of\noperator modeling, but lack an inherent notion of predictive uncertainty. We\nintroduce Local Spectral Conformal Inference (LSCI), a new framework for\nlocally adaptive, distribution-free uncertainty quantification for neural\noperator models. LSCI uses projection-based depth scoring and localized\nconformal inference to generate function-valued prediction sets with\nstatistical guarantees. We prove approximate finite-sample marginal coverage\nunder local exchangeability, and demonstrate significant gains in adaptivity\nand coverage across synthetic and real-world operator learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operator models are regression algorithms for functional data and have become\na key tool for emulating large-scale dynamical systems. Recent advances in deep\nneural operators have dramatically improved the accuracy and scalability of\noperator modeling, but lack an inherent notion of predictive uncertainty. We\nintroduce Local Spectral Conformal Inference (LSCI), a new framework for\nlocally adaptive, distribution-free uncertainty quantification for neural\noperator models. LSCI uses projection-based depth scoring and localized\nconformal inference to generate function-valued prediction sets with\nstatistical guarantees. We prove approximate finite-sample marginal coverage\nunder local exchangeability, and demonstrate significant gains in adaptivity\nand coverage across synthetic and real-world operator learning tasks."
                },
                "authors": [
                    {
                        "name": "Trevor Harris"
                    },
                    {
                        "name": "Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Liu"
                },
                "author": "Yan Liu",
                "arxiv_comment": "9 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20973v1",
                "updated": "2025-07-28T16:36:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    36,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:36:13Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    36,
                    13,
                    0,
                    209,
                    0
                ],
                "title": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via\n  Sparse Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via\n  Sparse Autoencoder"
                },
                "summary": "Text-to-image (T2I) diffusion models often exhibit gender bias, particularly\nby generating stereotypical associations between professions and gendered\nsubjects. This paper presents SAE Debias, a lightweight and model-agnostic\nframework for mitigating such bias in T2I generation. Unlike prior approaches\nthat rely on CLIP-based filtering or prompt engineering, which often require\nmodel-specific adjustments and offer limited control, SAE Debias operates\ndirectly within the feature space without retraining or architectural\nmodifications. By leveraging a k-sparse autoencoder pre-trained on a gender\nbias dataset, the method identifies gender-relevant directions within the\nsparse latent space, capturing professional stereotypes. Specifically, a biased\ndirection per profession is constructed from sparse latents and suppressed\nduring inference to steer generations toward more gender-balanced outputs.\nTrained only once, the sparse autoencoder provides a reusable debiasing\ndirection, offering effective control and interpretable insight into biased\nsubspaces. Extensive evaluations across multiple T2I models, including Stable\nDiffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially\nreduces gender bias while preserving generation quality. To the best of our\nknowledge, this is the first work to apply sparse autoencoders for identifying\nand intervening in gender bias within T2I models. These findings contribute\ntoward building socially responsible generative AI, providing an interpretable\nand model-agnostic tool to support fairness in text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models often exhibit gender bias, particularly\nby generating stereotypical associations between professions and gendered\nsubjects. This paper presents SAE Debias, a lightweight and model-agnostic\nframework for mitigating such bias in T2I generation. Unlike prior approaches\nthat rely on CLIP-based filtering or prompt engineering, which often require\nmodel-specific adjustments and offer limited control, SAE Debias operates\ndirectly within the feature space without retraining or architectural\nmodifications. By leveraging a k-sparse autoencoder pre-trained on a gender\nbias dataset, the method identifies gender-relevant directions within the\nsparse latent space, capturing professional stereotypes. Specifically, a biased\ndirection per profession is constructed from sparse latents and suppressed\nduring inference to steer generations toward more gender-balanced outputs.\nTrained only once, the sparse autoencoder provides a reusable debiasing\ndirection, offering effective control and interpretable insight into biased\nsubspaces. Extensive evaluations across multiple T2I models, including Stable\nDiffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially\nreduces gender bias while preserving generation quality. To the best of our\nknowledge, this is the first work to apply sparse autoencoders for identifying\nand intervening in gender bias within T2I models. These findings contribute\ntoward building socially responsible generative AI, providing an interpretable\nand model-agnostic tool to support fairness in text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Zhenyi Wang"
                    },
                    {
                        "name": "Kangxian Xie"
                    },
                    {
                        "name": "Naresh Kumar Devulapally"
                    },
                    {
                        "name": "Vishnu Suresh Lokhande"
                    },
                    {
                        "name": "Mingchen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingchen Gao"
                },
                "author": "Mingchen Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17137v2",
                "updated": "2025-07-28T16:30:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    30,
                    46,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-22T05:40:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    5,
                    40,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive\n  Decline via Longitudinal Voice Assistant Commands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive\n  Decline via Longitudinal Voice Assistant Commands"
                },
                "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline."
                },
                "authors": [
                    {
                        "name": "Kristin Qi"
                    },
                    {
                        "name": "Youxiang Zhu"
                    },
                    {
                        "name": "Caroline Summerour"
                    },
                    {
                        "name": "John A. Batsis"
                    },
                    {
                        "name": "Xiaohui Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Liang"
                },
                "author": "Xiaohui Liang",
                "arxiv_comment": "IEEE Global Communications Conference (GlobeCom) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20964v1",
                "updated": "2025-07-28T16:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    19,
                    25,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:19:25Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    19,
                    25,
                    0,
                    209,
                    0
                ],
                "title": "Core Safety Values for Provably Corrigible Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Core Safety Values for Provably Corrigible Agents"
                },
                "summary": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems."
                },
                "authors": [
                    {
                        "name": "Aran Nayebi"
                    }
                ],
                "author_detail": {
                    "name": "Aran Nayebi"
                },
                "author": "Aran Nayebi",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07119v3",
                "updated": "2025-07-28T16:17:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    17,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-11T21:05:33Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    21,
                    5,
                    33,
                    6,
                    131,
                    0
                ],
                "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression"
                },
                "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation."
                },
                "authors": [
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Marco Fabris"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v1",
                "updated": "2025-07-28T16:09:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08490v2",
                "updated": "2025-07-28T16:07:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    7,
                    44,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-11T12:42:01Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "title": "Adopting Large Language Models to Automated System Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large Language Models to Automated System Integration"
                },
                "summary": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    }
                ],
                "author_detail": {
                    "name": "Robin D. Pesl"
                },
                "author": "Robin D. Pesl",
                "arxiv_doi": "10.1007/978-3-031-94590-8_37",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94590-8_37",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.08490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Intelligent Information Systems. CAiSE 2025.\n  Lecture Notes in Business Information Processing, vol 557. Springer, Cham.,\n  and is available online at https://doi.org/10.1007/978-3-031-94590-8_37",
                "arxiv_journal_ref": "Intelligent Information Systems. CAiSE 2025. Lecture Notes in\n  Business Information Processing, vol 557. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20956v1",
                "updated": "2025-07-28T16:04:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    4,
                    25,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:04:25Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    4,
                    25,
                    0,
                    209,
                    0
                ],
                "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of\n  Instruction-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Conformative Decoding to Improve Output Diversity of\n  Instruction-Tuned Large Language Models"
                },
                "summary": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality."
                },
                "authors": [
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Dan Brown"
                    },
                    {
                        "name": "Anna Jordanous"
                    }
                ],
                "author_detail": {
                    "name": "Anna Jordanous"
                },
                "author": "Anna Jordanous",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19804v2",
                "updated": "2025-07-28T16:00:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    0,
                    1,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-29T16:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation"
                },
                "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    },
                    {
                        "name": "Jerin G. Mathew"
                    },
                    {
                        "name": "Massimo Mecella"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_doi": "10.1007/978-3-031-94571-7_8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94571-7_8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.19804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Advanced Information Systems Engineering. CAiSE\n  2025. Lecture Notes in Computer Science, vol 15702. Springer, Cham., and is\n  available online at https://doi.org/10.1007/978-3-031-94571-7_8",
                "arxiv_journal_ref": "Advanced Information Systems Engineering. CAiSE 2025. Lecture\n  Notes in Computer Science, vol 15702. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20939v1",
                "updated": "2025-07-28T15:52:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    52,
                    36,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:52:36Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    52,
                    36,
                    0,
                    209,
                    0
                ],
                "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts"
                },
                "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU."
                },
                "authors": [
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Junfu Pu"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Lu Qiu"
                    },
                    {
                        "name": "Jin Ma"
                    },
                    {
                        "name": "Lisheng Duan"
                    },
                    {
                        "name": "Xinyu Zuo"
                    },
                    {
                        "name": "Jinwen Luo"
                    },
                    {
                        "name": "Weibo Gu"
                    },
                    {
                        "name": "Zexuan Li"
                    },
                    {
                        "name": "Xiaojing Zhang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project Page:\n  https://tencentarc.github.io/posts/arc-video-announcement/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20938v1",
                "updated": "2025-07-28T15:51:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    51,
                    55,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:51:55Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    51,
                    55,
                    0,
                    209,
                    0
                ],
                "title": "The Concordance of Weak Lensing and Escape Velocity Mass Estimates for\n  Galaxy Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Concordance of Weak Lensing and Escape Velocity Mass Estimates for\n  Galaxy Clusters"
                },
                "summary": "In the $\\Lambda$CDM paradigm, the masses of the galaxy clusters inferred\nusing background galaxies via weak-lensing shear should agree with the masses\nmeasured using the galaxy projected radius-velocity phase-space data via the\nescape velocity profile. However, prior work indicates that the correlation\nbetween caustic-inferred escape masses and weak lensing masses is statistically\nconsistent with zero. Based on recent advancements in the measurement of the\nescape edge and its physical interpretation, we conduct a revised comparison\nbetween these two independent mass inference techniques for 46 galaxy clusters\nbetween $0.05 \\le z \\le 0.3$ and over an order of magnitude in mass, $14.4 \\le\n{\\rm log}_{10} M/M_{\\odot} \\le 15.4$. We find excellent agreement, with a\ncorrelation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between\nthe two mass measurements consistent with zero (0.02 $\\pm$ 0.02 dex). The\nobserved scatter between these direct mass estimates is 0.17 dex and is\nconsistent with the reported individual mass errors, suggesting that there is\nno need for an additional intrinsic component. We discuss the important\npractical consequences of these results, focusing on the systematic\nuncertainties inherent to each technique, and their implications for cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the $\\Lambda$CDM paradigm, the masses of the galaxy clusters inferred\nusing background galaxies via weak-lensing shear should agree with the masses\nmeasured using the galaxy projected radius-velocity phase-space data via the\nescape velocity profile. However, prior work indicates that the correlation\nbetween caustic-inferred escape masses and weak lensing masses is statistically\nconsistent with zero. Based on recent advancements in the measurement of the\nescape edge and its physical interpretation, we conduct a revised comparison\nbetween these two independent mass inference techniques for 46 galaxy clusters\nbetween $0.05 \\le z \\le 0.3$ and over an order of magnitude in mass, $14.4 \\le\n{\\rm log}_{10} M/M_{\\odot} \\le 15.4$. We find excellent agreement, with a\ncorrelation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between\nthe two mass measurements consistent with zero (0.02 $\\pm$ 0.02 dex). The\nobserved scatter between these direct mass estimates is 0.17 dex and is\nconsistent with the reported individual mass errors, suggesting that there is\nno need for an additional intrinsic component. We discuss the important\npractical consequences of these results, focusing on the systematic\nuncertainties inherent to each technique, and their implications for cosmology."
                },
                "authors": [
                    {
                        "name": "Alexander Rodriguez"
                    },
                    {
                        "name": "Christopher J. Miller"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. Miller"
                },
                "author": "Christopher J. Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20936v1",
                "updated": "2025-07-28T15:45:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    45,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:45:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    45,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation\n  Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Persona-Driven Reasoning in Language Models via Activation\n  Patching"
                },
                "summary": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities."
                },
                "authors": [
                    {
                        "name": "Ansh Poonia"
                    },
                    {
                        "name": "Maeghal Jain"
                    }
                ],
                "author_detail": {
                    "name": "Maeghal Jain"
                },
                "author": "Maeghal Jain",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14917v3",
                "updated": "2025-07-28T15:37:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    37,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2024-06-21T07:20:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    7,
                    20,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "LLM2TEA: An Agentic AI Designer for Discovery with Generative\n  Evolutionary Multitasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2TEA: An Agentic AI Designer for Discovery with Generative\n  Evolutionary Multitasking"
                },
                "summary": "This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs."
                },
                "authors": [
                    {
                        "name": "Melvin Wong"
                    },
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Thiago Rios"
                    },
                    {
                        "name": "Stefan Menzel"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "This work is accepted by IEEE CIM. IEEE copyrights applies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20924v1",
                "updated": "2025-07-28T15:30:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    30,
                    17,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:30:17Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    30,
                    17,
                    0,
                    209,
                    0
                ],
                "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech\n  Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech\n  Concept Bottleneck Models"
                },
                "summary": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish."
                },
                "authors": [
                    {
                        "name": "Roberto Labadie-Tamayo"
                    },
                    {
                        "name": "Adrian Jaques Böck"
                    },
                    {
                        "name": "Djordje Slijepčević"
                    },
                    {
                        "name": "Xihui Chen"
                    },
                    {
                        "name": "Andreas Babic"
                    },
                    {
                        "name": "Matthias Zeppelzauer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Zeppelzauer"
                },
                "author": "Matthias Zeppelzauer",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20923v1",
                "updated": "2025-07-28T15:26:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:26:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization"
                },
                "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE."
                },
                "authors": [
                    {
                        "name": "Minh Hieu Ha"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Tung Duy Doan"
                    },
                    {
                        "name": "Tung Dao"
                    },
                    {
                        "name": "Dao Tran"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "36 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20906v2",
                "updated": "2025-07-29T02:15:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    15,
                    56,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-28T14:59:17Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    59,
                    17,
                    0,
                    209,
                    0
                ],
                "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context\n  Learning"
                },
                "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.2%-14.3% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.2%-14.3% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space."
                },
                "authors": [
                    {
                        "name": "Jungwon Park"
                    },
                    {
                        "name": "Wonjong Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Wonjong Rhee"
                },
                "author": "Wonjong Rhee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13629v2",
                "updated": "2025-07-28T14:54:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    54,
                    15,
                    0,
                    209,
                    0
                ],
                "published": "2025-06-16T15:56:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding"
                },
                "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20900v1",
                "updated": "2025-07-28T14:52:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    52,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:52:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    52,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Music Arena: Live Evaluation for Text-to-Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music Arena: Live Evaluation for Text-to-Music"
                },
                "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org"
                },
                "authors": [
                    {
                        "name": "Yonghyun Kim"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Koichi Saito"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16117v2",
                "updated": "2025-07-28T14:51:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    51,
                    21,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-22T00:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    10,
                    55,
                    1,
                    203,
                    0
                ],
                "title": "BDIViz: An Interactive Visualization System for Biomedical Schema\n  Matching with LLM-Powered Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BDIViz: An Interactive Visualization System for Biomedical Schema\n  Matching with LLM-Powered Validation"
                },
                "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches."
                },
                "authors": [
                    {
                        "name": "Eden Wu"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Christos Koutras"
                    },
                    {
                        "name": "Sarah Keegan"
                    },
                    {
                        "name": "Wenke Liu"
                    },
                    {
                        "name": "Beata Szeitz"
                    },
                    {
                        "name": "David Fenyo"
                    },
                    {
                        "name": "Cláudio T. Silva"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "arxiv_comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12925v2",
                "updated": "2025-07-28T14:50:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    50,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-01-22T14:54:25Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    54,
                    25,
                    2,
                    22,
                    0
                ],
                "title": "A Denser Hydrogen Inferred from First-Principles Simulations Challenges\n  Jupiter's Interior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Denser Hydrogen Inferred from First-Principles Simulations Challenges\n  Jupiter's Interior Models"
                },
                "summary": "First-principle modeling of dense hydrogen is crucial in materials and\nplanetary sciences. Despite its apparent simplicity, predicting the ionic and\nelectronic structure of hydrogen is a formidable challenge, and it is connected\nwith the insulator-to-metal transition, a century-old problem in condensed\nmatter. Accurate simulations of liquid hydrogen are also essential for modeling\ngas giant planets. Here we perform an exhaustive study of the equation of state\nof hydrogen using Density Functional Theory and quantum Monte Carlo\nsimulations. We find that the pressure predicted by Density Functional Theory\nmay vary qualitatively when using different functionals. The predictive power\nof first-principle simulations is restored by validating each functional\nagainst higher-level wavefunction theories, represented by computationally\nintensive variational and diffusion Monte Carlo calculations. Our simulations\nprovide evidence that hydrogen is denser at planetary conditions, compared to\ncurrently used equations of state. For Jupiter, this implies a lower bulk\nmetallicity (i.e., a smaller mass of heavy elements). Our results further\namplify the inconsistency between Jupiter's atmospheric metallicity measured by\nthe Galileo probe and the envelope metallicity inferred from interior models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-principle modeling of dense hydrogen is crucial in materials and\nplanetary sciences. Despite its apparent simplicity, predicting the ionic and\nelectronic structure of hydrogen is a formidable challenge, and it is connected\nwith the insulator-to-metal transition, a century-old problem in condensed\nmatter. Accurate simulations of liquid hydrogen are also essential for modeling\ngas giant planets. Here we perform an exhaustive study of the equation of state\nof hydrogen using Density Functional Theory and quantum Monte Carlo\nsimulations. We find that the pressure predicted by Density Functional Theory\nmay vary qualitatively when using different functionals. The predictive power\nof first-principle simulations is restored by validating each functional\nagainst higher-level wavefunction theories, represented by computationally\nintensive variational and diffusion Monte Carlo calculations. Our simulations\nprovide evidence that hydrogen is denser at planetary conditions, compared to\ncurrently used equations of state. For Jupiter, this implies a lower bulk\nmetallicity (i.e., a smaller mass of heavy elements). Our results further\namplify the inconsistency between Jupiter's atmospheric metallicity measured by\nthe Galileo probe and the envelope metallicity inferred from interior models."
                },
                "authors": [
                    {
                        "name": "Cesare Cozza"
                    },
                    {
                        "name": "Kousuke Nakano"
                    },
                    {
                        "name": "Saburo Howard"
                    },
                    {
                        "name": "Hao Xie"
                    },
                    {
                        "name": "Ravit Helled"
                    },
                    {
                        "name": "Guglielmo Mazzola"
                    }
                ],
                "author_detail": {
                    "name": "Guglielmo Mazzola"
                },
                "author": "Guglielmo Mazzola",
                "arxiv_comment": "We add Huguniot calculation with SCAN+vv10, benchmarks with more\n  functionals, and provide additional tests",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18784v2",
                "updated": "2025-07-28T14:48:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    48,
                    17,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-26T03:33:14Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    3,
                    33,
                    14,
                    5,
                    116,
                    0
                ],
                "title": "Secret Breach Detection in Source Code with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Breach Detection in Source Code with Large Language Models"
                },
                "summary": "Background: Leaking sensitive information - such as API keys, tokens, and\ncredentials - in source code remains a persistent security threat. Traditional\nregex and entropy-based tools often generate high false positives due to\nlimited contextual understanding. Aims: This work aims to enhance secret\ndetection in source code using large language models (LLMs), reducing false\npositives while maintaining high recall. We also evaluate the feasibility of\nusing fine-tuned, smaller models for local deployment. Method: We propose a\nhybrid approach combining regex-based candidate extraction with LLM-based\nclassification. We evaluate pre-trained and fine-tuned variants of various\nLarge Language Models on a benchmark dataset from 818 GitHub repositories.\nVarious prompting strategies and efficient fine-tuning methods are employed for\nboth binary and multiclass classification. Results: The fine-tuned LLaMA-3.1 8B\nmodel achieved an F1-score of 0.9852 in binary classification, outperforming\nregex-only baselines. For multiclass classification, Mistral-7B reached 0.982\naccuracy. Fine-tuning significantly improved performance across all models.\nConclusions: Fine-tuned LLMs offer an effective and scalable solution for\nsecret detection, greatly reducing false positives. Open-source models provide\na practical alternative to commercial APIs, enabling secure and cost-efficient\ndeployment in development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Leaking sensitive information - such as API keys, tokens, and\ncredentials - in source code remains a persistent security threat. Traditional\nregex and entropy-based tools often generate high false positives due to\nlimited contextual understanding. Aims: This work aims to enhance secret\ndetection in source code using large language models (LLMs), reducing false\npositives while maintaining high recall. We also evaluate the feasibility of\nusing fine-tuned, smaller models for local deployment. Method: We propose a\nhybrid approach combining regex-based candidate extraction with LLM-based\nclassification. We evaluate pre-trained and fine-tuned variants of various\nLarge Language Models on a benchmark dataset from 818 GitHub repositories.\nVarious prompting strategies and efficient fine-tuning methods are employed for\nboth binary and multiclass classification. Results: The fine-tuned LLaMA-3.1 8B\nmodel achieved an F1-score of 0.9852 in binary classification, outperforming\nregex-only baselines. For multiclass classification, Mistral-7B reached 0.982\naccuracy. Fine-tuning significantly improved performance across all models.\nConclusions: Fine-tuned LLMs offer an effective and scalable solution for\nsecret detection, greatly reducing false positives. Open-source models provide\na practical alternative to commercial APIs, enabling secure and cost-efficient\ndeployment in development workflows."
                },
                "authors": [
                    {
                        "name": "Md Nafiu Rahman"
                    },
                    {
                        "name": "Sadif Ahmed"
                    },
                    {
                        "name": "Zahin Wahab"
                    },
                    {
                        "name": "S M Sohan"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "arxiv_comment": "19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM 2025) cameraready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20890v1",
                "updated": "2025-07-28T14:41:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    41,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:41:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    41,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\n  Attention-Guided Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\n  Attention-Guided Refinement"
                },
                "summary": "Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Guoxian Song"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Junsong Yuan"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20888v1",
                "updated": "2025-07-28T14:39:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    39,
                    46,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:39:46Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    39,
                    46,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Project-Specific Code Completion by Inferring Internal API\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Project-Specific Code Completion by Inferring Internal API\n  Information"
                },
                "summary": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%."
                },
                "authors": [
                    {
                        "name": "Le Deng"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Ming Liang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13952v2",
                "updated": "2025-07-28T14:34:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    34,
                    41,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    20,
                    54,
                    4,
                    199,
                    0
                ],
                "title": "Beyond Load: Understanding Cognitive Effort through Neural Efficiency\n  and Involvement using fNIRS and Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Load: Understanding Cognitive Effort through Neural Efficiency\n  and Involvement using fNIRS and Machine Learning"
                },
                "summary": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay."
                },
                "authors": [
                    {
                        "name": "Shayla Sharmin"
                    },
                    {
                        "name": "Roghayeh Leila Barmaki"
                    }
                ],
                "author_detail": {
                    "name": "Roghayeh Leila Barmaki"
                },
                "author": "Roghayeh Leila Barmaki",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.13883",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18318v3",
                "updated": "2025-07-28T14:32:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    32,
                    0,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-24T10:07:10Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    10,
                    7,
                    10,
                    1,
                    359,
                    0
                ],
                "title": "Search for a gravitational wave background from primordial black hole\n  binaries using data from the first three LIGO-Virgo-KAGRA observing runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for a gravitational wave background from primordial black hole\n  binaries using data from the first three LIGO-Virgo-KAGRA observing runs"
                },
                "summary": "Using the cross-correlation data from the first three observing runs of the\nLIGO-Virgo-KAGRA Collaboration, we search for a gravitational-wave background\n(GWB) from primordial black holes, arising from the superposition of compact\nbinary coalescence events. We consider both early and late binary formation\nmechanisms and perform Bayesian parameter inference. From the non-detection of\nthe GWB, we provide constraints on the fraction of primordial black holes\ncontributing to the present dark matter energy density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the cross-correlation data from the first three observing runs of the\nLIGO-Virgo-KAGRA Collaboration, we search for a gravitational-wave background\n(GWB) from primordial black holes, arising from the superposition of compact\nbinary coalescence events. We consider both early and late binary formation\nmechanisms and perform Bayesian parameter inference. From the non-detection of\nthe GWB, we provide constraints on the fraction of primordial black holes\ncontributing to the present dark matter energy density."
                },
                "authors": [
                    {
                        "name": "Tore Boybeyi"
                    },
                    {
                        "name": "Sebastien Clesse"
                    },
                    {
                        "name": "Sachiko Kuroyanagi"
                    },
                    {
                        "name": "Mairi Sakellariadou"
                    }
                ],
                "author_detail": {
                    "name": "Mairi Sakellariadou"
                },
                "author": "Mairi Sakellariadou",
                "arxiv_doi": "10.1103/zphk-3ld9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/zphk-3ld9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.18318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "v3, added references, fixed typos. 14 pages, 6 figures, published in\n  PRD",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20870v1",
                "updated": "2025-07-28T14:22:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:22:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM\n  Common-Sense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human-in-the-loop Approach to Robot Action Replanning through LLM\n  Common-Sense Reasoning"
                },
                "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness."
                },
                "authors": [
                    {
                        "name": "Elena Merlo"
                    },
                    {
                        "name": "Marta Lagomarsino"
                    },
                    {
                        "name": "Arash Ajoudani"
                    }
                ],
                "author_detail": {
                    "name": "Arash Ajoudani"
                },
                "author": "Arash Ajoudani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20869v1",
                "updated": "2025-07-28T14:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    0,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    0,
                    0,
                    209,
                    0
                ],
                "title": "Reconstructing Sparticle masses at the LHC using Generative Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing Sparticle masses at the LHC using Generative Machine\n  Learning"
                },
                "summary": "We explore a generative-model framework to infer the masses of heavy\nparticles from detector-level data over a broad parameter space. Our model\ncombines a transformer-based detector encoder and a diffusion neural network.\nWe apply our model to a new physics scenario involving the pair production of\nwino-like chargino-neutralino, $pp \\to \\tilde\\chi_1^{\\pm} \\tilde\\chi_2^0$, in\nthe $1\\ell + 2\\gamma + jets$ channel at the high luminosity LHC~(HL-LHC). We\nfind that our framework can achieve mass reconstruction efficiency of $\\gtrsim\n70\\%$ for the lightest neutralino $\\tilde\\chi_1^0$ and $\\gtrsim 40\\%$ for the\nsecond lightest neutralino $\\tilde\\chi_2^0$, for a mass tolerance of $\\Delta m\n= 30~$GeV, across the entire parameter space accessible at the HL-LHC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore a generative-model framework to infer the masses of heavy\nparticles from detector-level data over a broad parameter space. Our model\ncombines a transformer-based detector encoder and a diffusion neural network.\nWe apply our model to a new physics scenario involving the pair production of\nwino-like chargino-neutralino, $pp \\to \\tilde\\chi_1^{\\pm} \\tilde\\chi_2^0$, in\nthe $1\\ell + 2\\gamma + jets$ channel at the high luminosity LHC~(HL-LHC). We\nfind that our framework can achieve mass reconstruction efficiency of $\\gtrsim\n70\\%$ for the lightest neutralino $\\tilde\\chi_1^0$ and $\\gtrsim 40\\%$ for the\nsecond lightest neutralino $\\tilde\\chi_2^0$, for a mass tolerance of $\\Delta m\n= 30~$GeV, across the entire parameter space accessible at the HL-LHC."
                },
                "authors": [
                    {
                        "name": "Rahool Kumar Barman"
                    },
                    {
                        "name": "Arghya Choudhury"
                    },
                    {
                        "name": "Subhadeep Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Subhadeep Sarkar"
                },
                "author": "Subhadeep Sarkar",
                "arxiv_comment": "15 pages, 5 figures and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00831v2",
                "updated": "2025-07-28T14:17:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    17,
                    34,
                    0,
                    209,
                    0
                ],
                "published": "2025-06-01T04:33:34Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    4,
                    33,
                    34,
                    6,
                    152,
                    0
                ],
                "title": "A Large Language Model-Supported Threat Modeling Framework for\n  Transportation Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Supported Threat Modeling Framework for\n  Transportation Cyber-Physical Systems"
                },
                "summary": "Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository."
                },
                "authors": [
                    {
                        "name": "M Sabbir Salek"
                    },
                    {
                        "name": "Mashrur Chowdhury"
                    },
                    {
                        "name": "Muhaimin Bin Munir"
                    },
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Mohammad Imtiaz Hasan"
                    },
                    {
                        "name": "Jean-Michel Tine"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Mizanur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Mizanur Rahman"
                },
                "author": "Mizanur Rahman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20859v1",
                "updated": "2025-07-28T14:12:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    12,
                    37,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:12:37Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    12,
                    37,
                    0,
                    209,
                    0
                ],
                "title": "Leveraging Open-Source Large Language Models for Clinical Information\n  Extraction in Resource-Constrained Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Open-Source Large Language Models for Clinical Information\n  Extraction in Resource-Constrained Settings"
                },
                "summary": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Luc Builtjes"
                    },
                    {
                        "name": "Joeran Bosma"
                    },
                    {
                        "name": "Mathias Prokop"
                    },
                    {
                        "name": "Bram van Ginneken"
                    },
                    {
                        "name": "Alessa Hering"
                    }
                ],
                "author_detail": {
                    "name": "Alessa Hering"
                },
                "author": "Alessa Hering",
                "arxiv_comment": "34 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03511v2",
                "updated": "2025-07-28T14:04:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    4,
                    51,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-04T12:08:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    12,
                    8,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Nonparametric regression for cost-effectiveness analyses with\n  observational data -- a tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric regression for cost-effectiveness analyses with\n  observational data -- a tutorial"
                },
                "summary": "Healthcare decision-making often requires selecting among treatment options\nunder budget constraints, particularly when one option is more effective but\nalso more costly. Cost-effectiveness analysis (CEA) provides a framework for\nevaluating whether the health benefits of a treatment justify its additional\ncosts. A key component of CEA is the estimation of treatment effects on both\nhealth outcomes and costs, which becomes challenging when using observational\ndata, due to potential confounding. While advanced causal inference methods\nexist for use in such circumstances, their adoption in CEAs remains limited,\nwith many studies relying on overly simplistic methods such as linear\nregression or propensity score matching. We believe that this is mainly due to\nhealth economists being generally unfamiliar with superior methodology. In this\npaper, we address this gap by introducing cost-effectiveness researchers to\nmodern nonparametric regression models, with a particular focus on Bayesian\nAdditive Regression Trees (BART). We provide practical guidance on how to\nimplement BART in CEAs, including code examples, and discuss its advantages in\nproducing more robust and credible estimates from observational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare decision-making often requires selecting among treatment options\nunder budget constraints, particularly when one option is more effective but\nalso more costly. Cost-effectiveness analysis (CEA) provides a framework for\nevaluating whether the health benefits of a treatment justify its additional\ncosts. A key component of CEA is the estimation of treatment effects on both\nhealth outcomes and costs, which becomes challenging when using observational\ndata, due to potential confounding. While advanced causal inference methods\nexist for use in such circumstances, their adoption in CEAs remains limited,\nwith many studies relying on overly simplistic methods such as linear\nregression or propensity score matching. We believe that this is mainly due to\nhealth economists being generally unfamiliar with superior methodology. In this\npaper, we address this gap by introducing cost-effectiveness researchers to\nmodern nonparametric regression models, with a particular focus on Bayesian\nAdditive Regression Trees (BART). We provide practical guidance on how to\nimplement BART in CEAs, including code examples, and discuss its advantages in\nproducing more robust and credible estimates from observational data."
                },
                "authors": [
                    {
                        "name": "Jonas Esser"
                    },
                    {
                        "name": "Mateus Maia"
                    },
                    {
                        "name": "Judith Bosmans"
                    },
                    {
                        "name": "Johanna van Dongen"
                    }
                ],
                "author_detail": {
                    "name": "Johanna van Dongen"
                },
                "author": "Johanna van Dongen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20849v1",
                "updated": "2025-07-28T14:00:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    0,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:00:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    0,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Latent Inter-User Difference Modeling for LLM Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Inter-User Difference Modeling for LLM Personalization"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP."
                },
                "authors": [
                    {
                        "name": "Yilun Qiu"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20842v1",
                "updated": "2025-07-28T13:50:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    50,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:50:53Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    50,
                    53,
                    0,
                    209,
                    0
                ],
                "title": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision\n  Language Models"
                },
                "summary": "Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Bowen Shi"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Hongkai Xiong"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20832v1",
                "updated": "2025-07-28T13:39:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    39,
                    3,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:39:03Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    39,
                    3,
                    0,
                    209,
                    0
                ],
                "title": "Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics"
                },
                "summary": "Situationally-aware artificial agents operating with competence in natural\nenvironments face several challenges: spatial awareness, object affordance\ndetection, dynamic changes and unpredictability. A critical challenge is the\nagent's ability to identify and monitor environmental elements pertinent to its\nobjectives. Our research introduces a neurosymbolic modular architecture for\nreactive robotics. Our system combines a neural component performing object\nrecognition over the environment and image processing techniques such as\noptical flow, with symbolic representation and reasoning. The reasoning system\nis grounded in the embodied cognition paradigm, via integrating image schematic\nknowledge in an ontological structure. The ontology is operatively used to\ncreate queries for the perception system, decide on actions, and infer\nentities' capabilities derived from perceptual data. The combination of\nreasoning and image processing allows the agent to focus its perception for\nnormal operation as well as discover new concepts for parts of objects involved\nin particular interactions. The discovered concepts allow the robot to\nautonomously acquire training data and adjust its subsymbolic perception to\nrecognize the parts, as well as making planning for more complex tasks feasible\nby focusing search on those relevant object parts. We demonstrate our approach\nin a simulated world, in which an agent learns to recognize parts of objects\ninvolved in support relations. While the agent has no concept of handle\ninitially, by observing examples of supported objects hanging from a hook it\nlearns to recognize the parts involved in establishing support and becomes able\nto plan the establishment/destruction of the support relation. This underscores\nthe agent's capability to expand its knowledge through observation in a\nsystematic way, and illustrates the potential of combining deep reasoning\n[...].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situationally-aware artificial agents operating with competence in natural\nenvironments face several challenges: spatial awareness, object affordance\ndetection, dynamic changes and unpredictability. A critical challenge is the\nagent's ability to identify and monitor environmental elements pertinent to its\nobjectives. Our research introduces a neurosymbolic modular architecture for\nreactive robotics. Our system combines a neural component performing object\nrecognition over the environment and image processing techniques such as\noptical flow, with symbolic representation and reasoning. The reasoning system\nis grounded in the embodied cognition paradigm, via integrating image schematic\nknowledge in an ontological structure. The ontology is operatively used to\ncreate queries for the perception system, decide on actions, and infer\nentities' capabilities derived from perceptual data. The combination of\nreasoning and image processing allows the agent to focus its perception for\nnormal operation as well as discover new concepts for parts of objects involved\nin particular interactions. The discovered concepts allow the robot to\nautonomously acquire training data and adjust its subsymbolic perception to\nrecognize the parts, as well as making planning for more complex tasks feasible\nby focusing search on those relevant object parts. We demonstrate our approach\nin a simulated world, in which an agent learns to recognize parts of objects\ninvolved in support relations. While the agent has no concept of handle\ninitially, by observing examples of supported objects hanging from a hook it\nlearns to recognize the parts involved in establishing support and becomes able\nto plan the establishment/destruction of the support relation. This underscores\nthe agent's capability to expand its knowledge through observation in a\nsystematic way, and illustrates the potential of combining deep reasoning\n[...]."
                },
                "authors": [
                    {
                        "name": "Mihai Pomarlan"
                    },
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Rachel Ringe"
                    },
                    {
                        "name": "Maria M. Hedblom"
                    },
                    {
                        "name": "Nikolaos Tsiogkas"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Tsiogkas"
                },
                "author": "Nikolaos Tsiogkas",
                "arxiv_doi": "10.3233/FAIA241288",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA241288",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This article is published online with Open Access by IOS Press and\n  distributed under the terms of the Creative Commons Attribution\n  Non-Commercial License 4.0 (CC BY-NC 4.0)",
                "arxiv_journal_ref": "Frontiers in Artificial Intelligence and Applications; 2024; Vol.\n  394; pp. 2 - 15",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08064v2",
                "updated": "2025-07-28T13:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    36,
                    38,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-10T16:47:25Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    47,
                    25,
                    3,
                    191,
                    0
                ],
                "title": "PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal\n  Retrieval with Modality-Adaptive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal\n  Retrieval with Modality-Adaptive Learning"
                },
                "summary": "As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance."
                },
                "authors": [
                    {
                        "name": "Yibo Lyu"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Gongwei Chen"
                    },
                    {
                        "name": "Yijie Zhu"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Accepted to ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15748v2",
                "updated": "2025-07-28T13:13:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    13,
                    2,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-20T10:06:52Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models"
                },
                "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole."
                },
                "authors": [
                    {
                        "name": "Shamus Sim"
                    },
                    {
                        "name": "Tyrone Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone Chen"
                },
                "author": "Tyrone Chen",
                "arxiv_comment": "25 pages, 7 figures, 3 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20798v1",
                "updated": "2025-07-28T13:07:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    7,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:07:23Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    7,
                    23,
                    0,
                    209,
                    0
                ],
                "title": "An Efficient Machine Learning Framework for Forest Height Estimation\n  from Multi-Polarimetric Multi-Baseline SAR data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Machine Learning Framework for Forest Height Estimation\n  from Multi-Polarimetric Multi-Baseline SAR data"
                },
                "summary": "Accurate forest height estimation is crucial for climate change monitoring\nand carbon cycle assessment. Synthetic Aperture Radar (SAR), particularly in\nmulti-channel configurations, has provided support for a long time in 3D forest\nstructure reconstruction through model-based techniques. More recently,\ndata-driven approaches using Machine Learning (ML) and Deep Learning (DL) have\nenabled new opportunities for forest parameter retrieval. This paper introduces\nFGump, a forest height estimation framework by gradient boosting using\nmulti-channel SAR processing with LiDAR profiles as Ground Truth(GT). Unlike\ntypical ML and DL approaches that require large datasets and complex\narchitectures, FGump ensures a strong balance between accuracy and\ncomputational efficiency, using a limited set of hand-designed features and\navoiding heavy preprocessing (e.g., calibration and/or quantization). Evaluated\nunder both classification and regression paradigms, the proposed framework\ndemonstrates that the regression formulation enables fine-grained, continuous\nestimations and avoids quantization artifacts by resulting in more precise\nmeasurements without rounding. Experimental results confirm that FGump\noutperforms State-of-the-Art (SOTA) AI-based and classical methods, achieving\nhigher accuracy and significantly lower training and inference times, as\ndemonstrated in our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate forest height estimation is crucial for climate change monitoring\nand carbon cycle assessment. Synthetic Aperture Radar (SAR), particularly in\nmulti-channel configurations, has provided support for a long time in 3D forest\nstructure reconstruction through model-based techniques. More recently,\ndata-driven approaches using Machine Learning (ML) and Deep Learning (DL) have\nenabled new opportunities for forest parameter retrieval. This paper introduces\nFGump, a forest height estimation framework by gradient boosting using\nmulti-channel SAR processing with LiDAR profiles as Ground Truth(GT). Unlike\ntypical ML and DL approaches that require large datasets and complex\narchitectures, FGump ensures a strong balance between accuracy and\ncomputational efficiency, using a limited set of hand-designed features and\navoiding heavy preprocessing (e.g., calibration and/or quantization). Evaluated\nunder both classification and regression paradigms, the proposed framework\ndemonstrates that the regression formulation enables fine-grained, continuous\nestimations and avoids quantization artifacts by resulting in more precise\nmeasurements without rounding. Experimental results confirm that FGump\noutperforms State-of-the-Art (SOTA) AI-based and classical methods, achieving\nhigher accuracy and significantly lower training and inference times, as\ndemonstrated in our results."
                },
                "authors": [
                    {
                        "name": "Francesca Razzano"
                    },
                    {
                        "name": "Wenyu Yang"
                    },
                    {
                        "name": "Sergio Vitale"
                    },
                    {
                        "name": "Giampaolo Ferraioli"
                    },
                    {
                        "name": "Silvia Liberata Ullo"
                    },
                    {
                        "name": "Gilda Schirinzi"
                    }
                ],
                "author_detail": {
                    "name": "Gilda Schirinzi"
                },
                "author": "Gilda Schirinzi",
                "arxiv_comment": "13 pages, 12 figures, This paper has been submitted to IEEE TGRS. At\n  the moment is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20796v1",
                "updated": "2025-07-28T13:05:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    5,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:05:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    5,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "Aligning Large Language Model Agents with Rational and Moral\n  Preferences: A Supervised Fine-Tuning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Model Agents with Rational and Moral\n  Preferences: A Supervised Fine-Tuning Approach"
                },
                "summary": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Daniel L. Chen"
                    },
                    {
                        "name": "Christian B. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Christian B. Hansen"
                },
                "author": "Christian B. Hansen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06645v3",
                "updated": "2025-07-28T13:00:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    0,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2025-01-11T21:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    21,
                    41,
                    27,
                    5,
                    11,
                    0
                ],
                "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings"
                },
                "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00582v2",
                "updated": "2025-07-28T12:59:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    59,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-31T14:37:18Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    14,
                    37,
                    18,
                    5,
                    151,
                    0
                ],
                "title": "Do Language Models Mirror Human Confidence? Exploring Psychological\n  Insights to Address Overconfidence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Language Models Mirror Human Confidence? Exploring Psychological\n  Insights to Address Overconfidence in LLMs"
                },
                "summary": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty."
                },
                "authors": [
                    {
                        "name": "Chenjun Xu"
                    },
                    {
                        "name": "Bingbing Wen"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Robert Wolfe"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    },
                    {
                        "name": "Bill Howe"
                    }
                ],
                "author_detail": {
                    "name": "Bill Howe"
                },
                "author": "Bill Howe",
                "arxiv_comment": "Accepted by ACL 2025 Findings, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20786v1",
                "updated": "2025-07-28T12:56:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:56:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Automating Thematic Review of Prevention of Future Deaths Reports:\n  Replicating the ONS Child Suicide Study using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Thematic Review of Prevention of Future Deaths Reports:\n  Replicating the ONS Child Suicide Study using Large Language Models"
                },
                "summary": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research."
                },
                "authors": [
                    {
                        "name": "Sam Osian"
                    },
                    {
                        "name": "Arpan Dutta"
                    },
                    {
                        "name": "Sahil Bhandari"
                    },
                    {
                        "name": "Iain E. Buchan"
                    },
                    {
                        "name": "Dan W. Joyce"
                    }
                ],
                "author_detail": {
                    "name": "Dan W. Joyce"
                },
                "author": "Dan W. Joyce",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18557v2",
                "updated": "2025-07-28T12:56:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-24T16:30:46Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    30,
                    46,
                    3,
                    205,
                    0
                ],
                "title": "Deep Learning for Blood-Brain Barrier Permeability Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning for Blood-Brain Barrier Permeability Prediction"
                },
                "summary": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development."
                },
                "authors": [
                    {
                        "name": "Zihan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Yang"
                },
                "author": "Zihan Yang",
                "arxiv_comment": "Author list updated to reflect current contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20774v1",
                "updated": "2025-07-28T12:37:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    37,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:37:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    37,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract\n  Generated Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract\n  Generated Comments"
                },
                "summary": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Fatou Ndiaye Mbodji"
                    }
                ],
                "author_detail": {
                    "name": "Fatou Ndiaye Mbodji"
                },
                "author": "Fatou Ndiaye Mbodji",
                "arxiv_comment": "4 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19299v3",
                "updated": "2025-07-28T12:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    20,
                    24,
                    0,
                    209,
                    0
                ],
                "published": "2024-07-27T16:48:03Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    48,
                    3,
                    5,
                    209,
                    0
                ],
                "title": "The Impact of LoRA Adapters on LLMs for Clinical Text Classification\n  Under Computational and Data Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of LoRA Adapters on LLMs for Clinical Text Classification\n  Under Computational and Data Constraints"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to domain gap, limited data,\nand stringent hardware constraints. In this study, we evaluate four adapter\ntechniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note\nclassification under real-world, resource-constrained conditions. All\nexperiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512\nCUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and\nmaximum sequence length to 256 tokens. Our clinical corpus comprises only 580\n000 tokens, several orders of magnitude smaller than standard LLM pre-training\ndatasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio,\nAliBERT, DrBERT) and two lightweight Transformer models trained from scratch.\nResults show that 1) adapter structures provide no consistent gains when\nfine-tuning biomedical LLMs under these constraints, and 2) simpler\nTransformers, with minimal parameter counts and training times under six hours,\noutperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among\nadapters, GRN achieved the best metrics (accuracy, precision, recall, F1 =\n0.88). These findings demonstrate that, in low-resource clinical settings with\nlimited data and compute, lightweight Transformers trained from scratch offer a\nmore practical and efficient solution than large LLMs, while GRN remains a\nviable adapter choice when minimal adaptation is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to domain gap, limited data,\nand stringent hardware constraints. In this study, we evaluate four adapter\ntechniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note\nclassification under real-world, resource-constrained conditions. All\nexperiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512\nCUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and\nmaximum sequence length to 256 tokens. Our clinical corpus comprises only 580\n000 tokens, several orders of magnitude smaller than standard LLM pre-training\ndatasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio,\nAliBERT, DrBERT) and two lightweight Transformer models trained from scratch.\nResults show that 1) adapter structures provide no consistent gains when\nfine-tuning biomedical LLMs under these constraints, and 2) simpler\nTransformers, with minimal parameter counts and training times under six hours,\noutperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among\nadapters, GRN achieved the best metrics (accuracy, precision, recall, F1 =\n0.88). These findings demonstrate that, in low-resource clinical settings with\nlimited data and compute, lightweight Transformers trained from scratch offer a\nmore practical and efficient solution than large LLMs, while GRN remains a\nviable adapter choice when minimal adaptation is needed."
                },
                "authors": [
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    },
                    {
                        "name": "Philippe Jouvet"
                    },
                    {
                        "name": "Rita Noumeir"
                    }
                ],
                "author_detail": {
                    "name": "Rita Noumeir"
                },
                "author": "Rita Noumeir",
                "arxiv_doi": "10.1109/ACCESS.2025.3582037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3582037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in the IEEE Access",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20765v1",
                "updated": "2025-07-28T12:18:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    18,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:18:52Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    18,
                    52,
                    0,
                    209,
                    0
                ],
                "title": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural\n  Network"
                },
                "summary": "Hyperspectral imagers on satellites obtain the fine spectral signatures\nessential for distinguishing one material from another at the expense of\nlimited spatial resolution. Enhancing the latter is thus a desirable\npreprocessing step in order to further improve the detection capabilities\noffered by hyperspectral images on downstream tasks. At the same time, there is\na growing interest towards deploying inference methods directly onboard of\nsatellites, which calls for lightweight image super-resolution methods that can\nbe run on the payload in real time. In this paper, we present a novel neural\nnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches the\npushbroom acquisition of hyperspectral sensors by processing an image line by\nline in the along-track direction with a causal memory mechanism to exploit\npreviously acquired lines. This design greatly limits memory requirements and\ncomputational complexity, achieving onboard real-time performance, i.e., the\nability to super-resolve a line in the time it takes to acquire the next one,\non low-power hardware. Experiments show that the quality of the super-resolved\nimages is competitive or even outperforms state-of-the-art methods that are\nsignificantly more complex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperspectral imagers on satellites obtain the fine spectral signatures\nessential for distinguishing one material from another at the expense of\nlimited spatial resolution. Enhancing the latter is thus a desirable\npreprocessing step in order to further improve the detection capabilities\noffered by hyperspectral images on downstream tasks. At the same time, there is\na growing interest towards deploying inference methods directly onboard of\nsatellites, which calls for lightweight image super-resolution methods that can\nbe run on the payload in real time. In this paper, we present a novel neural\nnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches the\npushbroom acquisition of hyperspectral sensors by processing an image line by\nline in the along-track direction with a causal memory mechanism to exploit\npreviously acquired lines. This design greatly limits memory requirements and\ncomputational complexity, achieving onboard real-time performance, i.e., the\nability to super-resolve a line in the time it takes to acquire the next one,\non low-power hardware. Experiments show that the quality of the super-resolved\nimages is competitive or even outperforms state-of-the-art methods that are\nsignificantly more complex."
                },
                "authors": [
                    {
                        "name": "Davide Piccinini"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20762v1",
                "updated": "2025-07-28T12:16:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    16,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:16:52Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    16,
                    52,
                    0,
                    209,
                    0
                ],
                "title": "Watermarking Large Language Model-based Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Large Language Model-based Time Series Forecasting"
                },
                "summary": "Large Language Model-based Time Series Forecasting (LLMTS) has shown\nremarkable promise in handling complex and diverse temporal data, representing\na significant step toward foundation models for time series analysis. However,\nthis emerging paradigm introduces two critical challenges. First, the\nsubstantial commercial potential and resource-intensive development raise\nurgent concerns about intellectual property (IP) protection. Second, their\npowerful time series forecasting capabilities may be misused to produce\nmisleading or fabricated deepfake time series data. To address these concerns,\nwe explore watermarking the outputs of LLMTS models, that is, embedding\nimperceptible signals into the generated time series data that remain\ndetectable by specialized algorithms. We propose a novel post-hoc watermarking\nframework, Waltz, which is broadly compatible with existing LLMTS models. Waltz\nis inspired by the empirical observation that time series patch embeddings are\nrarely aligned with a specific set of LLM tokens, which we term ``cold\ntokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the\nsimilarity statistics between patch embeddings and cold token embeddings, and\ndetects watermarks using similarity z-scores. To minimize potential side\neffects, we introduce a similarity-based embedding position identification\nstrategy and employ projected gradient descent to constrain the watermark noise\nwithin a defined boundary. Extensive experiments using two popular LLMTS models\nacross seven benchmark datasets demonstrate that Waltz achieves high watermark\ndetection accuracy with minimal impact on the quality of the generated time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Time Series Forecasting (LLMTS) has shown\nremarkable promise in handling complex and diverse temporal data, representing\na significant step toward foundation models for time series analysis. However,\nthis emerging paradigm introduces two critical challenges. First, the\nsubstantial commercial potential and resource-intensive development raise\nurgent concerns about intellectual property (IP) protection. Second, their\npowerful time series forecasting capabilities may be misused to produce\nmisleading or fabricated deepfake time series data. To address these concerns,\nwe explore watermarking the outputs of LLMTS models, that is, embedding\nimperceptible signals into the generated time series data that remain\ndetectable by specialized algorithms. We propose a novel post-hoc watermarking\nframework, Waltz, which is broadly compatible with existing LLMTS models. Waltz\nis inspired by the empirical observation that time series patch embeddings are\nrarely aligned with a specific set of LLM tokens, which we term ``cold\ntokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the\nsimilarity statistics between patch embeddings and cold token embeddings, and\ndetects watermarks using similarity z-scores. To minimize potential side\neffects, we introduce a similarity-based embedding position identification\nstrategy and employ projected gradient descent to constrain the watermark noise\nwithin a defined boundary. Extensive experiments using two popular LLMTS models\nacross seven benchmark datasets demonstrate that Waltz achieves high watermark\ndetection accuracy with minimal impact on the quality of the generated time\nseries."
                },
                "authors": [
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Yu Xing"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20757v1",
                "updated": "2025-07-28T12:11:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    11,
                    12,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:11:12Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    11,
                    12,
                    0,
                    209,
                    0
                ],
                "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry"
                },
                "summary": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers."
                },
                "authors": [
                    {
                        "name": "Matan Kichler"
                    },
                    {
                        "name": "Shai Bagon"
                    },
                    {
                        "name": "Mark Sheinin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Sheinin"
                },
                "author": "Mark Sheinin",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20752v1",
                "updated": "2025-07-28T12:01:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    1,
                    59,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:01:59Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    1,
                    59,
                    0,
                    209,
                    0
                ],
                "title": "Multilingual Self-Taught Faithfulness Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Self-Taught Faithfulness Evaluators"
                },
                "summary": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Carlo Alfano"
                    },
                    {
                        "name": "Aymen Al Marjani"
                    },
                    {
                        "name": "Zeno Jonke"
                    },
                    {
                        "name": "Amin Mantrach"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17332v3",
                "updated": "2025-07-28T11:53:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    53,
                    55,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-23T09:00:13Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    0,
                    13,
                    2,
                    204,
                    0
                ],
                "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image"
                },
                "summary": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/."
                },
                "authors": [
                    {
                        "name": "Hyeongjin Nam"
                    },
                    {
                        "name": "Donghwan Kim"
                    },
                    {
                        "name": "Gyeongsik Moon"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyoung Mu Lee"
                },
                "author": "Kyoung Mu Lee",
                "arxiv_comment": "Published at ICCV 2025, 22 pages including the supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20745v1",
                "updated": "2025-07-28T11:52:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    52,
                    56,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T11:52:56Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    52,
                    56,
                    0,
                    209,
                    0
                ],
                "title": "Regularizing Subspace Redundancy of Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularizing Subspace Redundancy of Low-Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Jiazuo Yu"
                    },
                    {
                        "name": "Jiawen Zhu"
                    },
                    {
                        "name": "Yunzhi Zhuge"
                    },
                    {
                        "name": "Shuai Hao"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_doi": "10.1145/3746027.3755359",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755359",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures, Accepted by ACMMM2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10508v3",
                "updated": "2025-07-28T11:46:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    46,
                    29,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-13T16:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    9,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Hoi2Threat: An Interpretable Threat Detection Method for Human Violence\n  Scenarios Guided by Human-Object Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hoi2Threat: An Interpretable Threat Detection Method for Human Violence\n  Scenarios Guided by Human-Object Interaction"
                },
                "summary": "In light of the mounting imperative for public security, the necessity for\nautomated threat detection in high-risk scenarios is becoming increasingly\npressing. However, existing methods generally suffer from the problems of\nuninterpretable inference and biased semantic understanding, which severely\nlimits their reliability in practical deployment. In order to address the\naforementioned challenges, this article proposes a threat detection method\nbased on human-object interaction pairs (HOI-pairs), Hoi2Threat. This method is\nbased on the fine-grained multimodal TD-Hoi dataset, enhancing the model's\nsemantic modeling ability for key entities and their behavioral interactions by\nusing structured HOI tags to guide language generation. Furthermore, a set of\nmetrics is designed for the evaluation of text response quality, with the\nobjective of systematically measuring the model's representation accuracy and\ncomprehensibility during threat interpretation. The experimental results have\ndemonstrated that Hoi2Threat attains substantial enhancement in several threat\ndetection tasks, particularly in the core metrics of Correctness of Information\n(CoI), Behavioral Mapping Accuracy (BMA), and Threat Detailed Orientation\n(TDO), which are 5.08, 5.04, and 4.76, and 7.10%, 6.80%, and 2.63%,\nrespectively, in comparison with the Gemma3 (4B). The aforementioned results\nprovide comprehensive validation of the merits of this approach in the domains\nof semantic understanding, entity behavior mapping, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the mounting imperative for public security, the necessity for\nautomated threat detection in high-risk scenarios is becoming increasingly\npressing. However, existing methods generally suffer from the problems of\nuninterpretable inference and biased semantic understanding, which severely\nlimits their reliability in practical deployment. In order to address the\naforementioned challenges, this article proposes a threat detection method\nbased on human-object interaction pairs (HOI-pairs), Hoi2Threat. This method is\nbased on the fine-grained multimodal TD-Hoi dataset, enhancing the model's\nsemantic modeling ability for key entities and their behavioral interactions by\nusing structured HOI tags to guide language generation. Furthermore, a set of\nmetrics is designed for the evaluation of text response quality, with the\nobjective of systematically measuring the model's representation accuracy and\ncomprehensibility during threat interpretation. The experimental results have\ndemonstrated that Hoi2Threat attains substantial enhancement in several threat\ndetection tasks, particularly in the core metrics of Correctness of Information\n(CoI), Behavioral Mapping Accuracy (BMA), and Threat Detailed Orientation\n(TDO), which are 5.08, 5.04, and 4.76, and 7.10%, 6.80%, and 2.63%,\nrespectively, in comparison with the Gemma3 (4B). The aforementioned results\nprovide comprehensive validation of the merits of this approach in the domains\nof semantic understanding, entity behavior mapping, and interpretability."
                },
                "authors": [
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Daou Zhang"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Jinyang Chen"
                    },
                    {
                        "name": "Purui Dong"
                    },
                    {
                        "name": "Zuyuan Yu"
                    },
                    {
                        "name": "Ziru Wang"
                    },
                    {
                        "name": "Weichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wu"
                },
                "author": "Weichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18039v3",
                "updated": "2025-07-28T11:31:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    31,
                    55,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-25T03:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Nuoqian Xiao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted by ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04632v3",
                "updated": "2025-07-28T11:30:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    30,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-07T03:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    3,
                    20,
                    52,
                    0,
                    188,
                    0
                ],
                "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?"
                },
                "summary": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Björn Ommer"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13360v2",
                "updated": "2025-07-28T11:28:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    28,
                    8,
                    0,
                    209,
                    0
                ],
                "published": "2024-07-18T10:01:22Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    10,
                    1,
                    22,
                    3,
                    200,
                    0
                ],
                "title": "Ultra-Low-Latency Edge Inference for Distributed Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Low-Latency Edge Inference for Distributed Sensing"
                },
                "summary": "There is a broad consensus that artificial intelligence (AI) will be a\ndefining component of the sixth-generation (6G) networks. As a specific\ninstance, AI-empowered sensing will gather and process environmental perception\ndata at the network edge, giving rise to integrated sensing and edge AI (ISEA).\nMany applications, such as autonomous driving and industrial manufacturing, are\nlatency-sensitive and require end-to-end (E2E) performance guarantees under\nstringent deadlines. However, the 5G-style ultra-reliable and low-latency\ncommunication (URLLC) techniques designed with communication reliability and\nagnostic to the data may fall short in achieving the optimal E2E performance of\nperceptive wireless systems. In this work, we introduce an ultra-low-latency\n(ultra-LoLa) inference framework for perceptive networks that facilitates the\nanalysis of the E2E sensing accuracy in distributed sensing by jointly\nconsidering communication reliability and inference accuracy. By characterizing\nthe tradeoff between packet length and the number of sensing observations, we\nderive an efficient optimization procedure that closely approximates the\noptimal tradeoff. We validate the accuracy of the proposed method through\nexperimental results, and show that the proposed ultra-Lola inference framework\noutperforms conventional reliability-oriented protocols with respect to sensing\nperformance under a latency constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a broad consensus that artificial intelligence (AI) will be a\ndefining component of the sixth-generation (6G) networks. As a specific\ninstance, AI-empowered sensing will gather and process environmental perception\ndata at the network edge, giving rise to integrated sensing and edge AI (ISEA).\nMany applications, such as autonomous driving and industrial manufacturing, are\nlatency-sensitive and require end-to-end (E2E) performance guarantees under\nstringent deadlines. However, the 5G-style ultra-reliable and low-latency\ncommunication (URLLC) techniques designed with communication reliability and\nagnostic to the data may fall short in achieving the optimal E2E performance of\nperceptive wireless systems. In this work, we introduce an ultra-low-latency\n(ultra-LoLa) inference framework for perceptive networks that facilitates the\nanalysis of the E2E sensing accuracy in distributed sensing by jointly\nconsidering communication reliability and inference accuracy. By characterizing\nthe tradeoff between packet length and the number of sensing observations, we\nderive an efficient optimization procedure that closely approximates the\noptimal tradeoff. We validate the accuracy of the proposed method through\nexperimental results, and show that the proposed ultra-Lola inference framework\noutperforms conventional reliability-oriented protocols with respect to sensing\nperformance under a latency constraint."
                },
                "authors": [
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Anders E. Kalør"
                    },
                    {
                        "name": "You Zhou"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20730v1",
                "updated": "2025-07-28T11:26:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    26,
                    39,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T11:26:39Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    26,
                    39,
                    0,
                    209,
                    0
                ],
                "title": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice\n  Competitions"
                },
                "summary": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals."
                },
                "authors": [
                    {
                        "name": "Edvin Teskeredzic"
                    },
                    {
                        "name": "Muamer Paric"
                    },
                    {
                        "name": "Adna Sestic"
                    },
                    {
                        "name": "Petra Fribert"
                    },
                    {
                        "name": "Anamarija Lukac"
                    },
                    {
                        "name": "Hadzem Hadzic"
                    },
                    {
                        "name": "Kemal Altwlkany"
                    },
                    {
                        "name": "Emanuel Lacic"
                    }
                ],
                "author_detail": {
                    "name": "Emanuel Lacic"
                },
                "author": "Emanuel Lacic",
                "arxiv_doi": "10.1145/3720533.3750059",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720533.3750059",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Hypertext 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01460v3",
                "updated": "2025-07-28T11:12:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    12,
                    59,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-31T10:43:19Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    10,
                    43,
                    19,
                    1,
                    366,
                    0
                ],
                "title": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution"
                },
                "summary": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR by\nparalleling RWKV and convolutional operations to handle large-scale RSIs.\nFurthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an\nintermediary between the two branches to bridge their complementary roles. In\naddition, we propose the Dual-Group Multi-Scale Wavelet Loss, a wavelet-domain\nconstraint mechanism via dual-group subband strategy and cross-resolution\nfrequency alignment for enhanced reconstruction fidelity in RSI-SR. Extensive\nexperiments under two degradation methods on several benchmarks, including AID,\nUCMerced, and RSSRD-QH, demonstrate that GSDR outperforms the state-of-the-art\nTransformer-based method HAT by an average of 0.09 dB in PSNR, while using only\n63% of its parameters and 51% of its FLOPs, achieving an inference speed 3.2\ntimes faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR by\nparalleling RWKV and convolutional operations to handle large-scale RSIs.\nFurthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an\nintermediary between the two branches to bridge their complementary roles. In\naddition, we propose the Dual-Group Multi-Scale Wavelet Loss, a wavelet-domain\nconstraint mechanism via dual-group subband strategy and cross-resolution\nfrequency alignment for enhanced reconstruction fidelity in RSI-SR. Extensive\nexperiments under two degradation methods on several benchmarks, including AID,\nUCMerced, and RSSRD-QH, demonstrate that GSDR outperforms the state-of-the-art\nTransformer-based method HAT by an average of 0.09 dB in PSNR, while using only\n63% of its parameters and 51% of its FLOPs, achieving an inference speed 3.2\ntimes faster."
                },
                "authors": [
                    {
                        "name": "Qiwei Zhu"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Guojing Zhang"
                    },
                    {
                        "name": "Xiaoying Wang"
                    },
                    {
                        "name": "Jianqiang Huang"
                    },
                    {
                        "name": "Xilai Li"
                    }
                ],
                "author_detail": {
                    "name": "Xilai Li"
                },
                "author": "Xilai Li",
                "arxiv_comment": "GDSR: Global-Detail Integration through Dual-Branch Network with\n  Wavelet Losses for Remote Sensing Image Super-Resolution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20700v1",
                "updated": "2025-07-28T10:49:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    49,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:49:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    49,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained\n  Multilingual Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained\n  Multilingual Claim Verification"
                },
                "summary": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems."
                },
                "authors": [
                    {
                        "name": "Hanna Shcharbakova"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Natalia Skachkova"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "arxiv_comment": "Published at the FEVER Workshop, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03214v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03214v4",
                "updated": "2025-07-28T10:27:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    27,
                    45,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-04T11:05:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    5,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Continual Low-Rank Scaled Dot-product Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Low-Rank Scaled Dot-product Attention"
                },
                "summary": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models."
                },
                "authors": [
                    {
                        "name": "Ginés Carreto Picón"
                    },
                    {
                        "name": "Illia Oleksiienko"
                    },
                    {
                        "name": "Lukas Hedegaard"
                    },
                    {
                        "name": "Arian Bakhtiarnia"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Iosifidis"
                },
                "author": "Alexandros Iosifidis",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03214v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03214v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20696v2",
                "updated": "2025-07-29T12:05:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    5,
                    47,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-28T10:27:42Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    27,
                    42,
                    0,
                    209,
                    0
                ],
                "title": "Measuring coherence factors of states in superconductors through local\n  current",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring coherence factors of states in superconductors through local\n  current"
                },
                "summary": "The coherence factors of quasiparticles in a superconductor determine their\nproperties, including transport and susceptibility to electric fields. In this\nwork, we propose a way to infer the local coherence factors using local\ntransport to normal leads. Our method is based on measuring the local current\nthrough a lead as the coupling to a second one is varied: the shape of the\ncurrent is determined by the ratio between the local coherence factors,\nbecoming independent of the coupling to the second lead in the presence of\nlocal electron-hole symmetry, {\\it i.e.} coherence factors $|u|=|v|$. We apply\nour method to minimal Kitaev chains: arrays of quantum dots coupled via narrow\nsuperconducting segments. These chains feature Majorana-like quasiparticles\n(zero-energy states with $|u|=|v|$) at discrete points in parameter space. We\ndemonstrate that the local current allows us to estimate the local Majorana\npolarization (MP) -- a measurement of the local Majorana properties of the\nstate. We derive an analytical expression for the MP in terms of local currents\nand benchmark it against numerical calculations for 2- and 3-sites chains that\ninclude a finite Zeeman field and electron-electron interactions. These results\nprovide a way to quantitatively assess the quality of Majorana states in short\nKitaev chains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coherence factors of quasiparticles in a superconductor determine their\nproperties, including transport and susceptibility to electric fields. In this\nwork, we propose a way to infer the local coherence factors using local\ntransport to normal leads. Our method is based on measuring the local current\nthrough a lead as the coupling to a second one is varied: the shape of the\ncurrent is determined by the ratio between the local coherence factors,\nbecoming independent of the coupling to the second lead in the presence of\nlocal electron-hole symmetry, {\\it i.e.} coherence factors $|u|=|v|$. We apply\nour method to minimal Kitaev chains: arrays of quantum dots coupled via narrow\nsuperconducting segments. These chains feature Majorana-like quasiparticles\n(zero-energy states with $|u|=|v|$) at discrete points in parameter space. We\ndemonstrate that the local current allows us to estimate the local Majorana\npolarization (MP) -- a measurement of the local Majorana properties of the\nstate. We derive an analytical expression for the MP in terms of local currents\nand benchmark it against numerical calculations for 2- and 3-sites chains that\ninclude a finite Zeeman field and electron-electron interactions. These results\nprovide a way to quantitatively assess the quality of Majorana states in short\nKitaev chains."
                },
                "authors": [
                    {
                        "name": "Rodrigo A. Dourado"
                    },
                    {
                        "name": "Jeroen Danon"
                    },
                    {
                        "name": "Martin Leijnse"
                    },
                    {
                        "name": "Rubén Seoane Souto"
                    }
                ],
                "author_detail": {
                    "name": "Rubén Seoane Souto"
                },
                "author": "Rubén Seoane Souto",
                "arxiv_comment": "7 + 8 pages, 5 + 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06003v3",
                "updated": "2025-07-28T10:09:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    9,
                    27,
                    0,
                    209,
                    0
                ],
                "published": "2024-08-12T08:52:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM\n  Inference"
                },
                "summary": "Large Language Model (LLM) inference becomes resource-intensive, prompting a\nshift toward low-bit model weights to reduce the memory footprint and improve\nefficiency. Such low-bit LLMs necessitate the mixed-precision matrix\nmultiplication (mpGEMM), an important yet underexplored operation involving the\nmultiplication of lower-precision weights with higher-precision activations.\nOff-the-shelf hardware does not support this operation natively, leading to\nindirect, thus inefficient, dequantization-based implementations.\n  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and\nfind that a conventional LUT implementation fails to achieve the promised\ngains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor\nCore, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core\ndifferentiates itself from conventional LUT designs through: 1) software-based\noptimizations to minimize table precompute overhead and weight reinterpretation\nto reduce table storage; 2) a LUT-based Tensor Core hardware design with an\nelongated tiling shape to maximize table reuse and a bit-serial design to\nsupport diverse precision combinations in mpGEMM; 3) a new instruction set and\ncompilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly\noutperforms existing pure software LUT implementations and achieves a\n1.44$\\times$ improvement in compute density and energy efficiency compared to\nprevious state-of-the-art LUT-based accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference becomes resource-intensive, prompting a\nshift toward low-bit model weights to reduce the memory footprint and improve\nefficiency. Such low-bit LLMs necessitate the mixed-precision matrix\nmultiplication (mpGEMM), an important yet underexplored operation involving the\nmultiplication of lower-precision weights with higher-precision activations.\nOff-the-shelf hardware does not support this operation natively, leading to\nindirect, thus inefficient, dequantization-based implementations.\n  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and\nfind that a conventional LUT implementation fails to achieve the promised\ngains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor\nCore, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core\ndifferentiates itself from conventional LUT designs through: 1) software-based\noptimizations to minimize table precompute overhead and weight reinterpretation\nto reduce table storage; 2) a LUT-based Tensor Core hardware design with an\nelongated tiling shape to maximize table reuse and a bit-serial design to\nsupport diverse precision combinations in mpGEMM; 3) a new instruction set and\ncompilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly\noutperforms existing pure software LUT implementations and achieves a\n1.44$\\times$ improvement in compute density and energy efficiency compared to\nprevious state-of-the-art LUT-based accelerators."
                },
                "authors": [
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_doi": "10.1145/3695053.3731057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conference Version (ISCA'25). Fixed a typo",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.0; C.3; B.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20678v1",
                "updated": "2025-07-28T10:01:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    1,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:01:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    1,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process\n  Inference"
                },
                "summary": "The Cholesky decomposition is a fundamental tool for solving linear systems\nwith symmetric and positive definite matrices which are ubiquitous in linear\nalgebra, optimization, and machine learning. Its numerical stability can be\nimproved by introducing a pivoting strategy that iteratively permutes the rows\nand columns of the matrix. The order of pivoting indices determines how\naccurately the intermediate decomposition can reconstruct the original matrix,\nthus is decisive for the algorithm's efficiency in the case of early\ntermination. Standard implementations select the next pivot from the largest\nvalue on the diagonal. In the case of Bayesian nonparametric inference, this\nstrategy corresponds to greedy entropy maximization, which is often used in\nactive learning and design of experiments. We explore this connection in detail\nand deduce novel pivoting strategies for the Cholesky decomposition. The\nresulting algorithms are more efficient at reducing the uncertainty over a data\nset, can be updated to include information about observations, and additionally\nbenefit from a tailored implementation. We benchmark the effectiveness of the\nnew selection strategies on two tasks important to Gaussian processes: sparse\nregression and inference based on preconditioned iterative solvers. Our results\nshow that the proposed selection strategies are either on par or, in most\ncases, outperform traditional baselines while requiring a negligible amount of\nadditional computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cholesky decomposition is a fundamental tool for solving linear systems\nwith symmetric and positive definite matrices which are ubiquitous in linear\nalgebra, optimization, and machine learning. Its numerical stability can be\nimproved by introducing a pivoting strategy that iteratively permutes the rows\nand columns of the matrix. The order of pivoting indices determines how\naccurately the intermediate decomposition can reconstruct the original matrix,\nthus is decisive for the algorithm's efficiency in the case of early\ntermination. Standard implementations select the next pivot from the largest\nvalue on the diagonal. In the case of Bayesian nonparametric inference, this\nstrategy corresponds to greedy entropy maximization, which is often used in\nactive learning and design of experiments. We explore this connection in detail\nand deduce novel pivoting strategies for the Cholesky decomposition. The\nresulting algorithms are more efficient at reducing the uncertainty over a data\nset, can be updated to include information about observations, and additionally\nbenefit from a tailored implementation. We benchmark the effectiveness of the\nnew selection strategies on two tasks important to Gaussian processes: sparse\nregression and inference based on preconditioned iterative solvers. Our results\nshow that the proposed selection strategies are either on par or, in most\ncases, outperform traditional baselines while requiring a negligible amount of\nadditional computation."
                },
                "authors": [
                    {
                        "name": "Filip de Roos"
                    },
                    {
                        "name": "Fabio Muratore"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Muratore"
                },
                "author": "Fabio Muratore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08685v2",
                "updated": "2025-07-28T09:59:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-11T17:59:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "\"Principal Components\" Enable A New Language of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Principal Components\" Enable A New Language of Images"
                },
                "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space--a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, autoregressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space--a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, autoregressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference."
                },
                "authors": [
                    {
                        "name": "Xin Wen"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojuan Qi"
                },
                "author": "Xiaojuan Qi",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20674v1",
                "updated": "2025-07-28T09:55:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    55,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:55:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    55,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "LLM-Based Repair of Static Nullability Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Repair of Static Nullability Errors"
                },
                "summary": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects."
                },
                "authors": [
                    {
                        "name": "Nima Karimipour"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Martin Kellogg"
                    },
                    {
                        "name": "Manu Sridharan"
                    }
                ],
                "author_detail": {
                    "name": "Manu Sridharan"
                },
                "author": "Manu Sridharan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17762v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17762v4",
                "updated": "2025-07-28T09:54:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    54,
                    49,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-26T03:33:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    33,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding"
                },
                "summary": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks."
                },
                "authors": [
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Chen Du"
                    },
                    {
                        "name": "Ping Song"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17762v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17762v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20672v1",
                "updated": "2025-07-28T09:53:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    53,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:53:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    53,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Program Analysis for High-Value Smart Contract Vulnerabilities:\n  Techniques and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program Analysis for High-Value Smart Contract Vulnerabilities:\n  Techniques and Insights"
                },
                "summary": "A widespread belief in the blockchain security community is that automated\ntechniques are only good for detecting shallow bugs, typically of small value.\nIn this paper, we present the techniques and insights that have led us to\nrepeatable success in automatically discovering high-value smart contract\nvulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,\nfor a total of over $3M, over high-profile deployed code, as well as hundreds\nof bugs detected in pre-deployment or under-audit code.\n  We argue that the elements of this surprising success are a) a very\nhigh-completeness static analysis approach that manages to maintain acceptable\nprecision; b) domain knowledge, provided by experts or captured via statistical\ninference. We present novel techniques for automatically inferring domain\nknowledge from statistical analysis of a large corpus of deployed contracts, as\nwell as discuss insights on the ideal precision and warning rate of a promising\nvulnerability detector. In contrast to academic literature in program analysis,\nwhich routinely expects false-positive rates below 50% for publishable results,\nwe posit that a useful analysis for high-value real-world vulnerabilities will\nlikely flag very few programs (under 1%) and will do so with a high\nfalse-positive rate (e.g., 95%, meaning that only one-of-twenty human\ninspections will yield an exploitable vulnerability).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A widespread belief in the blockchain security community is that automated\ntechniques are only good for detecting shallow bugs, typically of small value.\nIn this paper, we present the techniques and insights that have led us to\nrepeatable success in automatically discovering high-value smart contract\nvulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,\nfor a total of over $3M, over high-profile deployed code, as well as hundreds\nof bugs detected in pre-deployment or under-audit code.\n  We argue that the elements of this surprising success are a) a very\nhigh-completeness static analysis approach that manages to maintain acceptable\nprecision; b) domain knowledge, provided by experts or captured via statistical\ninference. We present novel techniques for automatically inferring domain\nknowledge from statistical analysis of a large corpus of deployed contracts, as\nwell as discuss insights on the ideal precision and warning rate of a promising\nvulnerability detector. In contrast to academic literature in program analysis,\nwhich routinely expects false-positive rates below 50% for publishable results,\nwe posit that a useful analysis for high-value real-world vulnerabilities will\nlikely flag very few programs (under 1%) and will do so with a high\nfalse-positive rate (e.g., 95%, meaning that only one-of-twenty human\ninspections will yield an exploitable vulnerability)."
                },
                "authors": [
                    {
                        "name": "Yannis Smaragdakis"
                    },
                    {
                        "name": "Neville Grech"
                    },
                    {
                        "name": "Sifis Lagouvardos"
                    },
                    {
                        "name": "Konstantinos Triantafyllou"
                    },
                    {
                        "name": "Ilias Tsatiris"
                    },
                    {
                        "name": "Yannis Bollanos"
                    },
                    {
                        "name": "Tony Rocco Valentine"
                    }
                ],
                "author_detail": {
                    "name": "Tony Rocco Valentine"
                },
                "author": "Tony Rocco Valentine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20666v1",
                "updated": "2025-07-28T09:42:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    42,
                    41,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:42:41Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    42,
                    41,
                    0,
                    209,
                    0
                ],
                "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative\n  Evaluation of Anomalous Sound Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative\n  Evaluation of Anomalous Sound Detection"
                },
                "summary": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems."
                },
                "authors": [
                    {
                        "name": "Harsh Purohit"
                    },
                    {
                        "name": "Tomoya Nishida"
                    },
                    {
                        "name": "Kota Dohi"
                    },
                    {
                        "name": "Takashi Endo"
                    },
                    {
                        "name": "Yohei Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Kawaguchi"
                },
                "author": "Yohei Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20661v1",
                "updated": "2025-07-28T09:31:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    31,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:31:23Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    31,
                    23,
                    0,
                    209,
                    0
                ],
                "title": "Geometric reflective boundary conditions for asymptotically Anti-de\n  Sitter spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric reflective boundary conditions for asymptotically Anti-de\n  Sitter spaces"
                },
                "summary": "This article solves the initial boundary value problem for the vacuum\nEinstein equations with a negative cosmological constant in dimension 4, giving\nrise to asymptotically Anti-de Sitter spaces. We introduce a new family of\ngeometric reflective boundary conditions, which can be regarded as the\nhomogeneous Robin boundary conditions, involving both the conformal class and\nthe stress-energy tensor of the timelike conformal boundary. This family\nincludes as a special case the homogeneous Neumann boundary condition,\nconsisting of setting the boundary stress-energy tensor to zero. It also\nagrees, in a limit case, with the homogeneous Dirichlet boundary condition,\nwhere one fixes a locally conformally flat conformal class on the boundary,\nalready covered in Friedrich's pioneering work of 1995.\n  The proof of local existence and uniqueness for this family of boundary\nconditions relies notably on Friedrich's framework and his extended conformal\nEinstein equations. These are rewritten in a tensorial formalism, rather than a\nspinorial one, as it facilitates the comparison with the Fefferman-Graham\nexpansion of asymptotically Anti-de Sitter metrics. Similarly to Friedrich's\nproof, our geometric boundary conditions are eventually inferred from\ngauge-dependent boundary conditions by means of an auxiliary system on the\nconformal boundary.\n  In addition, our analysis also comprises new necessary and sufficient\nconditions for the unphysical fields associated to the initial data to be\nsmooth up to the conformal boundary. Finally, the paper contains examples of\nasymptotically Anti-de Sitter spaces, with a focus on their conformal boundary\ndata. This provides valuable insight into the possible shapes of boundary\nstress-energy tensors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article solves the initial boundary value problem for the vacuum\nEinstein equations with a negative cosmological constant in dimension 4, giving\nrise to asymptotically Anti-de Sitter spaces. We introduce a new family of\ngeometric reflective boundary conditions, which can be regarded as the\nhomogeneous Robin boundary conditions, involving both the conformal class and\nthe stress-energy tensor of the timelike conformal boundary. This family\nincludes as a special case the homogeneous Neumann boundary condition,\nconsisting of setting the boundary stress-energy tensor to zero. It also\nagrees, in a limit case, with the homogeneous Dirichlet boundary condition,\nwhere one fixes a locally conformally flat conformal class on the boundary,\nalready covered in Friedrich's pioneering work of 1995.\n  The proof of local existence and uniqueness for this family of boundary\nconditions relies notably on Friedrich's framework and his extended conformal\nEinstein equations. These are rewritten in a tensorial formalism, rather than a\nspinorial one, as it facilitates the comparison with the Fefferman-Graham\nexpansion of asymptotically Anti-de Sitter metrics. Similarly to Friedrich's\nproof, our geometric boundary conditions are eventually inferred from\ngauge-dependent boundary conditions by means of an auxiliary system on the\nconformal boundary.\n  In addition, our analysis also comprises new necessary and sufficient\nconditions for the unphysical fields associated to the initial data to be\nsmooth up to the conformal boundary. Finally, the paper contains examples of\nasymptotically Anti-de Sitter spaces, with a focus on their conformal boundary\ndata. This provides valuable insight into the possible shapes of boundary\nstress-energy tensors."
                },
                "authors": [
                    {
                        "name": "Ludovic Souêtre"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Souêtre"
                },
                "author": "Ludovic Souêtre",
                "arxiv_comment": "120 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "83C05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20655v1",
                "updated": "2025-07-28T09:21:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    21,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:21:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    21,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "CoGrader: Transforming Instructors' Assessment of Project Reports\n  through Collaborative LLM Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoGrader: Transforming Instructors' Assessment of Project Reports\n  through Collaborative LLM Integration"
                },
                "summary": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems."
                },
                "authors": [
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Haobo Li"
                    },
                    {
                        "name": "Chuhan Shi"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_doi": "10.1145/3746059.3747670",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747670",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ACM UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v2",
                "updated": "2025-07-28T09:19:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    19,
                    42,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Testora: Using Natural Language Intent to Detect Behavioral Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testora: Using Natural Language Intent to Detect Behavioral Regressions"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted at IEEE/ACM International Conference on Software Engineering\n  (ICSE) 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20643v1",
                "updated": "2025-07-28T09:00:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    0,
                    48,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:00:48Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    0,
                    48,
                    0,
                    209,
                    0
                ],
                "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Wenbin Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Zirui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Chen"
                },
                "author": "Zirui Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22518v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22518v3",
                "updated": "2025-07-28T08:58:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    58,
                    45,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-28T16:04:17Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    4,
                    17,
                    2,
                    148,
                    0
                ],
                "title": "IGNIS: A Robust Neural Network Framework for Constrained Parameter\n  Estimation in Archimedean Copulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGNIS: A Robust Neural Network Framework for Constrained Parameter\n  Estimation in Archimedean Copulas"
                },
                "summary": "Classical estimators, the cornerstones of statistical inference, face\ninsurmountable challenges when applied to important emerging classes of\nArchimedean copulas. These models exhibit pathological properties, including\nnumerically unstable densities, non-monotonic parameter-to-dependence mappings,\nand vanishingly small likelihood gradients, rendering methods like Maximum\nLikelihood (MLE) and Method of Moments (MoM) inconsistent or computationally\ninfeasible. We introduce IGNIS, a unified neural estimation framework that\nsidesteps these barriers by learning a direct, robust mapping from data-driven\ndependency measures to the underlying copula parameter theta. IGNIS utilizes a\nmulti-input architecture and a theory-guided output layer (softplus(z) + 1) to\nautomatically enforce the domain constraint theta_hat >= 1. Trained and\nvalidated on four families (Gumbel, Joe, and the numerically challenging\nA1/A2), IGNIS delivers accurate and stable estimates for real-world financial\nand health datasets, demonstrating its necessity for reliable inference in\nmodern, complex dependence models where traditional methods fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical estimators, the cornerstones of statistical inference, face\ninsurmountable challenges when applied to important emerging classes of\nArchimedean copulas. These models exhibit pathological properties, including\nnumerically unstable densities, non-monotonic parameter-to-dependence mappings,\nand vanishingly small likelihood gradients, rendering methods like Maximum\nLikelihood (MLE) and Method of Moments (MoM) inconsistent or computationally\ninfeasible. We introduce IGNIS, a unified neural estimation framework that\nsidesteps these barriers by learning a direct, robust mapping from data-driven\ndependency measures to the underlying copula parameter theta. IGNIS utilizes a\nmulti-input architecture and a theory-guided output layer (softplus(z) + 1) to\nautomatically enforce the domain constraint theta_hat >= 1. Trained and\nvalidated on four families (Gumbel, Joe, and the numerically challenging\nA1/A2), IGNIS delivers accurate and stable estimates for real-world financial\nand health datasets, demonstrating its necessity for reliable inference in\nmodern, complex dependence models where traditional methods fail."
                },
                "authors": [
                    {
                        "name": "Agnideep Aich"
                    }
                ],
                "author_detail": {
                    "name": "Agnideep Aich"
                },
                "author": "Agnideep Aich",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22518v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22518v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H05, 62H12, 62F10, 68T07, 62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18181v2",
                "updated": "2025-07-28T08:55:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    55,
                    9,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-24T08:27:53Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    27,
                    53,
                    3,
                    205,
                    0
                ],
                "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding"
                },
                "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy."
                },
                "authors": [
                    {
                        "name": "Linye Wei"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Songqiang Xu"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by Design Automation Conference (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20630v1",
                "updated": "2025-07-28T08:44:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    44,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:44:58Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    44,
                    58,
                    0,
                    209,
                    0
                ],
                "title": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language\n  Model"
                },
                "summary": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but\nface high computational costs due to the large number of visual tokens,\nmotivating token pruning to improve inference efficiency. The key challenge\nlies in identifying which tokens are truly important. Most existing approaches\nrely on attention-based criteria to estimate token importance. However, they\ninherently suffer from certain limitations, such as positional bias. In this\nwork, we explore a new perspective on token importance based on token\ntransitions in LVLMs. We observe that the transition of token representations\nprovides a meaningful signal of semantic information. Based on this insight, we\npropose TransPrune, a training-free and efficient token pruning method.\nSpecifically, TransPrune progressively prunes tokens by assessing their\nimportance through a combination of Token Transition Variation (TTV)-which\nmeasures changes in both the magnitude and direction of token\nrepresentations-and Instruction-Guided Attention (IGA), which measures how\nstrongly the instruction attends to image tokens via attention. Extensive\nexperiments demonstrate that TransPrune achieves comparable multimodal\nperformance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight\nbenchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV\nalone can serve as an effective criterion without relying on attention,\nachieving performance comparable to attention-based methods. The code will be\nmade publicly available upon acceptance of the paper at\nhttps://github.com/liaolea/TransPrune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but\nface high computational costs due to the large number of visual tokens,\nmotivating token pruning to improve inference efficiency. The key challenge\nlies in identifying which tokens are truly important. Most existing approaches\nrely on attention-based criteria to estimate token importance. However, they\ninherently suffer from certain limitations, such as positional bias. In this\nwork, we explore a new perspective on token importance based on token\ntransitions in LVLMs. We observe that the transition of token representations\nprovides a meaningful signal of semantic information. Based on this insight, we\npropose TransPrune, a training-free and efficient token pruning method.\nSpecifically, TransPrune progressively prunes tokens by assessing their\nimportance through a combination of Token Transition Variation (TTV)-which\nmeasures changes in both the magnitude and direction of token\nrepresentations-and Instruction-Guided Attention (IGA), which measures how\nstrongly the instruction attends to image tokens via attention. Extensive\nexperiments demonstrate that TransPrune achieves comparable multimodal\nperformance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight\nbenchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV\nalone can serve as an effective criterion without relying on attention,\nachieving performance comparable to attention-based methods. The code will be\nmade publicly available upon acceptance of the paper at\nhttps://github.com/liaolea/TransPrune."
                },
                "authors": [
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Yuxiang Duan"
                    },
                    {
                        "name": "Jinghui Zhang"
                    },
                    {
                        "name": "Congbo Ma"
                    },
                    {
                        "name": "Yutong Xie"
                    },
                    {
                        "name": "Gustavo Carneiro"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    },
                    {
                        "name": "Hu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hu Wang"
                },
                "author": "Hu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20629v1",
                "updated": "2025-07-28T08:42:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    42,
                    0,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:42:00Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    42,
                    0,
                    0,
                    209,
                    0
                ],
                "title": "DAMS:Dual-Branch Adaptive Multiscale Spatiotemporal Framework for Video\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAMS:Dual-Branch Adaptive Multiscale Spatiotemporal Framework for Video\n  Anomaly Detection"
                },
                "summary": "The goal of video anomaly detection is tantamount to performing\nspatio-temporal localization of abnormal events in the video. The multiscale\ntemporal dependencies, visual-semantic heterogeneity, and the scarcity of\nlabeled data exhibited by video anomalies collectively present a challenging\nresearch problem in computer vision. This study offers a dual-path architecture\ncalled the Dual-Branch Adaptive Multiscale Spatiotemporal Framework (DAMS),\nwhich is based on multilevel feature decoupling and fusion, enabling efficient\nanomaly detection modeling by integrating hierarchical feature learning and\ncomplementary information. The main processing path of this framework\nintegrates the Adaptive Multiscale Time Pyramid Network (AMTPN) with the\nConvolutional Block Attention Mechanism (CBAM). AMTPN enables multigrained\nrepresentation and dynamically weighted reconstruction of temporal features\nthrough a three-level cascade structure (time pyramid pooling, adaptive feature\nfusion, and temporal context enhancement). CBAM maximizes the entropy\ndistribution of feature channels and spatial dimensions through dual attention\nmapping. Simultaneously, the parallel path driven by CLIP introduces a\ncontrastive language-visual pre-training paradigm. Cross-modal semantic\nalignment and a multiscale instance selection mechanism provide high-order\nsemantic guidance for spatio-temporal features. This creates a complete\ninference chain from the underlying spatio-temporal features to high-level\nsemantic concepts. The orthogonal complementarity of the two paths and the\ninformation fusion mechanism jointly construct a comprehensive representation\nand identification capability for anomalous events. Extensive experimental\nresults on the UCF-Crime and XD-Violence benchmarks establish the effectiveness\nof the DAMS framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of video anomaly detection is tantamount to performing\nspatio-temporal localization of abnormal events in the video. The multiscale\ntemporal dependencies, visual-semantic heterogeneity, and the scarcity of\nlabeled data exhibited by video anomalies collectively present a challenging\nresearch problem in computer vision. This study offers a dual-path architecture\ncalled the Dual-Branch Adaptive Multiscale Spatiotemporal Framework (DAMS),\nwhich is based on multilevel feature decoupling and fusion, enabling efficient\nanomaly detection modeling by integrating hierarchical feature learning and\ncomplementary information. The main processing path of this framework\nintegrates the Adaptive Multiscale Time Pyramid Network (AMTPN) with the\nConvolutional Block Attention Mechanism (CBAM). AMTPN enables multigrained\nrepresentation and dynamically weighted reconstruction of temporal features\nthrough a three-level cascade structure (time pyramid pooling, adaptive feature\nfusion, and temporal context enhancement). CBAM maximizes the entropy\ndistribution of feature channels and spatial dimensions through dual attention\nmapping. Simultaneously, the parallel path driven by CLIP introduces a\ncontrastive language-visual pre-training paradigm. Cross-modal semantic\nalignment and a multiscale instance selection mechanism provide high-order\nsemantic guidance for spatio-temporal features. This creates a complete\ninference chain from the underlying spatio-temporal features to high-level\nsemantic concepts. The orthogonal complementarity of the two paths and the\ninformation fusion mechanism jointly construct a comprehensive representation\nand identification capability for anomalous events. Extensive experimental\nresults on the UCF-Crime and XD-Violence benchmarks establish the effectiveness\nof the DAMS framework."
                },
                "authors": [
                    {
                        "name": "Dezhi An"
                    },
                    {
                        "name": "Wenqiang Liu"
                    },
                    {
                        "name": "Kefan Wang"
                    },
                    {
                        "name": "Zening Chen"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Shengcai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengcai Zhang"
                },
                "author": "Shengcai Zhang",
                "arxiv_comment": "13 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20623v1",
                "updated": "2025-07-28T08:36:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    36,
                    36,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:36:36Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    36,
                    36,
                    0,
                    209,
                    0
                ],
                "title": "Lightweight Remote Sensing Scene Classification on Edge Devices via\n  Knowledge Distillation and Early-exit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Remote Sensing Scene Classification on Edge Devices via\n  Knowledge Distillation and Early-exit"
                },
                "summary": "As the development of lightweight deep learning algorithms, various deep\nneural network (DNN) models have been proposed for the remote sensing scene\nclassification (RSSC) application. However, it is still challenging for these\nRSSC models to achieve optimal performance among model accuracy, inference\nlatency, and energy consumption on resource-constrained edge devices. In this\npaper, we propose a lightweight RSSC framework, which includes a distilled\nglobal filter network (GFNet) model and an early-exit mechanism designed for\nedge devices to achieve state-of-the-art performance. Specifically, we first\napply frequency domain distillation on the GFNet model to reduce model size.\nThen we design a dynamic early-exit model tailored for DNN models on edge\ndevices to further improve model inference efficiency. We evaluate our E3C\nmodel on three edge devices across four datasets. Extensive experimental\nresults show that it achieves an average of 1.3x speedup on model inference and\nover 40% improvement on energy efficiency, while maintaining high\nclassification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the development of lightweight deep learning algorithms, various deep\nneural network (DNN) models have been proposed for the remote sensing scene\nclassification (RSSC) application. However, it is still challenging for these\nRSSC models to achieve optimal performance among model accuracy, inference\nlatency, and energy consumption on resource-constrained edge devices. In this\npaper, we propose a lightweight RSSC framework, which includes a distilled\nglobal filter network (GFNet) model and an early-exit mechanism designed for\nedge devices to achieve state-of-the-art performance. Specifically, we first\napply frequency domain distillation on the GFNet model to reduce model size.\nThen we design a dynamic early-exit model tailored for DNN models on edge\ndevices to further improve model inference efficiency. We evaluate our E3C\nmodel on three edge devices across four datasets. Extensive experimental\nresults show that it achieves an average of 1.3x speedup on model inference and\nover 40% improvement on energy efficiency, while maintaining high\nclassification accuracy."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Shusheng Li"
                    },
                    {
                        "name": "Xueshang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xueshang Feng"
                },
                "author": "Xueshang Feng",
                "arxiv_comment": "9 pages, 5 figures, to be published in ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13940v2",
                "updated": "2025-07-28T08:10:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    10,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-20T05:18:15Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    18,
                    15,
                    1,
                    140,
                    0
                ],
                "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery"
                },
                "summary": "Large language models (LLMs) integrated with autonomous agents hold\nsignificant potential for advancing scientific discovery through automated\nreasoning and task execution. However, applying LLM agents to drug discovery is\nstill constrained by challenges such as large-scale multimodal data processing,\nlimited task automation, and poor support for domain-specific tools. To\novercome these limitations, we introduce DrugPilot, a LLM-based agent system\nwith a parameterized reasoning architecture designed for end-to-end scientific\nworkflows in drug discovery. DrugPilot enables multi-stage research processes\nby integrating structured tool use with a novel parameterized memory pool. The\nmemory pool converts heterogeneous data from both public sources and\nuser-defined inputs into standardized representations. This design supports\nefficient multi-turn dialogue, reduces information loss during data exchange,\nand enhances complex scientific decision-making. To support training and\nbenchmarking, we construct a drug instruction dataset covering eight core drug\ndiscovery tasks. Under the Berkeley function-calling benchmark, DrugPilot\nsignificantly outperforms state-of-the-art agents such as ReAct and LoT,\nachieving task completion rates of 98.0%, 93.5%, and 64.0% for simple,\nmulti-tool, and multi-turn scenarios, respectively. These results highlight\nDrugPilot's potential as a versatile agent framework for computational science\ndomains requiring automated, interactive, and data-integrated reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated with autonomous agents hold\nsignificant potential for advancing scientific discovery through automated\nreasoning and task execution. However, applying LLM agents to drug discovery is\nstill constrained by challenges such as large-scale multimodal data processing,\nlimited task automation, and poor support for domain-specific tools. To\novercome these limitations, we introduce DrugPilot, a LLM-based agent system\nwith a parameterized reasoning architecture designed for end-to-end scientific\nworkflows in drug discovery. DrugPilot enables multi-stage research processes\nby integrating structured tool use with a novel parameterized memory pool. The\nmemory pool converts heterogeneous data from both public sources and\nuser-defined inputs into standardized representations. This design supports\nefficient multi-turn dialogue, reduces information loss during data exchange,\nand enhances complex scientific decision-making. To support training and\nbenchmarking, we construct a drug instruction dataset covering eight core drug\ndiscovery tasks. Under the Berkeley function-calling benchmark, DrugPilot\nsignificantly outperforms state-of-the-art agents such as ReAct and LoT,\nachieving task completion rates of 98.0%, 93.5%, and 64.0% for simple,\nmulti-tool, and multi-turn scenarios, respectively. These results highlight\nDrugPilot's potential as a versatile agent framework for computational science\ndomains requiring automated, interactive, and data-integrated reasoning."
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Zhennan Wu"
                    },
                    {
                        "name": "Shoupeng Wang"
                    },
                    {
                        "name": "Jia Wu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Wenbin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Hu"
                },
                "author": "Wenbin Hu",
                "arxiv_comment": "29 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20592v1",
                "updated": "2025-07-28T08:02:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    2,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:02:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    2,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase\n  Adaptation"
                },
                "summary": "Neural Architecture Search (NAS) is challenged by the trade-off between\nsearch space exploration and efficiency, especially for complex tasks. While\nrecent LLM-based NAS methods have shown promise, they often suffer from static\nsearch strategies and ambiguous architecture representations. We propose\nPhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by\nreal-time score thresholds and a structured architecture template language for\nconsistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS\nconsistently discovers architectures with higher accuracy and better rank. For\nimage classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%\nwhile maintaining or improving accuracy. In object detection, it automatically\nproduces YOLOv8 variants with higher mAP and lower resource cost. These results\ndemonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS\nacross diverse vision tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) is challenged by the trade-off between\nsearch space exploration and efficiency, especially for complex tasks. While\nrecent LLM-based NAS methods have shown promise, they often suffer from static\nsearch strategies and ambiguous architecture representations. We propose\nPhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by\nreal-time score thresholds and a structured architecture template language for\nconsistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS\nconsistently discovers architectures with higher accuracy and better rank. For\nimage classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%\nwhile maintaining or improving accuracy. In object detection, it automatically\nproduces YOLOv8 variants with higher mAP and lower resource cost. These results\ndemonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS\nacross diverse vision tasks."
                },
                "authors": [
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Xiaohan Shan"
                    },
                    {
                        "name": "Yanwei Hu"
                    },
                    {
                        "name": "Jianmin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Li"
                },
                "author": "Jianmin Li",
                "arxiv_comment": "14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.21046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v1",
                "updated": "2025-07-28T17:59:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenghailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21035v1",
                "updated": "2025-07-28T17:55:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    55,
                    8,
                    0,
                    209,
                    0
                ],
                "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis"
                },
                "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "51 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21028v1",
                "updated": "2025-07-28T17:48:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    48,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:48:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    48,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with\n  Multi-Dimensional Human Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with\n  Multi-Dimensional Human Evaluation"
                },
                "summary": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Huimin Zeng"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12854v3",
                "updated": "2025-07-28T17:46:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    46,
                    50,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-17T06:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    28,
                    25,
                    0,
                    76,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation"
                },
                "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations."
                },
                "authors": [
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Jiahao Lin"
                    },
                    {
                        "name": "Xiangyu Tian"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Dongmei Jiang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "arxiv_comment": "23pages",
                "arxiv_journal_ref": "COLM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21017v1",
                "updated": "2025-07-28T17:38:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    38,
                    29,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:38:29Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    38,
                    29,
                    0,
                    209,
                    0
                ],
                "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them"
                },
                "summary": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments."
                },
                "authors": [
                    {
                        "name": "Weichen Zhang"
                    },
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Pohao Huang"
                    },
                    {
                        "name": "Jiayue Pu"
                    },
                    {
                        "name": "Heyue Lin"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "Code and data: https://github.com/sunblaze-ucb/mirage-bench.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02087v2",
                "updated": "2025-07-28T17:26:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    26,
                    1,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-02T19:02:18Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    19,
                    2,
                    18,
                    2,
                    183,
                    0
                ],
                "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions"
                },
                "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes."
                },
                "authors": [
                    {
                        "name": "Eitan Anzenberg"
                    },
                    {
                        "name": "Arunava Samajpati"
                    },
                    {
                        "name": "Sivasankaran Chandrasekar"
                    },
                    {
                        "name": "Varun Kacholia"
                    }
                ],
                "author_detail": {
                    "name": "Varun Kacholia"
                },
                "author": "Varun Kacholia",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21012v1",
                "updated": "2025-07-28T17:23:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    23,
                    34,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:23:34Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    23,
                    34,
                    0,
                    209,
                    0
                ],
                "title": "User-Centered Design with AI in the Loop: A Case Study of Rapid User\n  Interface Prototyping with \"Vibe Coding\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-Centered Design with AI in the Loop: A Case Study of Rapid User\n  Interface Prototyping with \"Vibe Coding\""
                },
                "summary": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Tanay Maheshwari"
                    },
                    {
                        "name": "Alex Voelker"
                    }
                ],
                "author_detail": {
                    "name": "Alex Voelker"
                },
                "author": "Alex Voelker",
                "arxiv_journal_ref": "ACM Collective Intelligence 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21009v1",
                "updated": "2025-07-28T17:22:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    22,
                    10,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:22:10Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    22,
                    10,
                    0,
                    209,
                    0
                ],
                "title": "Memorization in Fine-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Fine-Tuned Large Language Models"
                },
                "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."
                },
                "authors": [
                    {
                        "name": "Danil Savine"
                    },
                    {
                        "name": "Muni Sreenivas Pydi"
                    },
                    {
                        "name": "Jamal Atif"
                    },
                    {
                        "name": "Olivier Cappé"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Cappé"
                },
                "author": "Olivier Cappé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21004v1",
                "updated": "2025-07-28T17:18:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    18,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:18:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    18,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Compositional Function Networks: A High-Performance Alternative to Deep\n  Neural Networks with Built-in Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Function Networks: A High-Performance Alternative to Deep\n  Neural Networks with Built-in Interpretability"
                },
                "summary": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount."
                },
                "authors": [
                    {
                        "name": "Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Fang Li"
                },
                "author": "Fang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20999v1",
                "updated": "2025-07-28T17:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:11:26Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    11,
                    26,
                    0,
                    209,
                    0
                ],
                "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning"
                },
                "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."
                },
                "authors": [
                    {
                        "name": "Yining Huang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Keke Tang"
                    },
                    {
                        "name": "Meilian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Meilian Chen"
                },
                "author": "Meilian Chen",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20997v1",
                "updated": "2025-07-28T17:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    8,
                    49,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    8,
                    49,
                    0,
                    209,
                    0
                ],
                "title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework\n  for Continual and Reversible Model Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework\n  for Continual and Reversible Model Composition"
                },
                "summary": "In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design."
                },
                "authors": [
                    {
                        "name": "Haris Khan"
                    },
                    {
                        "name": "Shumaila Asif"
                    },
                    {
                        "name": "Sadia Asif"
                    }
                ],
                "author_detail": {
                    "name": "Sadia Asif"
                },
                "author": "Sadia Asif",
                "arxiv_comment": "11 pages, 6 figures, 3 tables. Will be Submitted to ICLR 2025 for\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20995v1",
                "updated": "2025-07-28T17:04:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    4,
                    28,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T17:04:28Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    4,
                    28,
                    0,
                    209,
                    0
                ],
                "title": "VArsity: Can Large Language Models Keep Power Engineering Students in\n  Phase?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VArsity: Can Large Language Models Keep Power Engineering Students in\n  Phase?"
                },
                "summary": "This paper provides an educational case study regarding our experience in\ndeploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023\nofferings of ECE 4320: Power System Analysis and Control at Georgia Tech. As\npart of course assessments, students were tasked with identifying, explaining,\nand correcting errors in the ChatGPT outputs corresponding to power factor\ncorrection problems. While most students successfully identified the errors in\nthe outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found\nthe errors from the ChatGPT o1 version much more difficult to identify in\nSpring 2025. As shown in this case study, the role of LLMs in pedagogy,\nassessment, and learning in power engineering classrooms is an important topic\ndeserving further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an educational case study regarding our experience in\ndeploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023\nofferings of ECE 4320: Power System Analysis and Control at Georgia Tech. As\npart of course assessments, students were tasked with identifying, explaining,\nand correcting errors in the ChatGPT outputs corresponding to power factor\ncorrection problems. While most students successfully identified the errors in\nthe outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found\nthe errors from the ChatGPT o1 version much more difficult to identify in\nSpring 2025. As shown in this case study, the role of LLMs in pedagogy,\nassessment, and learning in power engineering classrooms is an important topic\ndeserving further investigation."
                },
                "authors": [
                    {
                        "name": "Samuel Talkington"
                    },
                    {
                        "name": "Daniel K. Molzahn"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. Molzahn"
                },
                "author": "Daniel K. Molzahn",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20994v1",
                "updated": "2025-07-28T16:59:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    59,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:59:53Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    59,
                    53,
                    0,
                    209,
                    0
                ],
                "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM"
                },
                "summary": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Wujia Niu"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "arxiv_comment": "Codes and data are available at\n  https://github.com/listen0425/Security-Tensors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00022v3",
                "updated": "2025-07-28T16:50:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    50,
                    18,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-21T17:06:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    6,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "Scaling Physical Reasoning with the PHYSICS Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Physical Reasoning with the PHYSICS Dataset"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "arxiv_comment": "Work on physical datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16815v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16815v3",
                "updated": "2025-07-28T16:47:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    47,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-25T15:35:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREE-Merging: Fourier Transform for Efficient Model Merging"
                },
                "summary": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of deep learning, there is an increasing availability\nof open-source models for various tasks. However, single fine-tuned models\noften fall short of meeting the diverse needs of users. Model merging has thus\nemerged as an efficient method to integrate the capabilities of existing models\ninto a unified model. Nevertheless, existing model merging methods face\nchallenging trade-offs between performance and deployment costs, primarily due\nto task interference. For the first time, we reveal that task interference is\nevident in the frequency domain of model parameters, yet current efforts only\nfocus on spatial domain solutions, which are largely ineffective in addressing\nfrequency domain interference. To mitigate the impact of frequency domain\ninterference, we propose FR-Merging, an innovative method that effectively\nfilters harmful frequency domain interference on the backbone with minimal\ncomputational overhead. Since performance loss is inevitable with cost-free\nmethods, we propose a lightweight task-specific expert module that dynamically\ncompensates for information loss during merging. This proposed framework,\nFREE-Merging (FR-Merging with experts), strikes a balanced trade-off between\ntraining cost, inference latency, storage requirements, and performance. We\ndemonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple\ntasks across CV, NLP, and Multi-Modal domains and show that they can be\nflexibly adapted to specific needs."
                },
                "authors": [
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16815v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16815v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v1",
                "updated": "2025-07-28T16:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.16298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.16298v3",
                "updated": "2025-07-28T16:40:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    40,
                    24,
                    0,
                    209,
                    0
                ],
                "published": "2023-08-30T19:58:56Z",
                "published_parsed": [
                    2023,
                    8,
                    30,
                    19,
                    58,
                    56,
                    2,
                    242,
                    0
                ],
                "title": "Publishing Wikipedia usage data with strong privacy guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publishing Wikipedia usage data with strong privacy guarantees"
                },
                "summary": "For almost 20 years, the Wikimedia Foundation has been publishing statistics\nabout how many people visited each Wikipedia page on each day. This data helps\nWikipedia editors determine where to focus their efforts to improve the online\nencyclopedia, and enables academic research. In June 2023, the Wikimedia\nFoundation, helped by Tumult Labs, addressed a long-standing request from\nWikipedia editors and academic researchers: it started publishing these\nstatistics with finer granularity, including the country of origin in the daily\ncounts of page views. This new data publication uses differential privacy to\nprovide robust guarantees to people browsing or editing Wikipedia. This paper\ndescribes this data publication: its goals, the process followed from its\ninception to its deployment, the algorithms used to produce the data, and the\noutcomes of the data release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For almost 20 years, the Wikimedia Foundation has been publishing statistics\nabout how many people visited each Wikipedia page on each day. This data helps\nWikipedia editors determine where to focus their efforts to improve the online\nencyclopedia, and enables academic research. In June 2023, the Wikimedia\nFoundation, helped by Tumult Labs, addressed a long-standing request from\nWikipedia editors and academic researchers: it started publishing these\nstatistics with finer granularity, including the country of origin in the daily\ncounts of page views. This new data publication uses differential privacy to\nprovide robust guarantees to people browsing or editing Wikipedia. This paper\ndescribes this data publication: its goals, the process followed from its\ninception to its deployment, the algorithms used to produce the data, and the\noutcomes of the data release."
                },
                "authors": [
                    {
                        "name": "Temilola Adeleye"
                    },
                    {
                        "name": "Skye Berghel"
                    },
                    {
                        "name": "Damien Desfontaines"
                    },
                    {
                        "name": "Michael Hay"
                    },
                    {
                        "name": "Isaac Johnson"
                    },
                    {
                        "name": "Cléo Lemoisson"
                    },
                    {
                        "name": "Ashwin Machanavajjhala"
                    },
                    {
                        "name": "Tom Magerlein"
                    },
                    {
                        "name": "Gabriele Modena"
                    },
                    {
                        "name": "David Pujol"
                    },
                    {
                        "name": "Daniel Simmons-Marengo"
                    },
                    {
                        "name": "Hal Triedman"
                    }
                ],
                "author_detail": {
                    "name": "Hal Triedman"
                },
                "author": "Hal Triedman",
                "arxiv_comment": "11 pages, 10 figures, Theory and Practice of Differential Privacy\n  (TPDP) 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.16298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.16298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20977v1",
                "updated": "2025-07-28T16:39:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    39,
                    16,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:39:16Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    39,
                    16,
                    0,
                    209,
                    0
                ],
                "title": "Repairing vulnerabilities without invisible hands. A differentiated\n  replication study on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repairing vulnerabilities without invisible hands. A differentiated\n  replication study on LLMs"
                },
                "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method."
                },
                "authors": [
                    {
                        "name": "Maria Camporese"
                    },
                    {
                        "name": "Fabio Massacci"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massacci"
                },
                "author": "Fabio Massacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17137v2",
                "updated": "2025-07-28T16:30:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    30,
                    46,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-22T05:40:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    5,
                    40,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive\n  Decline via Longitudinal Voice Assistant Commands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive\n  Decline via Longitudinal Voice Assistant Commands"
                },
                "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline."
                },
                "authors": [
                    {
                        "name": "Kristin Qi"
                    },
                    {
                        "name": "Youxiang Zhu"
                    },
                    {
                        "name": "Caroline Summerour"
                    },
                    {
                        "name": "John A. Batsis"
                    },
                    {
                        "name": "Xiaohui Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Liang"
                },
                "author": "Xiaohui Liang",
                "arxiv_comment": "IEEE Global Communications Conference (GlobeCom) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20964v1",
                "updated": "2025-07-28T16:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    19,
                    25,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:19:25Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    19,
                    25,
                    0,
                    209,
                    0
                ],
                "title": "Core Safety Values for Provably Corrigible Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Core Safety Values for Provably Corrigible Agents"
                },
                "summary": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems."
                },
                "authors": [
                    {
                        "name": "Aran Nayebi"
                    }
                ],
                "author_detail": {
                    "name": "Aran Nayebi"
                },
                "author": "Aran Nayebi",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07119v3",
                "updated": "2025-07-28T16:17:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    17,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-11T21:05:33Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    21,
                    5,
                    33,
                    6,
                    131,
                    0
                ],
                "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression"
                },
                "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation."
                },
                "authors": [
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Marco Fabris"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v1",
                "updated": "2025-07-28T16:09:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08490v2",
                "updated": "2025-07-28T16:07:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    7,
                    44,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-11T12:42:01Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "title": "Adopting Large Language Models to Automated System Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large Language Models to Automated System Integration"
                },
                "summary": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    }
                ],
                "author_detail": {
                    "name": "Robin D. Pesl"
                },
                "author": "Robin D. Pesl",
                "arxiv_doi": "10.1007/978-3-031-94590-8_37",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94590-8_37",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.08490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Intelligent Information Systems. CAiSE 2025.\n  Lecture Notes in Business Information Processing, vol 557. Springer, Cham.,\n  and is available online at https://doi.org/10.1007/978-3-031-94590-8_37",
                "arxiv_journal_ref": "Intelligent Information Systems. CAiSE 2025. Lecture Notes in\n  Business Information Processing, vol 557. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20956v1",
                "updated": "2025-07-28T16:04:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    4,
                    25,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T16:04:25Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    4,
                    25,
                    0,
                    209,
                    0
                ],
                "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of\n  Instruction-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Conformative Decoding to Improve Output Diversity of\n  Instruction-Tuned Large Language Models"
                },
                "summary": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality."
                },
                "authors": [
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Dan Brown"
                    },
                    {
                        "name": "Anna Jordanous"
                    }
                ],
                "author_detail": {
                    "name": "Anna Jordanous"
                },
                "author": "Anna Jordanous",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19804v2",
                "updated": "2025-07-28T16:00:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    0,
                    1,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-29T16:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation"
                },
                "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    },
                    {
                        "name": "Jerin G. Mathew"
                    },
                    {
                        "name": "Massimo Mecella"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_doi": "10.1007/978-3-031-94571-7_8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-94571-7_8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.19804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Advanced Information Systems Engineering. CAiSE\n  2025. Lecture Notes in Computer Science, vol 15702. Springer, Cham., and is\n  available online at https://doi.org/10.1007/978-3-031-94571-7_8",
                "arxiv_journal_ref": "Advanced Information Systems Engineering. CAiSE 2025. Lecture\n  Notes in Computer Science, vol 15702. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20939v1",
                "updated": "2025-07-28T15:52:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    52,
                    36,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:52:36Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    52,
                    36,
                    0,
                    209,
                    0
                ],
                "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts"
                },
                "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU."
                },
                "authors": [
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Junfu Pu"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Lu Qiu"
                    },
                    {
                        "name": "Jin Ma"
                    },
                    {
                        "name": "Lisheng Duan"
                    },
                    {
                        "name": "Xinyu Zuo"
                    },
                    {
                        "name": "Jinwen Luo"
                    },
                    {
                        "name": "Weibo Gu"
                    },
                    {
                        "name": "Zexuan Li"
                    },
                    {
                        "name": "Xiaojing Zhang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project Page:\n  https://tencentarc.github.io/posts/arc-video-announcement/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20936v1",
                "updated": "2025-07-28T15:45:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    45,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:45:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    45,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation\n  Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Persona-Driven Reasoning in Language Models via Activation\n  Patching"
                },
                "summary": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities."
                },
                "authors": [
                    {
                        "name": "Ansh Poonia"
                    },
                    {
                        "name": "Maeghal Jain"
                    }
                ],
                "author_detail": {
                    "name": "Maeghal Jain"
                },
                "author": "Maeghal Jain",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14917v3",
                "updated": "2025-07-28T15:37:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    37,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2024-06-21T07:20:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    7,
                    20,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "LLM2TEA: An Agentic AI Designer for Discovery with Generative\n  Evolutionary Multitasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2TEA: An Agentic AI Designer for Discovery with Generative\n  Evolutionary Multitasking"
                },
                "summary": "This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs."
                },
                "authors": [
                    {
                        "name": "Melvin Wong"
                    },
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Thiago Rios"
                    },
                    {
                        "name": "Stefan Menzel"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "This work is accepted by IEEE CIM. IEEE copyrights applies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20924v1",
                "updated": "2025-07-28T15:30:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    30,
                    17,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:30:17Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    30,
                    17,
                    0,
                    209,
                    0
                ],
                "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech\n  Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech\n  Concept Bottleneck Models"
                },
                "summary": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish."
                },
                "authors": [
                    {
                        "name": "Roberto Labadie-Tamayo"
                    },
                    {
                        "name": "Adrian Jaques Böck"
                    },
                    {
                        "name": "Djordje Slijepčević"
                    },
                    {
                        "name": "Xihui Chen"
                    },
                    {
                        "name": "Andreas Babic"
                    },
                    {
                        "name": "Matthias Zeppelzauer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Zeppelzauer"
                },
                "author": "Matthias Zeppelzauer",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20923v1",
                "updated": "2025-07-28T15:26:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:26:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    26,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization"
                },
                "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE."
                },
                "authors": [
                    {
                        "name": "Minh Hieu Ha"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Tung Duy Doan"
                    },
                    {
                        "name": "Tung Dao"
                    },
                    {
                        "name": "Dao Tran"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "36 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20919v1",
                "updated": "2025-07-28T15:19:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    19,
                    54,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T15:19:54Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    19,
                    54,
                    0,
                    209,
                    0
                ],
                "title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling User Behavior from Adaptive Surveys with Supplemental Context"
                },
                "summary": "Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications."
                },
                "authors": [
                    {
                        "name": "Aman Shukla"
                    },
                    {
                        "name": "Daniel Patrick Scantlebury"
                    },
                    {
                        "name": "Rishabh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Kumar"
                },
                "author": "Rishabh Kumar",
                "arxiv_comment": "Best Paper, NewInML @ ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18808v2",
                "updated": "2025-07-28T15:12:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    15,
                    12,
                    54,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-24T21:11:23Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    21,
                    11,
                    23,
                    3,
                    205,
                    0
                ],
                "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static\n  Environments"
                },
                "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations."
                },
                "authors": [
                    {
                        "name": "Miguel Saavedra-Ruiz"
                    },
                    {
                        "name": "Samer B. Nashed"
                    },
                    {
                        "name": "Charlie Gauthier"
                    },
                    {
                        "name": "Liam Paull"
                    }
                ],
                "author_detail": {
                    "name": "Liam Paull"
                },
                "author": "Liam Paull",
                "arxiv_comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20906v2",
                "updated": "2025-07-29T02:15:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    15,
                    56,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-28T14:59:17Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    59,
                    17,
                    0,
                    209,
                    0
                ],
                "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context\n  Learning"
                },
                "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.2%-14.3% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.2%-14.3% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space."
                },
                "authors": [
                    {
                        "name": "Jungwon Park"
                    },
                    {
                        "name": "Wonjong Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Wonjong Rhee"
                },
                "author": "Wonjong Rhee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13629v2",
                "updated": "2025-07-28T14:54:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    54,
                    15,
                    0,
                    209,
                    0
                ],
                "published": "2025-06-16T15:56:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding"
                },
                "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20900v1",
                "updated": "2025-07-28T14:52:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    52,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:52:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    52,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Music Arena: Live Evaluation for Text-to-Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music Arena: Live Evaluation for Text-to-Music"
                },
                "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org"
                },
                "authors": [
                    {
                        "name": "Yonghyun Kim"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Koichi Saito"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16117v2",
                "updated": "2025-07-28T14:51:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    51,
                    21,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-22T00:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    10,
                    55,
                    1,
                    203,
                    0
                ],
                "title": "BDIViz: An Interactive Visualization System for Biomedical Schema\n  Matching with LLM-Powered Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BDIViz: An Interactive Visualization System for Biomedical Schema\n  Matching with LLM-Powered Validation"
                },
                "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches."
                },
                "authors": [
                    {
                        "name": "Eden Wu"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Christos Koutras"
                    },
                    {
                        "name": "Sarah Keegan"
                    },
                    {
                        "name": "Wenke Liu"
                    },
                    {
                        "name": "Beata Szeitz"
                    },
                    {
                        "name": "David Fenyo"
                    },
                    {
                        "name": "Cláudio T. Silva"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "arxiv_comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18784v2",
                "updated": "2025-07-28T14:48:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    48,
                    17,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-26T03:33:14Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    3,
                    33,
                    14,
                    5,
                    116,
                    0
                ],
                "title": "Secret Breach Detection in Source Code with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Breach Detection in Source Code with Large Language Models"
                },
                "summary": "Background: Leaking sensitive information - such as API keys, tokens, and\ncredentials - in source code remains a persistent security threat. Traditional\nregex and entropy-based tools often generate high false positives due to\nlimited contextual understanding. Aims: This work aims to enhance secret\ndetection in source code using large language models (LLMs), reducing false\npositives while maintaining high recall. We also evaluate the feasibility of\nusing fine-tuned, smaller models for local deployment. Method: We propose a\nhybrid approach combining regex-based candidate extraction with LLM-based\nclassification. We evaluate pre-trained and fine-tuned variants of various\nLarge Language Models on a benchmark dataset from 818 GitHub repositories.\nVarious prompting strategies and efficient fine-tuning methods are employed for\nboth binary and multiclass classification. Results: The fine-tuned LLaMA-3.1 8B\nmodel achieved an F1-score of 0.9852 in binary classification, outperforming\nregex-only baselines. For multiclass classification, Mistral-7B reached 0.982\naccuracy. Fine-tuning significantly improved performance across all models.\nConclusions: Fine-tuned LLMs offer an effective and scalable solution for\nsecret detection, greatly reducing false positives. Open-source models provide\na practical alternative to commercial APIs, enabling secure and cost-efficient\ndeployment in development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Leaking sensitive information - such as API keys, tokens, and\ncredentials - in source code remains a persistent security threat. Traditional\nregex and entropy-based tools often generate high false positives due to\nlimited contextual understanding. Aims: This work aims to enhance secret\ndetection in source code using large language models (LLMs), reducing false\npositives while maintaining high recall. We also evaluate the feasibility of\nusing fine-tuned, smaller models for local deployment. Method: We propose a\nhybrid approach combining regex-based candidate extraction with LLM-based\nclassification. We evaluate pre-trained and fine-tuned variants of various\nLarge Language Models on a benchmark dataset from 818 GitHub repositories.\nVarious prompting strategies and efficient fine-tuning methods are employed for\nboth binary and multiclass classification. Results: The fine-tuned LLaMA-3.1 8B\nmodel achieved an F1-score of 0.9852 in binary classification, outperforming\nregex-only baselines. For multiclass classification, Mistral-7B reached 0.982\naccuracy. Fine-tuning significantly improved performance across all models.\nConclusions: Fine-tuned LLMs offer an effective and scalable solution for\nsecret detection, greatly reducing false positives. Open-source models provide\na practical alternative to commercial APIs, enabling secure and cost-efficient\ndeployment in development workflows."
                },
                "authors": [
                    {
                        "name": "Md Nafiu Rahman"
                    },
                    {
                        "name": "Sadif Ahmed"
                    },
                    {
                        "name": "Zahin Wahab"
                    },
                    {
                        "name": "S M Sohan"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "arxiv_comment": "19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM 2025) cameraready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20888v1",
                "updated": "2025-07-28T14:39:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    39,
                    46,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:39:46Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    39,
                    46,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Project-Specific Code Completion by Inferring Internal API\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Project-Specific Code Completion by Inferring Internal API\n  Information"
                },
                "summary": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%."
                },
                "authors": [
                    {
                        "name": "Le Deng"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Ming Liang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20873v1",
                "updated": "2025-07-28T14:24:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    24,
                    20,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:24:20Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    24,
                    20,
                    0,
                    209,
                    0
                ],
                "title": "Testbed and Software Architecture for Enhancing Security in Industrial\n  Private 5G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testbed and Software Architecture for Enhancing Security in Industrial\n  Private 5G Networks"
                },
                "summary": "In the era of Industry 4.0, the growing need for secure and efficient\ncommunication systems has driven the development of fifth-generation (5G)\nnetworks characterized by extremely low latency, massive device connectivity\nand high data transfer speeds. However, the deployment of 5G networks presents\nsignificant security challenges, requiring advanced and robust solutions to\ncounter increasingly sophisticated cyber threats. This paper proposes a testbed\nand software architecture to strengthen the security of Private 5G Networks,\nparticularly in industrial communication environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Industry 4.0, the growing need for secure and efficient\ncommunication systems has driven the development of fifth-generation (5G)\nnetworks characterized by extremely low latency, massive device connectivity\nand high data transfer speeds. However, the deployment of 5G networks presents\nsignificant security challenges, requiring advanced and robust solutions to\ncounter increasingly sophisticated cyber threats. This paper proposes a testbed\nand software architecture to strengthen the security of Private 5G Networks,\nparticularly in industrial communication environments."
                },
                "authors": [
                    {
                        "name": "Song Son Ha"
                    },
                    {
                        "name": "Florian Foerster"
                    },
                    {
                        "name": "Thomas Robert Doebbert"
                    },
                    {
                        "name": "Tim Kittel"
                    },
                    {
                        "name": "Dominik Merli"
                    },
                    {
                        "name": "Gerd Scholl"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Scholl"
                },
                "author": "Gerd Scholl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20870v1",
                "updated": "2025-07-28T14:22:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:22:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    22,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM\n  Common-Sense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human-in-the-loop Approach to Robot Action Replanning through LLM\n  Common-Sense Reasoning"
                },
                "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness."
                },
                "authors": [
                    {
                        "name": "Elena Merlo"
                    },
                    {
                        "name": "Marta Lagomarsino"
                    },
                    {
                        "name": "Arash Ajoudani"
                    }
                ],
                "author_detail": {
                    "name": "Arash Ajoudani"
                },
                "author": "Arash Ajoudani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13807v4",
                "updated": "2025-07-28T14:21:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    21,
                    55,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-18T17:20:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    20,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability"
                },
                "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. For more details, please\nvisit the project website: https://zhengtongxu.github.io/diffog-website/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. For more details, please\nvisit the project website: https://zhengtongxu.github.io/diffog-website/."
                },
                "authors": [
                    {
                        "name": "Zhengtong Xu"
                    },
                    {
                        "name": "Zichen Miao"
                    },
                    {
                        "name": "Qiang Qiu"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Yu She"
                    }
                ],
                "author_detail": {
                    "name": "Yu She"
                },
                "author": "Yu She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00831v2",
                "updated": "2025-07-28T14:17:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    17,
                    34,
                    0,
                    209,
                    0
                ],
                "published": "2025-06-01T04:33:34Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    4,
                    33,
                    34,
                    6,
                    152,
                    0
                ],
                "title": "A Large Language Model-Supported Threat Modeling Framework for\n  Transportation Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Supported Threat Modeling Framework for\n  Transportation Cyber-Physical Systems"
                },
                "summary": "Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository."
                },
                "authors": [
                    {
                        "name": "M Sabbir Salek"
                    },
                    {
                        "name": "Mashrur Chowdhury"
                    },
                    {
                        "name": "Muhaimin Bin Munir"
                    },
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Mohammad Imtiaz Hasan"
                    },
                    {
                        "name": "Jean-Michel Tine"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Mizanur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Mizanur Rahman"
                },
                "author": "Mizanur Rahman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20859v1",
                "updated": "2025-07-28T14:12:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    12,
                    37,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:12:37Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    12,
                    37,
                    0,
                    209,
                    0
                ],
                "title": "Leveraging Open-Source Large Language Models for Clinical Information\n  Extraction in Resource-Constrained Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Open-Source Large Language Models for Clinical Information\n  Extraction in Resource-Constrained Settings"
                },
                "summary": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Luc Builtjes"
                    },
                    {
                        "name": "Joeran Bosma"
                    },
                    {
                        "name": "Mathias Prokop"
                    },
                    {
                        "name": "Bram van Ginneken"
                    },
                    {
                        "name": "Alessa Hering"
                    }
                ],
                "author_detail": {
                    "name": "Alessa Hering"
                },
                "author": "Alessa Hering",
                "arxiv_comment": "34 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20849v1",
                "updated": "2025-07-28T14:00:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    0,
                    57,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T14:00:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    0,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Latent Inter-User Difference Modeling for LLM Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Inter-User Difference Modeling for LLM Personalization"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP."
                },
                "authors": [
                    {
                        "name": "Yilun Qiu"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20844v1",
                "updated": "2025-07-28T13:53:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    53,
                    17,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:53:17Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    53,
                    17,
                    0,
                    209,
                    0
                ],
                "title": "Route Optimization Over Scheduled Services For Large-Scale Package\n  Delivery Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route Optimization Over Scheduled Services For Large-Scale Package\n  Delivery Networks"
                },
                "summary": "This paper introduces the Trailer Path Optimization with Schedule Services\nProblem (TPOSSP) and proposes a column-generation heuristic (CG-heuristic) to\nfind high-quality solutions to large-scale instances. The TPOSSP aims at\ndetermining trailer routes over a time-dependent network using existing\nscheduled services, while considering tractor capacity constraints and time\nwindows for trailer pickups and deliveries. The objective is to minimize both\nthe number of schedules used and the total miles traveled. To address the large\nscale of industrial instances, the paper proposes a network reduction technique\nthat identifies the set of feasible schedule-legs for each requests. Moreover,\nto address the resulting MIP models, that still contains hundred of millions\nvariables, the paper proposes a stabilized column-generation, whose pricing\nproblem is a time-dependent shortest path. The approach is evaluated on\nindustrial instances both for tactical planning where requests for the entire\nnetwork are re-optimized and for real-time operations where new requests are\ninserted. In the tactical planning setting, the column-generation heuristic\nreturns solutions with a 3.7%-5.7% optimality gap (based on a MIP relaxation)\nin under 1.7-10.3 hours, and improves the current practice by 2.3-3.2%, with\ntranslates into savings of tens of millions of dollars a year. In the real-time\nsetting, the column-generation heuristic returns solution within 3% of\noptimality in under 1 minute, which makes it adequate for real-time deployment.\nThe results also show that the network reduction decreases run times by 85% for\nthe column-generation heuristic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Trailer Path Optimization with Schedule Services\nProblem (TPOSSP) and proposes a column-generation heuristic (CG-heuristic) to\nfind high-quality solutions to large-scale instances. The TPOSSP aims at\ndetermining trailer routes over a time-dependent network using existing\nscheduled services, while considering tractor capacity constraints and time\nwindows for trailer pickups and deliveries. The objective is to minimize both\nthe number of schedules used and the total miles traveled. To address the large\nscale of industrial instances, the paper proposes a network reduction technique\nthat identifies the set of feasible schedule-legs for each requests. Moreover,\nto address the resulting MIP models, that still contains hundred of millions\nvariables, the paper proposes a stabilized column-generation, whose pricing\nproblem is a time-dependent shortest path. The approach is evaluated on\nindustrial instances both for tactical planning where requests for the entire\nnetwork are re-optimized and for real-time operations where new requests are\ninserted. In the tactical planning setting, the column-generation heuristic\nreturns solutions with a 3.7%-5.7% optimality gap (based on a MIP relaxation)\nin under 1.7-10.3 hours, and improves the current practice by 2.3-3.2%, with\ntranslates into savings of tens of millions of dollars a year. In the real-time\nsetting, the column-generation heuristic returns solution within 3% of\noptimality in under 1 minute, which makes it adequate for real-time deployment.\nThe results also show that the network reduction decreases run times by 85% for\nthe column-generation heuristic."
                },
                "authors": [
                    {
                        "name": "Mohammed Faisal Ahmed"
                    },
                    {
                        "name": "Pascal Van Hentenryck"
                    },
                    {
                        "name": "Ahmed El Nashar"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed El Nashar"
                },
                "author": "Ahmed El Nashar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20842v1",
                "updated": "2025-07-28T13:50:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    50,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:50:53Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    50,
                    53,
                    0,
                    209,
                    0
                ],
                "title": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision\n  Language Models"
                },
                "summary": "Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Bowen Shi"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Hongkai Xiong"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18975v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18975v4",
                "updated": "2025-07-28T13:28:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    28,
                    6,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-25T04:54:53Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    4,
                    54,
                    53,
                    6,
                    145,
                    0
                ],
                "title": "FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with\n  Accurate Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with\n  Accurate Quantization"
                },
                "summary": "State Space Models (SSMs), like recent Mamba2, have achieved remarkable\nperformance and received extensive attention. However, deploying Mamba2 on\nresource-constrained edge devices encounters many problems: severe outliers\nwithin the linear layer challenging the quantization, diverse and irregular\nelement-wise tensor operations, and hardware-unfriendly nonlinear functions in\nthe SSM block. To address these issues, this paper presents FastMamba, a\ndedicated accelerator on FPGA with hardware-algorithm co-design to promote the\ndeployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit\nquantization for linear layers through Hadamard transformation to eliminate\noutliers. Moreover, a hardware-friendly and fine-grained power-of-two\nquantization framework is presented for the SSM block and convolution layer,\nand a first-order linear approximation is developed to optimize the nonlinear\nfunctions. Based on the accurate algorithm quantization, we propose an\naccelerator that integrates parallel vector processing units, pipelined\nexecution dataflow, and an efficient SSM Nonlinear Approximation Unit, which\nenhances computational efficiency and reduces hardware complexity. Finally, we\nevaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on\nMamba2-130M, FastMamba achieves 68.80\\times and 8.90\\times speedup over Intel\nXeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode\nexperiment with Mamba2-2.7B, FastMamba attains 6\\times higher energy efficiency\nthan RTX 3090 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models (SSMs), like recent Mamba2, have achieved remarkable\nperformance and received extensive attention. However, deploying Mamba2 on\nresource-constrained edge devices encounters many problems: severe outliers\nwithin the linear layer challenging the quantization, diverse and irregular\nelement-wise tensor operations, and hardware-unfriendly nonlinear functions in\nthe SSM block. To address these issues, this paper presents FastMamba, a\ndedicated accelerator on FPGA with hardware-algorithm co-design to promote the\ndeployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit\nquantization for linear layers through Hadamard transformation to eliminate\noutliers. Moreover, a hardware-friendly and fine-grained power-of-two\nquantization framework is presented for the SSM block and convolution layer,\nand a first-order linear approximation is developed to optimize the nonlinear\nfunctions. Based on the accurate algorithm quantization, we propose an\naccelerator that integrates parallel vector processing units, pipelined\nexecution dataflow, and an efficient SSM Nonlinear Approximation Unit, which\nenhances computational efficiency and reduces hardware complexity. Finally, we\nevaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on\nMamba2-130M, FastMamba achieves 68.80\\times and 8.90\\times speedup over Intel\nXeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode\nexperiment with Mamba2-2.7B, FastMamba attains 6\\times higher energy efficiency\nthan RTX 3090 GPU."
                },
                "authors": [
                    {
                        "name": "Aotao Wang"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18975v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18975v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15748v2",
                "updated": "2025-07-28T13:13:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    13,
                    2,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-20T10:06:52Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    52,
                    4,
                    355,
                    0
                ],
                "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models"
                },
                "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole."
                },
                "authors": [
                    {
                        "name": "Shamus Sim"
                    },
                    {
                        "name": "Tyrone Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone Chen"
                },
                "author": "Tyrone Chen",
                "arxiv_comment": "25 pages, 7 figures, 3 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20800v1",
                "updated": "2025-07-28T13:08:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    8,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:08:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    8,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted\n  Lanternfly Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted\n  Lanternfly Populations"
                },
                "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes."
                },
                "authors": [
                    {
                        "name": "Vinil Polepalli"
                    }
                ],
                "author_detail": {
                    "name": "Vinil Polepalli"
                },
                "author": "Vinil Polepalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20796v1",
                "updated": "2025-07-28T13:05:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    5,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T13:05:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    5,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "Aligning Large Language Model Agents with Rational and Moral\n  Preferences: A Supervised Fine-Tuning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Model Agents with Rational and Moral\n  Preferences: A Supervised Fine-Tuning Approach"
                },
                "summary": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language model (LLM) agents behave in strategic\ninteractions is essential as these systems increasingly participate\nautonomously in economically and morally consequential decisions. We evaluate\nLLM preferences using canonical economic games, finding substantial deviations\nfrom human behavior. Models like GPT-4o show excessive cooperation and limited\nincentive sensitivity, while reasoning models, such as o3-mini, align more\nconsistently with payoff-maximizing strategies. We propose a supervised\nfine-tuning pipeline that uses synthetic datasets derived from economic\nreasoning to align LLM agents with economic preferences, focusing on two\nstylized preference structures. In the first, utility depends only on\nindividual payoffs (homo economicus), while utility also depends on a notion of\nKantian universalizability in the second preference structure (homo moralis).\nWe find that fine-tuning based on small datasets shifts LLM agent behavior\ntoward the corresponding economic agent. We further assess the fine-tuned\nagents' behavior in two applications: Moral dilemmas involving autonomous\nvehicles and algorithmic pricing in competitive markets. These examples\nillustrate how different normative objectives embedded via realizations from\nstructured preference structures can influence market and moral outcomes. This\nwork contributes a replicable, cost-efficient, and economically grounded\npipeline to align AI preferences using moral-economic principles."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Daniel L. Chen"
                    },
                    {
                        "name": "Christian B. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Christian B. Hansen"
                },
                "author": "Christian B. Hansen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06645v3",
                "updated": "2025-07-28T13:00:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    0,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2025-01-11T21:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    21,
                    41,
                    27,
                    5,
                    11,
                    0
                ],
                "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings"
                },
                "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00582v2",
                "updated": "2025-07-28T12:59:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    59,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-31T14:37:18Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    14,
                    37,
                    18,
                    5,
                    151,
                    0
                ],
                "title": "Do Language Models Mirror Human Confidence? Exploring Psychological\n  Insights to Address Overconfidence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Language Models Mirror Human Confidence? Exploring Psychological\n  Insights to Address Overconfidence in LLMs"
                },
                "summary": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty."
                },
                "authors": [
                    {
                        "name": "Chenjun Xu"
                    },
                    {
                        "name": "Bingbing Wen"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Robert Wolfe"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    },
                    {
                        "name": "Bill Howe"
                    }
                ],
                "author_detail": {
                    "name": "Bill Howe"
                },
                "author": "Bill Howe",
                "arxiv_comment": "Accepted by ACL 2025 Findings, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20786v1",
                "updated": "2025-07-28T12:56:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:56:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Automating Thematic Review of Prevention of Future Deaths Reports:\n  Replicating the ONS Child Suicide Study using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Thematic Review of Prevention of Future Deaths Reports:\n  Replicating the ONS Child Suicide Study using Large Language Models"
                },
                "summary": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research."
                },
                "authors": [
                    {
                        "name": "Sam Osian"
                    },
                    {
                        "name": "Arpan Dutta"
                    },
                    {
                        "name": "Sahil Bhandari"
                    },
                    {
                        "name": "Iain E. Buchan"
                    },
                    {
                        "name": "Dan W. Joyce"
                    }
                ],
                "author_detail": {
                    "name": "Dan W. Joyce"
                },
                "author": "Dan W. Joyce",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12776v3",
                "updated": "2025-07-28T12:56:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    56,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2024-12-17T10:37:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    37,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "Physical simulation of Marsupial UAV-UGV Systems Connected by a\n  Variable-Length Hanging Tether",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical simulation of Marsupial UAV-UGV Systems Connected by a\n  Variable-Length Hanging Tether"
                },
                "summary": "This paper presents a simulation framework able of modeling the dynamics of a\nhanging tether with adjustable length, connecting a UAV to a UGV. The model\nincorporates the interaction between the UAV, UGV, and a winch, allowing for\ndynamic tether adjustments based on the relative motion of the robots. The\naccuracy and reliability of the simulator are assessed through extensive\nexperiments, including comparisons with real-world experiment, to evaluate its\nability to reproduce the complex tether dynamics observed in physical\ndeployments. The results demonstrate that the simulation closely aligns with\nreal-world behavior, particularly in constrained environments where tether\neffects are significant. This work provides a validated tool for studying\ntethered robotic systems, offering valuable insights into their motion dynamics\nand control strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simulation framework able of modeling the dynamics of a\nhanging tether with adjustable length, connecting a UAV to a UGV. The model\nincorporates the interaction between the UAV, UGV, and a winch, allowing for\ndynamic tether adjustments based on the relative motion of the robots. The\naccuracy and reliability of the simulator are assessed through extensive\nexperiments, including comparisons with real-world experiment, to evaluate its\nability to reproduce the complex tether dynamics observed in physical\ndeployments. The results demonstrate that the simulation closely aligns with\nreal-world behavior, particularly in constrained environments where tether\neffects are significant. This work provides a validated tool for studying\ntethered robotic systems, offering valuable insights into their motion dynamics\nand control strategies."
                },
                "authors": [
                    {
                        "name": "Jose Enrique Maese"
                    },
                    {
                        "name": "Fernando Caballero"
                    },
                    {
                        "name": "Luis Merino"
                    }
                ],
                "author_detail": {
                    "name": "Luis Merino"
                },
                "author": "Luis Merino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20774v1",
                "updated": "2025-07-28T12:37:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    37,
                    43,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:37:43Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    37,
                    43,
                    0,
                    209,
                    0
                ],
                "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract\n  Generated Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract\n  Generated Comments"
                },
                "summary": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Fatou Ndiaye Mbodji"
                    }
                ],
                "author_detail": {
                    "name": "Fatou Ndiaye Mbodji"
                },
                "author": "Fatou Ndiaye Mbodji",
                "arxiv_comment": "4 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19989v3",
                "updated": "2025-07-28T12:21:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    21,
                    18,
                    0,
                    209,
                    0
                ],
                "published": "2025-02-27T11:14:14Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    11,
                    14,
                    14,
                    3,
                    58,
                    0
                ],
                "title": "Satellite-Surface-Area Machine-Learning Models for Reservoir Storage\n  Estimation: Regime-Sensitive Evaluation and Operational Deployment at Loskop\n  Dam, South Africa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite-Surface-Area Machine-Learning Models for Reservoir Storage\n  Estimation: Regime-Sensitive Evaluation and Operational Deployment at Loskop\n  Dam, South Africa"
                },
                "summary": "Reliable daily estimates of reservoir storage are pivotal for water\nallocation and drought response decisions in semiarid regions. Conventional\nrating curves at Loskop Dam, the primary storage on South Africa's Olifants\nRiver, have become increasingly uncertain owing to sedimentation and episodic\ndrawdown. A 40 year Digital Earth Africa (DEA) surface area archive (1984-2024)\nfused with gauged water levels to develop data driven volume predictors that\noperate under a maximum 9.14%, a 90 day drawdown constraint. Four nested\nfeature sets were examined: (i) raw water area, (ii) +a power law \"calculated\nvolume\" proxy, (iii) +six river geometry metrics, and (iv) +full supply\nelevation. Five candidate algorithms, Gradient Boosting (GB), Random Forest\n(RF), Ridge (RI), Lasso (LA) and Elastic Net (EN), were tuned using a 20 draw\nrandom search and assessed with a five fold Timeseries Split to eliminate look\nahead bias. Prediction errors were decomposed into two regimes: Low (<250 x\n10^6 cubic meters) and High (>250 x 10^6 cubic meters) storage regimes. Ridge\nregression achieved the lowest cross validated RMSE (12.3 x 10^6 cubic meters),\noutperforming GB by 16% and RF by 7%. In regime terms, Ridge was superior in\nthe Low band (18.0 ver. 22.7 MCM for GB) and tied RF in the High band (~12\nMCM). In sample diagnostics showed GB's apparent dominance (6.8-5.4 MCM) to be\nan artefact of overfitting. A Ridge meta stacked ensemble combining GB, RF, and\nRidge reduced full series RMSE to ~ 11 MCM (~ 3% of live capacity). We\nrecommend (i) GB retrained daily for routine operations, (ii) Ridge for drought\nearly warning, and (iii) the stacked blend for all weather dashboards.\nQuarterly rolling retraining and regime specific metrics are advised to\nmaintain operational accuracy below the 5% threshold mandated by the Department\nof Water and Sanitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable daily estimates of reservoir storage are pivotal for water\nallocation and drought response decisions in semiarid regions. Conventional\nrating curves at Loskop Dam, the primary storage on South Africa's Olifants\nRiver, have become increasingly uncertain owing to sedimentation and episodic\ndrawdown. A 40 year Digital Earth Africa (DEA) surface area archive (1984-2024)\nfused with gauged water levels to develop data driven volume predictors that\noperate under a maximum 9.14%, a 90 day drawdown constraint. Four nested\nfeature sets were examined: (i) raw water area, (ii) +a power law \"calculated\nvolume\" proxy, (iii) +six river geometry metrics, and (iv) +full supply\nelevation. Five candidate algorithms, Gradient Boosting (GB), Random Forest\n(RF), Ridge (RI), Lasso (LA) and Elastic Net (EN), were tuned using a 20 draw\nrandom search and assessed with a five fold Timeseries Split to eliminate look\nahead bias. Prediction errors were decomposed into two regimes: Low (<250 x\n10^6 cubic meters) and High (>250 x 10^6 cubic meters) storage regimes. Ridge\nregression achieved the lowest cross validated RMSE (12.3 x 10^6 cubic meters),\noutperforming GB by 16% and RF by 7%. In regime terms, Ridge was superior in\nthe Low band (18.0 ver. 22.7 MCM for GB) and tied RF in the High band (~12\nMCM). In sample diagnostics showed GB's apparent dominance (6.8-5.4 MCM) to be\nan artefact of overfitting. A Ridge meta stacked ensemble combining GB, RF, and\nRidge reduced full series RMSE to ~ 11 MCM (~ 3% of live capacity). We\nrecommend (i) GB retrained daily for routine operations, (ii) Ridge for drought\nearly warning, and (iii) the stacked blend for all weather dashboards.\nQuarterly rolling retraining and regime specific metrics are advised to\nmaintain operational accuracy below the 5% threshold mandated by the Department\nof Water and Sanitation."
                },
                "authors": [
                    {
                        "name": "Hugo Retief"
                    },
                    {
                        "name": "Kayathri"
                    },
                    {
                        "name": "Vigneswaran"
                    },
                    {
                        "name": "Surajit Ghosh"
                    },
                    {
                        "name": "Mariangel Garcia Andarcia"
                    },
                    {
                        "name": "Chris Dickens"
                    }
                ],
                "author_detail": {
                    "name": "Chris Dickens"
                },
                "author": "Chris Dickens",
                "arxiv_comment": "20 pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19299v3",
                "updated": "2025-07-28T12:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    20,
                    24,
                    0,
                    209,
                    0
                ],
                "published": "2024-07-27T16:48:03Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    48,
                    3,
                    5,
                    209,
                    0
                ],
                "title": "The Impact of LoRA Adapters on LLMs for Clinical Text Classification\n  Under Computational and Data Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of LoRA Adapters on LLMs for Clinical Text Classification\n  Under Computational and Data Constraints"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to domain gap, limited data,\nand stringent hardware constraints. In this study, we evaluate four adapter\ntechniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note\nclassification under real-world, resource-constrained conditions. All\nexperiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512\nCUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and\nmaximum sequence length to 256 tokens. Our clinical corpus comprises only 580\n000 tokens, several orders of magnitude smaller than standard LLM pre-training\ndatasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio,\nAliBERT, DrBERT) and two lightweight Transformer models trained from scratch.\nResults show that 1) adapter structures provide no consistent gains when\nfine-tuning biomedical LLMs under these constraints, and 2) simpler\nTransformers, with minimal parameter counts and training times under six hours,\noutperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among\nadapters, GRN achieved the best metrics (accuracy, precision, recall, F1 =\n0.88). These findings demonstrate that, in low-resource clinical settings with\nlimited data and compute, lightweight Transformers trained from scratch offer a\nmore practical and efficient solution than large LLMs, while GRN remains a\nviable adapter choice when minimal adaptation is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to domain gap, limited data,\nand stringent hardware constraints. In this study, we evaluate four adapter\ntechniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note\nclassification under real-world, resource-constrained conditions. All\nexperiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512\nCUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and\nmaximum sequence length to 256 tokens. Our clinical corpus comprises only 580\n000 tokens, several orders of magnitude smaller than standard LLM pre-training\ndatasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio,\nAliBERT, DrBERT) and two lightweight Transformer models trained from scratch.\nResults show that 1) adapter structures provide no consistent gains when\nfine-tuning biomedical LLMs under these constraints, and 2) simpler\nTransformers, with minimal parameter counts and training times under six hours,\noutperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among\nadapters, GRN achieved the best metrics (accuracy, precision, recall, F1 =\n0.88). These findings demonstrate that, in low-resource clinical settings with\nlimited data and compute, lightweight Transformers trained from scratch offer a\nmore practical and efficient solution than large LLMs, while GRN remains a\nviable adapter choice when minimal adaptation is needed."
                },
                "authors": [
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    },
                    {
                        "name": "Philippe Jouvet"
                    },
                    {
                        "name": "Rita Noumeir"
                    }
                ],
                "author_detail": {
                    "name": "Rita Noumeir"
                },
                "author": "Rita Noumeir",
                "arxiv_doi": "10.1109/ACCESS.2025.3582037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3582037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in the IEEE Access",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20764v1",
                "updated": "2025-07-28T12:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    18,
                    41,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    18,
                    41,
                    0,
                    209,
                    0
                ],
                "title": "ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image\n  Registration under Complex Imaging Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image\n  Registration under Complex Imaging Conditions"
                },
                "summary": "Multimodal fusion has become a key enabler for UAV-based object detection, as\neach modality provides complementary cues for robust feature extraction.\nHowever, due to significant differences in resolution, field of view, and\nsensing characteristics across modalities, accurate registration is a\nprerequisite before fusion. Despite its importance, there is currently no\npublicly available benchmark specifically designed for multimodal registration\nin UAV-based aerial scenarios, which severely limits the development and\nevaluation of advanced registration methods under real-world conditions. To\nbridge this gap, we present ATR-UMMIM, the first benchmark dataset specifically\ntailored for multimodal image registration in UAV-based applications. This\ndataset includes 7,969 triplets of raw visible, infrared, and precisely\nregistered visible images captured covers diverse scenarios including flight\naltitudes from 80m to 300m, camera angles from 0{\\deg} to 75{\\deg}, and\nall-day, all-year temporal variations under rich weather and illumination\nconditions. To ensure high registration quality, we design a semi-automated\nannotation pipeline to introduce reliable pixel-level ground truth to each\ntriplet. In addition, each triplet is annotated with six imaging condition\nattributes, enabling benchmarking of registration robustness under real-world\ndeployment settings. To further support downstream tasks, we provide\nobject-level annotations on all registered images, covering 11 object\ncategories with 77,753 visible and 78,409 infrared bounding boxes. We believe\nATR-UMMIM will serve as a foundational benchmark for advancing multimodal\nregistration, fusion, and perception in real-world UAV scenarios. The datatset\ncan be download from https://github.com/supercpy/ATR-UMMIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal fusion has become a key enabler for UAV-based object detection, as\neach modality provides complementary cues for robust feature extraction.\nHowever, due to significant differences in resolution, field of view, and\nsensing characteristics across modalities, accurate registration is a\nprerequisite before fusion. Despite its importance, there is currently no\npublicly available benchmark specifically designed for multimodal registration\nin UAV-based aerial scenarios, which severely limits the development and\nevaluation of advanced registration methods under real-world conditions. To\nbridge this gap, we present ATR-UMMIM, the first benchmark dataset specifically\ntailored for multimodal image registration in UAV-based applications. This\ndataset includes 7,969 triplets of raw visible, infrared, and precisely\nregistered visible images captured covers diverse scenarios including flight\naltitudes from 80m to 300m, camera angles from 0{\\deg} to 75{\\deg}, and\nall-day, all-year temporal variations under rich weather and illumination\nconditions. To ensure high registration quality, we design a semi-automated\nannotation pipeline to introduce reliable pixel-level ground truth to each\ntriplet. In addition, each triplet is annotated with six imaging condition\nattributes, enabling benchmarking of registration robustness under real-world\ndeployment settings. To further support downstream tasks, we provide\nobject-level annotations on all registered images, covering 11 object\ncategories with 77,753 visible and 78,409 infrared bounding boxes. We believe\nATR-UMMIM will serve as a foundational benchmark for advancing multimodal\nregistration, fusion, and perception in real-world UAV scenarios. The datatset\ncan be download from https://github.com/supercpy/ATR-UMMIM"
                },
                "authors": [
                    {
                        "name": "Kangcheng Bin"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Jiahao Qi"
                    },
                    {
                        "name": "Ping Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhong"
                },
                "author": "Ping Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20762v1",
                "updated": "2025-07-28T12:16:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    16,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:16:52Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    16,
                    52,
                    0,
                    209,
                    0
                ],
                "title": "Watermarking Large Language Model-based Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Large Language Model-based Time Series Forecasting"
                },
                "summary": "Large Language Model-based Time Series Forecasting (LLMTS) has shown\nremarkable promise in handling complex and diverse temporal data, representing\na significant step toward foundation models for time series analysis. However,\nthis emerging paradigm introduces two critical challenges. First, the\nsubstantial commercial potential and resource-intensive development raise\nurgent concerns about intellectual property (IP) protection. Second, their\npowerful time series forecasting capabilities may be misused to produce\nmisleading or fabricated deepfake time series data. To address these concerns,\nwe explore watermarking the outputs of LLMTS models, that is, embedding\nimperceptible signals into the generated time series data that remain\ndetectable by specialized algorithms. We propose a novel post-hoc watermarking\nframework, Waltz, which is broadly compatible with existing LLMTS models. Waltz\nis inspired by the empirical observation that time series patch embeddings are\nrarely aligned with a specific set of LLM tokens, which we term ``cold\ntokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the\nsimilarity statistics between patch embeddings and cold token embeddings, and\ndetects watermarks using similarity z-scores. To minimize potential side\neffects, we introduce a similarity-based embedding position identification\nstrategy and employ projected gradient descent to constrain the watermark noise\nwithin a defined boundary. Extensive experiments using two popular LLMTS models\nacross seven benchmark datasets demonstrate that Waltz achieves high watermark\ndetection accuracy with minimal impact on the quality of the generated time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Time Series Forecasting (LLMTS) has shown\nremarkable promise in handling complex and diverse temporal data, representing\na significant step toward foundation models for time series analysis. However,\nthis emerging paradigm introduces two critical challenges. First, the\nsubstantial commercial potential and resource-intensive development raise\nurgent concerns about intellectual property (IP) protection. Second, their\npowerful time series forecasting capabilities may be misused to produce\nmisleading or fabricated deepfake time series data. To address these concerns,\nwe explore watermarking the outputs of LLMTS models, that is, embedding\nimperceptible signals into the generated time series data that remain\ndetectable by specialized algorithms. We propose a novel post-hoc watermarking\nframework, Waltz, which is broadly compatible with existing LLMTS models. Waltz\nis inspired by the empirical observation that time series patch embeddings are\nrarely aligned with a specific set of LLM tokens, which we term ``cold\ntokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the\nsimilarity statistics between patch embeddings and cold token embeddings, and\ndetects watermarks using similarity z-scores. To minimize potential side\neffects, we introduce a similarity-based embedding position identification\nstrategy and employ projected gradient descent to constrain the watermark noise\nwithin a defined boundary. Extensive experiments using two popular LLMTS models\nacross seven benchmark datasets demonstrate that Waltz achieves high watermark\ndetection accuracy with minimal impact on the quality of the generated time\nseries."
                },
                "authors": [
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Yu Xing"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20752v1",
                "updated": "2025-07-28T12:01:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    1,
                    59,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T12:01:59Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    12,
                    1,
                    59,
                    0,
                    209,
                    0
                ],
                "title": "Multilingual Self-Taught Faithfulness Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Self-Taught Faithfulness Evaluators"
                },
                "summary": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Carlo Alfano"
                    },
                    {
                        "name": "Aymen Al Marjani"
                    },
                    {
                        "name": "Zeno Jonke"
                    },
                    {
                        "name": "Amin Mantrach"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20749v1",
                "updated": "2025-07-28T11:57:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    57,
                    52,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T11:57:52Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    57,
                    52,
                    0,
                    209,
                    0
                ],
                "title": "Investigating Structural Pruning and Recovery Techniques for Compressing\n  Multimodal Large Language Models: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Structural Pruning and Recovery Techniques for Compressing\n  Multimodal Large Language Models: An Empirical Study"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data."
                },
                "authors": [
                    {
                        "name": "Yiran Huang"
                    },
                    {
                        "name": "Lukas Thede"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "name": "Wenjia Xu"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Accepted at GCPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10508v3",
                "updated": "2025-07-28T11:46:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    46,
                    29,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-13T16:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    9,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Hoi2Threat: An Interpretable Threat Detection Method for Human Violence\n  Scenarios Guided by Human-Object Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hoi2Threat: An Interpretable Threat Detection Method for Human Violence\n  Scenarios Guided by Human-Object Interaction"
                },
                "summary": "In light of the mounting imperative for public security, the necessity for\nautomated threat detection in high-risk scenarios is becoming increasingly\npressing. However, existing methods generally suffer from the problems of\nuninterpretable inference and biased semantic understanding, which severely\nlimits their reliability in practical deployment. In order to address the\naforementioned challenges, this article proposes a threat detection method\nbased on human-object interaction pairs (HOI-pairs), Hoi2Threat. This method is\nbased on the fine-grained multimodal TD-Hoi dataset, enhancing the model's\nsemantic modeling ability for key entities and their behavioral interactions by\nusing structured HOI tags to guide language generation. Furthermore, a set of\nmetrics is designed for the evaluation of text response quality, with the\nobjective of systematically measuring the model's representation accuracy and\ncomprehensibility during threat interpretation. The experimental results have\ndemonstrated that Hoi2Threat attains substantial enhancement in several threat\ndetection tasks, particularly in the core metrics of Correctness of Information\n(CoI), Behavioral Mapping Accuracy (BMA), and Threat Detailed Orientation\n(TDO), which are 5.08, 5.04, and 4.76, and 7.10%, 6.80%, and 2.63%,\nrespectively, in comparison with the Gemma3 (4B). The aforementioned results\nprovide comprehensive validation of the merits of this approach in the domains\nof semantic understanding, entity behavior mapping, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the mounting imperative for public security, the necessity for\nautomated threat detection in high-risk scenarios is becoming increasingly\npressing. However, existing methods generally suffer from the problems of\nuninterpretable inference and biased semantic understanding, which severely\nlimits their reliability in practical deployment. In order to address the\naforementioned challenges, this article proposes a threat detection method\nbased on human-object interaction pairs (HOI-pairs), Hoi2Threat. This method is\nbased on the fine-grained multimodal TD-Hoi dataset, enhancing the model's\nsemantic modeling ability for key entities and their behavioral interactions by\nusing structured HOI tags to guide language generation. Furthermore, a set of\nmetrics is designed for the evaluation of text response quality, with the\nobjective of systematically measuring the model's representation accuracy and\ncomprehensibility during threat interpretation. The experimental results have\ndemonstrated that Hoi2Threat attains substantial enhancement in several threat\ndetection tasks, particularly in the core metrics of Correctness of Information\n(CoI), Behavioral Mapping Accuracy (BMA), and Threat Detailed Orientation\n(TDO), which are 5.08, 5.04, and 4.76, and 7.10%, 6.80%, and 2.63%,\nrespectively, in comparison with the Gemma3 (4B). The aforementioned results\nprovide comprehensive validation of the merits of this approach in the domains\nof semantic understanding, entity behavior mapping, and interpretability."
                },
                "authors": [
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Daou Zhang"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Jinyang Chen"
                    },
                    {
                        "name": "Purui Dong"
                    },
                    {
                        "name": "Zuyuan Yu"
                    },
                    {
                        "name": "Ziru Wang"
                    },
                    {
                        "name": "Weichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wu"
                },
                "author": "Weichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18039v3",
                "updated": "2025-07-28T11:31:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    31,
                    55,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-25T03:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Nuoqian Xiao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted by ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04632v3",
                "updated": "2025-07-28T11:30:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    30,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-07T03:20:52Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    3,
                    20,
                    52,
                    0,
                    188,
                    0
                ],
                "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?"
                },
                "summary": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Björn Ommer"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20730v1",
                "updated": "2025-07-28T11:26:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    26,
                    39,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T11:26:39Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    26,
                    39,
                    0,
                    209,
                    0
                ],
                "title": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice\n  Competitions"
                },
                "summary": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals."
                },
                "authors": [
                    {
                        "name": "Edvin Teskeredzic"
                    },
                    {
                        "name": "Muamer Paric"
                    },
                    {
                        "name": "Adna Sestic"
                    },
                    {
                        "name": "Petra Fribert"
                    },
                    {
                        "name": "Anamarija Lukac"
                    },
                    {
                        "name": "Hadzem Hadzic"
                    },
                    {
                        "name": "Kemal Altwlkany"
                    },
                    {
                        "name": "Emanuel Lacic"
                    }
                ],
                "author_detail": {
                    "name": "Emanuel Lacic"
                },
                "author": "Emanuel Lacic",
                "arxiv_doi": "10.1145/3720533.3750059",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720533.3750059",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Hypertext 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01494v2",
                "updated": "2025-07-28T11:13:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    13,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-02T08:52:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    52,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Crop Pest Classification Using Deep Learning Techniques: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crop Pest Classification Using Deep Learning Techniques: A Review"
                },
                "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems."
                },
                "authors": [
                    {
                        "name": "Muhammad Hassam Ejaz"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Usman Habib"
                    }
                ],
                "author_detail": {
                    "name": "Usman Habib"
                },
                "author": "Usman Habib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20708v1",
                "updated": "2025-07-28T11:01:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    1,
                    48,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T11:01:48Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    11,
                    1,
                    48,
                    0,
                    209,
                    0
                ],
                "title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to\n  Distributional Manipulation Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to\n  Distributional Manipulation Attacks"
                },
                "summary": "Proving the compliance of AI algorithms has become an important challenge\nwith the growing deployment of such algorithms for real-life applications.\nInspecting possible biased behaviors is mandatory to satisfy the constraints of\nthe regulations of the EU Artificial Intelligence's Act. Regulation-driven\naudits increasingly rely on global fairness metrics, with Disparate Impact\nbeing the most widely used. Yet such global measures depend highly on the\ndistribution of the sample on which the measures are computed. We investigate\nfirst how to manipulate data samples to artificially satisfy fairness criteria,\ncreating minimally perturbed datasets that remain statistically\nindistinguishable from the original distribution while satisfying prescribed\nfairness constraints. Then we study how to detect such manipulation. Our\nanalysis (i) introduces mathematically sound methods for modifying empirical\ndistributions under fairness constraints using entropic or optimal transport\nprojections, (ii) examines how an auditee could potentially circumvent fairness\ninspections, and (iii) offers recommendations to help auditors detect such data\nmanipulations. These results are validated through experiments on classical\ntabular datasets in bias detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving the compliance of AI algorithms has become an important challenge\nwith the growing deployment of such algorithms for real-life applications.\nInspecting possible biased behaviors is mandatory to satisfy the constraints of\nthe regulations of the EU Artificial Intelligence's Act. Regulation-driven\naudits increasingly rely on global fairness metrics, with Disparate Impact\nbeing the most widely used. Yet such global measures depend highly on the\ndistribution of the sample on which the measures are computed. We investigate\nfirst how to manipulate data samples to artificially satisfy fairness criteria,\ncreating minimally perturbed datasets that remain statistically\nindistinguishable from the original distribution while satisfying prescribed\nfairness constraints. Then we study how to detect such manipulation. Our\nanalysis (i) introduces mathematically sound methods for modifying empirical\ndistributions under fairness constraints using entropic or optimal transport\nprojections, (ii) examines how an auditee could potentially circumvent fairness\ninspections, and (iii) offers recommendations to help auditors detect such data\nmanipulations. These results are validated through experiments on classical\ntabular datasets in bias detection."
                },
                "authors": [
                    {
                        "name": "Valentin Lafargue"
                    },
                    {
                        "name": "Adriana Laurindo Monteiro"
                    },
                    {
                        "name": "Emmanuelle Claeys"
                    },
                    {
                        "name": "Laurent Risser"
                    },
                    {
                        "name": "Jean-Michel Loubes"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Loubes"
                },
                "author": "Jean-Michel Loubes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20704v1",
                "updated": "2025-07-28T10:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    57,
                    44,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    57,
                    44,
                    0,
                    209,
                    0
                ],
                "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in\n  Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in\n  Visual Language Models"
                },
                "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Gabriel Downer"
                    },
                    {
                        "name": "Sean Craven"
                    },
                    {
                        "name": "Damian Ruck"
                    },
                    {
                        "name": "Jake Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jake Thomas"
                },
                "author": "Jake Thomas",
                "arxiv_comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20700v1",
                "updated": "2025-07-28T10:49:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    49,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:49:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    49,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained\n  Multilingual Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained\n  Multilingual Claim Verification"
                },
                "summary": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems."
                },
                "authors": [
                    {
                        "name": "Hanna Shcharbakova"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Natalia Skachkova"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "arxiv_comment": "Published at the FEVER Workshop, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20685v1",
                "updated": "2025-07-28T10:10:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    10,
                    25,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:10:25Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    10,
                    25,
                    0,
                    209,
                    0
                ],
                "title": "What's Really Different with AI? -- A Behavior-based Perspective on\n  System Safety for Automated Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What's Really Different with AI? -- A Behavior-based Perspective on\n  System Safety for Automated Driving Systems"
                },
                "summary": "Assuring safety for ``AI-based'' systems is one of the current challenges in\nsafety engineering. For automated driving systems, in particular, further\nassurance challenges result from the open context that the systems need to\noperate in after deployment. The current standardization and regulation\nlandscape for ``AI-based'' systems is becoming ever more complex, as standards\nand regulations are being released at high frequencies.\n  This position paper seeks to provide guidance for making qualified arguments\nwhich standards should meaningfully be applied to (``AI-based'') automated\ndriving systems. Furthermore, we argue for clearly differentiating sources of\nrisk between AI-specific and general uncertainties related to the open context.\nIn our view, a clear conceptual separation can help to exploit commonalities\nthat can close the gap between system-level and AI-specific safety analyses,\nwhile ensuring the required rigor for engineering safe ``AI-based'' systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assuring safety for ``AI-based'' systems is one of the current challenges in\nsafety engineering. For automated driving systems, in particular, further\nassurance challenges result from the open context that the systems need to\noperate in after deployment. The current standardization and regulation\nlandscape for ``AI-based'' systems is becoming ever more complex, as standards\nand regulations are being released at high frequencies.\n  This position paper seeks to provide guidance for making qualified arguments\nwhich standards should meaningfully be applied to (``AI-based'') automated\ndriving systems. Furthermore, we argue for clearly differentiating sources of\nrisk between AI-specific and general uncertainties related to the open context.\nIn our view, a clear conceptual separation can help to exploit commonalities\nthat can close the gap between system-level and AI-specific safety analyses,\nwhile ensuring the required rigor for engineering safe ``AI-based'' systems."
                },
                "authors": [
                    {
                        "name": "Marcus Nolte"
                    },
                    {
                        "name": "Nayel Fabian Salem"
                    },
                    {
                        "name": "Olaf Franke"
                    },
                    {
                        "name": "Jan Heckmann"
                    },
                    {
                        "name": "Christoph Höhmann"
                    },
                    {
                        "name": "Georg Stettinger"
                    },
                    {
                        "name": "Markus Maurer"
                    }
                ],
                "author_detail": {
                    "name": "Markus Maurer"
                },
                "author": "Markus Maurer",
                "arxiv_comment": "8 pages, 1 figure, 1 table, to be published in 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06003v3",
                "updated": "2025-07-28T10:09:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    9,
                    27,
                    0,
                    209,
                    0
                ],
                "published": "2024-08-12T08:52:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM\n  Inference"
                },
                "summary": "Large Language Model (LLM) inference becomes resource-intensive, prompting a\nshift toward low-bit model weights to reduce the memory footprint and improve\nefficiency. Such low-bit LLMs necessitate the mixed-precision matrix\nmultiplication (mpGEMM), an important yet underexplored operation involving the\nmultiplication of lower-precision weights with higher-precision activations.\nOff-the-shelf hardware does not support this operation natively, leading to\nindirect, thus inefficient, dequantization-based implementations.\n  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and\nfind that a conventional LUT implementation fails to achieve the promised\ngains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor\nCore, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core\ndifferentiates itself from conventional LUT designs through: 1) software-based\noptimizations to minimize table precompute overhead and weight reinterpretation\nto reduce table storage; 2) a LUT-based Tensor Core hardware design with an\nelongated tiling shape to maximize table reuse and a bit-serial design to\nsupport diverse precision combinations in mpGEMM; 3) a new instruction set and\ncompilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly\noutperforms existing pure software LUT implementations and achieves a\n1.44$\\times$ improvement in compute density and energy efficiency compared to\nprevious state-of-the-art LUT-based accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference becomes resource-intensive, prompting a\nshift toward low-bit model weights to reduce the memory footprint and improve\nefficiency. Such low-bit LLMs necessitate the mixed-precision matrix\nmultiplication (mpGEMM), an important yet underexplored operation involving the\nmultiplication of lower-precision weights with higher-precision activations.\nOff-the-shelf hardware does not support this operation natively, leading to\nindirect, thus inefficient, dequantization-based implementations.\n  In this paper, we study the lookup table (LUT)-based approach for mpGEMM and\nfind that a conventional LUT implementation fails to achieve the promised\ngains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor\nCore, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core\ndifferentiates itself from conventional LUT designs through: 1) software-based\noptimizations to minimize table precompute overhead and weight reinterpretation\nto reduce table storage; 2) a LUT-based Tensor Core hardware design with an\nelongated tiling shape to maximize table reuse and a bit-serial design to\nsupport diverse precision combinations in mpGEMM; 3) a new instruction set and\ncompilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly\noutperforms existing pure software LUT implementations and achieves a\n1.44$\\times$ improvement in compute density and energy efficiency compared to\nprevious state-of-the-art LUT-based accelerators."
                },
                "authors": [
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_doi": "10.1145/3695053.3731057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conference Version (ISCA'25). Fixed a typo",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.0; C.3; B.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20680v1",
                "updated": "2025-07-28T10:05:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    5,
                    13,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T10:05:13Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    5,
                    13,
                    0,
                    209,
                    0
                ],
                "title": "Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails\n  in Solar PV Thermal Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails\n  in Solar PV Thermal Imagery"
                },
                "summary": "Accurate detection of defects such as hotspots and snail trails in\nphotovoltaic modules is essential for maintaining energy efficiency and system\nreliablility. This work presents a supervised deep learning framework for\nsegmenting thermal infrared images of PV panels, using a dataset of 277 aerial\nthermographic images captured by zenmuse XT infrared camera mounted on a DJI\nMatrice 100 drone. The preprocessing pipeline includes image resizing, CLAHE\nbased contrast enhancement, denoising, and normalisation. A lightweight\nsemantic segmentation model based on SegFormer is developed, featuring a\ncustomised Transformwer encoder and streamlined decoder, and fine-tuned on\nannotated images with manually labeled defect regions. To evaluate performance,\nwe benchmark our model against U-Net, DeepLabV3, PSPNet, and Mask2Former using\nconsistent preprocessing and augmentation. Evaluation metrices includes\nper-class Dice score, F1-score, Cohen's kappa, mean IoU, and pixel accuracy.\nThe SegFormer-based model outperforms baselines in accuracy and efficiency,\nparticularly for segmenting small and irregular defects. Its lightweight design\nreal-time deployment on edge devices and seamless integration with drone-based\nsystems for automated inspection of large-scale solar farms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of defects such as hotspots and snail trails in\nphotovoltaic modules is essential for maintaining energy efficiency and system\nreliablility. This work presents a supervised deep learning framework for\nsegmenting thermal infrared images of PV panels, using a dataset of 277 aerial\nthermographic images captured by zenmuse XT infrared camera mounted on a DJI\nMatrice 100 drone. The preprocessing pipeline includes image resizing, CLAHE\nbased contrast enhancement, denoising, and normalisation. A lightweight\nsemantic segmentation model based on SegFormer is developed, featuring a\ncustomised Transformwer encoder and streamlined decoder, and fine-tuned on\nannotated images with manually labeled defect regions. To evaluate performance,\nwe benchmark our model against U-Net, DeepLabV3, PSPNet, and Mask2Former using\nconsistent preprocessing and augmentation. Evaluation metrices includes\nper-class Dice score, F1-score, Cohen's kappa, mean IoU, and pixel accuracy.\nThe SegFormer-based model outperforms baselines in accuracy and efficiency,\nparticularly for segmenting small and irregular defects. Its lightweight design\nreal-time deployment on edge devices and seamless integration with drone-based\nsystems for automated inspection of large-scale solar farms."
                },
                "authors": [
                    {
                        "name": "Deepak Joshi"
                    },
                    {
                        "name": "Mayukha Pal"
                    }
                ],
                "author_detail": {
                    "name": "Mayukha Pal"
                },
                "author": "Mayukha Pal",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20674v1",
                "updated": "2025-07-28T09:55:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    55,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:55:04Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    55,
                    4,
                    0,
                    209,
                    0
                ],
                "title": "LLM-Based Repair of Static Nullability Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Repair of Static Nullability Errors"
                },
                "summary": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects."
                },
                "authors": [
                    {
                        "name": "Nima Karimipour"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Martin Kellogg"
                    },
                    {
                        "name": "Manu Sridharan"
                    }
                ],
                "author_detail": {
                    "name": "Manu Sridharan"
                },
                "author": "Manu Sridharan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17762v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17762v4",
                "updated": "2025-07-28T09:54:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    54,
                    49,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-26T03:33:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    33,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding"
                },
                "summary": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks."
                },
                "authors": [
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Chen Du"
                    },
                    {
                        "name": "Ping Song"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17762v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17762v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20672v1",
                "updated": "2025-07-28T09:53:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    53,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:53:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    53,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "Program Analysis for High-Value Smart Contract Vulnerabilities:\n  Techniques and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program Analysis for High-Value Smart Contract Vulnerabilities:\n  Techniques and Insights"
                },
                "summary": "A widespread belief in the blockchain security community is that automated\ntechniques are only good for detecting shallow bugs, typically of small value.\nIn this paper, we present the techniques and insights that have led us to\nrepeatable success in automatically discovering high-value smart contract\nvulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,\nfor a total of over $3M, over high-profile deployed code, as well as hundreds\nof bugs detected in pre-deployment or under-audit code.\n  We argue that the elements of this surprising success are a) a very\nhigh-completeness static analysis approach that manages to maintain acceptable\nprecision; b) domain knowledge, provided by experts or captured via statistical\ninference. We present novel techniques for automatically inferring domain\nknowledge from statistical analysis of a large corpus of deployed contracts, as\nwell as discuss insights on the ideal precision and warning rate of a promising\nvulnerability detector. In contrast to academic literature in program analysis,\nwhich routinely expects false-positive rates below 50% for publishable results,\nwe posit that a useful analysis for high-value real-world vulnerabilities will\nlikely flag very few programs (under 1%) and will do so with a high\nfalse-positive rate (e.g., 95%, meaning that only one-of-twenty human\ninspections will yield an exploitable vulnerability).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A widespread belief in the blockchain security community is that automated\ntechniques are only good for detecting shallow bugs, typically of small value.\nIn this paper, we present the techniques and insights that have led us to\nrepeatable success in automatically discovering high-value smart contract\nvulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,\nfor a total of over $3M, over high-profile deployed code, as well as hundreds\nof bugs detected in pre-deployment or under-audit code.\n  We argue that the elements of this surprising success are a) a very\nhigh-completeness static analysis approach that manages to maintain acceptable\nprecision; b) domain knowledge, provided by experts or captured via statistical\ninference. We present novel techniques for automatically inferring domain\nknowledge from statistical analysis of a large corpus of deployed contracts, as\nwell as discuss insights on the ideal precision and warning rate of a promising\nvulnerability detector. In contrast to academic literature in program analysis,\nwhich routinely expects false-positive rates below 50% for publishable results,\nwe posit that a useful analysis for high-value real-world vulnerabilities will\nlikely flag very few programs (under 1%) and will do so with a high\nfalse-positive rate (e.g., 95%, meaning that only one-of-twenty human\ninspections will yield an exploitable vulnerability)."
                },
                "authors": [
                    {
                        "name": "Yannis Smaragdakis"
                    },
                    {
                        "name": "Neville Grech"
                    },
                    {
                        "name": "Sifis Lagouvardos"
                    },
                    {
                        "name": "Konstantinos Triantafyllou"
                    },
                    {
                        "name": "Ilias Tsatiris"
                    },
                    {
                        "name": "Yannis Bollanos"
                    },
                    {
                        "name": "Tony Rocco Valentine"
                    }
                ],
                "author_detail": {
                    "name": "Tony Rocco Valentine"
                },
                "author": "Tony Rocco Valentine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20666v1",
                "updated": "2025-07-28T09:42:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    42,
                    41,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:42:41Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    42,
                    41,
                    0,
                    209,
                    0
                ],
                "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative\n  Evaluation of Anomalous Sound Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative\n  Evaluation of Anomalous Sound Detection"
                },
                "summary": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems."
                },
                "authors": [
                    {
                        "name": "Harsh Purohit"
                    },
                    {
                        "name": "Tomoya Nishida"
                    },
                    {
                        "name": "Kota Dohi"
                    },
                    {
                        "name": "Takashi Endo"
                    },
                    {
                        "name": "Yohei Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Kawaguchi"
                },
                "author": "Yohei Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20655v1",
                "updated": "2025-07-28T09:21:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    21,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:21:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    21,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "CoGrader: Transforming Instructors' Assessment of Project Reports\n  through Collaborative LLM Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoGrader: Transforming Instructors' Assessment of Project Reports\n  through Collaborative LLM Integration"
                },
                "summary": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems."
                },
                "authors": [
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Haobo Li"
                    },
                    {
                        "name": "Chuhan Shi"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_doi": "10.1145/3746059.3747670",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747670",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ACM UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v2",
                "updated": "2025-07-28T09:19:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    19,
                    42,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Testora: Using Natural Language Intent to Detect Behavioral Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testora: Using Natural Language Intent to Detect Behavioral Regressions"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted at IEEE/ACM International Conference on Software Engineering\n  (ICSE) 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20650v1",
                "updated": "2025-07-28T09:14:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    14,
                    21,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:14:21Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    14,
                    21,
                    0,
                    209,
                    0
                ],
                "title": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for\n  Large-scale Model Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for\n  Large-scale Model Distribution"
                },
                "summary": "Recently, Deep Learning (DL) models have been increasingly deployed on\nend-user devices as On-Device AI, offering improved efficiency and privacy.\nHowever, this deployment trend poses more serious Intellectual Property (IP)\nrisks, as models are distributed on numerous local devices, making them\nvulnerable to theft and redistribution. Most existing ownership protection\nsolutions (e.g., backdoor-based watermarking) are designed for cloud-based\nAI-as-a-Service (AIaaS) and are not directly applicable to large-scale\ndistribution scenarios, where each user-specific model instance must carry a\nunique watermark. These methods typically embed a fixed watermark, and\nmodifying the embedded watermark requires retraining the model. To address\nthese challenges, we propose Hot-Swap MarkBoard, an efficient watermarking\nmethod. It encodes user-specific $n$-bit binary signatures by independently\nembedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)\nmodule, enabling efficient watermark customization without retraining through\nbranch swapping. A parameter obfuscation mechanism further entangles the\nwatermark weights with those of the base model, preventing removal without\ndegrading model performance. The method supports black-box verification and is\ncompatible with various model architectures and DL tasks, including\nclassification, image generation, and text generation. Extensive experiments\nacross three types of tasks and six backbone models demonstrate our method's\nsuperior efficiency and adaptability compared to existing approaches, achieving\n100\\% verification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Deep Learning (DL) models have been increasingly deployed on\nend-user devices as On-Device AI, offering improved efficiency and privacy.\nHowever, this deployment trend poses more serious Intellectual Property (IP)\nrisks, as models are distributed on numerous local devices, making them\nvulnerable to theft and redistribution. Most existing ownership protection\nsolutions (e.g., backdoor-based watermarking) are designed for cloud-based\nAI-as-a-Service (AIaaS) and are not directly applicable to large-scale\ndistribution scenarios, where each user-specific model instance must carry a\nunique watermark. These methods typically embed a fixed watermark, and\nmodifying the embedded watermark requires retraining the model. To address\nthese challenges, we propose Hot-Swap MarkBoard, an efficient watermarking\nmethod. It encodes user-specific $n$-bit binary signatures by independently\nembedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)\nmodule, enabling efficient watermark customization without retraining through\nbranch swapping. A parameter obfuscation mechanism further entangles the\nwatermark weights with those of the base model, preventing removal without\ndegrading model performance. The method supports black-box verification and is\ncompatible with various model architectures and DL tasks, including\nclassification, image generation, and text generation. Extensive experiments\nacross three types of tasks and six backbone models demonstrate our method's\nsuperior efficiency and adaptability compared to existing approaches, achieving\n100\\% verification accuracy."
                },
                "authors": [
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Mengke Wan"
                    },
                    {
                        "name": "Jiang Fang"
                    },
                    {
                        "name": "Diandian Guo"
                    },
                    {
                        "name": "Yezeng Chen"
                    },
                    {
                        "name": "Yinlong Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jiyan Sun"
                    },
                    {
                        "name": "Liru Geng"
                    }
                ],
                "author_detail": {
                    "name": "Liru Geng"
                },
                "author": "Liru Geng",
                "arxiv_doi": "10.1145/3746027.3755345",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755345",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.20650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20643v1",
                "updated": "2025-07-28T09:00:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    0,
                    48,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:00:48Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    0,
                    48,
                    0,
                    209,
                    0
                ],
                "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Wenbin Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Zirui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Chen"
                },
                "author": "Zirui Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18181v2",
                "updated": "2025-07-28T08:55:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    55,
                    9,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-24T08:27:53Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    27,
                    53,
                    3,
                    205,
                    0
                ],
                "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding"
                },
                "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy."
                },
                "authors": [
                    {
                        "name": "Linye Wei"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Songqiang Xu"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by Design Automation Conference (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13940v2",
                "updated": "2025-07-28T08:10:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    10,
                    33,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-20T05:18:15Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    18,
                    15,
                    1,
                    140,
                    0
                ],
                "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery"
                },
                "summary": "Large language models (LLMs) integrated with autonomous agents hold\nsignificant potential for advancing scientific discovery through automated\nreasoning and task execution. However, applying LLM agents to drug discovery is\nstill constrained by challenges such as large-scale multimodal data processing,\nlimited task automation, and poor support for domain-specific tools. To\novercome these limitations, we introduce DrugPilot, a LLM-based agent system\nwith a parameterized reasoning architecture designed for end-to-end scientific\nworkflows in drug discovery. DrugPilot enables multi-stage research processes\nby integrating structured tool use with a novel parameterized memory pool. The\nmemory pool converts heterogeneous data from both public sources and\nuser-defined inputs into standardized representations. This design supports\nefficient multi-turn dialogue, reduces information loss during data exchange,\nand enhances complex scientific decision-making. To support training and\nbenchmarking, we construct a drug instruction dataset covering eight core drug\ndiscovery tasks. Under the Berkeley function-calling benchmark, DrugPilot\nsignificantly outperforms state-of-the-art agents such as ReAct and LoT,\nachieving task completion rates of 98.0%, 93.5%, and 64.0% for simple,\nmulti-tool, and multi-turn scenarios, respectively. These results highlight\nDrugPilot's potential as a versatile agent framework for computational science\ndomains requiring automated, interactive, and data-integrated reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated with autonomous agents hold\nsignificant potential for advancing scientific discovery through automated\nreasoning and task execution. However, applying LLM agents to drug discovery is\nstill constrained by challenges such as large-scale multimodal data processing,\nlimited task automation, and poor support for domain-specific tools. To\novercome these limitations, we introduce DrugPilot, a LLM-based agent system\nwith a parameterized reasoning architecture designed for end-to-end scientific\nworkflows in drug discovery. DrugPilot enables multi-stage research processes\nby integrating structured tool use with a novel parameterized memory pool. The\nmemory pool converts heterogeneous data from both public sources and\nuser-defined inputs into standardized representations. This design supports\nefficient multi-turn dialogue, reduces information loss during data exchange,\nand enhances complex scientific decision-making. To support training and\nbenchmarking, we construct a drug instruction dataset covering eight core drug\ndiscovery tasks. Under the Berkeley function-calling benchmark, DrugPilot\nsignificantly outperforms state-of-the-art agents such as ReAct and LoT,\nachieving task completion rates of 98.0%, 93.5%, and 64.0% for simple,\nmulti-tool, and multi-turn scenarios, respectively. These results highlight\nDrugPilot's potential as a versatile agent framework for computational science\ndomains requiring automated, interactive, and data-integrated reasoning."
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Zhennan Wu"
                    },
                    {
                        "name": "Shoupeng Wang"
                    },
                    {
                        "name": "Jia Wu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Wenbin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Hu"
                },
                "author": "Wenbin Hu",
                "arxiv_comment": "29 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05858v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05858v6",
                "updated": "2025-07-28T08:09:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    9,
                    6,
                    0,
                    209,
                    0
                ],
                "published": "2023-10-09T16:52:48Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    16,
                    52,
                    48,
                    0,
                    282,
                    0
                ],
                "title": "Distributional Soft Actor-Critic with Three Refinements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Soft Actor-Critic with Three Refinements"
                },
                "summary": "Reinforcement learning (RL) has shown remarkable success in solving complex\ndecision-making and control tasks. However, many model-free RL algorithms\nexperience performance degradation due to inaccurate value estimation,\nparticularly the overestimation of Q-values, which can lead to suboptimal\npolicies. To address this issue, we previously proposed the Distributional Soft\nActor-Critic (DSAC or DSACv1), an off-policy RL algorithm that enhances value\nestimation accuracy by learning a continuous Gaussian value distribution.\nDespite its effectiveness, DSACv1 faces challenges such as training instability\nand sensitivity to reward scaling, caused by high variance in critic gradients\ndue to return randomness. In this paper, we introduce three key refinements to\nDSACv1 to overcome these limitations and further improve Q-value estimation\naccuracy: expected value substitution, twin value distribution learning, and\nvariance-based critic gradient adjustment. The enhanced algorithm, termed DSAC\nwith Three refinements (DSAC-T or DSACv2), is systematically evaluated across a\ndiverse set of benchmark tasks. Without the need for task-specific\nhyperparameter tuning, DSAC-T consistently matches or outperforms leading\nmodel-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all\ntested environments. Additionally, DSAC-T ensures a stable learning process and\nmaintains robust performance across varying reward scales. Its effectiveness is\nfurther demonstrated through real-world application in controlling a wheeled\nrobot, highlighting its potential for deployment in practical robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has shown remarkable success in solving complex\ndecision-making and control tasks. However, many model-free RL algorithms\nexperience performance degradation due to inaccurate value estimation,\nparticularly the overestimation of Q-values, which can lead to suboptimal\npolicies. To address this issue, we previously proposed the Distributional Soft\nActor-Critic (DSAC or DSACv1), an off-policy RL algorithm that enhances value\nestimation accuracy by learning a continuous Gaussian value distribution.\nDespite its effectiveness, DSACv1 faces challenges such as training instability\nand sensitivity to reward scaling, caused by high variance in critic gradients\ndue to return randomness. In this paper, we introduce three key refinements to\nDSACv1 to overcome these limitations and further improve Q-value estimation\naccuracy: expected value substitution, twin value distribution learning, and\nvariance-based critic gradient adjustment. The enhanced algorithm, termed DSAC\nwith Three refinements (DSAC-T or DSACv2), is systematically evaluated across a\ndiverse set of benchmark tasks. Without the need for task-specific\nhyperparameter tuning, DSAC-T consistently matches or outperforms leading\nmodel-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all\ntested environments. Additionally, DSAC-T ensures a stable learning process and\nmaintains robust performance across varying reward scales. Its effectiveness is\nfurther demonstrated through real-world application in controlling a wheeled\nrobot, highlighting its potential for deployment in practical robotic tasks."
                },
                "authors": [
                    {
                        "name": "Jingliang Duan"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Liming Xiao"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Keqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiang Li"
                },
                "author": "Keqiang Li",
                "arxiv_doi": "10.1109/TPAMI.2025.3537087",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3537087",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.05858v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05858v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Title updated in this version. The previous version was titled\n  \"DSAC-T: Distributional Soft Actor-Critic With Three Refinements\". No other\n  major changes",
                "arxiv_journal_ref": "IEEE Trans. PAMI,47(5): 3935-3946,2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20592v1",
                "updated": "2025-07-28T08:02:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    2,
                    31,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:02:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    2,
                    31,
                    0,
                    209,
                    0
                ],
                "title": "PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase\n  Adaptation"
                },
                "summary": "Neural Architecture Search (NAS) is challenged by the trade-off between\nsearch space exploration and efficiency, especially for complex tasks. While\nrecent LLM-based NAS methods have shown promise, they often suffer from static\nsearch strategies and ambiguous architecture representations. We propose\nPhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by\nreal-time score thresholds and a structured architecture template language for\nconsistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS\nconsistently discovers architectures with higher accuracy and better rank. For\nimage classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%\nwhile maintaining or improving accuracy. In object detection, it automatically\nproduces YOLOv8 variants with higher mAP and lower resource cost. These results\ndemonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS\nacross diverse vision tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) is challenged by the trade-off between\nsearch space exploration and efficiency, especially for complex tasks. While\nrecent LLM-based NAS methods have shown promise, they often suffer from static\nsearch strategies and ambiguous architecture representations. We propose\nPhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by\nreal-time score thresholds and a structured architecture template language for\nconsistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS\nconsistently discovers architectures with higher accuracy and better rank. For\nimage classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%\nwhile maintaining or improving accuracy. In object detection, it automatically\nproduces YOLOv8 variants with higher mAP and lower resource cost. These results\ndemonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS\nacross diverse vision tasks."
                },
                "authors": [
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Xiaohan Shan"
                    },
                    {
                        "name": "Yanwei Hu"
                    },
                    {
                        "name": "Jianmin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Li"
                },
                "author": "Jianmin Li",
                "arxiv_comment": "14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20587v1",
                "updated": "2025-07-28T07:52:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    52,
                    18,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T07:52:18Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    52,
                    18,
                    0,
                    209,
                    0
                ],
                "title": "Real-Time Distributed Optical Fiber Vibration Recognition via Extreme\n  Lightweight Model and Cross-Domain Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Distributed Optical Fiber Vibration Recognition via Extreme\n  Lightweight Model and Cross-Domain Distillation"
                },
                "summary": "Distributed optical fiber vibration sensing (DVS) systems offer a promising\nsolution for large-scale monitoring and intrusion event recognition. However,\ntheir practical deployment remains hindered by two major challenges:\ndegradation of recognition accuracy in dynamic conditions, and the\ncomputational bottleneck of real-time processing for mass sensing data. This\npaper presents a new solution to these challenges, through a FPGA-accelerated\nextreme lightweight model along with a newly proposed knowledge distillation\nframework. The proposed three-layer depthwise separable convolution network\ncontains only 4141 parameters, which is the most compact architecture in this\nfield to date, and achieves a maximum processing speed of 0.019 ms for each\nsample covering a 12.5 m fiber length over 0.256 s. This performance\ncorresponds to real-time processing capabilities for sensing fibers extending\nup to 168.68 km. To improve generalizability under changing environments, the\nproposed cross-domain distillation framework guided by physical priors is used\nhere to embed frequency-domain insights into the time-domain model. This allows\nfor time-frequency representation learning without increasing complexity and\nboosts recognition accuracy from 51.93% to 95.72% under unseen environmental\nconditions. The proposed methodology provides key advancements including a\nframework combining interpretable signal processing technique with deep\nlearning and a reference architecture for real-time processing and\nedge-computing in DVS systems, and more general distributed optical fiber\nsensing (DOFS) area. It mitigates the trade-off between sensing range and\nreal-time capability, bridging the gap between theoretical capabilities and\npractical deployment requirements. Furthermore, this work reveals a new\ndirection for building more efficient, robust and explainable artificial\nintelligence systems for DOFS technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed optical fiber vibration sensing (DVS) systems offer a promising\nsolution for large-scale monitoring and intrusion event recognition. However,\ntheir practical deployment remains hindered by two major challenges:\ndegradation of recognition accuracy in dynamic conditions, and the\ncomputational bottleneck of real-time processing for mass sensing data. This\npaper presents a new solution to these challenges, through a FPGA-accelerated\nextreme lightweight model along with a newly proposed knowledge distillation\nframework. The proposed three-layer depthwise separable convolution network\ncontains only 4141 parameters, which is the most compact architecture in this\nfield to date, and achieves a maximum processing speed of 0.019 ms for each\nsample covering a 12.5 m fiber length over 0.256 s. This performance\ncorresponds to real-time processing capabilities for sensing fibers extending\nup to 168.68 km. To improve generalizability under changing environments, the\nproposed cross-domain distillation framework guided by physical priors is used\nhere to embed frequency-domain insights into the time-domain model. This allows\nfor time-frequency representation learning without increasing complexity and\nboosts recognition accuracy from 51.93% to 95.72% under unseen environmental\nconditions. The proposed methodology provides key advancements including a\nframework combining interpretable signal processing technique with deep\nlearning and a reference architecture for real-time processing and\nedge-computing in DVS systems, and more general distributed optical fiber\nsensing (DOFS) area. It mitigates the trade-off between sensing range and\nreal-time capability, bridging the gap between theoretical capabilities and\npractical deployment requirements. Furthermore, this work reveals a new\ndirection for building more efficient, robust and explainable artificial\nintelligence systems for DOFS technologies."
                },
                "authors": [
                    {
                        "name": "Zhongyao Luo"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Zhao Ge"
                    },
                    {
                        "name": "Ming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Tang"
                },
                "author": "Ming Tang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05815v2",
                "updated": "2025-07-28T07:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    51,
                    16,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-08T09:36:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "Beyond Manual Annotation: A Human-AI Collaborative Framework for Medical\n  Image Segmentation Using Only \"Better or Worse\" Expert Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Manual Annotation: A Human-AI Collaborative Framework for Medical\n  Image Segmentation Using Only \"Better or Worse\" Expert Feedback"
                },
                "summary": "Manual annotation of medical images is a labor-intensive and time-consuming\nprocess, posing a significant bottleneck in the development and deployment of\nrobust medical imaging AI systems. This paper introduces a novel hands-free\nHuman-AI collaborative framework for medical image segmentation that\nsubstantially reduces the annotation burden by eliminating the need for\nexplicit manual pixel-level labeling. The core innovation lies in a preference\nlearning paradigm, where human experts provide minimal, intuitive feedback --\nsimply indicating whether an AI-generated segmentation is better or worse than\na previous version. The framework comprises four key components: (1) an\nadaptable foundation model (FM) for feature extraction, (2) label propagation\nbased on feature similarity, (3) a clicking agent that learns from human\nbetter-or-worse feedback to decide where to click and with which label, and (4)\na multi-round segmentation learning procedure that trains a state-of-the-art\nsegmentation network using pseudo-labels generated by the clicking agent and\nFM-based label propagation. Experiments on three public datasets demonstrate\nthat the proposed approach achieves competitive segmentation performance using\nonly binary preference feedback, without requiring experts to directly manually\nannotate the images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual annotation of medical images is a labor-intensive and time-consuming\nprocess, posing a significant bottleneck in the development and deployment of\nrobust medical imaging AI systems. This paper introduces a novel hands-free\nHuman-AI collaborative framework for medical image segmentation that\nsubstantially reduces the annotation burden by eliminating the need for\nexplicit manual pixel-level labeling. The core innovation lies in a preference\nlearning paradigm, where human experts provide minimal, intuitive feedback --\nsimply indicating whether an AI-generated segmentation is better or worse than\na previous version. The framework comprises four key components: (1) an\nadaptable foundation model (FM) for feature extraction, (2) label propagation\nbased on feature similarity, (3) a clicking agent that learns from human\nbetter-or-worse feedback to decide where to click and with which label, and (4)\na multi-round segmentation learning procedure that trains a state-of-the-art\nsegmentation network using pseudo-labels generated by the clicking agent and\nFM-based label propagation. Experiments on three public datasets demonstrate\nthat the proposed approach achieves competitive segmentation performance using\nonly binary preference feedback, without requiring experts to directly manually\nannotate the images."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02820v2",
                "updated": "2025-07-28T07:46:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    46,
                    27,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-05T17:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLibra: Agent Metric Induction from Open-Ended Feedback"
                },
                "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback e.g. \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback e.g. \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."
                },
                "authors": [
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Phil Cuvin"
                    },
                    {
                        "name": "Xinkai Yu"
                    },
                    {
                        "name": "Charlotte Ka Yee Yan"
                    },
                    {
                        "name": "Jason Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "https://opensocial.world/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03387v3",
                "updated": "2025-07-29T16:23:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    16,
                    23,
                    2,
                    1,
                    210,
                    0
                ],
                "published": "2025-02-05T17:23:45Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    23,
                    45,
                    2,
                    36,
                    0
                ],
                "title": "LIMO: Less is More for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIMO: Less is More for Reasoning"
                },
                "summary": "We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning."
                },
                "authors": [
                    {
                        "name": "Yixin Ye"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Ethan Chern"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06102v3",
                "updated": "2025-07-28T07:16:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    16,
                    4,
                    0,
                    209,
                    0
                ],
                "published": "2024-11-09T07:32:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    7,
                    32,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence"
                },
                "summary": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Haining Xie"
                    },
                    {
                        "name": "Siqi Shen"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Meng Lei"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chunyou Li"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yinjun Wu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Peng Chen"
                },
                "author": "Peng Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16143v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16143v3",
                "updated": "2025-07-28T07:14:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    14,
                    36,
                    0,
                    209,
                    0
                ],
                "published": "2025-01-27T15:36:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    36,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments"
                },
                "summary": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met."
                },
                "authors": [
                    {
                        "name": "Marco Zambianco"
                    },
                    {
                        "name": "Silvio Cretti"
                    },
                    {
                        "name": "Domenico Siracusa"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Siracusa"
                },
                "author": "Domenico Siracusa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16143v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16143v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14111v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14111v4",
                "updated": "2025-07-28T07:04:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    7,
                    4,
                    11,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-18T17:43:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    17,
                    43,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning"
                },
                "summary": "The exponential growth in demand for GPU computing resources has created an\nurgent need for automated CUDA optimization strategies. While recent advances\nin LLMs show promise for code generation, current SOTA models achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization that employs a\nnovel contrastive RL algorithm.\n  CUDA-L1 achieves significant performance improvements on the CUDA\noptimization task: trained on NVIDIA A100, it delivers an average speedup of\nx3.12 with a median speedup of x1.42 across all 250 CUDA kernels of\nKernelBench, with peak speedups reaching x120. Furthermore, the model also\ndemonstrates portability across GPU architectures, achieving average speedups\nof x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite\nbeing optimized specifically for A100.\n  The capabilities of CUDA-L1 demonstrate that, RL can transform an initially\npoor-performing LLM into an effective CUDA optimizer through speedup-based\nreward signals alone, without human expertise or domain knowledge. This\nparadigm opens possibilities for automated optimization of CUDA operations, and\nholds promise to substantially promote GPU efficiency and alleviate the rising\npressure on GPU computing resources. We also identify important challenges\nposed by training RL models for tasks like CUDA development, where RL often\nlearns to exploit loopholes in reward functions rather than solve the intended\noptimization problems. By identifying these failure modes and analyzing their\nroot causes, we develop practical methods for creating more robust training\nprocedures that prevent reward hacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth in demand for GPU computing resources has created an\nurgent need for automated CUDA optimization strategies. While recent advances\nin LLMs show promise for code generation, current SOTA models achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization that employs a\nnovel contrastive RL algorithm.\n  CUDA-L1 achieves significant performance improvements on the CUDA\noptimization task: trained on NVIDIA A100, it delivers an average speedup of\nx3.12 with a median speedup of x1.42 across all 250 CUDA kernels of\nKernelBench, with peak speedups reaching x120. Furthermore, the model also\ndemonstrates portability across GPU architectures, achieving average speedups\nof x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite\nbeing optimized specifically for A100.\n  The capabilities of CUDA-L1 demonstrate that, RL can transform an initially\npoor-performing LLM into an effective CUDA optimizer through speedup-based\nreward signals alone, without human expertise or domain knowledge. This\nparadigm opens possibilities for automated optimization of CUDA operations, and\nholds promise to substantially promote GPU efficiency and alleviate the rising\npressure on GPU computing resources. We also identify important challenges\nposed by training RL models for tasks like CUDA development, where RL often\nlearns to exploit loopholes in reward functions rather than solve the intended\noptimization problems. By identifying these failure modes and analyzing their\nroot causes, we develop practical methods for creating more robust training\nprocedures that prevent reward hacking."
                },
                "authors": [
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Chris Shum"
                    }
                ],
                "author_detail": {
                    "name": "Chris Shum"
                },
                "author": "Chris Shum",
                "arxiv_comment": "Project Page: https://deepreinforce-ai.github.io/cudal1_blog/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14111v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14111v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20553v1",
                "updated": "2025-07-28T06:38:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    6,
                    38,
                    38,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T06:38:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    6,
                    38,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "GeoJSEval: An Automated Evaluation Framework for Large Language Models\n  on JavaScript-Based Geospatial Computation and Visualization Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoJSEval: An Automated Evaluation Framework for Large Language Models\n  on JavaScript-Based Geospatial Computation and Visualization Code Generation"
                },
                "summary": "With the widespread adoption of large language models (LLMs) in code\ngeneration tasks, geospatial code generation has emerged as a critical frontier\nin the integration of artificial intelligence and geoscientific analysis. This\ntrend underscores the urgent need for systematic evaluation methodologies to\nassess LLMs generation capabilities in geospatial contexts. In particular,\ngeospatial computation and visualization tasks in JavaScript environments rely\nheavily on orchestrating diverse frontend libraries and ecosystems, placing\nelevated demands on a model's semantic understanding and code synthesis\nabilities. To address this challenge, we propose GeoJSEval--the first\nmultimodal, function-level automatic evaluation framework for LLMs in\nJavaScript-based geospatial code generation. GeoJSEval comprises three core\ncomponents: a standardized test suite (GeoJSEval-Bench), a code submission\nengine, and an evaluation module. It includes 432 function-level tasks and\n2,071 structured test cases spanning five widely used JavaScript geospatial\nlibraries and 25 mainstream geospatial data types. GeoJSEval enables\nmultidimensional quantitative evaluation across metrics such as accuracy,\noutput stability, execution efficiency, resource consumption, and error type\ndistribution, and integrates boundary testing mechanisms to enhance robustness\nand coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs\nusing GeoJSEval, revealing significant performance disparities and bottlenecks\nin spatial semantic understanding, code reliability, and function invocation\naccuracy. GeoJSEval provides a foundational methodology, evaluation resource,\nand practical toolkit for the standardized assessment and optimization of\ngeospatial code generation models, with strong extensibility and applicability\nin real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of large language models (LLMs) in code\ngeneration tasks, geospatial code generation has emerged as a critical frontier\nin the integration of artificial intelligence and geoscientific analysis. This\ntrend underscores the urgent need for systematic evaluation methodologies to\nassess LLMs generation capabilities in geospatial contexts. In particular,\ngeospatial computation and visualization tasks in JavaScript environments rely\nheavily on orchestrating diverse frontend libraries and ecosystems, placing\nelevated demands on a model's semantic understanding and code synthesis\nabilities. To address this challenge, we propose GeoJSEval--the first\nmultimodal, function-level automatic evaluation framework for LLMs in\nJavaScript-based geospatial code generation. GeoJSEval comprises three core\ncomponents: a standardized test suite (GeoJSEval-Bench), a code submission\nengine, and an evaluation module. It includes 432 function-level tasks and\n2,071 structured test cases spanning five widely used JavaScript geospatial\nlibraries and 25 mainstream geospatial data types. GeoJSEval enables\nmultidimensional quantitative evaluation across metrics such as accuracy,\noutput stability, execution efficiency, resource consumption, and error type\ndistribution, and integrates boundary testing mechanisms to enhance robustness\nand coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs\nusing GeoJSEval, revealing significant performance disparities and bottlenecks\nin spatial semantic understanding, code reliability, and function invocation\naccuracy. GeoJSEval provides a foundational methodology, evaluation resource,\nand practical toolkit for the standardized assessment and optimization of\ngeospatial code generation models, with strong extensibility and applicability\nin real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Haoyue Jiao"
                    },
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Lutong Xie"
                    },
                    {
                        "name": "Shaowen Wu"
                    },
                    {
                        "name": "Huayi Wu"
                    },
                    {
                        "name": "Xuefeng Guan"
                    },
                    {
                        "name": "Zhipeng Gui"
                    }
                ],
                "author_detail": {
                    "name": "Zhipeng Gui"
                },
                "author": "Zhipeng Gui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20546v1",
                "updated": "2025-07-28T06:13:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    6,
                    13,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T06:13:23Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    6,
                    13,
                    23,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Hallucination Detection via Future Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Hallucination Detection via Future Context"
                },
                "summary": "Large Language Models (LLMs) are widely used to generate plausible text on\nonline platforms, without revealing the generation process. As users\nincreasingly encounter such black-box outputs, detecting hallucinations has\nbecome a critical challenge. To address this challenge, we focus on developing\na hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample\nfuture contexts. The sampled future contexts provide valuable clues for\nhallucination detection and can be effectively integrated with various\nsampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used to generate plausible text on\nonline platforms, without revealing the generation process. As users\nincreasingly encounter such black-box outputs, detecting hallucinations has\nbecome a critical challenge. To address this challenge, we focus on developing\na hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample\nfuture contexts. The sampled future contexts provide valuable clues for\nhallucination detection and can be effectively integrated with various\nsampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach."
                },
                "authors": [
                    {
                        "name": "Joosung Lee"
                    },
                    {
                        "name": "Cheonbok Park"
                    },
                    {
                        "name": "Hwiyeol Jo"
                    },
                    {
                        "name": "Jeonghoon Kim"
                    },
                    {
                        "name": "Joonsuk Park"
                    },
                    {
                        "name": "Kang Min Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Kang Min Yoo"
                },
                "author": "Kang Min Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20541v1",
                "updated": "2025-07-28T05:56:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    56,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T05:56:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    56,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design"
                },
                "summary": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD."
                },
                "authors": [
                    {
                        "name": "Zishang Qiu"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Ruibin Bai"
                    }
                ],
                "author_detail": {
                    "name": "Ruibin Bai"
                },
                "author": "Ruibin Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13092v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13092v3",
                "updated": "2025-07-28T05:51:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    51,
                    49,
                    0,
                    209,
                    0
                ],
                "published": "2025-04-17T16:59:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    59,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventVAD: Training-Free Event-Aware Video Anomaly Detection"
                },
                "summary": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Haojin He"
                    },
                    {
                        "name": "Sijie Li"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fanhu Zeng"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Muyang Zhang"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Shuyan Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuyan Li"
                },
                "author": "Shuyan Li",
                "arxiv_comment": "Paper was accepted by ACM MM 2025; Code:\n  https://github.com/YihuaJerry/EventVAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13092v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13092v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02348v3",
                "updated": "2025-07-29T06:48:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    6,
                    48,
                    21,
                    1,
                    210,
                    0
                ],
                "published": "2025-03-04T07:17:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    7,
                    17,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "YOLO-PRO: Enhancing Instance-Specific Object Detection with Full-Channel\n  Global Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLO-PRO: Enhancing Instance-Specific Object Detection with Full-Channel\n  Global Self-Attention"
                },
                "summary": "This paper addresses the inherent limitations of conventional bottleneck\nstructures (diminished instance discriminability due to overemphasis on batch\nstatistics) and decoupled heads (computational redundancy) in object detection\nframeworks by proposing two novel modules: the Instance-Specific Bottleneck\nwith full-channel global self-attention (ISB) and the Instance-Specific\nAsymmetric Decoupled Head (ISADH). The ISB module innovatively reconstructs\nfeature maps to establish an efficient full-channel global attention mechanism\nthrough synergistic fusion of batch-statistical and instance-specific features.\nComplementing this, the ISADH module pioneers an asymmetric decoupled\narchitecture enabling hierarchical multi-dimensional feature integration via\ndual-stream batch-instance representation fusion. Extensive experiments on the\nMS-COCO benchmark demonstrate that the coordinated deployment of ISB and ISADH\nin the YOLO-PRO framework achieves state-of-the-art performance across all\ncomputational scales. Specifically, YOLO-PRO surpasses YOLOv8 by 1.0-1.6% AP\n(N/S/M/L/X scales) and outperforms YOLO11 by 0.1-0.5% AP in critical N/M/L/X\ngroups, while maintaining competitive computational efficiency. This work\nprovides practical insights for developing high-precision detectors deployable\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the inherent limitations of conventional bottleneck\nstructures (diminished instance discriminability due to overemphasis on batch\nstatistics) and decoupled heads (computational redundancy) in object detection\nframeworks by proposing two novel modules: the Instance-Specific Bottleneck\nwith full-channel global self-attention (ISB) and the Instance-Specific\nAsymmetric Decoupled Head (ISADH). The ISB module innovatively reconstructs\nfeature maps to establish an efficient full-channel global attention mechanism\nthrough synergistic fusion of batch-statistical and instance-specific features.\nComplementing this, the ISADH module pioneers an asymmetric decoupled\narchitecture enabling hierarchical multi-dimensional feature integration via\ndual-stream batch-instance representation fusion. Extensive experiments on the\nMS-COCO benchmark demonstrate that the coordinated deployment of ISB and ISADH\nin the YOLO-PRO framework achieves state-of-the-art performance across all\ncomputational scales. Specifically, YOLO-PRO surpasses YOLOv8 by 1.0-1.6% AP\n(N/S/M/L/X scales) and outperforms YOLO11 by 0.1-0.5% AP in critical N/M/L/X\ngroups, while maintaining competitive computational efficiency. This work\nprovides practical insights for developing high-precision detectors deployable\non edge devices."
                },
                "authors": [
                    {
                        "name": "Lin Huang"
                    },
                    {
                        "name": "Yujuan Tan"
                    },
                    {
                        "name": "Weisheng Li"
                    },
                    {
                        "name": "Shitai Shan"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Yue Niu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Niu"
                },
                "author": "Yue Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20535v1",
                "updated": "2025-07-28T05:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    37,
                    19,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T05:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    37,
                    19,
                    0,
                    209,
                    0
                ],
                "title": "Learning Explainable Stock Predictions with Tweets Using Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Explainable Stock Predictions with Tweets Using Mixture of\n  Experts"
                },
                "summary": "Stock price movements are influenced by many factors, and alongside\nhistorical price data, tex-tual information is a key source. Public news and\nsocial media offer valuable insights into market sentiment and emerging events.\nThese sources are fast-paced, diverse, and significantly impact future stock\ntrends. Recently, LLMs have enhanced financial analysis, but prompt-based\nmethods still have limitations, such as input length restrictions and\ndifficulties in predicting sequences of varying lengths. Additionally, most\nmodels rely on dense computational layers, which are resource-intensive. To\naddress these challenges, we propose the FTS- Text-MoE model, which combines\nnumerical data with key summaries from news and tweets using point embeddings,\nboosting prediction accuracy through the integration of factual textual data.\nThe model uses a Mixture of Experts (MoE) Transformer decoder to process both\ndata types. By activating only a subset of model parameters, it reduces\ncomputational costs. Furthermore, the model features multi-resolution\nprediction heads, enabling flexible forecasting of financial time series at\ndifferent scales. Experimental results show that FTS-Text-MoE outperforms\nbaseline methods in terms of investment returns and Sharpe ratio, demonstrating\nits superior accuracy and ability to predict future market trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stock price movements are influenced by many factors, and alongside\nhistorical price data, tex-tual information is a key source. Public news and\nsocial media offer valuable insights into market sentiment and emerging events.\nThese sources are fast-paced, diverse, and significantly impact future stock\ntrends. Recently, LLMs have enhanced financial analysis, but prompt-based\nmethods still have limitations, such as input length restrictions and\ndifficulties in predicting sequences of varying lengths. Additionally, most\nmodels rely on dense computational layers, which are resource-intensive. To\naddress these challenges, we propose the FTS- Text-MoE model, which combines\nnumerical data with key summaries from news and tweets using point embeddings,\nboosting prediction accuracy through the integration of factual textual data.\nThe model uses a Mixture of Experts (MoE) Transformer decoder to process both\ndata types. By activating only a subset of model parameters, it reduces\ncomputational costs. Furthermore, the model features multi-resolution\nprediction heads, enabling flexible forecasting of financial time series at\ndifferent scales. Experimental results show that FTS-Text-MoE outperforms\nbaseline methods in terms of investment returns and Sharpe ratio, demonstrating\nits superior accuracy and ability to predict future market trends."
                },
                "authors": [
                    {
                        "name": "Wenyan Xu"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Rundong Wang"
                    },
                    {
                        "name": "Yonghong Hu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Zhonghua Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhonghua Lu"
                },
                "author": "Zhonghua Lu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20308v2",
                "updated": "2025-07-28T05:26:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    26,
                    47,
                    0,
                    209,
                    0
                ],
                "published": "2025-05-20T18:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    27,
                    22,
                    1,
                    140,
                    0
                ],
                "title": "Large Language Model Powered Decision Support for a Metal Additive\n  Manufacturing Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Powered Decision Support for a Metal Additive\n  Manufacturing Knowledge Graph"
                },
                "summary": "Metal additive manufacturing (AM) involves complex interdependencies among\nprocesses, materials, feedstock, and post-processing steps. However, the\nunderlying relationships and domain knowledge remain fragmented across\nliterature and static databases that often require expert-level queries,\nlimiting their applicability in design and planning. To address these\nlimitations, we develop a novel and structured knowledge graph (KG),\nrepresenting 53 distinct metals and alloys across seven material categories,\nnine AM processes, four feedstock types, and corresponding post-processing\nrequirements. A large language model (LLM) interface, guided by a few-shot\nprompting strategy, enables natural language querying without the need for\nformal query syntax. The system supports a range of tasks, including\ncompatibility evaluation, constraint-based filtering, and design for AM (DfAM)\nguidance. User queries in natural language are normalized, translated into\nCypher, and executed on the KG, with results returned in a structured format.\nThis work introduces the first interactive system that connects a\ndomain-specific metal AM KG with an LLM interface, delivering accessible and\nexplainable decision support for engineers and promoting human-centered tools\nin manufacturing knowledge systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metal additive manufacturing (AM) involves complex interdependencies among\nprocesses, materials, feedstock, and post-processing steps. However, the\nunderlying relationships and domain knowledge remain fragmented across\nliterature and static databases that often require expert-level queries,\nlimiting their applicability in design and planning. To address these\nlimitations, we develop a novel and structured knowledge graph (KG),\nrepresenting 53 distinct metals and alloys across seven material categories,\nnine AM processes, four feedstock types, and corresponding post-processing\nrequirements. A large language model (LLM) interface, guided by a few-shot\nprompting strategy, enables natural language querying without the need for\nformal query syntax. The system supports a range of tasks, including\ncompatibility evaluation, constraint-based filtering, and design for AM (DfAM)\nguidance. User queries in natural language are normalized, translated into\nCypher, and executed on the KG, with results returned in a structured format.\nThis work introduces the first interactive system that connects a\ndomain-specific metal AM KG with an LLM interface, delivering accessible and\nexplainable decision support for engineers and promoting human-centered tools\nin manufacturing knowledge systems."
                },
                "authors": [
                    {
                        "name": "Muhammad Tayyab Khan"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Wenhe Feng"
                    },
                    {
                        "name": "Seung Ki Moon"
                    }
                ],
                "author_detail": {
                    "name": "Seung Ki Moon"
                },
                "author": "Seung Ki Moon",
                "arxiv_comment": "The paper has been accepted at 11th International Conference of Asian\n  Society for Precision Engineering and Nanotechnology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20527v2",
                "updated": "2025-07-29T17:02:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    17,
                    2,
                    27,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-28T05:17:48Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    17,
                    48,
                    0,
                    209,
                    0
                ],
                "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful\n  Mathematics Questions and Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful\n  Mathematics Questions and Answers"
                },
                "summary": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce \\textbf{SAND-Math} (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new \\textbf{Difficulty\nHiking} step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\n\\textbf{$\\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\n\\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce \\textbf{SAND-Math} (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new \\textbf{Difficulty\nHiking} step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\n\\textbf{$\\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\n\\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}"
                },
                "authors": [
                    {
                        "name": "Chaitanya Manem"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Prakamya Mishra"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]