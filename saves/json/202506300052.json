[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Søren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Søren Stobbe"
                },
                "author": "Søren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rúben Adão"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "João Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.21550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21550v1",
                "updated": "2025-06-26T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale"
                },
                "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection."
                },
                "authors": [
                    {
                        "name": "Xiaona Zhou"
                    },
                    {
                        "name": "Constantin Brif"
                    },
                    {
                        "name": "Ismini Lourentzou"
                    }
                ],
                "author_detail": {
                    "name": "Ismini Lourentzou"
                },
                "author": "Ismini Lourentzou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21551v1",
                "updated": "2025-06-26T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test"
                },
                "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Chenrui Fan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21535v1",
                "updated": "2025-06-26T17:54:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    20,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:54:20Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    20,
                    3,
                    177,
                    0
                ],
                "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Design Space of 3D MLLMs for CT Report Generation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution"
                },
                "authors": [
                    {
                        "name": "Mohammed Baharoon"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Congyu Fang"
                    },
                    {
                        "name": "Augustin Toma"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21532v1",
                "updated": "2025-06-26T17:52:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    52,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:52:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    52,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets"
                },
                "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat"
                },
                "authors": [
                    {
                        "name": "Akshay Paruchuri"
                    },
                    {
                        "name": "Maryam Aziz"
                    },
                    {
                        "name": "Rohit Vartak"
                    },
                    {
                        "name": "Ayman Ali"
                    },
                    {
                        "name": "Best Uchehara"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Ishan Chatterjee"
                    },
                    {
                        "name": "Monica Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Monica Agrawal"
                },
                "author": "Monica Agrawal",
                "arxiv_comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09587v2",
                "updated": "2025-06-26T17:51:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    51,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-12T18:55:53Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    53,
                    3,
                    347,
                    0
                ],
                "title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets\n  in 50+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets\n  in 50+ Languages"
                },
                "summary": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task."
                },
                "authors": [
                    {
                        "name": "Chester Palen-Michel"
                    },
                    {
                        "name": "Maxwell Pickering"
                    },
                    {
                        "name": "Maya Kruse"
                    },
                    {
                        "name": "Jonne Sälevä"
                    },
                    {
                        "name": "Constantine Lignos"
                    }
                ],
                "author_detail": {
                    "name": "Constantine Lignos"
                },
                "author": "Constantine Lignos",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08165v2",
                "updated": "2025-06-26T17:48:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    48,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2024-10-10T17:44:13Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    44,
                    13,
                    3,
                    284,
                    0
                ],
                "title": "Chain-of-Sketch: Enabling Global Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Sketch: Enabling Global Visual Reasoning"
                },
                "summary": "Modern vision models have achieved remarkable success in benchmarks where\nlocal features provide critical information about the target. There is now a\ngrowing interest in tackling tasks requiring more global reasoning, where local\nfeatures do not provide significant information. Minsky and Papert put forward\nsuch tasks in 1969 with their connectivity study, exposing the limitations of\nthe perceptron model. In this paper, we introduce an expanded set of global\nvisual datasets involving graphs, strings, mazes, and image grids. We show that\nlarge vision models still struggle to learn these tasks efficiently. Similarly,\nstate-of-the-art multi-modal LLMs perform poorly on these datasets. We explain\nthis learning inefficiency by means of the 'globality degree' measure. To\nmitigate this, we propose a method called chain-of-sketch (CoS). Similar to the\nchain-of-thought and scratchpad techniques used in language models, CoS breaks\nthe original task into intermediate visual steps to help learn a complex task.\nIn addition, we show that not all CoS strategies perform equally well. Our key\ninsight is to impose a Markovian structure on the CoS frames. This leads to the\nintroduction of 'inductive CoS' which achieves better out-of-distribution\ngeneralization and performs well even with smaller models compared to\nnon-inductive variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern vision models have achieved remarkable success in benchmarks where\nlocal features provide critical information about the target. There is now a\ngrowing interest in tackling tasks requiring more global reasoning, where local\nfeatures do not provide significant information. Minsky and Papert put forward\nsuch tasks in 1969 with their connectivity study, exposing the limitations of\nthe perceptron model. In this paper, we introduce an expanded set of global\nvisual datasets involving graphs, strings, mazes, and image grids. We show that\nlarge vision models still struggle to learn these tasks efficiently. Similarly,\nstate-of-the-art multi-modal LLMs perform poorly on these datasets. We explain\nthis learning inefficiency by means of the 'globality degree' measure. To\nmitigate this, we propose a method called chain-of-sketch (CoS). Similar to the\nchain-of-thought and scratchpad techniques used in language models, CoS breaks\nthe original task into intermediate visual steps to help learn a complex task.\nIn addition, we show that not all CoS strategies perform equally well. Our key\ninsight is to impose a Markovian structure on the CoS frames. This leads to the\nintroduction of 'inductive CoS' which achieves better out-of-distribution\ngeneralization and performs well even with smaller models compared to\nnon-inductive variants."
                },
                "authors": [
                    {
                        "name": "Aryo Lotfi"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Abbe"
                },
                "author": "Emmanuel Abbe",
                "arxiv_comment": "additional experiments added, title changed from \"Visual Scratchpads:\n  Enabling Global Reasoning in Vision\" to \"Chain-of-Sketch: Enabling Global\n  Visual Reasoning\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21527v1",
                "updated": "2025-06-26T17:48:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    48,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:48:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    48,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "Asymptotic Inference for Exchangeable Gibbs Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Inference for Exchangeable Gibbs Partition"
                },
                "summary": "We study the asymptotic properties of parameter estimation and predictive\ninference under the exchangeable Gibbs partition, characterized by a discount\nparameter $\\alpha\\in(0,1)$ and a triangular array $v_{n,k}$ satisfying a\nbackward recursion. Assuming that $v_{n,k}$ admits a mixture representation\nover the Ewens--Pitman family $(\\alpha, \\theta)$, with $\\theta$ integrated by\nan unknown mixing distribution, we show that the (quasi) maximum likelihood\nestimator $\\hat\\alpha_n$ (QMLE) for $\\alpha$ is asymptotically mixed normal.\nThis generalizes earlier results for the Ewens--Pitman model to a more general\nclass. We further study the predictive task of estimating the probability\nsimplex $\\mathsf{p}_n$, which governs the allocation of the $(n+1)$-th item,\nconditional on the current partition of $[n]$. Based on the asymptotics of the\nQMLE $\\hat{\\alpha}_n$, we construct an estimator $\\hat{\\mathsf{p}}_n$ and\nderive the limit distributions of the $f$-divergence\n$\\mathsf{D}_f(\\hat{\\mathsf{p}}_n||\\mathsf{p}_n)$ for general convex functions\n$f$, including explicit results for the TV distance and KL divergence. These\nresults lead to asymptotically valid confidence intervals for both parameter\nestimation and prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the asymptotic properties of parameter estimation and predictive\ninference under the exchangeable Gibbs partition, characterized by a discount\nparameter $\\alpha\\in(0,1)$ and a triangular array $v_{n,k}$ satisfying a\nbackward recursion. Assuming that $v_{n,k}$ admits a mixture representation\nover the Ewens--Pitman family $(\\alpha, \\theta)$, with $\\theta$ integrated by\nan unknown mixing distribution, we show that the (quasi) maximum likelihood\nestimator $\\hat\\alpha_n$ (QMLE) for $\\alpha$ is asymptotically mixed normal.\nThis generalizes earlier results for the Ewens--Pitman model to a more general\nclass. We further study the predictive task of estimating the probability\nsimplex $\\mathsf{p}_n$, which governs the allocation of the $(n+1)$-th item,\nconditional on the current partition of $[n]$. Based on the asymptotics of the\nQMLE $\\hat{\\alpha}_n$, we construct an estimator $\\hat{\\mathsf{p}}_n$ and\nderive the limit distributions of the $f$-divergence\n$\\mathsf{D}_f(\\hat{\\mathsf{p}}_n||\\mathsf{p}_n)$ for general convex functions\n$f$, including explicit results for the TV distance and KL divergence. These\nresults lead to asymptotically valid confidence intervals for both parameter\nestimation and prediction."
                },
                "authors": [
                    {
                        "name": "Takuya Koriyama"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Koriyama"
                },
                "author": "Takuya Koriyama",
                "arxiv_comment": "35 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21521v1",
                "updated": "2025-06-26T17:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    41,
                    35,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    41,
                    35,
                    3,
                    177,
                    0
                ],
                "title": "Potemkin Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potemkin Understanding in Large Language Models"
                },
                "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Marina Mancoridis"
                    },
                    {
                        "name": "Bec Weeks"
                    },
                    {
                        "name": "Keyon Vafa"
                    },
                    {
                        "name": "Sendhil Mullainathan"
                    }
                ],
                "author_detail": {
                    "name": "Sendhil Mullainathan"
                },
                "author": "Sendhil Mullainathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21509v1",
                "updated": "2025-06-26T17:35:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    35,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:35:40Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    35,
                    40,
                    3,
                    177,
                    0
                ],
                "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic\n  Logits Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucination of Large Vision-Language Models via Dynamic\n  Logits Calibration"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github."
                },
                "authors": [
                    {
                        "name": "Jiahe Chen"
                    },
                    {
                        "name": "Jiaying He"
                    },
                    {
                        "name": "Qian Shao"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Jiahe Ying"
                    },
                    {
                        "name": "Hongxia Xu"
                    },
                    {
                        "name": "Jintai Chen"
                    },
                    {
                        "name": "Jianwei Zheng"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21501v1",
                "updated": "2025-06-26T17:29:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    29,
                    36,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:29:36Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    29,
                    36,
                    3,
                    177,
                    0
                ],
                "title": "Causal inference via implied interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference via implied interventions"
                },
                "summary": "In the context of having an instrumental variable, the standard practice in\ncausal inference begins by targeting an effect of interest and proceeds by\nformulating assumptions enabling identification of this effect. We turn this\naround by simply not making assumptions anymore and just adhere to the\ninterventions we can identify, rather than starting with a desired causal\nestimand and imposing untestable hypotheses. The randomization of an instrument\nand its exclusion restriction define a class of auxiliary stochastic\ninterventions on the treatment that are implied by stochastic interventions on\nthe instrument. This mapping effectively characterizes the identifiable causal\neffects of the treatment on the outcome given the observable probability\ndistribution, leading to an explicit transparent G-computation formula under\nhidden confounding. Alternatively, searching for an intervention on the\ninstrument whose implied one best approximates a desired target -- whose causal\neffect the user aims to estimate -- naturally leads to a projection on a\nfunction space representing the closest identifiable treatment effect. The\ngenerality of this projection allows to select different norms and indexing\nsets for the function class that turn optimization into different estimation\nprocedures with the Highly Adaptive Lasso. This shift from identification under\nassumptions to identification under observation redefines how the problem of\ncausal inference is approached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of having an instrumental variable, the standard practice in\ncausal inference begins by targeting an effect of interest and proceeds by\nformulating assumptions enabling identification of this effect. We turn this\naround by simply not making assumptions anymore and just adhere to the\ninterventions we can identify, rather than starting with a desired causal\nestimand and imposing untestable hypotheses. The randomization of an instrument\nand its exclusion restriction define a class of auxiliary stochastic\ninterventions on the treatment that are implied by stochastic interventions on\nthe instrument. This mapping effectively characterizes the identifiable causal\neffects of the treatment on the outcome given the observable probability\ndistribution, leading to an explicit transparent G-computation formula under\nhidden confounding. Alternatively, searching for an intervention on the\ninstrument whose implied one best approximates a desired target -- whose causal\neffect the user aims to estimate -- naturally leads to a projection on a\nfunction space representing the closest identifiable treatment effect. The\ngenerality of this projection allows to select different norms and indexing\nsets for the function class that turn optimization into different estimation\nprocedures with the Highly Adaptive Lasso. This shift from identification under\nassumptions to identification under observation redefines how the problem of\ncausal inference is approached."
                },
                "authors": [
                    {
                        "name": "Carlos García Meixide"
                    },
                    {
                        "name": "Mark J. van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. van der Laan"
                },
                "author": "Mark J. van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21497v1",
                "updated": "2025-06-26T17:26:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    26,
                    17,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:26:17Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    26,
                    17,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing User Engagement in Socially-Driven Dialogue through\n  Interactive LLM Alignments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing User Engagement in Socially-Driven Dialogue through\n  Interactive LLM Alignments"
                },
                "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs."
                },
                "authors": [
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Changhe Song"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21495v1",
                "updated": "2025-06-26T17:25:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    25,
                    49,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:25:49Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    25,
                    49,
                    3,
                    177,
                    0
                ],
                "title": "Bridging Offline and Online Reinforcement Learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Offline and Online Reinforcement Learning for LLMs"
                },
                "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types."
                },
                "authors": [
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Ilia Kulikov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Kulikov"
                },
                "author": "Ilia Kulikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02398v3",
                "updated": "2025-06-26T17:22:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    22,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2024-11-04T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages"
                },
                "summary": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval."
                },
                "authors": [
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Khyati Mahajan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Maheshwary"
                },
                "author": "Rishabh Maheshwary",
                "arxiv_doi": "10.18653/v1/2025.naacl-long.599",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.naacl-long.599",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to NAACL 2025 (Main Conference). This version contains minor\n  improvements to the camera-ready",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies (Volume 1: Long Papers)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18959v2",
                "updated": "2025-06-26T17:18:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    0,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-23T17:27:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    27,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents"
                },
                "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chenxuan Xie"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyang Zhang"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05716v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05716v3",
                "updated": "2025-06-26T17:02:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    2,
                    47,
                    3,
                    177,
                    0
                ],
                "published": "2024-09-09T15:25:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    25,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "Implications of feedback solutions to the $S_8$ tension for the baryon\n  fractions of galaxy groups and clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implications of feedback solutions to the $S_8$ tension for the baryon\n  fractions of galaxy groups and clusters"
                },
                "summary": "Recent large-scale structure (LSS) surveys have revealed a persistent tension\nin the value of $S_8$ compared to predictions from the standard cosmological\nmodel. This tension may suggest the need for new physics beyond the standard\nmodel, but an accurate characterisation of baryonic effects is essential to\navoid biases. Although some studies indicate that baryonic effects are too\nsmall to resolve this tension, others propose that more aggressive feedback\nmechanisms could reconcile differences between cosmic microwave background\n(CMB) measurements and low-redshift LSS observations. In this paper, we\ninvestigate the role of baryonic effects in alleviating the $S_8$ tension. We\nextend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of\ncosmological hydrodynamical simulations to map the suppression of the matter\npower spectrum to the baryon fraction in groups and clusters, to predict the\nrequired baryon fraction for a given $P(k)$ suppression. We then compare\npredictions from recent cosmic shear (weak lensing) analyses with the latest\nbaryon budget measurements from X-ray and weak gravitational lensing studies.\nOur findings show that studies marginalising over baryonic effects while fixing\ncosmological parameters to a Planck-like cosmology predict strong $P(k)$\nsuppression and baryon fractions that are much lower than existing low-redshift\nbaryon budget estimates of galaxy groups and clusters. Conversely, most studies\nthat marginalise over both cosmological parameters and baryonic effects imply\nbaryon fractions that are consistent with observations but lower values of\n$S_8$ than inferred from the CMB. Unless the observed baryon fractions are\nbiased high by a factor of several, these results suggest that a mechanism\nbeyond baryonic physics alone is required to modify or slow down the growth of\nstructure in the universe in order to resolve the $S_8$ tension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale structure (LSS) surveys have revealed a persistent tension\nin the value of $S_8$ compared to predictions from the standard cosmological\nmodel. This tension may suggest the need for new physics beyond the standard\nmodel, but an accurate characterisation of baryonic effects is essential to\navoid biases. Although some studies indicate that baryonic effects are too\nsmall to resolve this tension, others propose that more aggressive feedback\nmechanisms could reconcile differences between cosmic microwave background\n(CMB) measurements and low-redshift LSS observations. In this paper, we\ninvestigate the role of baryonic effects in alleviating the $S_8$ tension. We\nextend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of\ncosmological hydrodynamical simulations to map the suppression of the matter\npower spectrum to the baryon fraction in groups and clusters, to predict the\nrequired baryon fraction for a given $P(k)$ suppression. We then compare\npredictions from recent cosmic shear (weak lensing) analyses with the latest\nbaryon budget measurements from X-ray and weak gravitational lensing studies.\nOur findings show that studies marginalising over baryonic effects while fixing\ncosmological parameters to a Planck-like cosmology predict strong $P(k)$\nsuppression and baryon fractions that are much lower than existing low-redshift\nbaryon budget estimates of galaxy groups and clusters. Conversely, most studies\nthat marginalise over both cosmological parameters and baryonic effects imply\nbaryon fractions that are consistent with observations but lower values of\n$S_8$ than inferred from the CMB. Unless the observed baryon fractions are\nbiased high by a factor of several, these results suggest that a mechanism\nbeyond baryonic physics alone is required to modify or slow down the growth of\nstructure in the universe in order to resolve the $S_8$ tension."
                },
                "authors": [
                    {
                        "name": "Jaime Salcido"
                    },
                    {
                        "name": "Ian G. McCarthy"
                    }
                ],
                "author_detail": {
                    "name": "Ian G. McCarthy"
                },
                "author": "Ian G. McCarthy",
                "arxiv_comment": "Refereed version accepted for publication in Monthly Notices of the\n  Royal Astronomical Society (MNRAS). 12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05716v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05716v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06947v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06947v3",
                "updated": "2025-06-26T16:55:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    55,
                    48,
                    3,
                    177,
                    0
                ],
                "published": "2024-08-13T15:05:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    5,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference"
                },
                "summary": "Rapid parameter estimation is critical when dealing with short lived signals\nsuch as kilonovae. We present a parameter estimation algorithm that combines\nlikelihood-free inference with a pre-trained embedding network, optimized to\nefficiently process kilonova light curves. Our method is capable of retrieving\nthe mass, velocity, and lanthanide fraction of the neutron star ejecta with an\naccuracy and precision on par with nested sampling methods while taking\nsignificantly less computational time. Our inference uniquely utilizes a\npre-trained embedding network that marginalizes the time of arrival and the\nluminosity distance of the signal, allowing inference of signals at distances\nup to 200 Mpc. We find that including a pre-trained embedding outperforms the\nuse of likelihood-free inference alone, reducing training time, model size, and\noffering the capability to marginalize over certain nuisance parameters. This\nframework has been integrated into the publicly available Nuclear\nMulti-Messenger Astronomy codebase, enabling the broader scientific community\nto deploy the model for their inference purposes. Our algorithm is broadly\napplicable to parameterized or simulated light curves of other transient\nobjects, and can be adapted for quick sky localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid parameter estimation is critical when dealing with short lived signals\nsuch as kilonovae. We present a parameter estimation algorithm that combines\nlikelihood-free inference with a pre-trained embedding network, optimized to\nefficiently process kilonova light curves. Our method is capable of retrieving\nthe mass, velocity, and lanthanide fraction of the neutron star ejecta with an\naccuracy and precision on par with nested sampling methods while taking\nsignificantly less computational time. Our inference uniquely utilizes a\npre-trained embedding network that marginalizes the time of arrival and the\nluminosity distance of the signal, allowing inference of signals at distances\nup to 200 Mpc. We find that including a pre-trained embedding outperforms the\nuse of likelihood-free inference alone, reducing training time, model size, and\noffering the capability to marginalize over certain nuisance parameters. This\nframework has been integrated into the publicly available Nuclear\nMulti-Messenger Astronomy codebase, enabling the broader scientific community\nto deploy the model for their inference purposes. Our algorithm is broadly\napplicable to parameterized or simulated light curves of other transient\nobjects, and can be adapted for quick sky localization."
                },
                "authors": [
                    {
                        "name": "Malina Desai"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "Sahil Jhawar"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Erik Katsavounidis"
                    },
                    {
                        "name": "Michael Coughlin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Coughlin"
                },
                "author": "Michael Coughlin",
                "arxiv_doi": "10.1093/mnras/staf1045",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1045",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06947v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06947v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17859v2",
                "updated": "2025-06-26T16:54:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    57,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-21T23:49:08Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    23,
                    49,
                    8,
                    5,
                    172,
                    0
                ],
                "title": "In-Context Learning Strategies Emerge Rationally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning Strategies Emerge Rationally"
                },
                "summary": "Recent work analyzing in-context learning (ICL) has identified a broad set of\nstrategies that describe model behavior in different experimental conditions.\nWe aim to unify these findings by asking why a model learns these disparate\nstrategies in the first place. Specifically, we start with the observation that\nwhen trained to learn a mixture of tasks, as is popular in the literature, the\nstrategies learned by a model for performing ICL can be captured by a family of\nBayesian predictors: a memorizing predictor, which assumes a discrete prior on\nthe set of seen tasks, and a generalizing predictor, where the prior matches\nthe underlying task distribution. Adopting the normative lens of rational\nanalysis, where a learner's behavior is explained as an optimal adaptation to\ndata given computational constraints, we develop a hierarchical Bayesian\nframework that almost perfectly predicts Transformer next-token predictions\nthroughout training -- without assuming access to its weights. Under this\nframework, pretraining is viewed as a process of updating the posterior\nprobability of different strategies, and inference-time behavior as a\nposterior-weighted average over these strategies' predictions. Our framework\ndraws on common assumptions about neural network learning dynamics, which make\nexplicit a tradeoff between loss and complexity among candidate strategies:\nbeyond how well it explains the data, a model's preference towards implementing\na strategy is dictated by its complexity. This helps explain well-known ICL\nphenomena, while offering novel predictions: e.g., we show a superlinear trend\nin the timescale for transitioning from generalization to memorization as task\ndiversity increases. Overall, our work advances an explanatory and predictive\naccount of ICL grounded in tradeoffs between strategy loss and complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work analyzing in-context learning (ICL) has identified a broad set of\nstrategies that describe model behavior in different experimental conditions.\nWe aim to unify these findings by asking why a model learns these disparate\nstrategies in the first place. Specifically, we start with the observation that\nwhen trained to learn a mixture of tasks, as is popular in the literature, the\nstrategies learned by a model for performing ICL can be captured by a family of\nBayesian predictors: a memorizing predictor, which assumes a discrete prior on\nthe set of seen tasks, and a generalizing predictor, where the prior matches\nthe underlying task distribution. Adopting the normative lens of rational\nanalysis, where a learner's behavior is explained as an optimal adaptation to\ndata given computational constraints, we develop a hierarchical Bayesian\nframework that almost perfectly predicts Transformer next-token predictions\nthroughout training -- without assuming access to its weights. Under this\nframework, pretraining is viewed as a process of updating the posterior\nprobability of different strategies, and inference-time behavior as a\nposterior-weighted average over these strategies' predictions. Our framework\ndraws on common assumptions about neural network learning dynamics, which make\nexplicit a tradeoff between loss and complexity among candidate strategies:\nbeyond how well it explains the data, a model's preference towards implementing\na strategy is dictated by its complexity. This helps explain well-known ICL\nphenomena, while offering novel predictions: e.g., we show a superlinear trend\nin the timescale for transitioning from generalization to memorization as task\ndiversity increases. Overall, our work advances an explanatory and predictive\naccount of ICL grounded in tradeoffs between strategy loss and complexity."
                },
                "authors": [
                    {
                        "name": "Daniel Wurgaft"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    },
                    {
                        "name": "Gautam Reddy"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21467v1",
                "updated": "2025-06-26T16:54:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    39,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:54:39Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    39,
                    3,
                    177,
                    0
                ],
                "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery\n  Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Reuseable Cloud Configuration Search Using Discovery\n  Spaces"
                },
                "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%."
                },
                "authors": [
                    {
                        "name": "Michael Johnston"
                    },
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Christoph Hagleitner"
                    },
                    {
                        "name": "Alessandro Pomponio"
                    },
                    {
                        "name": "Vassilis Vassiliadis"
                    },
                    {
                        "name": "Christian Pinto"
                    },
                    {
                        "name": "Srikumar Venugopal"
                    }
                ],
                "author_detail": {
                    "name": "Srikumar Venugopal"
                },
                "author": "Srikumar Venugopal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18019v2",
                "updated": "2025-06-26T16:54:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    14,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-22T12:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    12,
                    59,
                    12,
                    6,
                    173,
                    0
                ],
                "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities"
                },
                "summary": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Siwen Wang"
                    },
                    {
                        "name": "Weizhi Chen"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Fakhri Karray"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18728v2",
                "updated": "2025-06-26T16:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    35,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-23T15:05:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries"
                },
                "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Pratiksha Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Pratiksha Thaker"
                },
                "author": "Pratiksha Thaker",
                "arxiv_comment": "In Adaptive Foundation Models: Evolving AI for Personalized and\n  Efficient Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21445v1",
                "updated": "2025-06-26T16:31:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    31,
                    10,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:31:10Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    31,
                    10,
                    3,
                    177,
                    0
                ],
                "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English"
                },
                "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    },
                    {
                        "name": "William Tai"
                    }
                ],
                "author_detail": {
                    "name": "William Tai"
                },
                "author": "William Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21443v1",
                "updated": "2025-06-26T16:29:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    29,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:29:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    29,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection"
                },
                "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications."
                },
                "authors": [
                    {
                        "name": "Ali Şenol"
                    },
                    {
                        "name": "Garima Agrawal"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21657v3",
                "updated": "2025-06-26T16:16:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    16,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-27T18:32:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    32,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations"
                },
                "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."
                },
                "authors": [
                    {
                        "name": "Zeinab Dehghani"
                    },
                    {
                        "name": "Mohammed Naveed Akram"
                    },
                    {
                        "name": "Koorosh Aslansefat"
                    },
                    {
                        "name": "Adil Khan"
                    }
                ],
                "author_detail": {
                    "name": "Adil Khan"
                },
                "author": "Adil Khan",
                "arxiv_comment": "The submission contains incorrect references that require substantial\n  revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11872v2",
                "updated": "2025-06-26T16:15:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    15,
                    31,
                    3,
                    177,
                    0
                ],
                "published": "2024-03-18T15:26:05Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    15,
                    26,
                    5,
                    0,
                    78,
                    0
                ],
                "title": "Graph Neural Network for Neutrino Physics Event Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Network for Neutrino Physics Event Reconstruction"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a\nwealth of high-resolution information on particle interactions, and leveraging\nthat information to its full potential requires sophisticated automated\nreconstruction techniques. This article describes NuGraph2, a Graph Neural\nNetwork (GNN) for low-level reconstruction of simulated neutrino interactions\nin a LArTPC detector. Simulated neutrino interactions in the MicroBooNE\ndetector geometry are described as heterogeneous graphs, with energy\ndepositions on each detector plane forming nodes on planar subgraphs. The\nnetwork utilizes a multi-head attention message-passing mechanism to perform\nbackground filtering and semantic labelling on these graph nodes, identifying\nthose associated with the primary physics interaction with 98.0\\% efficiency\nand labelling them according to particle type with 94.9\\% efficiency. The\nnetwork operates directly on detector observables across multiple 2D\nrepresentations, but utilizes a 3D-context-aware mechanism to encourage\nconsistency between these representations. Model inference takes 0.12~s/event\non a CPU, and 0.005s/event batched on a GPU. This architecture is designed to\nbe a general-purpose solution for particle reconstruction in neutrino physics,\nwith the potential for deployment across a broad range of detector\ntechnologies, and offers a core convolution engine that can be leveraged for a\nvariety of tasks beyond the two described in this article.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a\nwealth of high-resolution information on particle interactions, and leveraging\nthat information to its full potential requires sophisticated automated\nreconstruction techniques. This article describes NuGraph2, a Graph Neural\nNetwork (GNN) for low-level reconstruction of simulated neutrino interactions\nin a LArTPC detector. Simulated neutrino interactions in the MicroBooNE\ndetector geometry are described as heterogeneous graphs, with energy\ndepositions on each detector plane forming nodes on planar subgraphs. The\nnetwork utilizes a multi-head attention message-passing mechanism to perform\nbackground filtering and semantic labelling on these graph nodes, identifying\nthose associated with the primary physics interaction with 98.0\\% efficiency\nand labelling them according to particle type with 94.9\\% efficiency. The\nnetwork operates directly on detector observables across multiple 2D\nrepresentations, but utilizes a 3D-context-aware mechanism to encourage\nconsistency between these representations. Model inference takes 0.12~s/event\non a CPU, and 0.005s/event batched on a GPU. This architecture is designed to\nbe a general-purpose solution for particle reconstruction in neutrino physics,\nwith the potential for deployment across a broad range of detector\ntechnologies, and offers a core convolution engine that can be leveraged for a\nvariety of tasks beyond the two described in this article."
                },
                "authors": [
                    {
                        "name": "V Hewes"
                    },
                    {
                        "name": "Adam Aurisano"
                    },
                    {
                        "name": "Giuseppe Cerati"
                    },
                    {
                        "name": "Jim Kowalkowski"
                    },
                    {
                        "name": "Claire Lee"
                    },
                    {
                        "name": "Wei-keng Liao"
                    },
                    {
                        "name": "Daniel Grzenda"
                    },
                    {
                        "name": "Kaushal Gumpula"
                    },
                    {
                        "name": "Xiaohe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohe Zhang"
                },
                "author": "Xiaohe Zhang",
                "arxiv_doi": "10.1103/PhysRevD.110.032008",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.032008",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures, published in Physical Review D",
                "arxiv_journal_ref": "Phys.Rev.D 110 (2024) 3, 032008",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15830v2",
                "updated": "2025-06-26T16:14:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    14,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-18T19:17:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    19,
                    17,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Training through Information Geometry and Quantum Metrics"
                },
                "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems."
                },
                "authors": [
                    {
                        "name": "Riccardo Di Sipio"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Di Sipio"
                },
                "author": "Riccardo Di Sipio",
                "arxiv_comment": "9 pages, 1 figure(s)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08005v3",
                "updated": "2025-06-26T16:11:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    11,
                    14,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-14T10:49:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    49,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved\n  Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved\n  Out-of-Distribution Detection"
                },
                "summary": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code is publicly available."
                },
                "authors": [
                    {
                        "name": "Francisco Caetano"
                    },
                    {
                        "name": "Christiaan Viviers"
                    },
                    {
                        "name": "Luis A. Zavala-Mondragón"
                    },
                    {
                        "name": "Peter H. N. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21427v1",
                "updated": "2025-06-26T16:09:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    9,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:09:53Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    9,
                    53,
                    3,
                    177,
                    0
                ],
                "title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-Based Single-Step Completion for Efficient and Expressive Policy\n  Learning"
                },
                "summary": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Prajwal Koirala"
                    },
                    {
                        "name": "Cody Fleming"
                    }
                ],
                "author_detail": {
                    "name": "Cody Fleming"
                },
                "author": "Cody Fleming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04202v3",
                "updated": "2025-06-26T16:09:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    9,
                    36,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-04T17:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracLLM: A Generic Framework for Attributing Long Context LLMs"
                },
                "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM."
                },
                "authors": [
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21408v1",
                "updated": "2025-06-26T15:54:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    54,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:54:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    54,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference"
                },
                "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work."
                },
                "authors": [
                    {
                        "name": "Colin Samplawski"
                    },
                    {
                        "name": "Adam D. Cobb"
                    },
                    {
                        "name": "Manoj Acharya"
                    },
                    {
                        "name": "Ramneet Kaur"
                    },
                    {
                        "name": "Susmit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Susmit Jha"
                },
                "author": "Susmit Jha",
                "arxiv_comment": "Accepted at UAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20639v2",
                "updated": "2025-06-26T15:46:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    46,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-25T17:35:47Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    35,
                    47,
                    2,
                    176,
                    0
                ],
                "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation"
                },
                "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR bias during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR bias during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder."
                },
                "authors": [
                    {
                        "name": "Shansan Gong"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Huangjie Zheng"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "minor update",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21387v1",
                "updated": "2025-06-26T15:36:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    36,
                    37,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:36:37Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    36,
                    37,
                    3,
                    177,
                    0
                ],
                "title": "Early Stopping Tabular In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Stopping Tabular In-Context Learning"
                },
                "summary": "Tabular foundation models have shown strong performance across various\ntabular learning tasks via in-context learning, offering robust generalization\nwithout any downstream finetuning. However, their inference-time costs remain\nhigh, particularly for larger datasets. To address this, we propose\nearly-stopping the in-context learning process. We achieve this by dynamically\nevaluating whether to stop in-context learning after each Transformer encoder\nlayer. Once stopped, we decode the embedding using a pre-trained layer-wise\ndecoder. Experiments across 34 small classification tasks size show that early\nstopping in-context learning accelerates inference by up to x1.3 with\nnegligible degradation in predictive performance. To assess scalability, we\nfurther evaluate our method on five larger classification tasks, achieving\nspeedups of up to x2.2. Our results demonstrate the potential of early exiting\nas an effective and practical strategy for improving the efficiency of tabular\nin-context learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation models have shown strong performance across various\ntabular learning tasks via in-context learning, offering robust generalization\nwithout any downstream finetuning. However, their inference-time costs remain\nhigh, particularly for larger datasets. To address this, we propose\nearly-stopping the in-context learning process. We achieve this by dynamically\nevaluating whether to stop in-context learning after each Transformer encoder\nlayer. Once stopped, we decode the embedding using a pre-trained layer-wise\ndecoder. Experiments across 34 small classification tasks size show that early\nstopping in-context learning accelerates inference by up to x1.3 with\nnegligible degradation in predictive performance. To assess scalability, we\nfurther evaluate our method on five larger classification tasks, achieving\nspeedups of up to x2.2. Our results demonstrate the potential of early exiting\nas an effective and practical strategy for improving the efficiency of tabular\nin-context learning."
                },
                "authors": [
                    {
                        "name": "Jaris Küken"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "ICML Workshop Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21384v1",
                "updated": "2025-06-26T15:35:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    35,
                    12,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:35:12Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    35,
                    12,
                    3,
                    177,
                    0
                ],
                "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation"
                },
                "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries."
                },
                "authors": [
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Yuyao Zhang"
                    },
                    {
                        "name": "Mengjie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Mengjie Deng"
                },
                "author": "Mengjie Deng",
                "arxiv_comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11924v2",
                "updated": "2025-06-26T15:26:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    26,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-13T16:19:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    19,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation"
                },
                "summary": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI."
                },
                "authors": [
                    {
                        "name": "Min-Seop Kwak"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Dongyoon Han"
                    },
                    {
                        "name": "Taekyoung Kim"
                    },
                    {
                        "name": "Seungryong Kim"
                    },
                    {
                        "name": "Jin-Hwa Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jin-Hwa Kim"
                },
                "author": "Jin-Hwa Kim",
                "arxiv_comment": "Project page at https://cvlab-kaist.github.io/MoAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15858v2",
                "updated": "2025-06-26T15:16:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    16,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-21T01:26:23Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    26,
                    23,
                    2,
                    141,
                    0
                ],
                "title": "Large Language Model-Powered Agent for C to Rust Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Powered Agent for C to Rust Code Translation"
                },
                "summary": "The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks."
                },
                "authors": [
                    {
                        "name": "HoHyun Sim"
                    },
                    {
                        "name": "Hyeonjoong Cho"
                    },
                    {
                        "name": "Yeonghyeon Go"
                    },
                    {
                        "name": "Zhoulai Fu"
                    },
                    {
                        "name": "Ali Shokri"
                    },
                    {
                        "name": "Binoy Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Binoy Ravindran"
                },
                "author": "Binoy Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21362v1",
                "updated": "2025-06-26T15:13:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    13,
                    35,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:13:35Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    13,
                    35,
                    3,
                    177,
                    0
                ],
                "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer\n  Voting in Online Platforms with Helpfulness Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Voting Adjustment for Quality Assessment and Fairer\n  Voting in Online Platforms with Helpfulness Evaluation"
                },
                "summary": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities."
                },
                "authors": [
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yixin Wang"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21360v1",
                "updated": "2025-06-26T15:10:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    10,
                    24,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:10:24Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    10,
                    24,
                    3,
                    177,
                    0
                ],
                "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement."
                },
                "authors": [
                    {
                        "name": "Fangzhou Dong"
                    },
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Yingpeng Sang"
                    },
                    {
                        "name": "Hong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Shen"
                },
                "author": "Hong Shen",
                "arxiv_comment": "Accepted in CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12113v3",
                "updated": "2025-06-26T15:09:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    9,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-13T13:39:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    39,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Semantic Preprocessing for LLM-based Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Preprocessing for LLM-based Malware Analysis"
                },
                "summary": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality."
                },
                "authors": [
                    {
                        "name": "Benjamin Marais"
                    },
                    {
                        "name": "Tony Quertier"
                    },
                    {
                        "name": "Grégoire Barrue"
                    }
                ],
                "author_detail": {
                    "name": "Grégoire Barrue"
                },
                "author": "Grégoire Barrue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10028v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10028v4",
                "updated": "2025-06-26T15:06:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    6,
                    14,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-13T10:39:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    39,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "Mr. DETR++: Instructive Multi-Route Training for Detection Transformers\n  with Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mr. DETR++: Instructive Multi-Route Training for Detection Transformers\n  with Mixture-of-Experts"
                },
                "summary": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We propose a novel instructive\nself-attention mechanism, integrated into the first auxiliary route, which\ndynamically and flexibly guides object queries for one-to-many prediction. For\nthe second auxiliary route, we introduce a route-aware Mixture-of-Experts (MoE)\nto facilitate knowledge sharing while mitigating potential conflicts between\nroutes. Additionally, we apply an MoE to low-scale features in the encoder,\noptimizing the balance between efficiency and effectiveness. The auxiliary\nroutes are discarded during inference. We conduct extensive experiments across\nvarious object detection baselines, achieving consistent improvements as\ndemonstrated in Fig. 1. Our method is highly flexible and can be readily\nadapted to other tasks. To demonstrate its versatility, we conduct experiments\non both instance segmentation and panoptic segmentation, further validating its\neffectiveness. Project page: https://visual-ai.github.io/mrdetr/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We propose a novel instructive\nself-attention mechanism, integrated into the first auxiliary route, which\ndynamically and flexibly guides object queries for one-to-many prediction. For\nthe second auxiliary route, we introduce a route-aware Mixture-of-Experts (MoE)\nto facilitate knowledge sharing while mitigating potential conflicts between\nroutes. Additionally, we apply an MoE to low-scale features in the encoder,\noptimizing the balance between efficiency and effectiveness. The auxiliary\nroutes are discarded during inference. We conduct extensive experiments across\nvarious object detection baselines, achieving consistent improvements as\ndemonstrated in Fig. 1. Our method is highly flexible and can be readily\nadapted to other tasks. To demonstrate its versatility, we conduct experiments\non both instance segmentation and panoptic segmentation, further validating its\neffectiveness. Project page: https://visual-ai.github.io/mrdetr/"
                },
                "authors": [
                    {
                        "name": "Chang-Bin Zhang"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "Under review. Extended version of our CVPR 2025 paper, see\n  arXiv:2412.10028v3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10028v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10028v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08747v2",
                "updated": "2025-06-26T15:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    4,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2024-11-13T16:26:58Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    58,
                    2,
                    318,
                    0
                ],
                "title": "Regression for Astronomical Data with Realistic Distributions, Errors\n  and Non-linearity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression for Astronomical Data with Realistic Distributions, Errors\n  and Non-linearity"
                },
                "summary": "We have developed a new regression technique, the maximum likelihood\n(ML)-based method and its variant, the KS-test based method, designed to obtain\nunbiased regression results from typical astronomical data. A normalizing flow\nmodel is employed to automatically estimate the unobservable intrinsic\ndistribution of the independent variable as well as the unobservable\ncorrelation between uncertainty level and intrinsic value of both independent\nand dependent variables from the observed data points in a variational\ninference based empirical Bayes approach. By incorporating these estimated\ndistributions, our method comprehensively accounts for the uncertainties\nassociated with both independent and dependent variables. Our test on both mock\ndata and real astronomical data from PHANGS-ALMA and PHANGS-JWST demonstrates\nthat, given a sufficiently large sample size (> 1000), both the ML-based method\nand the KS-test based method significantly outperform the existing widely-used\nmethods, particularly in cases of low signal-to-noise ratios. The KS-test based\nmethod exhibits remarkable robustness against deviations from underlying\nassumptions, complex intrinsic distributions, varying correlations between\nuncertainty levels and intrinsic values, inaccuracies in uncertainty\nestimations, outliers, and saturation effects. For sample sizes between 300 and\n1000, the ML-based method yields the best performance. In the low data regime\n(< 300), the ML-based method maintains comparable performance to other\nstate-of-the-art methods. A GPU-compatible Python implementation of our\nmethods, nicknamed ``raddest'', will be made publicly available upon acceptance\nof this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have developed a new regression technique, the maximum likelihood\n(ML)-based method and its variant, the KS-test based method, designed to obtain\nunbiased regression results from typical astronomical data. A normalizing flow\nmodel is employed to automatically estimate the unobservable intrinsic\ndistribution of the independent variable as well as the unobservable\ncorrelation between uncertainty level and intrinsic value of both independent\nand dependent variables from the observed data points in a variational\ninference based empirical Bayes approach. By incorporating these estimated\ndistributions, our method comprehensively accounts for the uncertainties\nassociated with both independent and dependent variables. Our test on both mock\ndata and real astronomical data from PHANGS-ALMA and PHANGS-JWST demonstrates\nthat, given a sufficiently large sample size (> 1000), both the ML-based method\nand the KS-test based method significantly outperform the existing widely-used\nmethods, particularly in cases of low signal-to-noise ratios. The KS-test based\nmethod exhibits remarkable robustness against deviations from underlying\nassumptions, complex intrinsic distributions, varying correlations between\nuncertainty levels and intrinsic values, inaccuracies in uncertainty\nestimations, outliers, and saturation effects. For sample sizes between 300 and\n1000, the ML-based method yields the best performance. In the low data regime\n(< 300), the ML-based method maintains comparable performance to other\nstate-of-the-art methods. A GPU-compatible Python implementation of our\nmethods, nicknamed ``raddest'', will be made publicly available upon acceptance\nof this paper."
                },
                "authors": [
                    {
                        "name": "Tao Jing"
                    },
                    {
                        "name": "Cheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Li"
                },
                "author": "Cheng Li",
                "arxiv_doi": "10.3847/1538-3881/add891",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-3881/add891",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.08747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 16 + 3 figures, 1 table; published in AJ; Code is avalible\n  at https://github.com/astro-jingtao/raddest",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17069v2",
                "updated": "2025-06-26T15:04:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    4,
                    3,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-28T16:53:28Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    53,
                    28,
                    1,
                    28,
                    0
                ],
                "title": "Amplifying microwave pulses with a single qubit engine fueled by quantum\n  measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying microwave pulses with a single qubit engine fueled by quantum\n  measurements"
                },
                "summary": "Recent progress in manipulating individual quantum systems enables the\nexploration of engines exploiting non-classical resources. One of the most\nappealing is the energy provided by the inherent backaction of quantum\nmeasurements. While a handful of experiments have investigated the inner\ndynamics of engines fueled by measurement backaction, powering a useful task by\nsuch an engine is missing. Here we demonstrate the amplification of microwave\nsignals by an engine fueled by repeated quantum measurements of a\nsuperconducting transmon qubit. Using feedback, the engine acts as a quantum\nMaxwell demon operating without a hot thermal source. Measuring the gain of\nthis amplification constitutes a direct probing of the work output of the\nengine, in contrast with inferring the work by measuring the qubit state along\nits evolution. Observing a good agreement between both work estimation methods,\nour experiment validates the accuracy of the indirect method. We characterize\nthe long-term stability of the engine as well as its robustness to transmon\ndecoherence, loss and drifts. Our experiment exemplifies a practical usage of\nthe energy brought by quantum measurement backaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in manipulating individual quantum systems enables the\nexploration of engines exploiting non-classical resources. One of the most\nappealing is the energy provided by the inherent backaction of quantum\nmeasurements. While a handful of experiments have investigated the inner\ndynamics of engines fueled by measurement backaction, powering a useful task by\nsuch an engine is missing. Here we demonstrate the amplification of microwave\nsignals by an engine fueled by repeated quantum measurements of a\nsuperconducting transmon qubit. Using feedback, the engine acts as a quantum\nMaxwell demon operating without a hot thermal source. Measuring the gain of\nthis amplification constitutes a direct probing of the work output of the\nengine, in contrast with inferring the work by measuring the qubit state along\nits evolution. Observing a good agreement between both work estimation methods,\nour experiment validates the accuracy of the indirect method. We characterize\nthe long-term stability of the engine as well as its robustness to transmon\ndecoherence, loss and drifts. Our experiment exemplifies a practical usage of\nthe energy brought by quantum measurement backaction."
                },
                "authors": [
                    {
                        "name": "Rémy Dassonneville"
                    },
                    {
                        "name": "Cyril Elouard"
                    },
                    {
                        "name": "Romain Cazali"
                    },
                    {
                        "name": "Réouven Assouly"
                    },
                    {
                        "name": "Audrey Bienfait"
                    },
                    {
                        "name": "Alexia Auffèves"
                    },
                    {
                        "name": "Benjamin Huard"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Huard"
                },
                "author": "Benjamin Huard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04556v2",
                "updated": "2025-06-26T15:00:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    0,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-08T15:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    7,
                    6,
                    2,
                    8,
                    0
                ],
                "title": "Nonlinear coupling between magnetar QPOs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear coupling between magnetar QPOs"
                },
                "summary": "The quasi-periodic oscillations (QPOs) observed in the tails of magnetar\ngiant $\\gamma$-ray flares have long been interpreted as normal oscillation\nmodes of these stars. However, most studies modelling QPOs have neglected some\nkey features in the analyses of the signals, namely that QPOs appear to be\ndetectable only intermittently and exhibit drifts in their frequencies. These\nare typical characteristics of nonlinear mode coupling, where, at leading\norder, the modes couple and evolve collectively as triplets. Using a\nrepresentative triplet of modes, we solve the system's nonlinear equations of\nmotion analytically and argue that the coupling is likely axial-axial-polar in\nnature, with the observed intermittence and frequency drifts providing a way to\ninfer details of the magnetar's internal magnetic field geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quasi-periodic oscillations (QPOs) observed in the tails of magnetar\ngiant $\\gamma$-ray flares have long been interpreted as normal oscillation\nmodes of these stars. However, most studies modelling QPOs have neglected some\nkey features in the analyses of the signals, namely that QPOs appear to be\ndetectable only intermittently and exhibit drifts in their frequencies. These\nare typical characteristics of nonlinear mode coupling, where, at leading\norder, the modes couple and evolve collectively as triplets. Using a\nrepresentative triplet of modes, we solve the system's nonlinear equations of\nmotion analytically and argue that the coupling is likely axial-axial-polar in\nnature, with the observed intermittence and frequency drifts providing a way to\ninfer details of the magnetar's internal magnetic field geometry."
                },
                "authors": [
                    {
                        "name": "Pantelis Pnigouras"
                    },
                    {
                        "name": "Samuel K. Lander"
                    }
                ],
                "author_detail": {
                    "name": "Samuel K. Lander"
                },
                "author": "Samuel K. Lander",
                "arxiv_doi": "10.3847/1538-4357/adceee",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adceee",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 3 figures; published version",
                "arxiv_journal_ref": "Astrophys. J. 987 (2025) 35",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10586v2",
                "updated": "2025-06-26T15:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    0,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2024-01-19T09:54:23Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    9,
                    54,
                    23,
                    4,
                    19,
                    0
                ],
                "title": "PuriDefense: Randomized Local Implicit Adversarial Purification for\n  Defending Black-box Query-based Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PuriDefense: Randomized Local Implicit Adversarial Purification for\n  Defending Black-box Query-based Attacks"
                },
                "summary": "Black-box query-based attacks constitute significant threats to Machine\nLearning as a Service (MLaaS) systems since they can generate adversarial\nexamples without accessing the target model's architecture and parameters.\nTraditional defense mechanisms, such as adversarial training, gradient masking,\nand input transformations, either impose substantial computational costs or\ncompromise the test accuracy of non-adversarial inputs. To address these\nchallenges, we propose an efficient defense mechanism, PuriDefense, that\nemploys random patch-wise purifications with an ensemble of lightweight\npurification models at a low level of inference cost. These models leverage the\nlocal implicit function and rebuild the natural image manifold. Our theoretical\nanalysis suggests that this approach slows down the convergence of query-based\nattacks by incorporating randomness into purifications. Extensive experiments\non CIFAR-10 and ImageNet validate the effectiveness of our proposed\npurifier-based defense mechanism, demonstrating significant improvements in\nrobustness against query-based attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box query-based attacks constitute significant threats to Machine\nLearning as a Service (MLaaS) systems since they can generate adversarial\nexamples without accessing the target model's architecture and parameters.\nTraditional defense mechanisms, such as adversarial training, gradient masking,\nand input transformations, either impose substantial computational costs or\ncompromise the test accuracy of non-adversarial inputs. To address these\nchallenges, we propose an efficient defense mechanism, PuriDefense, that\nemploys random patch-wise purifications with an ensemble of lightweight\npurification models at a low level of inference cost. These models leverage the\nlocal implicit function and rebuild the natural image manifold. Our theoretical\nanalysis suggests that this approach slows down the convergence of query-based\nattacks by incorporating randomness into purifications. Extensive experiments\non CIFAR-10 and ImageNet validate the effectiveness of our proposed\npurifier-based defense mechanism, demonstrating significant improvements in\nrobustness against query-based attacks."
                },
                "authors": [
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Qingchuan Zhao"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21343v1",
                "updated": "2025-06-26T14:53:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    53,
                    44,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:53:44Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    53,
                    44,
                    3,
                    177,
                    0
                ],
                "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models"
                },
                "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21338v1",
                "updated": "2025-06-26T14:49:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    49,
                    10,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:49:10Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    49,
                    10,
                    3,
                    177,
                    0
                ],
                "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG\n  Classification"
                },
                "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications."
                },
                "authors": [
                    {
                        "name": "Galvin Brice S. Lim"
                    },
                    {
                        "name": "Brian Godwin S. Lim"
                    },
                    {
                        "name": "Argel A. Bandala"
                    },
                    {
                        "name": "John Anthony C. Jose"
                    },
                    {
                        "name": "Timothy Scott C. Chu"
                    },
                    {
                        "name": "Edwin Sybingco"
                    }
                ],
                "author_detail": {
                    "name": "Edwin Sybingco"
                },
                "author": "Edwin Sybingco",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21329v1",
                "updated": "2025-06-26T14:43:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    43,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:43:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    43,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "Active Inference AI Systems for Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference AI Systems for Scientific Discovery"
                },
                "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component."
                },
                "authors": [
                    {
                        "name": "Karthik Duraisamy"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Duraisamy"
                },
                "author": "Karthik Duraisamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10245v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10245v3",
                "updated": "2025-06-26T14:41:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    41,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-13T16:14:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "Is checking for sequential positivity violations getting you down? Try\n  sPoRT!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is checking for sequential positivity violations getting you down? Try\n  sPoRT!"
                },
                "summary": "Background: Sequential positivity is often a necessary assumption for drawing\ncausal inferences, such as through marginal structural modeling. Unfortunately,\nverification of this assumption can be challenging because it usually relies on\nmultiple parametric propensity score models, unlikely all correctly specified.\nTherefore, we propose a new algorithm, called sequential Positivity Regression\nTree (sPoRT), to overcome this issue and identify the subgroups found to be\nviolating this assumption, allowing for insights about the nature of the\nviolations and potential solutions.\n  Methods: We present different versions of sPoRT based on either stratifying\nor pooling over time under static or dynamic treatment strategies. This\nmethodological development was motivated by a real-life application of the\nimpact of the timing of initiation of HIV treatment with and without smoothing\nover time, which we also use to demonstrate the method.\n  Results: The illustration of sPoRT demonstrates its easy use and the\ninterpretability of the results for applied epidemiologists. Furthermore, an R\nnotebook showing how to use sPoRT in practice is available at\ngithub.com/ArthurChatton/sPoRT-notebook.\n  Conclusions: The sPoRT algorithm provides interpretable subgroups violating\nthe sequential positivity violation, allowing patterns and trends in the\nconfounders to be easily identified. We finally provided practical implications\nand recommendations when positivity violations are identified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Sequential positivity is often a necessary assumption for drawing\ncausal inferences, such as through marginal structural modeling. Unfortunately,\nverification of this assumption can be challenging because it usually relies on\nmultiple parametric propensity score models, unlikely all correctly specified.\nTherefore, we propose a new algorithm, called sequential Positivity Regression\nTree (sPoRT), to overcome this issue and identify the subgroups found to be\nviolating this assumption, allowing for insights about the nature of the\nviolations and potential solutions.\n  Methods: We present different versions of sPoRT based on either stratifying\nor pooling over time under static or dynamic treatment strategies. This\nmethodological development was motivated by a real-life application of the\nimpact of the timing of initiation of HIV treatment with and without smoothing\nover time, which we also use to demonstrate the method.\n  Results: The illustration of sPoRT demonstrates its easy use and the\ninterpretability of the results for applied epidemiologists. Furthermore, an R\nnotebook showing how to use sPoRT in practice is available at\ngithub.com/ArthurChatton/sPoRT-notebook.\n  Conclusions: The sPoRT algorithm provides interpretable subgroups violating\nthe sequential positivity violation, allowing patterns and trends in the\nconfounders to be easily identified. We finally provided practical implications\nand recommendations when positivity violations are identified."
                },
                "authors": [
                    {
                        "name": "Arthur Chatton"
                    },
                    {
                        "name": "Michael Schomaker"
                    },
                    {
                        "name": "Miguel-Angel Luque-Fernandez"
                    },
                    {
                        "name": "Robert W. Platt"
                    },
                    {
                        "name": "Mireille E. Schnitzer"
                    }
                ],
                "author_detail": {
                    "name": "Mireille E. Schnitzer"
                },
                "author": "Mireille E. Schnitzer",
                "arxiv_comment": "10 pages, 4 Tables, 2 Figures. Accepted for publication in\n  Epidemiology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10245v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10245v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21328v1",
                "updated": "2025-06-26T14:41:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    41,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:41:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    41,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing."
                },
                "authors": [
                    {
                        "name": "Jiajie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Yang"
                },
                "author": "Jiajie Yang",
                "arxiv_comment": "15 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21319v1",
                "updated": "2025-06-26T14:35:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    35,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:35:59Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    35,
                    59,
                    3,
                    177,
                    0
                ],
                "title": "Multimodal LLMs for Visualization Reconstruction and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs for Visualization Reconstruction and Understanding"
                },
                "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality."
                },
                "authors": [
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Chunlin Da"
                    },
                    {
                        "name": "Xiaoxiao Long"
                    },
                    {
                        "name": "Yuxiao Yang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12090v2",
                "updated": "2025-06-26T14:29:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    29,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2024-06-17T21:04:47Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    4,
                    47,
                    0,
                    169,
                    0
                ],
                "title": "Data-Aware Hybrid Tableaux",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Aware Hybrid Tableaux"
                },
                "summary": "Labelled tableaux have been a traditional approach to define satisfiability\nchecking procedures for Modal Logics. In many cases, they can also be used to\nobtain tight complexity bounds and lead to efficient implementations of\nreasoning tools. More recently, it has been shown that the expressive power\nprovided by the operators characterizing Hybrid Logics (nominals and\nsatisfiability modalities) can be used to \\emph{internalize} labels, leading to\nwell-behaved inference procedures for fairly expressive logics. The resulting\nprocedures are attractive because they do not use external mechanisms outside\nthe language of the logic at hand, and have good logical and computational\nproperties.\n  Many tableau systems based on Hybrid Logic have been investigated, with more\nrecent efforts concentrating on Modal Logics that support data comparison\noperators. Here, we introduce an internalized tableau calculus for XPath,\narguably one of the most prominent approaches for querying semistructured data.\nMore precisely, we define data-aware tableaux for XPath featuring data\ncomparison operators and enriched with nominals and the satisfiability\nmodalities from Hybrid Logic. We prove that the calculus is sound, complete and\nterminating. Moreover, we show that tableaux can be explored in polynomial\nspace, therefore establishing that the satisfiability problem for the logic is\nPSPACE-complete. Finally, we explore different extensions of the calculus, in\nparticular how to handle data trees and other frame classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labelled tableaux have been a traditional approach to define satisfiability\nchecking procedures for Modal Logics. In many cases, they can also be used to\nobtain tight complexity bounds and lead to efficient implementations of\nreasoning tools. More recently, it has been shown that the expressive power\nprovided by the operators characterizing Hybrid Logics (nominals and\nsatisfiability modalities) can be used to \\emph{internalize} labels, leading to\nwell-behaved inference procedures for fairly expressive logics. The resulting\nprocedures are attractive because they do not use external mechanisms outside\nthe language of the logic at hand, and have good logical and computational\nproperties.\n  Many tableau systems based on Hybrid Logic have been investigated, with more\nrecent efforts concentrating on Modal Logics that support data comparison\noperators. Here, we introduce an internalized tableau calculus for XPath,\narguably one of the most prominent approaches for querying semistructured data.\nMore precisely, we define data-aware tableaux for XPath featuring data\ncomparison operators and enriched with nominals and the satisfiability\nmodalities from Hybrid Logic. We prove that the calculus is sound, complete and\nterminating. Moreover, we show that tableaux can be explored in polynomial\nspace, therefore establishing that the satisfiability problem for the logic is\nPSPACE-complete. Finally, we explore different extensions of the calculus, in\nparticular how to handle data trees and other frame classes."
                },
                "authors": [
                    {
                        "name": "Carlos Areces"
                    },
                    {
                        "name": "Valentin Cassano"
                    },
                    {
                        "name": "Raul Fervari"
                    }
                ],
                "author_detail": {
                    "name": "Raul Fervari"
                },
                "author": "Raul Fervari",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21304v1",
                "updated": "2025-06-26T14:23:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    23,
                    29,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:23:29Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    23,
                    29,
                    3,
                    177,
                    0
                ],
                "title": "Nonparametric Bayesian analysis for the Galton-Watson process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Bayesian analysis for the Galton-Watson process"
                },
                "summary": "The Galton-Watson process is a model for population growth which assumes that\nindividuals reproduce independently according to the same offspring\ndistribution. Inference usually focuses on the offspring average as it allows\nto classify the process with respect to extinction. We propose a fully\nnon-parametric approach for Bayesian inference on the GW model using a\nDirichlet Process prior. The prior naturally generalizes the Dirichlet\nconjugate prior distribution, and it allows learning the support of the\noffspring distribution from the data as well as taking into account possible\noverdispersion of the data. The performance of the proposed approach is\ncompared with both frequentist and Bayesian procedures via simulation. In\nparticular, we show that the use of a DP prior yields good classification\nperformance with both complete and incomplete data. A real-world data example\nconcerning COVID-19 data from Sardinia illustrates the use of the approach in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Galton-Watson process is a model for population growth which assumes that\nindividuals reproduce independently according to the same offspring\ndistribution. Inference usually focuses on the offspring average as it allows\nto classify the process with respect to extinction. We propose a fully\nnon-parametric approach for Bayesian inference on the GW model using a\nDirichlet Process prior. The prior naturally generalizes the Dirichlet\nconjugate prior distribution, and it allows learning the support of the\noffspring distribution from the data as well as taking into account possible\noverdispersion of the data. The performance of the proposed approach is\ncompared with both frequentist and Bayesian procedures via simulation. In\nparticular, we show that the use of a DP prior yields good classification\nperformance with both complete and incomplete data. A real-world data example\nconcerning COVID-19 data from Sardinia illustrates the use of the approach in\npractice."
                },
                "authors": [
                    {
                        "name": "Massimo Cannas"
                    },
                    {
                        "name": "Michele Guindani"
                    },
                    {
                        "name": "Nicola Piras"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Piras"
                },
                "author": "Nicola Piras",
                "arxiv_comment": "27 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19683v2",
                "updated": "2025-06-26T14:20:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    20,
                    13,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:49:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance"
                },
                "summary": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries."
                },
                "authors": [
                    {
                        "name": "Xuesong Li"
                    },
                    {
                        "name": "Dianye Huang"
                    },
                    {
                        "name": "Yameng Zhang"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Jiang"
                },
                "author": "Zhongliang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21294v1",
                "updated": "2025-06-26T14:14:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    14,
                    20,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:14:20Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    14,
                    20,
                    3,
                    177,
                    0
                ],
                "title": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models"
                },
                "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches."
                },
                "authors": [
                    {
                        "name": "Bram Willemsen"
                    },
                    {
                        "name": "Gabriel Skantze"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Skantze"
                },
                "author": "Gabriel Skantze",
                "arxiv_comment": "Accepted for publication at XLLM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21288v1",
                "updated": "2025-06-26T14:09:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    9,
                    41,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:09:41Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    9,
                    41,
                    3,
                    177,
                    0
                ],
                "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Encoders Can Rival Large Decoders in Detecting Groundedness"
                },
                "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less"
                },
                "authors": [
                    {
                        "name": "Istabrak Abbes"
                    },
                    {
                        "name": "Gabriele Prato"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Fernando Rodriguez"
                    },
                    {
                        "name": "Alaa Boukhary"
                    },
                    {
                        "name": "Adam Elwood"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13379v2",
                "updated": "2025-06-26T14:06:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    6,
                    49,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-19T17:24:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Thinkless: LLM Learns When to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinkless: LLM Learns When to Think"
                },
                "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"
                },
                "authors": [
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21285v1",
                "updated": "2025-06-26T14:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning"
                },
                "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Lu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Yin"
                },
                "author": "Lu Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21284v1",
                "updated": "2025-06-26T14:05:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    27,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:05:27Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    27,
                    3,
                    177,
                    0
                ],
                "title": "Wide-field Polarization Imaging and Numerical Modeling of the Coma and\n  Tail of Comet C/2023 A3 (Tsuchinshan-ATLAS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wide-field Polarization Imaging and Numerical Modeling of the Coma and\n  Tail of Comet C/2023 A3 (Tsuchinshan-ATLAS)"
                },
                "summary": "Imaging polarimetry is a well-known method for analysing the structure and\ncomposition of cometary dust. Accordingly, comets are classified based on the\nphase angle dependent degree of linear polarization of their comae. The goal of\nthis study is to examine the size and structure of the dust grains in the coma\nand in particular in the tail of the bright comet C/2023 A3 (Tsuchinshan-ATLAS)\nand to infer possible variations. For this purpose, we rely on the method of\ntelescopic wide-field polarimetric imaging of the comet in order to obtain the\ndependence of the degree of linear polarization (DoLP) of the coma and tail on\nthe phase angle across a broad range, using an off-the-shelf industrial grade\npolarization camera combined with a small telescope of short aperture ratio.\nThese observations are accompanied by T-matrix modeling using the MSTM4\nsoftware framework for simulation of light scattering by fractal agglomerates\nof spherical monomer particles. Our observations indicate that the coma\nexhibits a high maximum DoLP of 0.34, which is further exceeded by a factor of\nabout two by the DoLP of the comet's tail. Differences in the phase angle of\nmaximum DoLP between coma and tail indicate a higher carbon vs.\\ silicate\nfraction in the coma than in the tail. Our simulations are able to reproduce\nthe observations when assuming that the dust agglomerates are formed of larger\nmonomers in the coma than in the tail. A possible explanation of these results\nis that the coma monomers are coated by a thin layer of frozen volatiles, which\nsublimate due to solar heating when the dust moves from the coma towards the\ntail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imaging polarimetry is a well-known method for analysing the structure and\ncomposition of cometary dust. Accordingly, comets are classified based on the\nphase angle dependent degree of linear polarization of their comae. The goal of\nthis study is to examine the size and structure of the dust grains in the coma\nand in particular in the tail of the bright comet C/2023 A3 (Tsuchinshan-ATLAS)\nand to infer possible variations. For this purpose, we rely on the method of\ntelescopic wide-field polarimetric imaging of the comet in order to obtain the\ndependence of the degree of linear polarization (DoLP) of the coma and tail on\nthe phase angle across a broad range, using an off-the-shelf industrial grade\npolarization camera combined with a small telescope of short aperture ratio.\nThese observations are accompanied by T-matrix modeling using the MSTM4\nsoftware framework for simulation of light scattering by fractal agglomerates\nof spherical monomer particles. Our observations indicate that the coma\nexhibits a high maximum DoLP of 0.34, which is further exceeded by a factor of\nabout two by the DoLP of the comet's tail. Differences in the phase angle of\nmaximum DoLP between coma and tail indicate a higher carbon vs.\\ silicate\nfraction in the coma than in the tail. Our simulations are able to reproduce\nthe observations when assuming that the dust agglomerates are formed of larger\nmonomers in the coma than in the tail. A possible explanation of these results\nis that the coma monomers are coated by a thin layer of frozen volatiles, which\nsublimate due to solar heating when the dust moves from the coma towards the\ntail."
                },
                "authors": [
                    {
                        "name": "Mirza Arnaut"
                    },
                    {
                        "name": "Christian Wöhler"
                    },
                    {
                        "name": "Pritish Halder"
                    },
                    {
                        "name": "Goldy Ahuja"
                    },
                    {
                        "name": "Shashikiran Ganesh"
                    },
                    {
                        "name": "Megha Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Megha Bhatt"
                },
                "author": "Megha Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12590v2",
                "updated": "2025-06-26T14:03:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    3,
                    3,
                    3,
                    177,
                    0
                ],
                "published": "2024-10-16T14:10:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    10,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Flexible and Efficient Estimation of Causal Effects with Error-Prone\n  Exposures: A Control Variates Approach for Measurement Error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and Efficient Estimation of Causal Effects with Error-Prone\n  Exposures: A Control Variates Approach for Measurement Error"
                },
                "summary": "Exposure measurement error is a ubiquitous but often overlooked challenge in\ncausal inference with observational data. Existing methods accounting for\nexposure measurement error largely rely on restrictive parametric assumptions,\nwhile emerging data-adaptive estimation approaches allow for less restrictive\nassumptions but at the cost of flexibility, as they are typically tailored\ntowards rigidly-defined statistical quantities. There remains a critical need\nfor assumption-lean estimation methods that are both flexible and possess\ndesirable theoretical properties across a variety of study designs. In this\npaper, we introduce a general framework for estimation of causal quantities in\nthe presence of exposure measurement error, adapted from the control variates\napproach of Yang and Ding (2019). Our method can be implemented in various\ntwo-phase sampling study designs, where one obtains gold-standard exposure\nmeasurements for a small subset of the full study sample, called the validation\ndata. The control variates framework leverages both the error-prone and\nerror-free exposure measurements by augmenting an initial consistent estimator\nfrom the validation data with a variance reduction term formed from the full\ndata. We show that our method inherits double-robustness properties under\nstandard causal assumptions. Simulation studies show that our approach performs\nfavorably compared to leading methods under various two-phase sampling schemes.\nWe illustrate our method with observational electronic health record data on\nHIV outcomes from the Vanderbilt Comprehensive Care Clinic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure measurement error is a ubiquitous but often overlooked challenge in\ncausal inference with observational data. Existing methods accounting for\nexposure measurement error largely rely on restrictive parametric assumptions,\nwhile emerging data-adaptive estimation approaches allow for less restrictive\nassumptions but at the cost of flexibility, as they are typically tailored\ntowards rigidly-defined statistical quantities. There remains a critical need\nfor assumption-lean estimation methods that are both flexible and possess\ndesirable theoretical properties across a variety of study designs. In this\npaper, we introduce a general framework for estimation of causal quantities in\nthe presence of exposure measurement error, adapted from the control variates\napproach of Yang and Ding (2019). Our method can be implemented in various\ntwo-phase sampling study designs, where one obtains gold-standard exposure\nmeasurements for a small subset of the full study sample, called the validation\ndata. The control variates framework leverages both the error-prone and\nerror-free exposure measurements by augmenting an initial consistent estimator\nfrom the validation data with a variance reduction term formed from the full\ndata. We show that our method inherits double-robustness properties under\nstandard causal assumptions. Simulation studies show that our approach performs\nfavorably compared to leading methods under various two-phase sampling schemes.\nWe illustrate our method with observational electronic health record data on\nHIV outcomes from the Vanderbilt Comprehensive Care Clinic."
                },
                "authors": [
                    {
                        "name": "Keith Barnatchez"
                    },
                    {
                        "name": "Rachel Nethery"
                    },
                    {
                        "name": "Bryan E. Shepherd"
                    },
                    {
                        "name": "Giovanni Parmigiani"
                    },
                    {
                        "name": "Kevin P. Josey"
                    }
                ],
                "author_detail": {
                    "name": "Kevin P. Josey"
                },
                "author": "Kevin P. Josey",
                "arxiv_comment": "42 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21277v1",
                "updated": "2025-06-26T14:01:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    1,
                    3,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:01:03Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    1,
                    3,
                    3,
                    177,
                    0
                ],
                "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context"
                },
                "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models."
                },
                "authors": [
                    {
                        "name": "Qize Yang"
                    },
                    {
                        "name": "Shimin Yao"
                    },
                    {
                        "name": "Weixuan Chen"
                    },
                    {
                        "name": "Shenghao Fu"
                    },
                    {
                        "name": "Detao Bai"
                    },
                    {
                        "name": "Jiaxing Zhao"
                    },
                    {
                        "name": "Boyuan Sun"
                    },
                    {
                        "name": "Bowen Yin"
                    },
                    {
                        "name": "Xihan Wei"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21274v1",
                "updated": "2025-06-26T13:58:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    58,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:58:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    58,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?"
                },
                "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness"
                },
                "authors": [
                    {
                        "name": "Andrea McGlinchey"
                    },
                    {
                        "name": "Peter J Barclay"
                    }
                ],
                "author_detail": {
                    "name": "Peter J Barclay"
                },
                "author": "Peter J Barclay",
                "arxiv_comment": "(Submitted for publication)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10886v2",
                "updated": "2025-06-26T13:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    57,
                    37,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-18T21:50:43Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    21,
                    50,
                    43,
                    5,
                    18,
                    0
                ],
                "title": "Linear scaling causal discovery from high-dimensional time series by\n  dynamical community detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear scaling causal discovery from high-dimensional time series by\n  dynamical community detection"
                },
                "summary": "Understanding which parts of a dynamical system cause each other is extremely\nrelevant in fundamental and applied sciences. However, inferring causal links\nfrom observational data, namely without direct manipulations of the system, is\nstill computationally challenging, especially if the data are high-dimensional.\nIn this study we introduce a framework for constructing causal graphs from\nhigh-dimensional time series, whose computational cost scales linearly with the\nnumber of variables. The approach is based on the automatic identification of\ndynamical communities, groups of variables which mutually influence each other\nand can therefore be described as a single node in a causal graph. These\ncommunities are efficiently identified by optimizing the Information Imbalance,\na statistical quantity that assigns a weight to each putative causal variable\nbased on its information content relative to a target variable. The communities\nare then ordered starting from the fully autonomous ones, whose evolution is\nindependent from all the others, to those that are progressively dependent on\nother communities, building in this manner a community causal graph. We\ndemonstrate the computational efficiency and the accuracy of our approach on\ntime-discrete and time-continuous dynamical systems including up to 80\nvariables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding which parts of a dynamical system cause each other is extremely\nrelevant in fundamental and applied sciences. However, inferring causal links\nfrom observational data, namely without direct manipulations of the system, is\nstill computationally challenging, especially if the data are high-dimensional.\nIn this study we introduce a framework for constructing causal graphs from\nhigh-dimensional time series, whose computational cost scales linearly with the\nnumber of variables. The approach is based on the automatic identification of\ndynamical communities, groups of variables which mutually influence each other\nand can therefore be described as a single node in a causal graph. These\ncommunities are efficiently identified by optimizing the Information Imbalance,\na statistical quantity that assigns a weight to each putative causal variable\nbased on its information content relative to a target variable. The communities\nare then ordered starting from the fully autonomous ones, whose evolution is\nindependent from all the others, to those that are progressively dependent on\nother communities, building in this manner a community causal graph. We\ndemonstrate the computational efficiency and the accuracy of our approach on\ntime-discrete and time-continuous dynamical systems including up to 80\nvariables."
                },
                "authors": [
                    {
                        "name": "Matteo Allione"
                    },
                    {
                        "name": "Vittorio Del Tatto"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio",
                "arxiv_doi": "10.1103/kd73-93cg",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/kd73-93cg",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21263v1",
                "updated": "2025-06-26T13:45:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    45,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:45:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    45,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster"
                },
                "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters."
                },
                "authors": [
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "WenPeng Zhu"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Ming Wu"
                    },
                    {
                        "name": "YingJun Wu"
                    },
                    {
                        "name": "Wu He"
                    },
                    {
                        "name": "Xun Gao"
                    },
                    {
                        "name": "Jason Zeng"
                    },
                    {
                        "name": "Michael Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Heinrich"
                },
                "author": "Michael Heinrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21262v1",
                "updated": "2025-06-26T13:43:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    43,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:43:55Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    43,
                    55,
                    3,
                    177,
                    0
                ],
                "title": "Ambiguities, Built-in Biases and Flaws in Big Data Insight Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguities, Built-in Biases and Flaws in Big Data Insight Extraction"
                },
                "summary": "I address the challenge of extracting reliable insights from large datasets\nusing a simplified model that illustrates how hierarchical classification can\ndistort outcomes. The model consists of discrete pixels labeled red, blue, or\nwhite. Red and blue indicate distinct properties, and white represents\nunclassified or ambiguous data. A macro-color is assigned only if one color\nholds a strict majority among the pixels. Otherwise, the aggregate is labeled\nwhite, reflecting uncertainty. This setup mimics a percolation threshold at\nfifty percent. Assuming direct access of the various proportions of colors is\ninfeasible from the data, I implement a hierarchical coarse-graining procedure.\nElements (first pixels, then aggregates) are recursively grouped and\nreclassified via local majority rules, producing ultimately a single\nsuper-aggregate whose color represents the inferred macro-property of the\ncollection of pixels as a whole. Analytical results, supported by simulations,\nshow that the process introduces additional white aggregates beyond white\npixels, which could be initially present. These arise from groups lacking a\nclear majority, requiring arbitrary symmetry-breaking decisions to attribute a\ncolor to them. While each local resolution may appear minor and\ninconsequential, their repetitions introduce a growing systematic bias. Even\nwith complete data, unavoidable asymmetries in local rules are shown to skew\noutcomes. This study highlights a critical limitation of recursive data\nreduction. Insight extraction is shaped not only by data quality, but by how\nlocal ambiguity is handled. That results in built-in biases. Thus, the related\nflaws are not due to the data, but due to structural choices made during local\naggregations. Though based on a simple model, the findings expose the high\nlikelihood of inherent flaws in widely used hierarchical classification\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I address the challenge of extracting reliable insights from large datasets\nusing a simplified model that illustrates how hierarchical classification can\ndistort outcomes. The model consists of discrete pixels labeled red, blue, or\nwhite. Red and blue indicate distinct properties, and white represents\nunclassified or ambiguous data. A macro-color is assigned only if one color\nholds a strict majority among the pixels. Otherwise, the aggregate is labeled\nwhite, reflecting uncertainty. This setup mimics a percolation threshold at\nfifty percent. Assuming direct access of the various proportions of colors is\ninfeasible from the data, I implement a hierarchical coarse-graining procedure.\nElements (first pixels, then aggregates) are recursively grouped and\nreclassified via local majority rules, producing ultimately a single\nsuper-aggregate whose color represents the inferred macro-property of the\ncollection of pixels as a whole. Analytical results, supported by simulations,\nshow that the process introduces additional white aggregates beyond white\npixels, which could be initially present. These arise from groups lacking a\nclear majority, requiring arbitrary symmetry-breaking decisions to attribute a\ncolor to them. While each local resolution may appear minor and\ninconsequential, their repetitions introduce a growing systematic bias. Even\nwith complete data, unavoidable asymmetries in local rules are shown to skew\noutcomes. This study highlights a critical limitation of recursive data\nreduction. Insight extraction is shaped not only by data quality, but by how\nlocal ambiguity is handled. That results in built-in biases. Thus, the related\nflaws are not due to the data, but due to structural choices made during local\naggregations. Though based on a simple model, the findings expose the high\nlikelihood of inherent flaws in widely used hierarchical classification\ntechniques."
                },
                "authors": [
                    {
                        "name": "Serge Galam"
                    }
                ],
                "author_detail": {
                    "name": "Serge Galam"
                },
                "author": "Serge Galam",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21258v1",
                "updated": "2025-06-26T13:40:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    40,
                    32,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:40:32Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    40,
                    32,
                    3,
                    177,
                    0
                ],
                "title": "Low multipole mapmaking for global 21-cm experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low multipole mapmaking for global 21-cm experiments"
                },
                "summary": "The 21-cm global signal is obscured by very bright galactic and extra\ngalactic foreground emissions. Typical single-spectrum fit (SSF) based methods\nfor foreground/signal separation can result in biased estimates of the\ncosmological signal due to the presence of spectral oscillations induced by the\ninteraction between chromatic beams and the spatial shape of the foregrounds.\nModelling this interaction requires some amount of assumed foreground\ninformation. We present a mapmaking-based approach which describes the\nbeam-weighted observation of the sky by multiple globally-distributed antenna\nexperiments as an observation equation. This equation is inverted in order to\nestimate the low-order sky modes ($\\ell\\lesssim10$). The resulting\nchromaticity-free sky monopole is then fit with a smooth foreground function\nand a 21-cm model. Given the insensitivity of global 21-cm experiments to small\nangular scales, we rely on the mean and covariance of higher-order foreground\nmodes being known. We show that this mapmaking-based method is capable of\ninferring the cosmological signal in cases where a SSF with a simple\nbeam-factor based chromaticity correction fails, even when the foreground model\nused in the mapmaking method features uncertainty at the 10\\% level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21-cm global signal is obscured by very bright galactic and extra\ngalactic foreground emissions. Typical single-spectrum fit (SSF) based methods\nfor foreground/signal separation can result in biased estimates of the\ncosmological signal due to the presence of spectral oscillations induced by the\ninteraction between chromatic beams and the spatial shape of the foregrounds.\nModelling this interaction requires some amount of assumed foreground\ninformation. We present a mapmaking-based approach which describes the\nbeam-weighted observation of the sky by multiple globally-distributed antenna\nexperiments as an observation equation. This equation is inverted in order to\nestimate the low-order sky modes ($\\ell\\lesssim10$). The resulting\nchromaticity-free sky monopole is then fit with a smooth foreground function\nand a 21-cm model. Given the insensitivity of global 21-cm experiments to small\nangular scales, we rely on the mean and covariance of higher-order foreground\nmodes being known. We show that this mapmaking-based method is capable of\ninferring the cosmological signal in cases where a SSF with a simple\nbeam-factor based chromaticity correction fails, even when the foreground model\nused in the mapmaking method features uncertainty at the 10\\% level."
                },
                "authors": [
                    {
                        "name": "Yordan D. Ignatov"
                    },
                    {
                        "name": "Jonathan R. Pritchard"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Pritchard"
                },
                "author": "Jonathan R. Pritchard",
                "arxiv_comment": "15 pages, 11 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08886v4",
                "updated": "2025-06-26T13:39:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    39,
                    9,
                    3,
                    177,
                    0
                ],
                "published": "2025-04-11T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    18,
                    0,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "The kangaroo's first hop: the early fast cooling phase of EP250108a/SN\n  2025kg",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kangaroo's first hop: the early fast cooling phase of EP250108a/SN\n  2025kg"
                },
                "summary": "Fast X-ray transients (FXTs) are a rare and poorly understood population of\nevents. Previously difficult to detect in real time, the launch of the Einstein\nProbe with its wide field X-ray telescope has led to a rapid expansion in the\nsample and allowed the exploration of these transients across the\nelectromagnetic spectrum. EP250108a is a recently detected example linked to an\noptical counterpart, SN 2025kg, or 'the kangaroo'. Together with a companion\npaper (Rastinejad et al. 2025), we present our observing campaign and analysis\nof this event. In this letter, we focus on the early evolution of the optical\ncounterpart over the first six days, including our measurement of the redshift\nof $z=0.17641$. We find that the source is well-modelled by a rapidly expanding\ncooling blackbody. We show the observed X-ray and radio properties are\nconsistent with a collapsar-powered jet that is low energy ($\\lesssim10^{51}$\nerg) and/or fails to break out of the dense material surrounding it. While we\nexamine the possibility that the optical emission emerges from the shock\nproduced as the supernova ejecta expand into a dense shell of circumstellar\nmaterial, due to our X-ray and radio inferences, we favour a model where it\narises from a shocked cocoon resulting from the trapped jet. This makes SN\n2025kg one of the few examples of this currently observationally rare event.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast X-ray transients (FXTs) are a rare and poorly understood population of\nevents. Previously difficult to detect in real time, the launch of the Einstein\nProbe with its wide field X-ray telescope has led to a rapid expansion in the\nsample and allowed the exploration of these transients across the\nelectromagnetic spectrum. EP250108a is a recently detected example linked to an\noptical counterpart, SN 2025kg, or 'the kangaroo'. Together with a companion\npaper (Rastinejad et al. 2025), we present our observing campaign and analysis\nof this event. In this letter, we focus on the early evolution of the optical\ncounterpart over the first six days, including our measurement of the redshift\nof $z=0.17641$. We find that the source is well-modelled by a rapidly expanding\ncooling blackbody. We show the observed X-ray and radio properties are\nconsistent with a collapsar-powered jet that is low energy ($\\lesssim10^{51}$\nerg) and/or fails to break out of the dense material surrounding it. While we\nexamine the possibility that the optical emission emerges from the shock\nproduced as the supernova ejecta expand into a dense shell of circumstellar\nmaterial, due to our X-ray and radio inferences, we favour a model where it\narises from a shocked cocoon resulting from the trapped jet. This makes SN\n2025kg one of the few examples of this currently observationally rare event."
                },
                "authors": [
                    {
                        "name": "Rob A. J. Eyles-Ferris"
                    },
                    {
                        "name": "Peter G. Jonker"
                    },
                    {
                        "name": "Andrew J. Levan"
                    },
                    {
                        "name": "Daniele Bjørn Malesani"
                    },
                    {
                        "name": "Nikhil Sarin"
                    },
                    {
                        "name": "Christopher L. Fryer"
                    },
                    {
                        "name": "Jillian C. Rastinejad"
                    },
                    {
                        "name": "Eric Burns"
                    },
                    {
                        "name": "Nial R. Tanvir"
                    },
                    {
                        "name": "Paul T. O'Brien"
                    },
                    {
                        "name": "Wen-fai Fong"
                    },
                    {
                        "name": "Ilya Mandel"
                    },
                    {
                        "name": "Benjamin P. Gompertz"
                    },
                    {
                        "name": "Charles D. Kilpatrick"
                    },
                    {
                        "name": "Steven Bloemen"
                    },
                    {
                        "name": "Joe S. Bright"
                    },
                    {
                        "name": "Francesco Carotenuto"
                    },
                    {
                        "name": "Gregory Corcoran"
                    },
                    {
                        "name": "Laura Cotter"
                    },
                    {
                        "name": "Paul J. Groot"
                    },
                    {
                        "name": "Luca Izzo"
                    },
                    {
                        "name": "Tanmoy Laskar"
                    },
                    {
                        "name": "Antonio Martin-Carrillo"
                    },
                    {
                        "name": "Jesse Palmerio"
                    },
                    {
                        "name": "Maria E. Ravasio"
                    },
                    {
                        "name": "Jan van Roestel"
                    },
                    {
                        "name": "Andrea Saccardi"
                    },
                    {
                        "name": "Rhaana L. C. Starling"
                    },
                    {
                        "name": "Aishwarya Linesh Thakur"
                    },
                    {
                        "name": "Susanna D. Vergani"
                    },
                    {
                        "name": "Paul M. Vreeswijk"
                    },
                    {
                        "name": "Franz E. Bauer"
                    },
                    {
                        "name": "Sergio Campana"
                    },
                    {
                        "name": "Jennifer A. Chacón"
                    },
                    {
                        "name": "Ashley A. Chrimes"
                    },
                    {
                        "name": "Stefano Covino"
                    },
                    {
                        "name": "Joyce N. D. van Dalen"
                    },
                    {
                        "name": "Valerio D'Elia"
                    },
                    {
                        "name": "Massimiliano De Pasquale"
                    },
                    {
                        "name": "Nusrin Habeeb"
                    },
                    {
                        "name": "Dieter H. Hartmann"
                    },
                    {
                        "name": "Agnes P. C. van Hoof"
                    },
                    {
                        "name": "Páll Jakobsson"
                    },
                    {
                        "name": "Yashaswi Julakanti"
                    },
                    {
                        "name": "Giorgos Leloudas"
                    },
                    {
                        "name": "Daniel Mata Sánchez"
                    },
                    {
                        "name": "Christopher J. Nixon"
                    },
                    {
                        "name": "Daniëlle L. A. Pieterse"
                    },
                    {
                        "name": "Giavanna Pugliese"
                    },
                    {
                        "name": "Jonathan Quirola-Vásquez"
                    },
                    {
                        "name": "Ben C. Rayson"
                    },
                    {
                        "name": "Ruben Salvaterra"
                    },
                    {
                        "name": "Ben Schneider"
                    },
                    {
                        "name": "Manuel A. P. Torres"
                    },
                    {
                        "name": "Tayyaba Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Tayyaba Zafar"
                },
                "author": "Tayyaba Zafar",
                "arxiv_comment": "28 pages, 15 figures and 6 tables. Version accepted by ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21240v1",
                "updated": "2025-06-26T13:23:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    57,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:23:57Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    57,
                    3,
                    177,
                    0
                ],
                "title": "Zero-Shot Learning for Obsolescence Risk Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Learning for Obsolescence Risk Forecasting"
                },
                "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks."
                },
                "authors": [
                    {
                        "name": "Elie Saad"
                    },
                    {
                        "name": "Aya Mrabah"
                    },
                    {
                        "name": "Mariem Besbes"
                    },
                    {
                        "name": "Marc Zolghadri"
                    },
                    {
                        "name": "Victor Czmil"
                    },
                    {
                        "name": "Claude Baron"
                    },
                    {
                        "name": "Vincent Bourgeois"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Bourgeois"
                },
                "author": "Vincent Bourgeois",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21238v1",
                "updated": "2025-06-26T13:23:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    3,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:23:03Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    3,
                    3,
                    177,
                    0
                ],
                "title": "Optimizing Gaussian Process Kernels Using Nested Sampling and ABC\n  Rejection for H(z) Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Gaussian Process Kernels Using Nested Sampling and ABC\n  Rejection for H(z) Reconstruction"
                },
                "summary": "Recent cosmological observations have achieved high-precision measurements of\nthe Universe's expansion history, prompting the use of nonparametric methods\nsuch as Gaussian processes (GP) regression. We apply GP regression for\nreconstructing the Hubble parameter using CC data, with improved covariance\nmodeling and latest study in CC data. By comparing reconstructions in redshift\nspace $z$ and transformed space $\\log(z+1)$ , we evaluate six kernel functions\nusing nested sampling (NS) and approximate Bayesian computation rejection (ABC\nrejection) methods and analyze the construction of Hubble constant $H_0$ in\ndifferent models. Our analysis demonstrates that reconstructions in $\\log(z+1)$\nspace remain physically reasonable, offering a viable alternative to\nconventional $z$ space approaches, while the introduction of nondiagonal\ncovariance matrices leads to degraded reconstruction quality, suggesting that\nsimplified diagonal forms may be preferable for reconstruction. These findings\nunderscore the importance of task-specific kernel selection in GP-based\ncosmological inference. In particular, our findings suggest that careful\npreliminary screening of kernel functions, based on the physical quantities of\ninterest, is essential for reliable inference in cosmological research using\nGP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent cosmological observations have achieved high-precision measurements of\nthe Universe's expansion history, prompting the use of nonparametric methods\nsuch as Gaussian processes (GP) regression. We apply GP regression for\nreconstructing the Hubble parameter using CC data, with improved covariance\nmodeling and latest study in CC data. By comparing reconstructions in redshift\nspace $z$ and transformed space $\\log(z+1)$ , we evaluate six kernel functions\nusing nested sampling (NS) and approximate Bayesian computation rejection (ABC\nrejection) methods and analyze the construction of Hubble constant $H_0$ in\ndifferent models. Our analysis demonstrates that reconstructions in $\\log(z+1)$\nspace remain physically reasonable, offering a viable alternative to\nconventional $z$ space approaches, while the introduction of nondiagonal\ncovariance matrices leads to degraded reconstruction quality, suggesting that\nsimplified diagonal forms may be preferable for reconstruction. These findings\nunderscore the importance of task-specific kernel selection in GP-based\ncosmological inference. In particular, our findings suggest that careful\npreliminary screening of kernel functions, based on the physical quantities of\ninterest, is essential for reliable inference in cosmological research using\nGP."
                },
                "authors": [
                    {
                        "name": "Jia-yan Jiang"
                    },
                    {
                        "name": "Kang Jiao"
                    },
                    {
                        "name": "Tong-Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong-Jie Zhang"
                },
                "author": "Tong-Jie Zhang",
                "arxiv_comment": "19 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21222v1",
                "updated": "2025-06-26T13:14:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    14,
                    52,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:14:52Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    14,
                    52,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval"
                },
                "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks."
                },
                "authors": [
                    {
                        "name": "Yongchan Chun"
                    },
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Dongjun Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21220v1",
                "updated": "2025-06-26T13:13:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    13,
                    24,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:13:24Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    13,
                    24,
                    3,
                    177,
                    0
                ],
                "title": "Complexity-aware fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity-aware fine-tuning"
                },
                "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction."
                },
                "authors": [
                    {
                        "name": "Andrey Goncharov"
                    },
                    {
                        "name": "Daniil Vyazhev"
                    },
                    {
                        "name": "Petr Sychev"
                    },
                    {
                        "name": "Edvard Khalafyan"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21215v1",
                "updated": "2025-06-26T13:11:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    11,
                    1,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:11:01Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    11,
                    1,
                    3,
                    177,
                    0
                ],
                "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?"
                },
                "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2."
                },
                "authors": [
                    {
                        "name": "Haoang Chi"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Long Lan"
                    },
                    {
                        "name": "Xiaoguang Ren"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "arxiv_comment": "24 pages, accepted at NeurIPS 2024",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems, 2024, 37:\n  96640-96670",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v2",
                "updated": "2025-06-26T13:09:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    9,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21211v1",
                "updated": "2025-06-26T13:04:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    4,
                    28,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:04:28Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    4,
                    28,
                    3,
                    177,
                    0
                ],
                "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large\n  Language Models"
                },
                "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging."
                },
                "authors": [
                    {
                        "name": "Quanming Liu"
                    },
                    {
                        "name": "Xupeng Bu"
                    },
                    {
                        "name": "Zhichao Yan"
                    },
                    {
                        "name": "Ru Li"
                    }
                ],
                "author_detail": {
                    "name": "Ru Li"
                },
                "author": "Ru Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21199v1",
                "updated": "2025-06-26T12:57:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    57,
                    41,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:57:41Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    57,
                    41,
                    3,
                    177,
                    0
                ],
                "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image\n  Segmentation and Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image\n  Segmentation and Classification"
                },
                "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs."
                },
                "authors": [
                    {
                        "name": "Shadman Sobhan"
                    },
                    {
                        "name": "Kazi Abrar Mahmud"
                    },
                    {
                        "name": "Abduz Zami"
                    }
                ],
                "author_detail": {
                    "name": "Abduz Zami"
                },
                "author": "Abduz Zami",
                "arxiv_comment": "40 pages, 8 Tables, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00753v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00753v4",
                "updated": "2025-06-26T12:53:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    53,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-01T08:29:26Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    8,
                    29,
                    26,
                    3,
                    121,
                    0
                ],
                "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Yaozu Wu"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Paper lists and resources are available at\n  https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00753v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00753v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19748v2",
                "updated": "2025-06-26T12:49:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-25T15:11:14Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    11,
                    14,
                    1,
                    84,
                    0
                ],
                "title": "No-prior Bayesian inference reIMagined: probabilistic approximations of\n  inferential models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-prior Bayesian inference reIMagined: probabilistic approximations of\n  inferential models"
                },
                "summary": "When prior information is lacking, the go-to strategy for probabilistic\ninference is to combine a \"default prior\" and the likelihood via Bayes's\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\nthis umbrella. This construction is natural, but the corresponding posterior\ndistributions generally only offer limited, approximately valid uncertainty\nquantification. The present paper takes a reimagined approach offering\nposterior distributions with stronger reliability properties. The proposed\nconstruction starts with an inferential model (IM), one that takes the\nmathematical form of a data-driven possibility measure and features exactly\nvalid uncertainty quantification, and then returns a so-called inner\nprobabilistic approximation thereof. This inner probabilistic approximation\ninherits many of the original IM's desirable properties, including credible\nsets with exact coverage and asymptotic efficiency. The approximation also\nagrees with the familiar Bayes/fiducial solution obtained in applications where\nthe model has a group transformation structure. A Monte Carlo method for\nevaluating the probabilistic approximation is presented, along with numerical\nillustrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When prior information is lacking, the go-to strategy for probabilistic\ninference is to combine a \"default prior\" and the likelihood via Bayes's\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\nthis umbrella. This construction is natural, but the corresponding posterior\ndistributions generally only offer limited, approximately valid uncertainty\nquantification. The present paper takes a reimagined approach offering\nposterior distributions with stronger reliability properties. The proposed\nconstruction starts with an inferential model (IM), one that takes the\nmathematical form of a data-driven possibility measure and features exactly\nvalid uncertainty quantification, and then returns a so-called inner\nprobabilistic approximation thereof. This inner probabilistic approximation\ninherits many of the original IM's desirable properties, including credible\nsets with exact coverage and asymptotic efficiency. The approximation also\nagrees with the familiar Bayes/fiducial solution obtained in applications where\nthe model has a group transformation structure. A Monte Carlo method for\nevaluating the probabilistic approximation is presented, along with numerical\nillustrations."
                },
                "authors": [
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21191v1",
                "updated": "2025-06-26T12:49:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:49:07Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    7,
                    3,
                    177,
                    0
                ],
                "title": "Prompt-Guided Turn-Taking Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Guided Turn-Taking Prediction"
                },
                "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Yahui Fu"
                    },
                    {
                        "name": "Zi Haur Pang"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Keiko Ochi"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "arxiv_comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21190v1",
                "updated": "2025-06-26T12:49:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    2,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:49:02Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    2,
                    3,
                    177,
                    0
                ],
                "title": "Survival analysis under label shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival analysis under label shift"
                },
                "summary": "Let P represent the source population with complete data, containing\ncovariate $\\mathbf{Z}$ and response $T$, and Q the target population, where\nonly the covariate $\\mathbf{Z}$ is available. We consider a setting with both\nlabel shift and label censoring. Label shift assumes that the marginal\ndistribution of $T$ differs between $P$ and $Q$, while the conditional\ndistribution of $\\mathbf{Z}$ given $T$ remains the same. Label censoring refers\nto the case where the response $T$ in $P$ is subject to random censoring. Our\ngoal is to leverage information from the label-shifted and label-censored\nsource population $P$ to conduct statistical inference in the target population\n$Q$. We propose a parametric model for $T$ given $\\mathbf{Z}$ in $Q$ and\nestimate the model parameters by maximizing an approximate likelihood. This\nallows for statistical inference in $Q$ and accommodates a range of classical\nsurvival models. Under the label shift assumption, the likelihood depends not\nonly on the unknown parameters but also on the unknown distribution of $T$ in\n$P$ and $\\mathbf{Z}$ in $Q$, which we estimate nonparametrically. The\nasymptotic properties of the estimator are rigorously established and the\neffectiveness of the method is demonstrated through simulations and a real data\napplication. This work is the first to combine survival analysis with label\nshift, offering a new research direction in this emerging topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let P represent the source population with complete data, containing\ncovariate $\\mathbf{Z}$ and response $T$, and Q the target population, where\nonly the covariate $\\mathbf{Z}$ is available. We consider a setting with both\nlabel shift and label censoring. Label shift assumes that the marginal\ndistribution of $T$ differs between $P$ and $Q$, while the conditional\ndistribution of $\\mathbf{Z}$ given $T$ remains the same. Label censoring refers\nto the case where the response $T$ in $P$ is subject to random censoring. Our\ngoal is to leverage information from the label-shifted and label-censored\nsource population $P$ to conduct statistical inference in the target population\n$Q$. We propose a parametric model for $T$ given $\\mathbf{Z}$ in $Q$ and\nestimate the model parameters by maximizing an approximate likelihood. This\nallows for statistical inference in $Q$ and accommodates a range of classical\nsurvival models. Under the label shift assumption, the likelihood depends not\nonly on the unknown parameters but also on the unknown distribution of $T$ in\n$P$ and $\\mathbf{Z}$ in $Q$, which we estimate nonparametrically. The\nasymptotic properties of the estimator are rigorously established and the\neffectiveness of the method is demonstrated through simulations and a real data\napplication. This work is the first to combine survival analysis with label\nshift, offering a new research direction in this emerging topic."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zong"
                    },
                    {
                        "name": "Yanyuan Ma"
                    },
                    {
                        "name": "Ingrid Van Keilegom"
                    }
                ],
                "author_detail": {
                    "name": "Ingrid Van Keilegom"
                },
                "author": "Ingrid Van Keilegom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08829v2",
                "updated": "2025-06-26T12:48:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    48,
                    11,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-11T19:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    19,
                    8,
                    31,
                    1,
                    70,
                    0
                ],
                "title": "Seal Your Backdoor with Variational Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seal Your Backdoor with Variational Defense"
                },
                "summary": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios."
                },
                "authors": [
                    {
                        "name": "Ivan Sabolić"
                    },
                    {
                        "name": "Matej Grcić"
                    },
                    {
                        "name": "Siniša Šegvić"
                    }
                ],
                "author_detail": {
                    "name": "Siniša Šegvić"
                },
                "author": "Siniša Šegvić",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01244v2",
                "updated": "2025-06-26T12:44:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    44,
                    50,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-02T01:44:33Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    44,
                    33,
                    0,
                    153,
                    0
                ],
                "title": "Exact operator inference with minimal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact operator inference with minimal data"
                },
                "summary": "This work introduces a novel method to generate snapshot data for operator\ninference that guarantees the exact reconstruction of intrusive\nprojection-based reduced-order models (ROMs). To ensure exact reconstruction,\nthe operator inference least squares matrix must have full rank, without\nregularization. Existing works have achieved this full rank using heuristic\nstrategies to generate snapshot data and a-posteriori checks on full rank, but\nwithout a guarantee of success. Our novel snapshot data generation method\nprovides this guarantee thanks to two key ingredients: first we identify ROM\nstates that induce full rank, then we generate snapshots corresponding to\nexactly these states by simulating multiple trajectories for only a single time\nstep. This way, the number of required snapshots is minimal and orders of\nmagnitude lower than typically reported with existing methods. The method\navoids non-Markovian terms and does not require re-projection. Since the number\nof snapshots is minimal, the least squares problem simplifies to a linear\nsystem that is numerically more stable. In addition, because the inferred\noperators are exact, properties of the intrusive ROM operators such as symmetry\nor skew-symmetry are preserved. Numerical results for differential equations\ninvolving 2nd, 3rd and 8th order polynomials demonstrate that the novel\nsnapshot data generation method leads to exact reconstruction of the intrusive\nreduced order models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a novel method to generate snapshot data for operator\ninference that guarantees the exact reconstruction of intrusive\nprojection-based reduced-order models (ROMs). To ensure exact reconstruction,\nthe operator inference least squares matrix must have full rank, without\nregularization. Existing works have achieved this full rank using heuristic\nstrategies to generate snapshot data and a-posteriori checks on full rank, but\nwithout a guarantee of success. Our novel snapshot data generation method\nprovides this guarantee thanks to two key ingredients: first we identify ROM\nstates that induce full rank, then we generate snapshots corresponding to\nexactly these states by simulating multiple trajectories for only a single time\nstep. This way, the number of required snapshots is minimal and orders of\nmagnitude lower than typically reported with existing methods. The method\navoids non-Markovian terms and does not require re-projection. Since the number\nof snapshots is minimal, the least squares problem simplifies to a linear\nsystem that is numerically more stable. In addition, because the inferred\noperators are exact, properties of the intrusive ROM operators such as symmetry\nor skew-symmetry are preserved. Numerical results for differential equations\ninvolving 2nd, 3rd and 8th order polynomials demonstrate that the novel\nsnapshot data generation method leads to exact reconstruction of the intrusive\nreduced order models."
                },
                "authors": [
                    {
                        "name": "Henrik Rosenberger"
                    },
                    {
                        "name": "Benjamin Sanderse"
                    },
                    {
                        "name": "Giovanni Stabile"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Stabile"
                },
                "author": "Giovanni Stabile",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65Y99, 65F22, 35R30, 65D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21181v1",
                "updated": "2025-06-26T12:39:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    39,
                    6,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:39:06Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    39,
                    6,
                    3,
                    177,
                    0
                ],
                "title": "Systematic bias in LISA ringdown analysis due to waveform inaccuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic bias in LISA ringdown analysis due to waveform inaccuracy"
                },
                "summary": "Inaccurate modeling of gravitational-wave signals can introduce systematic\nbiases in the inferred source parameters. As detector sensitivities improve and\nsignals become louder, mitigating such waveform-induced systematics becomes\nincreasingly important. In this work, we assess the systematic biases\nintroduced by an incomplete description of the ringdown signal from massive\nblack hole binaries in the LISA band. Specifically, we investigate the impact\nof mode truncation in the ringdown template. Using a reference waveform\ncomposed of 13 modes, we establish a mode hierarchy and determine the minimum\nnumber of modes required to avoid parameter biases across a wide range of LISA\nsources. For typical systems with masses $\\sim 10^6$--$10^7\\,M_\\odot$ at\nredshifts $z \\sim 2$--$6$, we find that at least 3--6 modes are needed for\naccurate parameter estimation, while high-SNR events may need at least 10\nmodes. Our results are a window-insensitive lower bound on the minimum number\nof modes, as more modes may be needed depending on the choice of time-domain\nwindowing of the post-merger signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inaccurate modeling of gravitational-wave signals can introduce systematic\nbiases in the inferred source parameters. As detector sensitivities improve and\nsignals become louder, mitigating such waveform-induced systematics becomes\nincreasingly important. In this work, we assess the systematic biases\nintroduced by an incomplete description of the ringdown signal from massive\nblack hole binaries in the LISA band. Specifically, we investigate the impact\nof mode truncation in the ringdown template. Using a reference waveform\ncomposed of 13 modes, we establish a mode hierarchy and determine the minimum\nnumber of modes required to avoid parameter biases across a wide range of LISA\nsources. For typical systems with masses $\\sim 10^6$--$10^7\\,M_\\odot$ at\nredshifts $z \\sim 2$--$6$, we find that at least 3--6 modes are needed for\naccurate parameter estimation, while high-SNR events may need at least 10\nmodes. Our results are a window-insensitive lower bound on the minimum number\nof modes, as more modes may be needed depending on the choice of time-domain\nwindowing of the post-merger signal."
                },
                "authors": [
                    {
                        "name": "Lodovico Capuano"
                    },
                    {
                        "name": "Massimo Vaglio"
                    },
                    {
                        "name": "Rohit S. Chandramouli"
                    },
                    {
                        "name": "Chantal L Pitte"
                    },
                    {
                        "name": "Adrien Kuntz"
                    },
                    {
                        "name": "Enrico Barausse"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Barausse"
                },
                "author": "Enrico Barausse",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07413v2",
                "updated": "2025-06-26T12:27:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    27,
                    25,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-09T04:19:12Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    4,
                    19,
                    12,
                    0,
                    160,
                    0
                ],
                "title": "Variational Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Supervised Contrastive Learning"
                },
                "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies."
                },
                "authors": [
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Jiajun Fan"
                    },
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Ge Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Liu"
                },
                "author": "Ge Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21170v1",
                "updated": "2025-06-26T12:05:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    5,
                    13,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:05:13Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    5,
                    13,
                    3,
                    177,
                    0
                ],
                "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed and Smooth Latent Space for Text Diffusion Modeling"
                },
                "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference."
                },
                "authors": [
                    {
                        "name": "Viacheslav Meshchaninov"
                    },
                    {
                        "name": "Egor Chimbulatov"
                    },
                    {
                        "name": "Alexander Shabalin"
                    },
                    {
                        "name": "Aleksandr Abramov"
                    },
                    {
                        "name": "Dmitry Vetrov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Vetrov"
                },
                "author": "Dmitry Vetrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13056v2",
                "updated": "2025-06-26T11:45:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    45,
                    11,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-16T02:56:13Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    56,
                    13,
                    0,
                    167,
                    0
                ],
                "title": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model\n  Learning"
                },
                "summary": "Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall. Please refer\nto our project page for open-source information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall. Please refer\nto our project page for open-source information."
                },
                "authors": [
                    {
                        "name": "Haibo Qiu"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Fanfan Liu"
                    },
                    {
                        "name": "Xiaohu Sun"
                    },
                    {
                        "name": "Delian Ruan"
                    },
                    {
                        "name": "Peng Shi"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "Project Page: https://github.com/MM-Thinking/Metis-RISE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01495v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01495v4",
                "updated": "2025-06-26T11:34:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    34,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-02T09:56:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    56,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of\n  Large Language Models"
                },
                "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC."
                },
                "authors": [
                    {
                        "name": "Ping Wu"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Enmeng Lu"
                    },
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01495v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01495v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21157v1",
                "updated": "2025-06-26T11:27:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    27,
                    23,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T11:27:23Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    27,
                    23,
                    3,
                    177,
                    0
                ],
                "title": "Evaluating Randomness Assumption: A Novel Graph Theoretic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Randomness Assumption: A Novel Graph Theoretic Approach"
                },
                "summary": "Randomness or mutual independence is a fundamental assumption forming the\nbasis of statistical inference across disciplines such as economics, finance,\nand management. Consequently, validating this assumption is essential for the\nreliable application of statistical methods. However, verifying randomness\nremains a challenge, as existing tests in the literature are often restricted\nto detecting specific types of data dependencies. In this paper, we propose a\nnovel graph-theoretic approach to testing randomness using random interval\ngraphs (RIGs). The key advantage of RIGs is that their properties are\nindependent of the underlying distribution of the data, relying solely on the\nassumption of independence between observations. By using two key properties of\nRIGs-edge probability and vertex degree distribution-we develop two new\nrandomness tests: the RIG-Edge Probability test and the RIG-Degree Distribution\n(RIG-DD) test. Through extensive simulations, we demonstrate that these tests\ncan detect a broad range of dependencies, including complex phenomena such as\nconditional heteroskedasticity and chaotic behavior, beyond simple\ncorrelations. Furthermore, we show that the RIG-DD test outperforms most of the\nexisting tests of randomness in the literature. We also provide real-world\nexamples to illustrate the practical applicability of these tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomness or mutual independence is a fundamental assumption forming the\nbasis of statistical inference across disciplines such as economics, finance,\nand management. Consequently, validating this assumption is essential for the\nreliable application of statistical methods. However, verifying randomness\nremains a challenge, as existing tests in the literature are often restricted\nto detecting specific types of data dependencies. In this paper, we propose a\nnovel graph-theoretic approach to testing randomness using random interval\ngraphs (RIGs). The key advantage of RIGs is that their properties are\nindependent of the underlying distribution of the data, relying solely on the\nassumption of independence between observations. By using two key properties of\nRIGs-edge probability and vertex degree distribution-we develop two new\nrandomness tests: the RIG-Edge Probability test and the RIG-Degree Distribution\n(RIG-DD) test. Through extensive simulations, we demonstrate that these tests\ncan detect a broad range of dependencies, including complex phenomena such as\nconditional heteroskedasticity and chaotic behavior, beyond simple\ncorrelations. Furthermore, we show that the RIG-DD test outperforms most of the\nexisting tests of randomness in the literature. We also provide real-world\nexamples to illustrate the practical applicability of these tests."
                },
                "authors": [
                    {
                        "name": "Shriya Gehlot"
                    },
                    {
                        "name": "Arnab Kumar Laha"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Kumar Laha"
                },
                "author": "Arnab Kumar Laha",
                "arxiv_comment": "31 Pages, 2 Figures, 12 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62A09 (Primary) 62G99 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21155v1",
                "updated": "2025-06-26T11:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    25,
                    14,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T11:25:14Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    25,
                    14,
                    3,
                    177,
                    0
                ],
                "title": "Amortizing personalization in virtual brain twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortizing personalization in virtual brain twins"
                },
                "summary": "Virtual brain twins are personalized digital models of individual human\nsubject or patient's brains, allowing for mechanistic interpretation of\nneuroimaging data features. Training and inference with these models however\npresents a pair of challenges: large shared infrastructure do not allow for use\nof personal data and inference in clinical applications should not require\nsignificant resources. We introduce \"anonymized personalization\" to address\nboth by expanding model priors to include personalization which under amortized\ninference allows training to be performed anonymously, while inference is both\npersonalized and lightweight. We illustrate the basic approach, demonstrate\nreliability in an example, and discuss the impact on both experimental and\ncomputational neuroscience. Code is available at\nhttps://github.com/ins-amu/apvbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual brain twins are personalized digital models of individual human\nsubject or patient's brains, allowing for mechanistic interpretation of\nneuroimaging data features. Training and inference with these models however\npresents a pair of challenges: large shared infrastructure do not allow for use\nof personal data and inference in clinical applications should not require\nsignificant resources. We introduce \"anonymized personalization\" to address\nboth by expanding model priors to include personalization which under amortized\ninference allows training to be performed anonymously, while inference is both\npersonalized and lightweight. We illustrate the basic approach, demonstrate\nreliability in an example, and discuss the impact on both experimental and\ncomputational neuroscience. Code is available at\nhttps://github.com/ins-amu/apvbt."
                },
                "authors": [
                    {
                        "name": "Nina Baldy"
                    },
                    {
                        "name": "Marmaduke M Woodman"
                    },
                    {
                        "name": "Viktor K Jirsa"
                    }
                ],
                "author_detail": {
                    "name": "Viktor K Jirsa"
                },
                "author": "Viktor K Jirsa",
                "arxiv_comment": "12 pages, 5 figures, accepted for ICANN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21152v1",
                "updated": "2025-06-26T11:22:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    22,
                    6,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T11:22:06Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    22,
                    6,
                    3,
                    177,
                    0
                ],
                "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image"
                },
                "summary": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To takle these issues, we present a novel method that seamlessly\nintegrates geometry and perception priors without requiring additional model\ntraining to reconstruct detailed 3D objects from a single image. Specifically,\nwe train three different Gaussian branches initialized from the geometry prior,\nperception prior and Gaussian noise, respectively. The geometry prior captures\nthe rough 3D shapes, while the perception prior utilizes the 2D pretrained\ndiffusion model to enhance multiview information. Subsequently, we refine 3D\nGaussian branches through mutual interaction between geometry and perception\npriors, further enhanced by a reprojection-based strategy that enforces depth\nconsistency. Experiments demonstrate the higher-fidelity reconstruction results\nof our method, outperforming existing methods on novel view synthesis and 3D\nreconstruction, demonstrating robust and consistent 3D object generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To takle these issues, we present a novel method that seamlessly\nintegrates geometry and perception priors without requiring additional model\ntraining to reconstruct detailed 3D objects from a single image. Specifically,\nwe train three different Gaussian branches initialized from the geometry prior,\nperception prior and Gaussian noise, respectively. The geometry prior captures\nthe rough 3D shapes, while the perception prior utilizes the 2D pretrained\ndiffusion model to enhance multiview information. Subsequently, we refine 3D\nGaussian branches through mutual interaction between geometry and perception\npriors, further enhanced by a reprojection-based strategy that enforces depth\nconsistency. Experiments demonstrate the higher-fidelity reconstruction results\nof our method, outperforming existing methods on novel view synthesis and 3D\nreconstruction, demonstrating robust and consistent 3D object generation."
                },
                "authors": [
                    {
                        "name": "Pufan Li"
                    },
                    {
                        "name": "Bi'an Du"
                    },
                    {
                        "name": "Wei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hu"
                },
                "author": "Wei Hu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14501v2",
                "updated": "2025-06-26T11:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    3,
                    13,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-19T03:48:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    48,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "Do Large Language Models Advocate for Inferentialism?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Advocate for Inferentialism?"
                },
                "summary": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims."
                },
                "authors": [
                    {
                        "name": "Yuzuki Arai"
                    },
                    {
                        "name": "Sho Tsugawa"
                    }
                ],
                "author_detail": {
                    "name": "Sho Tsugawa"
                },
                "author": "Sho Tsugawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21138v1",
                "updated": "2025-06-26T10:52:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    52,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T10:52:07Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    52,
                    7,
                    3,
                    177,
                    0
                ],
                "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets\n  for AI4RE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets\n  for AI4RE"
                },
                "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation."
                },
                "authors": [
                    {
                        "name": "Abdelkarim El-Hajjami"
                    },
                    {
                        "name": "Camille Salinesi"
                    }
                ],
                "author_detail": {
                    "name": "Camille Salinesi"
                },
                "author": "Camille Salinesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21812v2",
                "updated": "2025-06-26T10:51:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    51,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-04-30T17:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    15,
                    55,
                    2,
                    120,
                    0
                ],
                "title": "Easily Computed Marginal Likelihoods for Multivariate Mixture Models\n  Using the THAMES Estimator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easily Computed Marginal Likelihoods for Multivariate Mixture Models\n  Using the THAMES Estimator"
                },
                "summary": "We present a new version of the truncated harmonic mean estimator (THAMES)\nfor univariate or multivariate mixture models. The estimator computes the\nmarginal likelihood from Markov chain Monte Carlo (MCMC) samples, is\nconsistent, asymptotically normal and of finite variance. In addition, it is\ninvariant to label switching, does not require posterior samples from hidden\nallocation vectors, and is easily approximated, even for an arbitrarily high\nnumber of components. Its computational efficiency is based on an\nasymptotically optimal ordering of the parameter space, which can in turn be\nused to provide useful visualisations. We test it in simulation settings where\nthe true marginal likelihood is available analytically. It performs well\nagainst state-of-the-art competitors, even in multivariate settings with a high\nnumber of components. We demonstrate its utility for inference and model\nselection on univariate and multivariate data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new version of the truncated harmonic mean estimator (THAMES)\nfor univariate or multivariate mixture models. The estimator computes the\nmarginal likelihood from Markov chain Monte Carlo (MCMC) samples, is\nconsistent, asymptotically normal and of finite variance. In addition, it is\ninvariant to label switching, does not require posterior samples from hidden\nallocation vectors, and is easily approximated, even for an arbitrarily high\nnumber of components. Its computational efficiency is based on an\nasymptotically optimal ordering of the parameter space, which can in turn be\nused to provide useful visualisations. We test it in simulation settings where\nthe true marginal likelihood is available analytically. It performs well\nagainst state-of-the-art competitors, even in multivariate settings with a high\nnumber of components. We demonstrate its utility for inference and model\nselection on univariate and multivariate data sets."
                },
                "authors": [
                    {
                        "name": "Martin Metodiev"
                    },
                    {
                        "name": "Nicholas J. Irons"
                    },
                    {
                        "name": "Marie Perrot-Dockès"
                    },
                    {
                        "name": "Pierre Latouche"
                    },
                    {
                        "name": "Adrian E. Raftery"
                    }
                ],
                "author_detail": {
                    "name": "Adrian E. Raftery"
                },
                "author": "Adrian E. Raftery",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v3",
                "updated": "2025-06-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    49,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "26 pages main text and 2 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21121v1",
                "updated": "2025-06-26T09:46:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    9,
                    46,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T09:46:53Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    9,
                    46,
                    53,
                    3,
                    177,
                    0
                ],
                "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal\n  Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal\n  Trajectory Prediction"
                },
                "summary": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models."
                },
                "authors": [
                    {
                        "name": "Muleilan Pei"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Peiliang Li"
                    },
                    {
                        "name": "Shaojie Shen"
                    }
                ],
                "author_detail": {
                    "name": "Shaojie Shen"
                },
                "author": "Shaojie Shen",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21102v1",
                "updated": "2025-06-26T08:56:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    56,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:56:55Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    56,
                    55,
                    3,
                    177,
                    0
                ],
                "title": "Interpretable Hierarchical Concept Reasoning through Attention-Guided\n  Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Hierarchical Concept Reasoning through Attention-Guided\n  Graph Learning"
                },
                "summary": "Concept-Based Models (CBMs) are a class of deep learning models that provide\ninterpretability by explaining predictions through high-level concepts. These\nmodels first predict concepts and then use them to perform a downstream task.\nHowever, current CBMs offer interpretability only for the final task\nprediction, while the concept predictions themselves are typically made via\nblack-box neural networks. To address this limitation, we propose Hierarchical\nConcept Memory Reasoner (H-CMR), a new CBM that provides interpretability for\nboth concept and task predictions. H-CMR models relationships between concepts\nusing a learned directed acyclic graph, where edges represent logic rules that\ndefine concepts in terms of other concepts. During inference, H-CMR employs a\nneural attention mechanism to select a subset of these rules, which are then\napplied hierarchically to predict all concepts and the final task. Experimental\nresults demonstrate that H-CMR matches state-of-the-art performance while\nenabling strong human interaction through concept and model interventions. The\nformer can significantly improve accuracy at inference time, while the latter\ncan enhance data efficiency during training when background knowledge is\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Based Models (CBMs) are a class of deep learning models that provide\ninterpretability by explaining predictions through high-level concepts. These\nmodels first predict concepts and then use them to perform a downstream task.\nHowever, current CBMs offer interpretability only for the final task\nprediction, while the concept predictions themselves are typically made via\nblack-box neural networks. To address this limitation, we propose Hierarchical\nConcept Memory Reasoner (H-CMR), a new CBM that provides interpretability for\nboth concept and task predictions. H-CMR models relationships between concepts\nusing a learned directed acyclic graph, where edges represent logic rules that\ndefine concepts in terms of other concepts. During inference, H-CMR employs a\nneural attention mechanism to select a subset of these rules, which are then\napplied hierarchically to predict all concepts and the final task. Experimental\nresults demonstrate that H-CMR matches state-of-the-art performance while\nenabling strong human interaction through concept and model interventions. The\nformer can significantly improve accuracy at inference time, while the latter\ncan enhance data efficiency during training when background knowledge is\navailable."
                },
                "authors": [
                    {
                        "name": "David Debot"
                    },
                    {
                        "name": "Pietro Barbiero"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Giuseppe Marra"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Marra"
                },
                "author": "Giuseppe Marra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17361v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17361v4",
                "updated": "2025-06-26T08:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    54,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2024-06-25T08:24:35Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    24,
                    35,
                    1,
                    177,
                    0
                ],
                "title": "Tree-based variational inference for Poisson log-normal models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based variational inference for Poisson log-normal models"
                },
                "summary": "When studying ecosystems, hierarchical trees are often used to organize\nentities based on proximity criteria, such as the taxonomy in microbiology,\nsocial classes in geography, or product types in retail businesses, offering\nvaluable insights into entity relationships. Despite their significance,\ncurrent count-data models do not leverage this structured information. In\nparticular, the widely used Poisson log-normal (PLN) model, known for its\nability to model interactions between entities from count data, lacks the\npossibility to incorporate such hierarchical tree structures, limiting its\napplicability in domains characterized by such complexities. To address this\nmatter, we introduce the PLN-Tree model as an extension of the PLN model,\nspecifically designed for modeling hierarchical count data. By integrating\nstructured variational inference techniques, we propose an adapted training\nprocedure and establish identifiability results, enhancing both theoretical\nfoundations and practical interpretability. Experiments on synthetic datasets\nand human gut microbiome data highlight generative improvements when using\nPLN-Tree, demonstrating the practical interest of knowledge graphs like the\ntaxonomy in microbiome modeling. Additionally, we present a proof-of-concept\nimplication of the identifiability results by illustrating the practical\nbenefits of using identifiable features for classification tasks, showcasing\nthe versatility of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When studying ecosystems, hierarchical trees are often used to organize\nentities based on proximity criteria, such as the taxonomy in microbiology,\nsocial classes in geography, or product types in retail businesses, offering\nvaluable insights into entity relationships. Despite their significance,\ncurrent count-data models do not leverage this structured information. In\nparticular, the widely used Poisson log-normal (PLN) model, known for its\nability to model interactions between entities from count data, lacks the\npossibility to incorporate such hierarchical tree structures, limiting its\napplicability in domains characterized by such complexities. To address this\nmatter, we introduce the PLN-Tree model as an extension of the PLN model,\nspecifically designed for modeling hierarchical count data. By integrating\nstructured variational inference techniques, we propose an adapted training\nprocedure and establish identifiability results, enhancing both theoretical\nfoundations and practical interpretability. Experiments on synthetic datasets\nand human gut microbiome data highlight generative improvements when using\nPLN-Tree, demonstrating the practical interest of knowledge graphs like the\ntaxonomy in microbiome modeling. Additionally, we present a proof-of-concept\nimplication of the identifiability results by illustrating the practical\nbenefits of using identifiable features for classification tasks, showcasing\nthe versatility of the framework."
                },
                "authors": [
                    {
                        "name": "Alexandre Chaussard"
                    },
                    {
                        "name": "Anna Bonnet"
                    },
                    {
                        "name": "Elisabeth Gassiat"
                    },
                    {
                        "name": "Sylvain Le Corff"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Le Corff"
                },
                "arxiv_affiliation": "LPSM",
                "author": "Sylvain Le Corff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17361v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17361v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17443v4",
                "updated": "2025-06-26T08:46:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    46,
                    37,
                    3,
                    177,
                    0
                ],
                "published": "2024-08-30T17:52:55Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    52,
                    55,
                    4,
                    243,
                    0
                ],
                "title": "HERMES: temporal-coHERent long-forM understanding with Episodes and\n  Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: temporal-coHERent long-forM understanding with Episodes and\n  Semantics"
                },
                "summary": "Long-form video understanding presents unique challenges that extend beyond\ntraditional short-video analysis approaches, particularly in capturing\nlong-range dependencies, processing redundant information efficiently, and\nextracting high-level semantic concepts. To address these challenges, we\npropose a novel approach that more accurately reflects human cognition. This\npaper introduces HERMES: temporal-coHERent long-forM understanding with\nEpisodes and Semantics, featuring two versatile modules that can enhance\nexisting video-language models or operate as a standalone system. Our Episodic\nCOmpressor (ECO) efficiently aggregates representations from micro to\nsemi-macro levels, reducing computational overhead while preserving temporal\ndependencies. Our Semantics ReTRiever (SeTR) enriches these representations\nwith semantic information by focusing on broader context, dramatically reducing\nfeature dimensionality while preserving relevant macro-level information. We\ndemonstrate that these modules can be seamlessly integrated into existing SOTA\nmodels, consistently improving their performance while reducing inference\nlatency by up to 43% and memory usage by 46%. As a standalone system, HERMES\nachieves state-of-the-art performance across multiple long-video understanding\nbenchmarks in both zero-shot and fully-supervised settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding presents unique challenges that extend beyond\ntraditional short-video analysis approaches, particularly in capturing\nlong-range dependencies, processing redundant information efficiently, and\nextracting high-level semantic concepts. To address these challenges, we\npropose a novel approach that more accurately reflects human cognition. This\npaper introduces HERMES: temporal-coHERent long-forM understanding with\nEpisodes and Semantics, featuring two versatile modules that can enhance\nexisting video-language models or operate as a standalone system. Our Episodic\nCOmpressor (ECO) efficiently aggregates representations from micro to\nsemi-macro levels, reducing computational overhead while preserving temporal\ndependencies. Our Semantics ReTRiever (SeTR) enriches these representations\nwith semantic information by focusing on broader context, dramatically reducing\nfeature dimensionality while preserving relevant macro-level information. We\ndemonstrate that these modules can be seamlessly integrated into existing SOTA\nmodels, consistently improving their performance while reducing inference\nlatency by up to 43% and memory usage by 46%. As a standalone system, HERMES\nachieves state-of-the-art performance across multiple long-video understanding\nbenchmarks in both zero-shot and fully-supervised settings."
                },
                "authors": [
                    {
                        "name": "Gueter Josmy Faure"
                    },
                    {
                        "name": "Jia-Fong Yeh"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Hung-Ting Su"
                    },
                    {
                        "name": "Shang-Hong Lai"
                    },
                    {
                        "name": "Winston H. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Winston H. Hsu"
                },
                "author": "Winston H. Hsu",
                "arxiv_comment": "Accepted for ICCV 2025. Project page:\n  https://joslefaure.github.io/assets/html/hermes.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16889v2",
                "updated": "2025-06-26T08:38:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    38,
                    2,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-20T10:21:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    21,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of\n  Music Mastering Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of\n  Music Mastering Processors"
                },
                "summary": "Music mastering style transfer aims to model and apply the mastering\ncharacteristics of a reference track to a target track, simulating the\nprofessional mastering process. However, existing methods apply fixed\nprocessing based on a reference track, limiting users' ability to fine-tune the\nresults to match their artistic intent. In this paper, we introduce the\nITO-Master framework, a reference-based mastering style transfer system that\nintegrates Inference-Time Optimization (ITO) to enable finer user control over\nthe mastering process. By optimizing the reference embedding during inference,\nour approach allows users to refine the output dynamically, making micro-level\nadjustments to achieve more precise mastering results. We explore both\nblack-box and white-box methods for modeling mastering processors and\ndemonstrate that ITO improves mastering performance across different styles.\nThrough objective evaluation, subjective listening tests, and qualitative\nanalysis using text-based conditioning with CLAP embeddings, we validate that\nITO enhances mastering style similarity while offering increased adaptability.\nOur framework provides an effective and user-controllable solution for\nmastering style transfer, allowing users to refine their results beyond the\ninitial style transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music mastering style transfer aims to model and apply the mastering\ncharacteristics of a reference track to a target track, simulating the\nprofessional mastering process. However, existing methods apply fixed\nprocessing based on a reference track, limiting users' ability to fine-tune the\nresults to match their artistic intent. In this paper, we introduce the\nITO-Master framework, a reference-based mastering style transfer system that\nintegrates Inference-Time Optimization (ITO) to enable finer user control over\nthe mastering process. By optimizing the reference embedding during inference,\nour approach allows users to refine the output dynamically, making micro-level\nadjustments to achieve more precise mastering results. We explore both\nblack-box and white-box methods for modeling mastering processors and\ndemonstrate that ITO improves mastering performance across different styles.\nThrough objective evaluation, subjective listening tests, and qualitative\nanalysis using text-based conditioning with CLAP embeddings, we validate that\nITO enhances mastering style similarity while offering increased adaptability.\nOur framework provides an effective and user-controllable solution for\nmastering style transfer, allowing users to refine their results beyond the\ninitial style transfer."
                },
                "authors": [
                    {
                        "name": "Junghyun Koo"
                    },
                    {
                        "name": "Marco A. Martinez-Ramirez"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Giorgio Fabbro"
                    },
                    {
                        "name": "Michele Mancusi"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "ISMIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21091v1",
                "updated": "2025-06-26T08:34:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    34,
                    51,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:34:51Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    34,
                    51,
                    3,
                    177,
                    0
                ],
                "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and\n  Accurate Stereo Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and\n  Accurate Stereo Matching"
                },
                "summary": "Stereo matching has become an increasingly important component of modern\nautonomous systems. Developing deep learning-based stereo matching models that\ndeliver high accuracy while operating in real-time continues to be a major\nchallenge in computer vision. In the domain of cost-volume-based stereo\nmatching, accurate disparity estimation depends heavily on large-scale cost\nvolumes. However, such large volumes store substantial redundant information\nand also require computationally intensive aggregation units for processing and\nregression, making real-time performance unattainable. Conversely, small-scale\ncost volumes followed by lightweight aggregation units provide a promising\nroute for real-time performance, but lack sufficient information to ensure\nhighly accurate disparity estimation. To address this challenge, we propose the\nEnhanced Shuffle Mixer (ESM) to mitigate information loss associated with\nsmall-scale cost volumes. ESM restores critical details by integrating primary\nfeatures into the disparity upsampling unit. It quickly extracts features from\nthe initial disparity estimation and fuses them with image features. These\nfeatures are mixed by shuffling and layer splitting then refined through a\ncompact feature-guided hourglass network to recover more detailed scene\ngeometry. The ESM focuses on local contextual connectivity with a large\nreceptive field and low computational cost, leading to the reconstruction of a\nhighly accurate disparity map at real-time. The compact version of ESMStereo\nachieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX\nOrin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo matching has become an increasingly important component of modern\nautonomous systems. Developing deep learning-based stereo matching models that\ndeliver high accuracy while operating in real-time continues to be a major\nchallenge in computer vision. In the domain of cost-volume-based stereo\nmatching, accurate disparity estimation depends heavily on large-scale cost\nvolumes. However, such large volumes store substantial redundant information\nand also require computationally intensive aggregation units for processing and\nregression, making real-time performance unattainable. Conversely, small-scale\ncost volumes followed by lightweight aggregation units provide a promising\nroute for real-time performance, but lack sufficient information to ensure\nhighly accurate disparity estimation. To address this challenge, we propose the\nEnhanced Shuffle Mixer (ESM) to mitigate information loss associated with\nsmall-scale cost volumes. ESM restores critical details by integrating primary\nfeatures into the disparity upsampling unit. It quickly extracts features from\nthe initial disparity estimation and fuses them with image features. These\nfeatures are mixed by shuffling and layer splitting then refined through a\ncompact feature-guided hourglass network to recover more detailed scene\ngeometry. The ESM focuses on local contextual connectivity with a large\nreceptive field and low computational cost, leading to the reconstruction of a\nhighly accurate disparity map at real-time. The compact version of ESMStereo\nachieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX\nOrin."
                },
                "authors": [
                    {
                        "name": "Mahmoud Tahmasebi"
                    },
                    {
                        "name": "Saif Huq"
                    },
                    {
                        "name": "Kevin Meehan"
                    },
                    {
                        "name": "Marion McAfee"
                    }
                ],
                "author_detail": {
                    "name": "Marion McAfee"
                },
                "author": "Marion McAfee",
                "arxiv_comment": "Under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21080v1",
                "updated": "2025-06-26T08:09:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    9,
                    16,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:09:16Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    9,
                    16,
                    3,
                    177,
                    0
                ],
                "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for\n  Efficient Egocentric Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for\n  Efficient Egocentric Perception"
                },
                "summary": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Sanjoy Chowdhury"
                    },
                    {
                        "name": "Subrata Biswas"
                    },
                    {
                        "name": "Sayan Nag"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Calvin Murdock"
                    },
                    {
                        "name": "Ishwarya Ananthabhotla"
                    },
                    {
                        "name": "Yijun Qian"
                    },
                    {
                        "name": "Vamsi Krishna Ithapu"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Ruohan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Ruohan Gao"
                },
                "author": "Ruohan Gao",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21074v1",
                "updated": "2025-06-26T07:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    59,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T07:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    59,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via\n  Dynamic Frame Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via\n  Dynamic Frame Rate"
                },
                "summary": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/."
                },
                "authors": [
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Chongtian Shao"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "16 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21071v1",
                "updated": "2025-06-26T07:45:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    45,
                    15,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T07:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    45,
                    15,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge\n  Graph"
                },
                "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Zai Zhang"
                    },
                    {
                        "name": "Hao Qian"
                    },
                    {
                        "name": "Chunjing Gan"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Bin Shi"
                    },
                    {
                        "name": "Bo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dong"
                },
                "author": "Bo Dong",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21053v1",
                "updated": "2025-06-26T06:59:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    59,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:59:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    59,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for\n  Conversational Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for\n  Conversational Stance Detection"
                },
                "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection."
                },
                "authors": [
                    {
                        "name": "Fuqiang Niu"
                    },
                    {
                        "name": "Genan Dai"
                    },
                    {
                        "name": "Yisha Lu"
                    },
                    {
                        "name": "Jiayu Liao"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Hu Huang"
                    },
                    {
                        "name": "Bowen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhang"
                },
                "author": "Bowen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.21550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21550v1",
                "updated": "2025-06-26T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale"
                },
                "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection."
                },
                "authors": [
                    {
                        "name": "Xiaona Zhou"
                    },
                    {
                        "name": "Constantin Brif"
                    },
                    {
                        "name": "Ismini Lourentzou"
                    }
                ],
                "author_detail": {
                    "name": "Ismini Lourentzou"
                },
                "author": "Ismini Lourentzou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21551v1",
                "updated": "2025-06-26T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    59,
                    58,
                    3,
                    177,
                    0
                ],
                "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test"
                },
                "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Chenrui Fan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21536v1",
                "updated": "2025-06-26T17:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:54:42Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    42,
                    3,
                    177,
                    0
                ],
                "title": "PsyLite Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyLite Technical Report"
                },
                "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Fangjun Ding"
                    },
                    {
                        "name": "Renyu Zhang"
                    },
                    {
                        "name": "Xinyu Feng"
                    },
                    {
                        "name": "Chengye Xie"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Yanting Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanting Zhang"
                },
                "author": "Yanting Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21535v1",
                "updated": "2025-06-26T17:54:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    20,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:54:20Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    54,
                    20,
                    3,
                    177,
                    0
                ],
                "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Design Space of 3D MLLMs for CT Report Generation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution"
                },
                "authors": [
                    {
                        "name": "Mohammed Baharoon"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Congyu Fang"
                    },
                    {
                        "name": "Augustin Toma"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21532v1",
                "updated": "2025-06-26T17:52:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    52,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:52:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    52,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets"
                },
                "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat"
                },
                "authors": [
                    {
                        "name": "Akshay Paruchuri"
                    },
                    {
                        "name": "Maryam Aziz"
                    },
                    {
                        "name": "Rohit Vartak"
                    },
                    {
                        "name": "Ayman Ali"
                    },
                    {
                        "name": "Best Uchehara"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Ishan Chatterjee"
                    },
                    {
                        "name": "Monica Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Monica Agrawal"
                },
                "author": "Monica Agrawal",
                "arxiv_comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09587v2",
                "updated": "2025-06-26T17:51:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    51,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-12T18:55:53Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    55,
                    53,
                    3,
                    347,
                    0
                ],
                "title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets\n  in 50+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets\n  in 50+ Languages"
                },
                "summary": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task."
                },
                "authors": [
                    {
                        "name": "Chester Palen-Michel"
                    },
                    {
                        "name": "Maxwell Pickering"
                    },
                    {
                        "name": "Maya Kruse"
                    },
                    {
                        "name": "Jonne Sälevä"
                    },
                    {
                        "name": "Constantine Lignos"
                    }
                ],
                "author_detail": {
                    "name": "Constantine Lignos"
                },
                "author": "Constantine Lignos",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08165v2",
                "updated": "2025-06-26T17:48:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    48,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2024-10-10T17:44:13Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    44,
                    13,
                    3,
                    284,
                    0
                ],
                "title": "Chain-of-Sketch: Enabling Global Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Sketch: Enabling Global Visual Reasoning"
                },
                "summary": "Modern vision models have achieved remarkable success in benchmarks where\nlocal features provide critical information about the target. There is now a\ngrowing interest in tackling tasks requiring more global reasoning, where local\nfeatures do not provide significant information. Minsky and Papert put forward\nsuch tasks in 1969 with their connectivity study, exposing the limitations of\nthe perceptron model. In this paper, we introduce an expanded set of global\nvisual datasets involving graphs, strings, mazes, and image grids. We show that\nlarge vision models still struggle to learn these tasks efficiently. Similarly,\nstate-of-the-art multi-modal LLMs perform poorly on these datasets. We explain\nthis learning inefficiency by means of the 'globality degree' measure. To\nmitigate this, we propose a method called chain-of-sketch (CoS). Similar to the\nchain-of-thought and scratchpad techniques used in language models, CoS breaks\nthe original task into intermediate visual steps to help learn a complex task.\nIn addition, we show that not all CoS strategies perform equally well. Our key\ninsight is to impose a Markovian structure on the CoS frames. This leads to the\nintroduction of 'inductive CoS' which achieves better out-of-distribution\ngeneralization and performs well even with smaller models compared to\nnon-inductive variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern vision models have achieved remarkable success in benchmarks where\nlocal features provide critical information about the target. There is now a\ngrowing interest in tackling tasks requiring more global reasoning, where local\nfeatures do not provide significant information. Minsky and Papert put forward\nsuch tasks in 1969 with their connectivity study, exposing the limitations of\nthe perceptron model. In this paper, we introduce an expanded set of global\nvisual datasets involving graphs, strings, mazes, and image grids. We show that\nlarge vision models still struggle to learn these tasks efficiently. Similarly,\nstate-of-the-art multi-modal LLMs perform poorly on these datasets. We explain\nthis learning inefficiency by means of the 'globality degree' measure. To\nmitigate this, we propose a method called chain-of-sketch (CoS). Similar to the\nchain-of-thought and scratchpad techniques used in language models, CoS breaks\nthe original task into intermediate visual steps to help learn a complex task.\nIn addition, we show that not all CoS strategies perform equally well. Our key\ninsight is to impose a Markovian structure on the CoS frames. This leads to the\nintroduction of 'inductive CoS' which achieves better out-of-distribution\ngeneralization and performs well even with smaller models compared to\nnon-inductive variants."
                },
                "authors": [
                    {
                        "name": "Aryo Lotfi"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Abbe"
                },
                "author": "Emmanuel Abbe",
                "arxiv_comment": "additional experiments added, title changed from \"Visual Scratchpads:\n  Enabling Global Reasoning in Vision\" to \"Chain-of-Sketch: Enabling Global\n  Visual Reasoning\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21521v1",
                "updated": "2025-06-26T17:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    41,
                    35,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    41,
                    35,
                    3,
                    177,
                    0
                ],
                "title": "Potemkin Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potemkin Understanding in Large Language Models"
                },
                "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Marina Mancoridis"
                    },
                    {
                        "name": "Bec Weeks"
                    },
                    {
                        "name": "Keyon Vafa"
                    },
                    {
                        "name": "Sendhil Mullainathan"
                    }
                ],
                "author_detail": {
                    "name": "Sendhil Mullainathan"
                },
                "author": "Sendhil Mullainathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03666v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03666v4",
                "updated": "2025-06-26T17:36:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    36,
                    29,
                    3,
                    177,
                    0
                ],
                "published": "2024-02-06T03:39:44Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    39,
                    44,
                    1,
                    37,
                    0
                ],
                "title": "QuEST: Low-bit Diffusion Model Quantization via Efficient Selective\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuEST: Low-bit Diffusion Model Quantization via Efficient Selective\n  Finetuning"
                },
                "summary": "The practical deployment of diffusion models is still hindered by the high\nmemory and computational overhead. Although quantization paves a way for model\ncompression and acceleration, existing methods face challenges in achieving\nlow-bit quantization efficiently. In this paper, we identify imbalanced\nactivation distributions as a primary source of quantization difficulty, and\npropose to adjust these distributions through weight finetuning to be more\nquantization-friendly. We provide both theoretical and empirical evidence\nsupporting finetuning as a practical and reliable solution. Building on this\napproach, we further distinguish two critical types of quantized layers: those\nresponsible for retaining essential temporal information and those particularly\nsensitive to bit-width reduction. By selectively finetuning these layers under\nboth local and global supervision, we mitigate performance degradation while\nenhancing quantization efficiency. Our method demonstrates its efficacy across\nthree high-resolution image generation tasks, obtaining state-of-the-art\nperformance across multiple bit-width settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical deployment of diffusion models is still hindered by the high\nmemory and computational overhead. Although quantization paves a way for model\ncompression and acceleration, existing methods face challenges in achieving\nlow-bit quantization efficiently. In this paper, we identify imbalanced\nactivation distributions as a primary source of quantization difficulty, and\npropose to adjust these distributions through weight finetuning to be more\nquantization-friendly. We provide both theoretical and empirical evidence\nsupporting finetuning as a practical and reliable solution. Building on this\napproach, we further distinguish two critical types of quantized layers: those\nresponsible for retaining essential temporal information and those particularly\nsensitive to bit-width reduction. By selectively finetuning these layers under\nboth local and global supervision, we mitigate performance degradation while\nenhancing quantization efficiency. Our method demonstrates its efficacy across\nthree high-resolution image generation tasks, obtaining state-of-the-art\nperformance across multiple bit-width settings."
                },
                "authors": [
                    {
                        "name": "Haoxuan Wang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Yan Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yan Yan"
                },
                "author": "Yan Yan",
                "arxiv_comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/QuEST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03666v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03666v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19780v2",
                "updated": "2025-06-26T17:28:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    28,
                    25,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T16:47:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    47,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference\n  Alignment"
                },
                "summary": "While large-scale unsupervised language models (LMs) capture broad world\nknowledge and reasoning capabilities, steering their behavior toward desired\nobjectives remains challenging due to the lack of explicit supervision.\nExisting alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on training a reward model and performing reinforcement\nlearning to align with human preferences. However, RLHF is often\ncomputationally intensive, unstable, and sensitive to hyperparameters.\n  To address these limitations, Direct Preference Optimization (DPO) was\nintroduced as a lightweight and stable alternative, enabling direct alignment\nof language models with pairwise preference data via classification loss.\nHowever, DPO and its extensions generally assume a single static preference\ndistribution, limiting flexibility in multi-objective or dynamic alignment\nsettings.\n  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted\nListwise DPO, which extends DPO to incorporate multiple human preference\ndimensions (e.g., helpfulness, harmlessness, informativeness) and enables\ndynamic interpolation through a controllable simplex-weighted formulation. Our\nmethod supports both listwise preference feedback and flexible alignment across\nvarying user intents without re-training. Empirical and theoretical analysis\ndemonstrates that our method is as effective as traditional DPO on static\nobjectives while offering greater generality and adaptability for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large-scale unsupervised language models (LMs) capture broad world\nknowledge and reasoning capabilities, steering their behavior toward desired\nobjectives remains challenging due to the lack of explicit supervision.\nExisting alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on training a reward model and performing reinforcement\nlearning to align with human preferences. However, RLHF is often\ncomputationally intensive, unstable, and sensitive to hyperparameters.\n  To address these limitations, Direct Preference Optimization (DPO) was\nintroduced as a lightweight and stable alternative, enabling direct alignment\nof language models with pairwise preference data via classification loss.\nHowever, DPO and its extensions generally assume a single static preference\ndistribution, limiting flexibility in multi-objective or dynamic alignment\nsettings.\n  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted\nListwise DPO, which extends DPO to incorporate multiple human preference\ndimensions (e.g., helpfulness, harmlessness, informativeness) and enables\ndynamic interpolation through a controllable simplex-weighted formulation. Our\nmethod supports both listwise preference feedback and flexible alignment across\nvarying user intents without re-training. Empirical and theoretical analysis\ndemonstrates that our method is as effective as traditional DPO on static\nobjectives while offering greater generality and adaptability for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yuhui Sun"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Zixi Li"
                    },
                    {
                        "name": "Jinman Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Zhao"
                },
                "arxiv_affiliation": "University of Toronto",
                "author": "Jinman Zhao",
                "arxiv_comment": "10 pages, 4 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11751v2",
                "updated": "2025-06-26T17:26:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    26,
                    44,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-16T23:24:45Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    23,
                    24,
                    45,
                    4,
                    136,
                    0
                ],
                "title": "Towards Navigation-Grade and Deployable Optomechanical Accelerometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Navigation-Grade and Deployable Optomechanical Accelerometry"
                },
                "summary": "We design and experimentally demonstrate an architecture for achieving\nnavigation-grade, fiber-packaged optomechanical accelerometers that can operate\nwith a large dynamic range, over a wide temperature range, and without\nsophisticated laser sources. Our accelerometer architecture is based on a novel\nset of design principles that take advantage of the strengths of optomechanical\naccelerometry while eliminating many of its historical weaknesses. Displacement\nreadout is provided by an integrated, differential strain-sensing Mach-Zehnder\ninterferometer (DSMZI) attached to an ultra-rigid, bulk-micromachined proof\nmass having a 93.4 kHz fundamental resonance frequency (22.5 pm/g\ndisplacement). Despite the extreme rigidity, the high displacement sensitivity\nprovides an insertion loss limited 4.2 $\\mu g/\\sqrt{\\mathrm{Hz}}$ acceleration\nresolution, with a straight-forward path to achieving 330 $n\ng/\\sqrt{\\mathrm{Hz}}$ by improving the fiber-to-chip coupling. Further, we show\nthat the combination of high rigidity and intrinsic differential optical\nreadout makes the device insensitive to the common causes of bias instability,\nand we measure a bias instability of 6.3 $\\mu g$ at 243 seconds. The DSMZI\nprovides a 17 nm optical bandwidth and a temperature operating range of greater\nthan 20 $^\\circ\\mathrm{C}$, both orders of magnitude larger than previous\ndemonstrations of optomechanical accelerometers. The high rigidity and large\noptical bandwidth yield an expected dynamic range of 165.4 dB. The combination\nof high acceleration resolution, high dynamic range, low bias instability, and\nintrinsic insensitivity to wavelength, temperature, and package stresses makes\nour device well suited for deployment in realistic environments demanded by\nreal-world applications and demonstrates a path for optomechanical\naccelerometers to ultimately exceed the performance of all other chip-based\naccelerometers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design and experimentally demonstrate an architecture for achieving\nnavigation-grade, fiber-packaged optomechanical accelerometers that can operate\nwith a large dynamic range, over a wide temperature range, and without\nsophisticated laser sources. Our accelerometer architecture is based on a novel\nset of design principles that take advantage of the strengths of optomechanical\naccelerometry while eliminating many of its historical weaknesses. Displacement\nreadout is provided by an integrated, differential strain-sensing Mach-Zehnder\ninterferometer (DSMZI) attached to an ultra-rigid, bulk-micromachined proof\nmass having a 93.4 kHz fundamental resonance frequency (22.5 pm/g\ndisplacement). Despite the extreme rigidity, the high displacement sensitivity\nprovides an insertion loss limited 4.2 $\\mu g/\\sqrt{\\mathrm{Hz}}$ acceleration\nresolution, with a straight-forward path to achieving 330 $n\ng/\\sqrt{\\mathrm{Hz}}$ by improving the fiber-to-chip coupling. Further, we show\nthat the combination of high rigidity and intrinsic differential optical\nreadout makes the device insensitive to the common causes of bias instability,\nand we measure a bias instability of 6.3 $\\mu g$ at 243 seconds. The DSMZI\nprovides a 17 nm optical bandwidth and a temperature operating range of greater\nthan 20 $^\\circ\\mathrm{C}$, both orders of magnitude larger than previous\ndemonstrations of optomechanical accelerometers. The high rigidity and large\noptical bandwidth yield an expected dynamic range of 165.4 dB. The combination\nof high acceleration resolution, high dynamic range, low bias instability, and\nintrinsic insensitivity to wavelength, temperature, and package stresses makes\nour device well suited for deployment in realistic environments demanded by\nreal-world applications and demonstrates a path for optomechanical\naccelerometers to ultimately exceed the performance of all other chip-based\naccelerometers."
                },
                "authors": [
                    {
                        "name": "Chang Ge"
                    },
                    {
                        "name": "Daniel Dominguez"
                    },
                    {
                        "name": "Allison Rubenok"
                    },
                    {
                        "name": "Michael Miller"
                    },
                    {
                        "name": "Matt Eichenfield"
                    }
                ],
                "author_detail": {
                    "name": "Matt Eichenfield"
                },
                "author": "Matt Eichenfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21497v1",
                "updated": "2025-06-26T17:26:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    26,
                    17,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:26:17Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    26,
                    17,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing User Engagement in Socially-Driven Dialogue through\n  Interactive LLM Alignments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing User Engagement in Socially-Driven Dialogue through\n  Interactive LLM Alignments"
                },
                "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs."
                },
                "authors": [
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Changhe Song"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21495v1",
                "updated": "2025-06-26T17:25:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    25,
                    49,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T17:25:49Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    25,
                    49,
                    3,
                    177,
                    0
                ],
                "title": "Bridging Offline and Online Reinforcement Learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Offline and Online Reinforcement Learning for LLMs"
                },
                "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types."
                },
                "authors": [
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Ilia Kulikov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Kulikov"
                },
                "author": "Ilia Kulikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02398v3",
                "updated": "2025-06-26T17:22:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    22,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2024-11-04T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages"
                },
                "summary": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval."
                },
                "authors": [
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Khyati Mahajan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Maheshwary"
                },
                "author": "Rishabh Maheshwary",
                "arxiv_doi": "10.18653/v1/2025.naacl-long.599",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.naacl-long.599",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to NAACL 2025 (Main Conference). This version contains minor\n  improvements to the camera-ready",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies (Volume 1: Long Papers)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18959v2",
                "updated": "2025-06-26T17:18:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    0,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-23T17:27:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    27,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Web Search towards Agentic Deep Research: Incentivizing Search with\n  Reasoning Agents"
                },
                "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chenxuan Xie"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Xinyang Zhang"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21467v1",
                "updated": "2025-06-26T16:54:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    39,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:54:39Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    39,
                    3,
                    177,
                    0
                ],
                "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery\n  Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Reuseable Cloud Configuration Search Using Discovery\n  Spaces"
                },
                "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%."
                },
                "authors": [
                    {
                        "name": "Michael Johnston"
                    },
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Christoph Hagleitner"
                    },
                    {
                        "name": "Alessandro Pomponio"
                    },
                    {
                        "name": "Vassilis Vassiliadis"
                    },
                    {
                        "name": "Christian Pinto"
                    },
                    {
                        "name": "Srikumar Venugopal"
                    }
                ],
                "author_detail": {
                    "name": "Srikumar Venugopal"
                },
                "author": "Srikumar Venugopal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18019v2",
                "updated": "2025-06-26T16:54:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    54,
                    14,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-22T12:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    12,
                    59,
                    12,
                    6,
                    173,
                    0
                ],
                "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities"
                },
                "summary": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Siwen Wang"
                    },
                    {
                        "name": "Weizhi Chen"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Fakhri Karray"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18728v2",
                "updated": "2025-06-26T16:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    35,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-23T15:05:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries"
                },
                "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Pratiksha Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Pratiksha Thaker"
                },
                "author": "Pratiksha Thaker",
                "arxiv_comment": "In Adaptive Foundation Models: Evolving AI for Personalized and\n  Efficient Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21445v1",
                "updated": "2025-06-26T16:31:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    31,
                    10,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:31:10Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    31,
                    10,
                    3,
                    177,
                    0
                ],
                "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English"
                },
                "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    },
                    {
                        "name": "William Tai"
                    }
                ],
                "author_detail": {
                    "name": "William Tai"
                },
                "author": "William Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21443v1",
                "updated": "2025-06-26T16:29:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    29,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:29:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    29,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection"
                },
                "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications."
                },
                "authors": [
                    {
                        "name": "Ali Şenol"
                    },
                    {
                        "name": "Garima Agrawal"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21657v3",
                "updated": "2025-06-26T16:16:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    16,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-27T18:32:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    32,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations"
                },
                "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."
                },
                "authors": [
                    {
                        "name": "Zeinab Dehghani"
                    },
                    {
                        "name": "Mohammed Naveed Akram"
                    },
                    {
                        "name": "Koorosh Aslansefat"
                    },
                    {
                        "name": "Adil Khan"
                    }
                ],
                "author_detail": {
                    "name": "Adil Khan"
                },
                "author": "Adil Khan",
                "arxiv_comment": "The submission contains incorrect references that require substantial\n  revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11872v2",
                "updated": "2025-06-26T16:15:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    15,
                    31,
                    3,
                    177,
                    0
                ],
                "published": "2024-03-18T15:26:05Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    15,
                    26,
                    5,
                    0,
                    78,
                    0
                ],
                "title": "Graph Neural Network for Neutrino Physics Event Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Network for Neutrino Physics Event Reconstruction"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a\nwealth of high-resolution information on particle interactions, and leveraging\nthat information to its full potential requires sophisticated automated\nreconstruction techniques. This article describes NuGraph2, a Graph Neural\nNetwork (GNN) for low-level reconstruction of simulated neutrino interactions\nin a LArTPC detector. Simulated neutrino interactions in the MicroBooNE\ndetector geometry are described as heterogeneous graphs, with energy\ndepositions on each detector plane forming nodes on planar subgraphs. The\nnetwork utilizes a multi-head attention message-passing mechanism to perform\nbackground filtering and semantic labelling on these graph nodes, identifying\nthose associated with the primary physics interaction with 98.0\\% efficiency\nand labelling them according to particle type with 94.9\\% efficiency. The\nnetwork operates directly on detector observables across multiple 2D\nrepresentations, but utilizes a 3D-context-aware mechanism to encourage\nconsistency between these representations. Model inference takes 0.12~s/event\non a CPU, and 0.005s/event batched on a GPU. This architecture is designed to\nbe a general-purpose solution for particle reconstruction in neutrino physics,\nwith the potential for deployment across a broad range of detector\ntechnologies, and offers a core convolution engine that can be leveraged for a\nvariety of tasks beyond the two described in this article.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a\nwealth of high-resolution information on particle interactions, and leveraging\nthat information to its full potential requires sophisticated automated\nreconstruction techniques. This article describes NuGraph2, a Graph Neural\nNetwork (GNN) for low-level reconstruction of simulated neutrino interactions\nin a LArTPC detector. Simulated neutrino interactions in the MicroBooNE\ndetector geometry are described as heterogeneous graphs, with energy\ndepositions on each detector plane forming nodes on planar subgraphs. The\nnetwork utilizes a multi-head attention message-passing mechanism to perform\nbackground filtering and semantic labelling on these graph nodes, identifying\nthose associated with the primary physics interaction with 98.0\\% efficiency\nand labelling them according to particle type with 94.9\\% efficiency. The\nnetwork operates directly on detector observables across multiple 2D\nrepresentations, but utilizes a 3D-context-aware mechanism to encourage\nconsistency between these representations. Model inference takes 0.12~s/event\non a CPU, and 0.005s/event batched on a GPU. This architecture is designed to\nbe a general-purpose solution for particle reconstruction in neutrino physics,\nwith the potential for deployment across a broad range of detector\ntechnologies, and offers a core convolution engine that can be leveraged for a\nvariety of tasks beyond the two described in this article."
                },
                "authors": [
                    {
                        "name": "V Hewes"
                    },
                    {
                        "name": "Adam Aurisano"
                    },
                    {
                        "name": "Giuseppe Cerati"
                    },
                    {
                        "name": "Jim Kowalkowski"
                    },
                    {
                        "name": "Claire Lee"
                    },
                    {
                        "name": "Wei-keng Liao"
                    },
                    {
                        "name": "Daniel Grzenda"
                    },
                    {
                        "name": "Kaushal Gumpula"
                    },
                    {
                        "name": "Xiaohe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohe Zhang"
                },
                "author": "Xiaohe Zhang",
                "arxiv_doi": "10.1103/PhysRevD.110.032008",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.032008",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures, published in Physical Review D",
                "arxiv_journal_ref": "Phys.Rev.D 110 (2024) 3, 032008",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15830v2",
                "updated": "2025-06-26T16:14:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    14,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-18T19:17:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    19,
                    17,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Training through Information Geometry and Quantum Metrics"
                },
                "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems."
                },
                "authors": [
                    {
                        "name": "Riccardo Di Sipio"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Di Sipio"
                },
                "author": "Riccardo Di Sipio",
                "arxiv_comment": "9 pages, 1 figure(s)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04202v3",
                "updated": "2025-06-26T16:09:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    9,
                    36,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-04T17:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracLLM: A Generic Framework for Attributing Long Context LLMs"
                },
                "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM."
                },
                "authors": [
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21422v1",
                "updated": "2025-06-26T16:07:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    7,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T16:07:07Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    16,
                    7,
                    7,
                    3,
                    177,
                    0
                ],
                "title": "Carbon-Aware Microservice Deployment for Optimal User Experience on a\n  Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon-Aware Microservice Deployment for Optimal User Experience on a\n  Budget"
                },
                "summary": "The carbon footprint of data centers has recently become a critical concern.\nSo far, most carbon-aware strategies have focused on leveraging the flexibility\nof scheduling decisions for batch processing by shifting the time and location\nof workload executions. However, such approaches cannot be applied to\nservice-oriented cloud applications, since they have to be reachable at every\npoint in time and often at low latencies. We propose a carbon-aware approach\nfor operating microservices under hourly carbon budgets. By choosing the most\nappropriate version and horizontal scaleout for each microservice, our strategy\nmaximizes user experience and revenue while staying within budget constraints.\nExperiments across various application configurations and carbon budgets\ndemonstrate that the approach adapts properly to changing workloads and carbon\nintensities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The carbon footprint of data centers has recently become a critical concern.\nSo far, most carbon-aware strategies have focused on leveraging the flexibility\nof scheduling decisions for batch processing by shifting the time and location\nof workload executions. However, such approaches cannot be applied to\nservice-oriented cloud applications, since they have to be reachable at every\npoint in time and often at low latencies. We propose a carbon-aware approach\nfor operating microservices under hourly carbon budgets. By choosing the most\nappropriate version and horizontal scaleout for each microservice, our strategy\nmaximizes user experience and revenue while staying within budget constraints.\nExperiments across various application configurations and carbon budgets\ndemonstrate that the approach adapts properly to changing workloads and carbon\nintensities."
                },
                "authors": [
                    {
                        "name": "Kevin Kreutz"
                    },
                    {
                        "name": "Philipp Wiesner"
                    },
                    {
                        "name": "Monica Vitali"
                    }
                ],
                "author_detail": {
                    "name": "Monica Vitali"
                },
                "author": "Monica Vitali",
                "arxiv_comment": "LOCO 2024, December 3, 2024, Glasgow/Online",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21408v1",
                "updated": "2025-06-26T15:54:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    54,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:54:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    54,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference"
                },
                "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work."
                },
                "authors": [
                    {
                        "name": "Colin Samplawski"
                    },
                    {
                        "name": "Adam D. Cobb"
                    },
                    {
                        "name": "Manoj Acharya"
                    },
                    {
                        "name": "Ramneet Kaur"
                    },
                    {
                        "name": "Susmit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Susmit Jha"
                },
                "author": "Susmit Jha",
                "arxiv_comment": "Accepted at UAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21384v1",
                "updated": "2025-06-26T15:35:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    35,
                    12,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:35:12Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    35,
                    12,
                    3,
                    177,
                    0
                ],
                "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation"
                },
                "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries."
                },
                "authors": [
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Yuyao Zhang"
                    },
                    {
                        "name": "Mengjie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Mengjie Deng"
                },
                "author": "Mengjie Deng",
                "arxiv_comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21369v1",
                "updated": "2025-06-26T15:18:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    18,
                    0,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:18:00Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    18,
                    0,
                    3,
                    177,
                    0
                ],
                "title": "GenFlow: Interactive Modular System for Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenFlow: Interactive Modular System for Image Generation"
                },
                "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art."
                },
                "authors": [
                    {
                        "name": "Duc-Hung Nguyen"
                    },
                    {
                        "name": "Huu-Phuc Huynh"
                    },
                    {
                        "name": "Minh-Triet Tran"
                    },
                    {
                        "name": "Trung-Nghia Le"
                    }
                ],
                "author_detail": {
                    "name": "Trung-Nghia Le"
                },
                "author": "Trung-Nghia Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15858v2",
                "updated": "2025-06-26T15:16:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    16,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-21T01:26:23Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    26,
                    23,
                    2,
                    141,
                    0
                ],
                "title": "Large Language Model-Powered Agent for C to Rust Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Powered Agent for C to Rust Code Translation"
                },
                "summary": "The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks."
                },
                "authors": [
                    {
                        "name": "HoHyun Sim"
                    },
                    {
                        "name": "Hyeonjoong Cho"
                    },
                    {
                        "name": "Yeonghyeon Go"
                    },
                    {
                        "name": "Zhoulai Fu"
                    },
                    {
                        "name": "Ali Shokri"
                    },
                    {
                        "name": "Binoy Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Binoy Ravindran"
                },
                "author": "Binoy Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21366v1",
                "updated": "2025-06-26T15:15:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    15,
                    56,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:15:56Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    15,
                    56,
                    3,
                    177,
                    0
                ],
                "title": "Computational Design of Two-Dimensional MoSi$_2$N$_4$ Family\n  Field-Effect Transistor for Future Ångström-Scale CMOS Technology Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Design of Two-Dimensional MoSi$_2$N$_4$ Family\n  Field-Effect Transistor for Future Ångström-Scale CMOS Technology Nodes"
                },
                "summary": "Advancing complementary metal-oxide-semiconductor (CMOS) technology into the\nsub-1-nm angstr\\\"om-scale technology nodes is expected to involve alternative\nsemiconductor channel materials, as silicon transistors encounter severe\nperformance degradation at physical gate lengths below 10 nm. Two-dimensional\n(2D) semiconductors have emerged as strong candidates for overcoming\nshort-channel effects due to their atomically thin bodies, which inherently\nsuppress electrostatic leakage and improve gate control in aggressively scaled\nfield-effect transistors (FETs). Among the growing library of 2D materials, the\nMoSi$_2$N$_4$ family -- a synthetic septuple-layered materials -- has attracted\nincreasing attention for its remarkable ambient stability, suitable bandgaps,\nand favorable carrier transport characteristics, making it a promising platform\nfor next-generation transistors. While experimental realization of sub-10-nm 2D\nFETs remains technologically demanding, computational device simulation using\nfirst-principles density functional theory combined with nonequilibrium Green's\nfunction transport simulations provide a powerful and cost-effective route for\nexploring the performance limits and optimal design of ultrascaled FET. This\nreview consolidates the current progress in the computational design of\nMoSi$_2$N$_4$ family FETs. We review the physical properties of MoSi$_2$N$_4$\nthat makes them compelling candidates for transistor applications, as well as\nthe simulated device performance and optimization strategy of MoSi$_2$N$_4$\nfamily FETs. Finally, we identify key challenges and research gaps, and outline\nfuture directions that could accelerate the practical deployment of\nMoSi$_2$N$_4$ family FET in the angstr\\\"om-scale CMOS era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing complementary metal-oxide-semiconductor (CMOS) technology into the\nsub-1-nm angstr\\\"om-scale technology nodes is expected to involve alternative\nsemiconductor channel materials, as silicon transistors encounter severe\nperformance degradation at physical gate lengths below 10 nm. Two-dimensional\n(2D) semiconductors have emerged as strong candidates for overcoming\nshort-channel effects due to their atomically thin bodies, which inherently\nsuppress electrostatic leakage and improve gate control in aggressively scaled\nfield-effect transistors (FETs). Among the growing library of 2D materials, the\nMoSi$_2$N$_4$ family -- a synthetic septuple-layered materials -- has attracted\nincreasing attention for its remarkable ambient stability, suitable bandgaps,\nand favorable carrier transport characteristics, making it a promising platform\nfor next-generation transistors. While experimental realization of sub-10-nm 2D\nFETs remains technologically demanding, computational device simulation using\nfirst-principles density functional theory combined with nonequilibrium Green's\nfunction transport simulations provide a powerful and cost-effective route for\nexploring the performance limits and optimal design of ultrascaled FET. This\nreview consolidates the current progress in the computational design of\nMoSi$_2$N$_4$ family FETs. We review the physical properties of MoSi$_2$N$_4$\nthat makes them compelling candidates for transistor applications, as well as\nthe simulated device performance and optimization strategy of MoSi$_2$N$_4$\nfamily FETs. Finally, we identify key challenges and research gaps, and outline\nfuture directions that could accelerate the practical deployment of\nMoSi$_2$N$_4$ family FET in the angstr\\\"om-scale CMOS era."
                },
                "authors": [
                    {
                        "name": "Che Chen Tho"
                    },
                    {
                        "name": "Zongmeng Yang"
                    },
                    {
                        "name": "Shibo Fang"
                    },
                    {
                        "name": "Shiying Guo"
                    },
                    {
                        "name": "Liemao Cao"
                    },
                    {
                        "name": "Chit Siong Lau"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Shengli Zhang"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "L. K. Ang"
                    },
                    {
                        "name": "Lain-Jong Li"
                    },
                    {
                        "name": "Yee Sin Ang"
                    }
                ],
                "author_detail": {
                    "name": "Yee Sin Ang"
                },
                "author": "Yee Sin Ang",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21360v1",
                "updated": "2025-06-26T15:10:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    10,
                    24,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T15:10:24Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    10,
                    24,
                    3,
                    177,
                    0
                ],
                "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement."
                },
                "authors": [
                    {
                        "name": "Fangzhou Dong"
                    },
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Yingpeng Sang"
                    },
                    {
                        "name": "Hong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Shen"
                },
                "author": "Hong Shen",
                "arxiv_comment": "Accepted in CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12113v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12113v3",
                "updated": "2025-06-26T15:09:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    9,
                    42,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-13T13:39:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    39,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Semantic Preprocessing for LLM-based Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Preprocessing for LLM-based Malware Analysis"
                },
                "summary": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality."
                },
                "authors": [
                    {
                        "name": "Benjamin Marais"
                    },
                    {
                        "name": "Tony Quertier"
                    },
                    {
                        "name": "Grégoire Barrue"
                    }
                ],
                "author_detail": {
                    "name": "Grégoire Barrue"
                },
                "author": "Grégoire Barrue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12113v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12113v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21343v1",
                "updated": "2025-06-26T14:53:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    53,
                    44,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:53:44Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    53,
                    44,
                    3,
                    177,
                    0
                ],
                "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models"
                },
                "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21338v1",
                "updated": "2025-06-26T14:49:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    49,
                    10,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:49:10Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    49,
                    10,
                    3,
                    177,
                    0
                ],
                "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG\n  Classification"
                },
                "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications."
                },
                "authors": [
                    {
                        "name": "Galvin Brice S. Lim"
                    },
                    {
                        "name": "Brian Godwin S. Lim"
                    },
                    {
                        "name": "Argel A. Bandala"
                    },
                    {
                        "name": "John Anthony C. Jose"
                    },
                    {
                        "name": "Timothy Scott C. Chu"
                    },
                    {
                        "name": "Edwin Sybingco"
                    }
                ],
                "author_detail": {
                    "name": "Edwin Sybingco"
                },
                "author": "Edwin Sybingco",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21328v1",
                "updated": "2025-06-26T14:41:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    41,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:41:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    41,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing."
                },
                "authors": [
                    {
                        "name": "Jiajie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Yang"
                },
                "author": "Jiajie Yang",
                "arxiv_comment": "15 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21322v1",
                "updated": "2025-06-26T14:37:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    37,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:37:54Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    37,
                    54,
                    3,
                    177,
                    0
                ],
                "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a\n  Family Healthcare Robot Contradicts Human Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a\n  Family Healthcare Robot Contradicts Human Memory"
                },
                "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare."
                },
                "authors": [
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Natalia Calvo-Barajas"
                    },
                    {
                        "name": "Katie Winkle"
                    },
                    {
                        "name": "Ginevra Castellano"
                    }
                ],
                "author_detail": {
                    "name": "Ginevra Castellano"
                },
                "author": "Ginevra Castellano",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21319v1",
                "updated": "2025-06-26T14:35:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    35,
                    59,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:35:59Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    35,
                    59,
                    3,
                    177,
                    0
                ],
                "title": "Multimodal LLMs for Visualization Reconstruction and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs for Visualization Reconstruction and Understanding"
                },
                "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality."
                },
                "authors": [
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Chunlin Da"
                    },
                    {
                        "name": "Xiaoxiao Long"
                    },
                    {
                        "name": "Yuxiao Yang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19683v2",
                "updated": "2025-06-26T14:20:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    20,
                    13,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:49:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance"
                },
                "summary": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries."
                },
                "authors": [
                    {
                        "name": "Xuesong Li"
                    },
                    {
                        "name": "Dianye Huang"
                    },
                    {
                        "name": "Yameng Zhang"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Jiang"
                },
                "author": "Zhongliang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21294v1",
                "updated": "2025-06-26T14:14:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    14,
                    20,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:14:20Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    14,
                    20,
                    3,
                    177,
                    0
                ],
                "title": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models"
                },
                "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches."
                },
                "authors": [
                    {
                        "name": "Bram Willemsen"
                    },
                    {
                        "name": "Gabriel Skantze"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Skantze"
                },
                "author": "Gabriel Skantze",
                "arxiv_comment": "Accepted for publication at XLLM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21288v1",
                "updated": "2025-06-26T14:09:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    9,
                    41,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:09:41Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    9,
                    41,
                    3,
                    177,
                    0
                ],
                "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Encoders Can Rival Large Decoders in Detecting Groundedness"
                },
                "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less"
                },
                "authors": [
                    {
                        "name": "Istabrak Abbes"
                    },
                    {
                        "name": "Gabriele Prato"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Fernando Rodriguez"
                    },
                    {
                        "name": "Alaa Boukhary"
                    },
                    {
                        "name": "Adam Elwood"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13379v2",
                "updated": "2025-06-26T14:06:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    6,
                    49,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-19T17:24:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Thinkless: LLM Learns When to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinkless: LLM Learns When to Think"
                },
                "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"
                },
                "authors": [
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21285v1",
                "updated": "2025-06-26T14:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning"
                },
                "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Lu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Yin"
                },
                "author": "Lu Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21277v1",
                "updated": "2025-06-26T14:01:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    1,
                    3,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T14:01:03Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    1,
                    3,
                    3,
                    177,
                    0
                ],
                "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context"
                },
                "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models."
                },
                "authors": [
                    {
                        "name": "Qize Yang"
                    },
                    {
                        "name": "Shimin Yao"
                    },
                    {
                        "name": "Weixuan Chen"
                    },
                    {
                        "name": "Shenghao Fu"
                    },
                    {
                        "name": "Detao Bai"
                    },
                    {
                        "name": "Jiaxing Zhao"
                    },
                    {
                        "name": "Boyuan Sun"
                    },
                    {
                        "name": "Bowen Yin"
                    },
                    {
                        "name": "Xihan Wei"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21274v1",
                "updated": "2025-06-26T13:58:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    58,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:58:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    58,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?"
                },
                "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness"
                },
                "authors": [
                    {
                        "name": "Andrea McGlinchey"
                    },
                    {
                        "name": "Peter J Barclay"
                    }
                ],
                "author_detail": {
                    "name": "Peter J Barclay"
                },
                "author": "Peter J Barclay",
                "arxiv_comment": "(Submitted for publication)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21263v1",
                "updated": "2025-06-26T13:45:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    45,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:45:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    45,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster"
                },
                "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters."
                },
                "authors": [
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "WenPeng Zhu"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Ming Wu"
                    },
                    {
                        "name": "YingJun Wu"
                    },
                    {
                        "name": "Wu He"
                    },
                    {
                        "name": "Xun Gao"
                    },
                    {
                        "name": "Jason Zeng"
                    },
                    {
                        "name": "Michael Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Heinrich"
                },
                "author": "Michael Heinrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21240v1",
                "updated": "2025-06-26T13:23:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    57,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:23:57Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    23,
                    57,
                    3,
                    177,
                    0
                ],
                "title": "Zero-Shot Learning for Obsolescence Risk Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Learning for Obsolescence Risk Forecasting"
                },
                "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks."
                },
                "authors": [
                    {
                        "name": "Elie Saad"
                    },
                    {
                        "name": "Aya Mrabah"
                    },
                    {
                        "name": "Mariem Besbes"
                    },
                    {
                        "name": "Marc Zolghadri"
                    },
                    {
                        "name": "Victor Czmil"
                    },
                    {
                        "name": "Claude Baron"
                    },
                    {
                        "name": "Vincent Bourgeois"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Bourgeois"
                },
                "author": "Vincent Bourgeois",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21222v1",
                "updated": "2025-06-26T13:14:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    14,
                    52,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:14:52Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    14,
                    52,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval"
                },
                "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks."
                },
                "authors": [
                    {
                        "name": "Yongchan Chun"
                    },
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Dongjun Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21220v1",
                "updated": "2025-06-26T13:13:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    13,
                    24,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:13:24Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    13,
                    24,
                    3,
                    177,
                    0
                ],
                "title": "Complexity-aware fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity-aware fine-tuning"
                },
                "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction."
                },
                "authors": [
                    {
                        "name": "Andrey Goncharov"
                    },
                    {
                        "name": "Daniil Vyazhev"
                    },
                    {
                        "name": "Petr Sychev"
                    },
                    {
                        "name": "Edvard Khalafyan"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14712v3",
                "updated": "2025-06-26T13:12:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    12,
                    25,
                    3,
                    177,
                    0
                ],
                "published": "2023-12-22T14:10:07Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    14,
                    10,
                    7,
                    4,
                    356,
                    0
                ],
                "title": "Balancing Privacy, Robustness, and Efficiency in Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Privacy, Robustness, and Efficiency in Machine Learning"
                },
                "summary": "This position paper argues that achieving robustness, privacy, and efficiency\nsimultaneously in machine learning systems is infeasible under prevailing\nthreat models. The tension between these goals arises not from algorithmic\nshortcomings but from structural limitations imposed by worst-case adversarial\nassumptions. We advocate for a systematic research agenda aimed at formalizing\nthe robustness-privacy-efficiency trilemma, exploring how principled\nrelaxations of threat models can unlock better trade-offs, and designing\nbenchmarks that expose rather than obscure the compromises made. By shifting\nfocus from aspirational universal guarantees to context-aware system design,\nthe machine learning community can build models that are truly appropriate for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues that achieving robustness, privacy, and efficiency\nsimultaneously in machine learning systems is infeasible under prevailing\nthreat models. The tension between these goals arises not from algorithmic\nshortcomings but from structural limitations imposed by worst-case adversarial\nassumptions. We advocate for a systematic research agenda aimed at formalizing\nthe robustness-privacy-efficiency trilemma, exploring how principled\nrelaxations of threat models can unlock better trade-offs, and designing\nbenchmarks that expose rather than obscure the compromises made. By shifting\nfocus from aspirational universal guarantees to context-aware system design,\nthe machine learning community can build models that are truly appropriate for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Youssef Allouah"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "John Stephan"
                    }
                ],
                "author_detail": {
                    "name": "John Stephan"
                },
                "author": "John Stephan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21215v1",
                "updated": "2025-06-26T13:11:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    11,
                    1,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:11:01Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    11,
                    1,
                    3,
                    177,
                    0
                ],
                "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?"
                },
                "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2."
                },
                "authors": [
                    {
                        "name": "Haoang Chi"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Long Lan"
                    },
                    {
                        "name": "Xiaoguang Ren"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "arxiv_comment": "24 pages, accepted at NeurIPS 2024",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems, 2024, 37:\n  96640-96670",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20409v2",
                "updated": "2025-06-26T13:09:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    9,
                    40,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-25T13:24:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    24,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPS: Tool-Augmented Personalisation via Structured Tagging"
                },
                "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."
                },
                "authors": [
                    {
                        "name": "Ekaterina Taktasheva"
                    },
                    {
                        "name": "Jeff Dalton"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Dalton"
                },
                "author": "Jeff Dalton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21211v1",
                "updated": "2025-06-26T13:04:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    4,
                    28,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:04:28Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    4,
                    28,
                    3,
                    177,
                    0
                ],
                "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large\n  Language Models"
                },
                "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging."
                },
                "authors": [
                    {
                        "name": "Quanming Liu"
                    },
                    {
                        "name": "Xupeng Bu"
                    },
                    {
                        "name": "Zhichao Yan"
                    },
                    {
                        "name": "Ru Li"
                    }
                ],
                "author_detail": {
                    "name": "Ru Li"
                },
                "author": "Ru Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21199v1",
                "updated": "2025-06-26T12:57:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    57,
                    41,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:57:41Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    57,
                    41,
                    3,
                    177,
                    0
                ],
                "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image\n  Segmentation and Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image\n  Segmentation and Classification"
                },
                "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs."
                },
                "authors": [
                    {
                        "name": "Shadman Sobhan"
                    },
                    {
                        "name": "Kazi Abrar Mahmud"
                    },
                    {
                        "name": "Abduz Zami"
                    }
                ],
                "author_detail": {
                    "name": "Abduz Zami"
                },
                "author": "Abduz Zami",
                "arxiv_comment": "40 pages, 8 Tables, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00753v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00753v4",
                "updated": "2025-06-26T12:53:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    53,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-01T08:29:26Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    8,
                    29,
                    26,
                    3,
                    121,
                    0
                ],
                "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Yaozu Wu"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Paper lists and resources are available at\n  https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00753v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00753v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21191v1",
                "updated": "2025-06-26T12:49:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:49:07Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    49,
                    7,
                    3,
                    177,
                    0
                ],
                "title": "Prompt-Guided Turn-Taking Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Guided Turn-Taking Prediction"
                },
                "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Yahui Fu"
                    },
                    {
                        "name": "Zi Haur Pang"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Keiko Ochi"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "arxiv_comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13056v2",
                "updated": "2025-06-26T11:45:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    45,
                    11,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-16T02:56:13Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    56,
                    13,
                    0,
                    167,
                    0
                ],
                "title": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model\n  Learning"
                },
                "summary": "Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall. Please refer\nto our project page for open-source information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall. Please refer\nto our project page for open-source information."
                },
                "authors": [
                    {
                        "name": "Haibo Qiu"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Fanfan Liu"
                    },
                    {
                        "name": "Xiaohu Sun"
                    },
                    {
                        "name": "Delian Ruan"
                    },
                    {
                        "name": "Peng Shi"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "Project Page: https://github.com/MM-Thinking/Metis-RISE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01495v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01495v4",
                "updated": "2025-06-26T11:34:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    34,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-02T09:56:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    56,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of\n  Large Language Models"
                },
                "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC."
                },
                "authors": [
                    {
                        "name": "Ping Wu"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Enmeng Lu"
                    },
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01495v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01495v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14501v2",
                "updated": "2025-06-26T11:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    3,
                    13,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-19T03:48:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    48,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "Do Large Language Models Advocate for Inferentialism?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Advocate for Inferentialism?"
                },
                "summary": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims."
                },
                "authors": [
                    {
                        "name": "Yuzuki Arai"
                    },
                    {
                        "name": "Sho Tsugawa"
                    }
                ],
                "author_detail": {
                    "name": "Sho Tsugawa"
                },
                "author": "Sho Tsugawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21138v1",
                "updated": "2025-06-26T10:52:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    52,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T10:52:07Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    52,
                    7,
                    3,
                    177,
                    0
                ],
                "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets\n  for AI4RE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets\n  for AI4RE"
                },
                "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation."
                },
                "authors": [
                    {
                        "name": "Abdelkarim El-Hajjami"
                    },
                    {
                        "name": "Camille Salinesi"
                    }
                ],
                "author_detail": {
                    "name": "Camille Salinesi"
                },
                "author": "Camille Salinesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v3",
                "updated": "2025-06-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    49,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "26 pages main text and 2 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21134v1",
                "updated": "2025-06-26T10:31:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    31,
                    44,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T10:31:44Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    10,
                    31,
                    44,
                    3,
                    177,
                    0
                ],
                "title": "Inside Job: Defending Kubernetes Clusters Against Network\n  Misconfigurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside Job: Defending Kubernetes Clusters Against Network\n  Misconfigurations"
                },
                "summary": "Kubernetes has emerged as the de facto standard for container orchestration.\nUnfortunately, its increasing popularity has also made it an attractive target\nfor malicious actors. Despite extensive research on securing Kubernetes, little\nattention has been paid to the impact of network configuration on the security\nof application deployments. This paper addresses this gap by conducting a\ncomprehensive analysis of network misconfigurations in a Kubernetes cluster\nwith specific reference to lateral movement. Accordingly, we carried out an\nextensive evaluation of 287 open-source applications belonging to six different\norganizations, ranging from IT companies and public entities to non-profits. As\na result, we identified 634 misconfigurations, well beyond what could be found\nby solutions in the state of the art. We responsibly disclosed our findings to\nthe concerned organizations and engaged in a discussion to assess their\nseverity. As of now, misconfigurations affecting more than thirty applications\nhave been fixed with the mitigations we proposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kubernetes has emerged as the de facto standard for container orchestration.\nUnfortunately, its increasing popularity has also made it an attractive target\nfor malicious actors. Despite extensive research on securing Kubernetes, little\nattention has been paid to the impact of network configuration on the security\nof application deployments. This paper addresses this gap by conducting a\ncomprehensive analysis of network misconfigurations in a Kubernetes cluster\nwith specific reference to lateral movement. Accordingly, we carried out an\nextensive evaluation of 287 open-source applications belonging to six different\norganizations, ranging from IT companies and public entities to non-profits. As\na result, we identified 634 misconfigurations, well beyond what could be found\nby solutions in the state of the art. We responsibly disclosed our findings to\nthe concerned organizations and engaged in a discussion to assess their\nseverity. As of now, misconfigurations affecting more than thirty applications\nhave been fixed with the mitigations we proposed."
                },
                "authors": [
                    {
                        "name": "Jacopo Bufalino"
                    },
                    {
                        "name": "Jose Luis Martin-Navarro"
                    },
                    {
                        "name": "Mario Di Francesco"
                    },
                    {
                        "name": "Tuomas Aura"
                    }
                ],
                "author_detail": {
                    "name": "Tuomas Aura"
                },
                "author": "Tuomas Aura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21126v1",
                "updated": "2025-06-26T09:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    9,
                    59,
                    26,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T09:59:26Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    9,
                    59,
                    26,
                    3,
                    177,
                    0
                ],
                "title": "Semantic-aware Digital Twin for AI-based CSI Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-aware Digital Twin for AI-based CSI Acquisition"
                },
                "summary": "Artificial intelligence (AI) substantially enhances channel state information\n(CSI) acquisition performance but is limited by its reliance on single-modality\ninformation and deployment challenges, particularly in dataset collection. This\npaper investigates the use of semantic-aware digital twin (DT) to enhance\nAI-based CSI acquisition. We first briefly introduce the motivation and recent\nadvancements in AI-driven CSI acquisition and semantic-aware DT employment for\nair interfaces. Then, we thoroughly explore how semantic-aware DT can bolster\nAI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI\nacquisition into two classes: enhancing AI-based CSI acquisition through\nintegration with DT and using DT to aid AI-based CSI deployment. Potential\nintegration frameworks are introduced in detail. Finally, we conclude by\noutlining potential research directions within the semantic-aware DT-assisted\nAI-based CSI acquisition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) substantially enhances channel state information\n(CSI) acquisition performance but is limited by its reliance on single-modality\ninformation and deployment challenges, particularly in dataset collection. This\npaper investigates the use of semantic-aware digital twin (DT) to enhance\nAI-based CSI acquisition. We first briefly introduce the motivation and recent\nadvancements in AI-driven CSI acquisition and semantic-aware DT employment for\nair interfaces. Then, we thoroughly explore how semantic-aware DT can bolster\nAI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI\nacquisition into two classes: enhancing AI-based CSI acquisition through\nintegration with DT and using DT to aid AI-based CSI deployment. Potential\nintegration frameworks are introduced in detail. Finally, we conclude by\noutlining potential research directions within the semantic-aware DT-assisted\nAI-based CSI acquisition."
                },
                "authors": [
                    {
                        "name": "Jiajia Guo"
                    },
                    {
                        "name": "Yiming Cui"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "This article has been accepted by IEEE Communications Standards\n  Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10044v3",
                "updated": "2025-06-26T08:59:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    59,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-15T07:41:40Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    41,
                    40,
                    3,
                    135,
                    0
                ],
                "title": "To what extent can current French mobile network support agricultural\n  robots?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To what extent can current French mobile network support agricultural\n  robots?"
                },
                "summary": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area."
                },
                "authors": [
                    {
                        "name": "Pierre La Rocca"
                    },
                    {
                        "name": "Gaël Guennebaud"
                    },
                    {
                        "name": "Aurélie Bugeau"
                    }
                ],
                "author_detail": {
                    "name": "Aurélie Bugeau"
                },
                "arxiv_affiliation": "IUF, LaBRI, UB",
                "author": "Aurélie Bugeau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21098v1",
                "updated": "2025-06-26T08:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    48,
                    16,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    48,
                    16,
                    3,
                    177,
                    0
                ],
                "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for\n  Real-time Community Question Answering in Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for\n  Real-time Community Question Answering in Industry"
                },
                "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations."
                },
                "authors": [
                    {
                        "name": "Qinwen Chen"
                    },
                    {
                        "name": "Wenbiao Tao"
                    },
                    {
                        "name": "Zhiwei Zhu"
                    },
                    {
                        "name": "Mingfan Xi"
                    },
                    {
                        "name": "Liangzhong Guo"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yunshi Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yunshi Lan"
                },
                "author": "Yunshi Lan",
                "arxiv_comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21093v1",
                "updated": "2025-06-26T08:41:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    41,
                    45,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:41:45Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    41,
                    45,
                    3,
                    177,
                    0
                ],
                "title": "Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol\n  Detection"
                },
                "summary": "Transformers have shown potential in solving wireless communication problems,\nparticularly via in-context learning (ICL), where models adapt to new tasks\nthrough prompts without requiring model updates. However, prior ICL-based\nTransformer models rely on deep architectures with many layers to achieve\nsatisfactory performance, resulting in substantial storage and computational\ncosts. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a\nCoT-enhanced shallow Transformer framework for wireless symbol detection. By\nintroducing autoregressive latent reasoning steps within the hidden space,\nCHOOSE significantly improves the reasoning capacity of shallow models (1-2\nlayers) without increasing model depth. This design enables lightweight\nTransformers to achieve detection performance comparable to much deeper models,\nmaking them well-suited for deployment on resource-constrained mobile devices.\nExperimental results demonstrate that our approach outperforms conventional\nshallow Transformers and achieves performance comparable to that of deep\nTransformers, while maintaining storage and computational efficiency. This\nrepresents a promising direction for implementing Transformer-based algorithms\nin wireless receivers with limited computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have shown potential in solving wireless communication problems,\nparticularly via in-context learning (ICL), where models adapt to new tasks\nthrough prompts without requiring model updates. However, prior ICL-based\nTransformer models rely on deep architectures with many layers to achieve\nsatisfactory performance, resulting in substantial storage and computational\ncosts. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a\nCoT-enhanced shallow Transformer framework for wireless symbol detection. By\nintroducing autoregressive latent reasoning steps within the hidden space,\nCHOOSE significantly improves the reasoning capacity of shallow models (1-2\nlayers) without increasing model depth. This design enables lightweight\nTransformers to achieve detection performance comparable to much deeper models,\nmaking them well-suited for deployment on resource-constrained mobile devices.\nExperimental results demonstrate that our approach outperforms conventional\nshallow Transformers and achieves performance comparable to that of deep\nTransformers, while maintaining storage and computational efficiency. This\nrepresents a promising direction for implementing Transformer-based algorithms\nin wireless receivers with limited computational resources."
                },
                "authors": [
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Cong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shen"
                },
                "author": "Cong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21080v1",
                "updated": "2025-06-26T08:09:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    9,
                    16,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T08:09:16Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    9,
                    16,
                    3,
                    177,
                    0
                ],
                "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for\n  Efficient Egocentric Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for\n  Efficient Egocentric Perception"
                },
                "summary": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Sanjoy Chowdhury"
                    },
                    {
                        "name": "Subrata Biswas"
                    },
                    {
                        "name": "Sayan Nag"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Calvin Murdock"
                    },
                    {
                        "name": "Ishwarya Ananthabhotla"
                    },
                    {
                        "name": "Yijun Qian"
                    },
                    {
                        "name": "Vamsi Krishna Ithapu"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Ruohan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Ruohan Gao"
                },
                "author": "Ruohan Gao",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21073v1",
                "updated": "2025-06-26T07:58:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    58,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T07:58:22Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    58,
                    22,
                    3,
                    177,
                    0
                ],
                "title": "Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G\n  Networks"
                },
                "summary": "The advent of 5G and beyond has brought increased performance networks,\nfacilitating the deployment of services closer to the user. To meet performance\nrequirements such services require specialized hardware, such as Field\nProgrammable Gate Arrays (FPGAs). However, FPGAs are often deployed in\nunprotected environments, leaving the user's applications vulnerable to\nmultiple attacks. With the rise of quantum computing, which threatens the\nintegrity of widely-used cryptographic algorithms, the need for a robust\nsecurity infrastructure is even more crucial. In this paper we introduce a\nhybrid hardware-software solution utilizing remote attestation to securely\nconfigure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms\nfor enhanced security. Additionally, to enable trustworthiness across the whole\nedge computing continuum, our solution integrates a blockchain infrastructure,\nensuring the secure storage of any security evidence. We evaluate the proposed\nsecure configuration process under different PQC algorithms in two FPGA\nfamilies, showcasing only 2% overheard compared to the non PQC approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of 5G and beyond has brought increased performance networks,\nfacilitating the deployment of services closer to the user. To meet performance\nrequirements such services require specialized hardware, such as Field\nProgrammable Gate Arrays (FPGAs). However, FPGAs are often deployed in\nunprotected environments, leaving the user's applications vulnerable to\nmultiple attacks. With the rise of quantum computing, which threatens the\nintegrity of widely-used cryptographic algorithms, the need for a robust\nsecurity infrastructure is even more crucial. In this paper we introduce a\nhybrid hardware-software solution utilizing remote attestation to securely\nconfigure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms\nfor enhanced security. Additionally, to enable trustworthiness across the whole\nedge computing continuum, our solution integrates a blockchain infrastructure,\nensuring the secure storage of any security evidence. We evaluate the proposed\nsecure configuration process under different PQC algorithms in two FPGA\nfamilies, showcasing only 2% overheard compared to the non PQC approach."
                },
                "authors": [
                    {
                        "name": "Ilias Papalamprou"
                    },
                    {
                        "name": "Nikolaos Fotos"
                    },
                    {
                        "name": "Nikolaos Chatzivasileiadis"
                    },
                    {
                        "name": "Anna Angelogianni"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21072v1",
                "updated": "2025-06-26T07:52:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    52,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T07:52:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    52,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Bridding OT and PaaS in Edge-to-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridding OT and PaaS in Edge-to-Cloud Continuum"
                },
                "summary": "The Operational Technology Platform as a Service (OTPaaS) initiative provides\na structured framework for the efficient management and storage of data. It\nensures excellent response times while improving security, reliability, data\nand technology sovereignty, robustness, and energy efficiency, which are\ncrucial for industrial transformation and data sovereignty. This paper\nillustrates successful deployment, adaptable application management, and\nvarious integration components catering to Edge and Cloud environments. It\nleverages the advantages of the Platform as a Service model and highlights key\nchallenges that have been addressed for specific use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Operational Technology Platform as a Service (OTPaaS) initiative provides\na structured framework for the efficient management and storage of data. It\nensures excellent response times while improving security, reliability, data\nand technology sovereignty, robustness, and energy efficiency, which are\ncrucial for industrial transformation and data sovereignty. This paper\nillustrates successful deployment, adaptable application management, and\nvarious integration components catering to Edge and Cloud environments. It\nleverages the advantages of the Platform as a Service model and highlights key\nchallenges that have been addressed for specific use cases."
                },
                "authors": [
                    {
                        "name": "Carlos J Barrios"
                    },
                    {
                        "name": "Yves Denneulin"
                    }
                ],
                "author_detail": {
                    "name": "Yves Denneulin"
                },
                "arxiv_affiliation": "LIG, Grenoble INP",
                "author": "Yves Denneulin",
                "arxiv_journal_ref": "Conf{\\'e}rence Francophone d'Informatique en Parall{\\'e}lisme,\n  Architecture et Syst{\\`e}me (COMPAS 2025), INRIA; UNIVERSITE DE BORDEAUX;\n  CNRS, Jun 2025, BORDEAUX, France",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21071v1",
                "updated": "2025-06-26T07:45:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    45,
                    15,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T07:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    7,
                    45,
                    15,
                    3,
                    177,
                    0
                ],
                "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge\n  Graph"
                },
                "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Zai Zhang"
                    },
                    {
                        "name": "Hao Qian"
                    },
                    {
                        "name": "Chunjing Gan"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Bin Shi"
                    },
                    {
                        "name": "Bo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dong"
                },
                "author": "Bo Dong",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21053v1",
                "updated": "2025-06-26T06:59:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    59,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:59:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    59,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for\n  Conversational Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for\n  Conversational Stance Detection"
                },
                "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection."
                },
                "authors": [
                    {
                        "name": "Fuqiang Niu"
                    },
                    {
                        "name": "Genan Dai"
                    },
                    {
                        "name": "Yisha Lu"
                    },
                    {
                        "name": "Jiayu Liao"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Hu Huang"
                    },
                    {
                        "name": "Bowen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhang"
                },
                "author": "Bowen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19750v3",
                "updated": "2025-06-26T06:52:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    52,
                    46,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T16:06:37Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    37,
                    1,
                    175,
                    0
                ],
                "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach"
                },
                "summary": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost."
                },
                "authors": [
                    {
                        "name": "Takashi Nishibayashi"
                    },
                    {
                        "name": "Seiji Kanazawa"
                    },
                    {
                        "name": "Kumpei Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Kumpei Yamada"
                },
                "author": "Kumpei Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11277v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11277v3",
                "updated": "2025-06-26T06:52:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    52,
                    37,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-16T14:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    11,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs"
                },
                "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."
                },
                "authors": [
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11277v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18113v2",
                "updated": "2025-06-26T06:33:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    33,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2024-05-28T12:23:16Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    12,
                    23,
                    16,
                    1,
                    149,
                    0
                ],
                "title": "MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job\n  Seeking and Recruiting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job\n  Seeking and Recruiting"
                },
                "summary": "Online recruitment platforms have reshaped job-seeking and recruiting\nprocesses, driving increased demand for applications that enhance person-job\nmatching. Traditional methods generally rely on analyzing textual data from\nresumes and job descriptions, limiting the dynamic, interactive aspects crucial\nto effective recruitment. Recent advances in Large Language Models (LLMs) have\nrevealed remarkable potential in simulating adaptive, role-based dialogues,\nmaking them well-suited for recruitment scenarios. In this paper, we propose\n\\textbf{MockLLM}, a novel framework to generate and evaluate mock interview\ninteractions. The system consists of two key components: mock interview\ngeneration and two-sided evaluation in handshake protocol. By simulating both\ninterviewer and candidate roles, MockLLM enables consistent and collaborative\ninteractions for real-time and two-sided matching. To further improve the\nmatching quality, MockLLM further incorporates reflection memory generation and\ndynamic strategy modification, refining behaviors based on previous experience.\nWe evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment\nplatform. The experimental results indicate that MockLLM outperforms existing\nmethods in matching accuracy, scalability, and adaptability across job domains,\nhighlighting its potential to advance candidate assessment and online\nrecruitment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online recruitment platforms have reshaped job-seeking and recruiting\nprocesses, driving increased demand for applications that enhance person-job\nmatching. Traditional methods generally rely on analyzing textual data from\nresumes and job descriptions, limiting the dynamic, interactive aspects crucial\nto effective recruitment. Recent advances in Large Language Models (LLMs) have\nrevealed remarkable potential in simulating adaptive, role-based dialogues,\nmaking them well-suited for recruitment scenarios. In this paper, we propose\n\\textbf{MockLLM}, a novel framework to generate and evaluate mock interview\ninteractions. The system consists of two key components: mock interview\ngeneration and two-sided evaluation in handshake protocol. By simulating both\ninterviewer and candidate roles, MockLLM enables consistent and collaborative\ninteractions for real-time and two-sided matching. To further improve the\nmatching quality, MockLLM further incorporates reflection memory generation and\ndynamic strategy modification, refining behaviors based on previous experience.\nWe evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment\nplatform. The experimental results indicate that MockLLM outperforms existing\nmethods in matching accuracy, scalability, and adaptability across job domains,\nhighlighting its potential to advance candidate assessment and online\nrecruitment."
                },
                "authors": [
                    {
                        "name": "Hongda Sun"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Haiyu Yan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "Accepted by KDD 2025 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21909v3",
                "updated": "2025-06-26T06:24:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    24,
                    8,
                    3,
                    177,
                    0
                ],
                "published": "2024-10-29T10:01:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"
                },
                "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."
                },
                "authors": [
                    {
                        "name": "Xiao Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Zibo Liao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Tianrui Sun"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ling Fu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21035v1",
                "updated": "2025-06-26T06:19:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    19,
                    5,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:19:05Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    19,
                    5,
                    3,
                    177,
                    0
                ],
                "title": "Little By Little: Continual Learning via Self-Activated Sparse\n  Mixture-of-Rank Adaptive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Little By Little: Continual Learning via Self-Activated Sparse\n  Mixture-of-Rank Adaptive Learning"
                },
                "summary": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting."
                },
                "authors": [
                    {
                        "name": "Haodong Lu"
                    },
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Jason Xue"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05432v2",
                "updated": "2025-06-26T06:17:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    17,
                    49,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-05T08:58:58Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    8,
                    58,
                    58,
                    3,
                    156,
                    0
                ],
                "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar\n  Coordinate Decoupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar\n  Coordinate Decoupling"
                },
                "summary": "Large Language Models (LLMs) face significant challenges in edge deployment\ndue to their massive parameter scale. Vector Quantization (VQ), a\nclustering-based quantization method, serves as a prevalent solution to this\nissue for its extremely low-bit (even at 2-bit) and considerable accuracy.\nSince a vector is a quantity in mathematics and physics that has both direction\nand magnitude, existing VQ works typically quantize them in a coupled manner.\nHowever, we find that direction exhibits significantly greater sensitivity to\nquantization compared to the magnitude. For instance, when separately\nclustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the\naccuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap\neven increases with the reduction of clustering centers. Further, Euclidean\ndistance, a common metric to access vector similarities in current VQ works,\nplaces greater emphasis on reducing the magnitude error. This property is\ncontrary to the above finding, unavoidably leading to larger quantization\nerrors. To these ends, this paper proposes Polar Coordinate Decoupled Vector\nQuantization (PCDVQ), an effective and efficient VQ framework consisting of two\nkey modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors\ninto their polar coordinate representations and perform independent\nquantization of the direction and magnitude parameters.2) Distribution Aligned\nCodebook Construction (DACC), which optimizes the direction and magnitude\ncodebooks in accordance with the source distribution. Experimental results show\nthat PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\%\nzero-shot accuracy, establishing a novel paradigm for accurate and highly\ncompressed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant challenges in edge deployment\ndue to their massive parameter scale. Vector Quantization (VQ), a\nclustering-based quantization method, serves as a prevalent solution to this\nissue for its extremely low-bit (even at 2-bit) and considerable accuracy.\nSince a vector is a quantity in mathematics and physics that has both direction\nand magnitude, existing VQ works typically quantize them in a coupled manner.\nHowever, we find that direction exhibits significantly greater sensitivity to\nquantization compared to the magnitude. For instance, when separately\nclustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the\naccuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap\neven increases with the reduction of clustering centers. Further, Euclidean\ndistance, a common metric to access vector similarities in current VQ works,\nplaces greater emphasis on reducing the magnitude error. This property is\ncontrary to the above finding, unavoidably leading to larger quantization\nerrors. To these ends, this paper proposes Polar Coordinate Decoupled Vector\nQuantization (PCDVQ), an effective and efficient VQ framework consisting of two\nkey modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors\ninto their polar coordinate representations and perform independent\nquantization of the direction and magnitude parameters.2) Distribution Aligned\nCodebook Construction (DACC), which optimizes the direction and magnitude\ncodebooks in accordance with the source distribution. Experimental results show\nthat PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\%\nzero-shot accuracy, establishing a novel paradigm for accurate and highly\ncompressed LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yue"
                    },
                    {
                        "name": "Zukang Xu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21033v1",
                "updated": "2025-06-26T06:16:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    16,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:16:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    16,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient\n  LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient\n  LLM Services"
                },
                "summary": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments."
                },
                "authors": [
                    {
                        "name": "Zhaojiacheng Zhou"
                    },
                    {
                        "name": "Hongze Liu"
                    },
                    {
                        "name": "Shijing Yuan"
                    },
                    {
                        "name": "Hanning Zhang"
                    },
                    {
                        "name": "Jiong Lou"
                    },
                    {
                        "name": "Chentao Wu"
                    },
                    {
                        "name": "Jie Li"
                    }
                ],
                "author_detail": {
                    "name": "Jie Li"
                },
                "author": "Jie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21031v1",
                "updated": "2025-06-26T06:10:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    10,
                    37,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:10:37Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    10,
                    37,
                    3,
                    177,
                    0
                ],
                "title": "Large Language Models Acing Chartered Accountancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Acing Chartered Accountancy"
                },
                "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation."
                },
                "authors": [
                    {
                        "name": "Jatin Gupta"
                    },
                    {
                        "name": "Akhil Sharma"
                    },
                    {
                        "name": "Saransh Singhania"
                    },
                    {
                        "name": "Mohammad Adnan"
                    },
                    {
                        "name": "Sakshi Deo"
                    },
                    {
                        "name": "Ali Imam Abidi"
                    },
                    {
                        "name": "Keshav Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Gupta"
                },
                "author": "Keshav Gupta",
                "arxiv_comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21030v1",
                "updated": "2025-06-26T06:10:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    10,
                    2,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:10:02Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    10,
                    2,
                    3,
                    177,
                    0
                ],
                "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an\n  embodied long-horizon task planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEP Planner: Constructing cross-hierarchical subgoal tree as an\n  embodied long-horizon task planner"
                },
                "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods."
                },
                "authors": [
                    {
                        "name": "Zhou Tianxing"
                    },
                    {
                        "name": "Wang Zhirui"
                    },
                    {
                        "name": "Ao Haojia"
                    },
                    {
                        "name": "Chen Guangyan"
                    },
                    {
                        "name": "Xing Boyang"
                    },
                    {
                        "name": "Cheng Jingwen"
                    },
                    {
                        "name": "Yang Yi"
                    },
                    {
                        "name": "Yue Yufeng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yufeng"
                },
                "author": "Yue Yufeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21029v1",
                "updated": "2025-06-26T06:09:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    9,
                    57,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T06:09:57Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    9,
                    57,
                    3,
                    177,
                    0
                ],
                "title": "Are Ultrathin Stents Optimal for Bifurcation Lesions? Insights from\n  Computational Modelling of Provisional and DK-Crush Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Ultrathin Stents Optimal for Bifurcation Lesions? Insights from\n  Computational Modelling of Provisional and DK-Crush Techniques"
                },
                "summary": "Complex coronary bifurcation lesions remain challenging in percutaneous\ncoronary intervention, with stent design and deployment strategy influencing\nclinical outcomes. This study compares the mechanical and hemodynamic\nperformance of the ultrathin-strut Orsiro and thin-strut Xience Sierra stent in\nProvisional Side Branch (PSB) and Double Kissing Crush (DKC) techniques. We\nused finite element analyses of bifurcation stent deployment to assess\nmalapposition, ostium clearance, and arterial wall stress for both techniques.\nComputational fluid dynamics simulations quantified the luminal exposure to low\nTime-Averaged Endothelial Shear Stress (TAESS below 0.4 Pa) and high shear\nrates (above 1000 1/s). In PSB, Orsiro showed higher malapposition (13.0% vs\n9.6%) but improved SB ostium clearance (77% vs 64%) and lower low-TAESS\nexposure (30.3% vs 33.6%) compared to Xience. Orsiro also produced higher\narterial wall stresses, particularly during kissing balloon inflation. In DKC,\ndifferences in malapposition and ostium clearance diminished between stents,\nthough Orsiro retained a hemodynamic advantage with lower low-TAESS (28.2% vs\n36.3%).Stent design influenced outcomes more strongly in PSB, where anatomical\ninteraction and platform-specific behavior impacted both structural and\nhemodynamic results. In DKC, procedural complexity minimized those differences,\nmaking the stenting technique the primary performance driver. Nonetheless,\nOrsiro consistently preserved more favorable flow conditions. These findings\nhighlight the need to match device selection with lesion characteristics in\nPSB, while in DKC, optimizing procedural steps may have a greater impact than\nthe choice of stent platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex coronary bifurcation lesions remain challenging in percutaneous\ncoronary intervention, with stent design and deployment strategy influencing\nclinical outcomes. This study compares the mechanical and hemodynamic\nperformance of the ultrathin-strut Orsiro and thin-strut Xience Sierra stent in\nProvisional Side Branch (PSB) and Double Kissing Crush (DKC) techniques. We\nused finite element analyses of bifurcation stent deployment to assess\nmalapposition, ostium clearance, and arterial wall stress for both techniques.\nComputational fluid dynamics simulations quantified the luminal exposure to low\nTime-Averaged Endothelial Shear Stress (TAESS below 0.4 Pa) and high shear\nrates (above 1000 1/s). In PSB, Orsiro showed higher malapposition (13.0% vs\n9.6%) but improved SB ostium clearance (77% vs 64%) and lower low-TAESS\nexposure (30.3% vs 33.6%) compared to Xience. Orsiro also produced higher\narterial wall stresses, particularly during kissing balloon inflation. In DKC,\ndifferences in malapposition and ostium clearance diminished between stents,\nthough Orsiro retained a hemodynamic advantage with lower low-TAESS (28.2% vs\n36.3%).Stent design influenced outcomes more strongly in PSB, where anatomical\ninteraction and platform-specific behavior impacted both structural and\nhemodynamic results. In DKC, procedural complexity minimized those differences,\nmaking the stenting technique the primary performance driver. Nonetheless,\nOrsiro consistently preserved more favorable flow conditions. These findings\nhighlight the need to match device selection with lesion characteristics in\nPSB, while in DKC, optimizing procedural steps may have a greater impact than\nthe choice of stent platform."
                },
                "authors": [
                    {
                        "name": "Andrea Colombo"
                    },
                    {
                        "name": "Dario Carbonaro"
                    },
                    {
                        "name": "Mingzi Zhang"
                    },
                    {
                        "name": "Chi Shen"
                    },
                    {
                        "name": "Ramtin Gharleghi"
                    },
                    {
                        "name": "Ankush Kapoor"
                    },
                    {
                        "name": "Claudio Chiastra"
                    },
                    {
                        "name": "Nigel Jepson"
                    },
                    {
                        "name": "Mark Webster"
                    },
                    {
                        "name": "Susann Beier"
                    }
                ],
                "author_detail": {
                    "name": "Susann Beier"
                },
                "author": "Susann Beier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21017v1",
                "updated": "2025-06-26T05:28:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    28,
                    57,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T05:28:57Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    28,
                    57,
                    3,
                    177,
                    0
                ],
                "title": "Multimodal Prompt Alignment for Facial Expression Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Prompt Alignment for Facial Expression Recognition"
                },
                "summary": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs."
                },
                "authors": [
                    {
                        "name": "Fuyan Ma"
                    },
                    {
                        "name": "Yiran He"
                    },
                    {
                        "name": "Bin Sun"
                    },
                    {
                        "name": "Shutao Li"
                    }
                ],
                "author_detail": {
                    "name": "Shutao Li"
                },
                "author": "Shutao Li",
                "arxiv_comment": "To appear in ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14539v2",
                "updated": "2025-06-26T05:18:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    18,
                    19,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-17T14:01:39Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    14,
                    1,
                    39,
                    1,
                    168,
                    0
                ],
                "title": "Doppelganger Method: Breaking Role Consistency in LLM Agent via\n  Prompt-based Transferable Adversarial Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppelganger Method: Breaking Role Consistency in LLM Agent via\n  Prompt-based Transferable Adversarial Attack"
                },
                "summary": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelganger method'' to demonstrate\nthe risk of an agent being hijacked, thereby exposing system instructions and\ninternal information. Next, we define the ''Prompt Alignment Collapse under\nAdversarial Transfer (PACAT)'' level to evaluate the vulnerability to this\nadversarial transfer attack. We also propose a ''Caution for Adversarial\nTransfer (CAT)'' prompt to counter the Doppelganger method. The experimental\nresults demonstrate that the Doppelganger method can compromise the agent's\nconsistency and expose its internal information. In contrast, CAT prompts\nenable effective defense against this adversarial attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelganger method'' to demonstrate\nthe risk of an agent being hijacked, thereby exposing system instructions and\ninternal information. Next, we define the ''Prompt Alignment Collapse under\nAdversarial Transfer (PACAT)'' level to evaluate the vulnerability to this\nadversarial transfer attack. We also propose a ''Caution for Adversarial\nTransfer (CAT)'' prompt to counter the Doppelganger method. The experimental\nresults demonstrate that the Doppelganger method can compromise the agent's\nconsistency and expose its internal information. In contrast, CAT prompts\nenable effective defense against this adversarial attack."
                },
                "authors": [
                    {
                        "name": "Daewon Kang"
                    },
                    {
                        "name": "YeongHwan Shin"
                    },
                    {
                        "name": "Doyeon Kim"
                    },
                    {
                        "name": "Kyu-Hwan Jung"
                    },
                    {
                        "name": "Meong Hi Son"
                    }
                ],
                "author_detail": {
                    "name": "Meong Hi Son"
                },
                "author": "Meong Hi Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21005v1",
                "updated": "2025-06-26T04:45:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    4,
                    45,
                    18,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T04:45:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    4,
                    45,
                    18,
                    3,
                    177,
                    0
                ],
                "title": "VisionGuard: Synergistic Framework for Helmet Violation Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionGuard: Synergistic Framework for Helmet Violation Detection"
                },
                "summary": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance."
                },
                "authors": [
                    {
                        "name": "Lam-Huy Nguyen"
                    },
                    {
                        "name": "Thinh-Phuc Nguyen"
                    },
                    {
                        "name": "Thanh-Hai Nguyen"
                    },
                    {
                        "name": "Gia-Huy Dinh"
                    },
                    {
                        "name": "Minh-Triet Tran"
                    },
                    {
                        "name": "Trung-Nghia Le"
                    }
                ],
                "author_detail": {
                    "name": "Trung-Nghia Le"
                },
                "author": "Trung-Nghia Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18389v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18389v3",
                "updated": "2025-06-26T04:31:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    4,
                    31,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-05-23T21:33:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    21,
                    33,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN"
                },
                "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types"
                },
                "authors": [
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Reshma Prasad"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Under submission to an IEEE journal, copyright may change without\n  notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18389v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18389v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20993v1",
                "updated": "2025-06-26T04:12:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    4,
                    12,
                    15,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T04:12:15Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    4,
                    12,
                    15,
                    3,
                    177,
                    0
                ],
                "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs\n  with Dynamic Intensity Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs\n  with Dynamic Intensity Control"
                },
                "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines."
                },
                "authors": [
                    {
                        "name": "Adithya Chittem"
                    },
                    {
                        "name": "Aishna Shrivastava"
                    },
                    {
                        "name": "Sai Tarun Pendela"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18313v2",
                "updated": "2025-06-26T03:57:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    57,
                    7,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-24T03:32:13Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    32,
                    13,
                    0,
                    83,
                    0
                ],
                "title": "Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision-making remains\ninadequately evaluated. Current benchmarks primarily assess LLMs' understanding\non financial documents rather than the ability to manage assets or dig out\ntrading opportunities in dynamic market conditions. Despite the release of new\nbenchmarks for evaluating diversified tasks on the financial domain, we\nidentified four major problems in these benchmarks, which are data leakage,\nnavel-gazing, over-intervention, and maintenance-hard. To pave the research\ngap, we introduce DeepFund, a comprehensive arena platform for evaluating\nLLM-based trading strategies in a live environment. Our approach implements a\nmulti-agent framework where they serve as multiple key roles that realize the\nreal-world investment decision processes. Moreover, we provide a web interface\nthat visualizes LLMs' performance with fund investment metrics across different\nmarket conditions, enabling detailed comparative analysis. Through DeepFund, we\naim to provide a more realistic and fair assessment on LLM's capabilities in\nfund investment, offering diversified insights and revealing their potential\napplications in real-world financial markets. Our code is publicly available at\nhttps://github.com/HKUSTDial/DeepFund.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision-making remains\ninadequately evaluated. Current benchmarks primarily assess LLMs' understanding\non financial documents rather than the ability to manage assets or dig out\ntrading opportunities in dynamic market conditions. Despite the release of new\nbenchmarks for evaluating diversified tasks on the financial domain, we\nidentified four major problems in these benchmarks, which are data leakage,\nnavel-gazing, over-intervention, and maintenance-hard. To pave the research\ngap, we introduce DeepFund, a comprehensive arena platform for evaluating\nLLM-based trading strategies in a live environment. Our approach implements a\nmulti-agent framework where they serve as multiple key roles that realize the\nreal-world investment decision processes. Moreover, we provide a web interface\nthat visualizes LLMs' performance with fund investment metrics across different\nmarket conditions, enabling detailed comparative analysis. Through DeepFund, we\naim to provide a more realistic and fair assessment on LLM's capabilities in\nfund investment, offering diversified insights and revealing their potential\napplications in real-world financial markets. Our code is publicly available at\nhttps://github.com/HKUSTDial/DeepFund."
                },
                "authors": [
                    {
                        "name": "Changlun Li"
                    },
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "6 pages, 3 figures, perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03359v2",
                "updated": "2025-06-26T03:55:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    55,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2024-12-04T14:45:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    45,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis"
                },
                "summary": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\nhttps://whoisspy.ai/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\nhttps://whoisspy.ai/."
                },
                "authors": [
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Jianhui Zheng"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Junguang Jiang"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20982v1",
                "updated": "2025-06-26T03:54:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    54,
                    25,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:54:25Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    54,
                    25,
                    3,
                    177,
                    0
                ],
                "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a\n  Tangible Programming Robot for Preschoolers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a\n  Tangible Programming Robot for Preschoolers"
                },
                "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings."
                },
                "authors": [
                    {
                        "name": "Martin Ruskov"
                    }
                ],
                "author_detail": {
                    "name": "Martin Ruskov"
                },
                "author": "Martin Ruskov",
                "arxiv_comment": "accepted at D-SAIL Workshop - Transformative Curriculum Design:\n  Digitalization, Sustainability, and AI Literacy for 21st Century Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20978v1",
                "updated": "2025-06-26T03:52:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    52,
                    56,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:52:56Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    52,
                    56,
                    3,
                    177,
                    0
                ],
                "title": "Response Quality Assessment for Retrieval-Augmented Generation via\n  Conditional Conformal Factuality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Quality Assessment for Retrieval-Augmented Generation via\n  Conditional Conformal Factuality"
                },
                "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee."
                },
                "authors": [
                    {
                        "name": "Naihe Feng"
                    },
                    {
                        "name": "Yi Sui"
                    },
                    {
                        "name": "Shiyi Hou"
                    },
                    {
                        "name": "Jesse C. Cresswell"
                    },
                    {
                        "name": "Ga Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ga Wu"
                },
                "author": "Ga Wu",
                "arxiv_doi": "10.1145/3726302.3730244",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730244",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.20978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20971v1",
                "updated": "2025-06-26T03:24:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    24,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:24:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    24,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)"
                },
                "summary": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices."
                },
                "authors": [
                    {
                        "name": "Shihui Feng"
                    },
                    {
                        "name": "Huilin Zhang"
                    },
                    {
                        "name": "Dragan Gašević"
                    }
                ],
                "author_detail": {
                    "name": "Dragan Gašević"
                },
                "author": "Dragan Gašević",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09510v2",
                "updated": "2025-06-26T03:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    19,
                    56,
                    3,
                    177,
                    0
                ],
                "published": "2024-09-14T19:18:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    19,
                    18,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models"
                },
                "summary": "Despite its substantial impact on various search, recommendation, and\nquestion answering tasks, privacy-preserving methods for personalizing large\nlanguage models (LLMs) have received relatively limited exploration. There is\none primary approach in this area through retrieval-augmented generation (RAG),\nwhich generates personalized outputs by enriching the input prompt with\ninformation retrieved from the user's personal data. This paper studies an\northogonal approach to RAG that involves learning user-dependent LLM parameters\nthrough parameter-efficient fine-tuning (PEFT). This paper presents the first\nsystematic study for exploration of PEFT for LLM personalization and provides\nan extensive comparisons between RAG- and PEFT-based solutions, across a broad\nset of seven diverse datasets from the LaMP benchmark. Our results demonstrate\nthat, on average, both RAG- and PEFT-based personalization methods yield 14.92%\nand 1.07% improvements over non-personalized LLMs, respectively. When combining\nRAG with PEFT, we observe a further improvement of 15.98%, highlighting the\neffectiveness of their integration in enhancing personalized text generation.\nAdditionally, we identify a positive correlation between the amount of user\ndata available and the effectiveness of PEFT. This finding suggests that RAG is\nparticularly beneficial for cold-start users -- users with limited personal\ndata -- while PEFT performs better when more user-specific data is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite its substantial impact on various search, recommendation, and\nquestion answering tasks, privacy-preserving methods for personalizing large\nlanguage models (LLMs) have received relatively limited exploration. There is\none primary approach in this area through retrieval-augmented generation (RAG),\nwhich generates personalized outputs by enriching the input prompt with\ninformation retrieved from the user's personal data. This paper studies an\northogonal approach to RAG that involves learning user-dependent LLM parameters\nthrough parameter-efficient fine-tuning (PEFT). This paper presents the first\nsystematic study for exploration of PEFT for LLM personalization and provides\nan extensive comparisons between RAG- and PEFT-based solutions, across a broad\nset of seven diverse datasets from the LaMP benchmark. Our results demonstrate\nthat, on average, both RAG- and PEFT-based personalization methods yield 14.92%\nand 1.07% improvements over non-personalized LLMs, respectively. When combining\nRAG with PEFT, we observe a further improvement of 15.98%, highlighting the\neffectiveness of their integration in enhancing personalized text generation.\nAdditionally, we identify a positive correlation between the amount of user\ndata available and the effectiveness of PEFT. This finding suggests that RAG is\nparticularly beneficial for cold-start users -- users with limited personal\ndata -- while PEFT performs better when more user-specific data is available."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20970v1",
                "updated": "2025-06-26T03:19:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    19,
                    27,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:19:27Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    19,
                    27,
                    3,
                    177,
                    0
                ],
                "title": "Co-Design of Sensing, Communications, and Control for Low-Altitude\n  Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Design of Sensing, Communications, and Control for Low-Altitude\n  Wireless Networks"
                },
                "summary": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance."
                },
                "authors": [
                    {
                        "name": "Haijia Jin"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Yuanhao Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yuanhao Cui"
                },
                "author": "Yuanhao Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19054v2",
                "updated": "2025-06-26T03:18:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    18,
                    8,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-18T01:35:33Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    35,
                    33,
                    2,
                    169,
                    0
                ],
                "title": "GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail\n  Dataset"
                },
                "summary": "As LLMs become widespread across diverse applications, concerns about the\nsecurity and safety of LLM interactions have intensified. Numerous guardrail\nmodels and benchmarks have been developed to ensure LLM content safety.\nHowever, existing guardrail benchmarks are often built upon ad hoc risk\ntaxonomies that lack a principled grounding in standardized safety policies,\nlimiting their alignment with real-world operational requirements. Moreover,\nthey tend to overlook domain-specific risks, while the same risk category can\ncarry different implications across different domains. To bridge these gaps, we\nintroduce GuardSet-X, the first massive multi-domain safety policy-grounded\nguardrail dataset. GuardSet-X offers: (1) broad domain coverage across eight\nsafety-critical domains, such as finance, law, and codeGen; (2) policy-grounded\nrisk construction based on authentic, domain-specific safety guidelines; (3)\ndiverse interaction formats, encompassing declarative statements, questions,\ninstructions, and multi-turn conversations; (4) advanced benign data curation\nvia detoxification prompting to challenge over-refusal behaviors; and (5)\n\\textbf{attack-enhanced instances} that simulate adversarial inputs designed to\nbypass guardrails. Based on GuardSet-X, we benchmark 19 advanced guardrail\nmodels and uncover a series of findings, such as: (1) All models achieve varied\nF1 scores, with many demonstrating high variance across risk categories,\nhighlighting their limited domain coverage and insufficient handling of\ndomain-specific safety concerns; (2) As models evolve, their coverage of safety\nrisks broadens, but performance on common risk categories may decrease; (3) All\nmodels remain vulnerable to optimized adversarial attacks. We believe that\n\\dataset and the unique insights derived from our evaluations will advance the\ndevelopment of policy-aligned and resilient guardrail systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become widespread across diverse applications, concerns about the\nsecurity and safety of LLM interactions have intensified. Numerous guardrail\nmodels and benchmarks have been developed to ensure LLM content safety.\nHowever, existing guardrail benchmarks are often built upon ad hoc risk\ntaxonomies that lack a principled grounding in standardized safety policies,\nlimiting their alignment with real-world operational requirements. Moreover,\nthey tend to overlook domain-specific risks, while the same risk category can\ncarry different implications across different domains. To bridge these gaps, we\nintroduce GuardSet-X, the first massive multi-domain safety policy-grounded\nguardrail dataset. GuardSet-X offers: (1) broad domain coverage across eight\nsafety-critical domains, such as finance, law, and codeGen; (2) policy-grounded\nrisk construction based on authentic, domain-specific safety guidelines; (3)\ndiverse interaction formats, encompassing declarative statements, questions,\ninstructions, and multi-turn conversations; (4) advanced benign data curation\nvia detoxification prompting to challenge over-refusal behaviors; and (5)\n\\textbf{attack-enhanced instances} that simulate adversarial inputs designed to\nbypass guardrails. Based on GuardSet-X, we benchmark 19 advanced guardrail\nmodels and uncover a series of findings, such as: (1) All models achieve varied\nF1 scores, with many demonstrating high variance across risk categories,\nhighlighting their limited domain coverage and insufficient handling of\ndomain-specific safety concerns; (2) As models evolve, their coverage of safety\nrisks broadens, but performance on common risk categories may decrease; (3) All\nmodels remain vulnerable to optimized adversarial attacks. We believe that\n\\dataset and the unique insights derived from our evaluations will advance the\ndevelopment of policy-aligned and resilient guardrail systems."
                },
                "authors": [
                    {
                        "name": "Mintong Kang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Chengquan Guo"
                    },
                    {
                        "name": "Minzhou Pan"
                    },
                    {
                        "name": "Ivan Revilla"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19324v3",
                "updated": "2025-06-26T03:14:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    14,
                    46,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-31T17:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
                },
                "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00968v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00968v3",
                "updated": "2025-06-26T03:14:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    14,
                    28,
                    3,
                    177,
                    0
                ],
                "published": "2024-08-02T01:25:14Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    1,
                    25,
                    14,
                    4,
                    215,
                    0
                ],
                "title": "ss2DNS: A Secure DNS Scheme in Stage 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ss2DNS: A Secure DNS Scheme in Stage 2"
                },
                "summary": "The absence of security and privacy measures between DNS recursive resolvers\nand authoritative nameservers has been exploited by both on-path and off-path\nattackers. Although numerous security proposals have been introduced in\npractice and in the literature, they often face deployability barriers and/or\nlack a compelling set of security and privacy properties, resulting in limited\nadoption. We introduce ss2DNS, a novel DNS scheme designed to mitigate the\nsecurity and privacy vulnerabilities in the resolution process between\nresolvers and authoritative nameservers, while preserving efficiency by\nmaintaining a single round-trip. ss2DNS takes advantage of a hierarchical trust\nmodel that does not rely on entities external to DNS zones, and delegates\nnameserver replicas within each zone to serve zone data securely for short,\nrenewable time intervals. This design enables real-time security properties for\nDNS messages without requiring the duplication of long-term private keys on\nreplicas, thereby minimizing exposure to compromise. We implement a proof of\nconcept of ss2DNS for evaluation and show that for server-side processing\nlatency, resolution time, and CPU usage, ss2DNS is comparable to less-secure\nschemes but significantly outperforms DNS-over-TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The absence of security and privacy measures between DNS recursive resolvers\nand authoritative nameservers has been exploited by both on-path and off-path\nattackers. Although numerous security proposals have been introduced in\npractice and in the literature, they often face deployability barriers and/or\nlack a compelling set of security and privacy properties, resulting in limited\nadoption. We introduce ss2DNS, a novel DNS scheme designed to mitigate the\nsecurity and privacy vulnerabilities in the resolution process between\nresolvers and authoritative nameservers, while preserving efficiency by\nmaintaining a single round-trip. ss2DNS takes advantage of a hierarchical trust\nmodel that does not rely on entities external to DNS zones, and delegates\nnameserver replicas within each zone to serve zone data securely for short,\nrenewable time intervals. This design enables real-time security properties for\nDNS messages without requiring the duplication of long-term private keys on\nreplicas, thereby minimizing exposure to compromise. We implement a proof of\nconcept of ss2DNS for evaluation and show that for server-side processing\nlatency, resolution time, and CPU usage, ss2DNS is comparable to less-secure\nschemes but significantly outperforms DNS-over-TLS."
                },
                "authors": [
                    {
                        "name": "Ali Sadeghi Jahromi"
                    },
                    {
                        "name": "AbdelRahman Abdou"
                    },
                    {
                        "name": "Paul C. van Oorschot"
                    }
                ],
                "author_detail": {
                    "name": "Paul C. van Oorschot"
                },
                "author": "Paul C. van Oorschot",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00968v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00968v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09942v2",
                "updated": "2025-06-26T03:06:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    6,
                    17,
                    3,
                    177,
                    0
                ],
                "published": "2024-10-13T17:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    17,
                    53,
                    50,
                    6,
                    287,
                    0
                ],
                "title": "Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization"
                },
                "summary": "This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and RAG strategy. We introduce an\niterative approach where the search engine generates retrieval results for the\nRAG agents and gathers feedback on the quality of the retrieved documents\nduring an offline phase. This feedback is then used to iteratively optimize the\nsearch engine using an expectation-maximization algorithm, with the goal of\nmaximizing each agent's utility function. Additionally, we adapt this to an\nonline setting, allowing the search engine to refine its behavior based on\nreal-time individual agents feedback to better serve the results for each of\nthem. Experiments on datasets from the Knowledge-Intensive Language Tasks\n(KILT) benchmark demonstrates that our approach significantly on average\noutperforms baselines across 18 RAG models. We demonstrate that our method\neffectively ``personalizes'' the retrieval for each RAG agent based on the\ncollected feedback. Finally, we provide a comprehensive ablation study to\nexplore various aspects of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and RAG strategy. We introduce an\niterative approach where the search engine generates retrieval results for the\nRAG agents and gathers feedback on the quality of the retrieved documents\nduring an offline phase. This feedback is then used to iteratively optimize the\nsearch engine using an expectation-maximization algorithm, with the goal of\nmaximizing each agent's utility function. Additionally, we adapt this to an\nonline setting, allowing the search engine to refine its behavior based on\nreal-time individual agents feedback to better serve the results for each of\nthem. Experiments on datasets from the Knowledge-Intensive Language Tasks\n(KILT) benchmark demonstrates that our approach significantly on average\noutperforms baselines across 18 RAG models. We demonstrate that our method\neffectively ``personalizes'' the retrieval for each RAG agent based on the\ncollected feedback. Finally, we provide a comprehensive ablation study to\nexplore various aspects of our method."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20963v1",
                "updated": "2025-06-26T03:01:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    1,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:01:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    1,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for\n  Growing Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for\n  Growing Corpora"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official."
                },
                "authors": [
                    {
                        "name": "Fangyuan Zhang"
                    },
                    {
                        "name": "Zhengjun Huang"
                    },
                    {
                        "name": "Yingli Zhou"
                    },
                    {
                        "name": "Qintian Guo"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Wensheng Luo"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Yixiang Fang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20954v1",
                "updated": "2025-06-26T02:41:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    41,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T02:41:55Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    41,
                    55,
                    3,
                    177,
                    0
                ],
                "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard\n  Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard\n  Sensing"
                },
                "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations."
                },
                "authors": [
                    {
                        "name": "Xueming Liu"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Xiang Zhou"
                    },
                    {
                        "name": "Qingrui Zhang"
                    },
                    {
                        "name": "Tianjiang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Tianjiang Hu"
                },
                "author": "Tianjiang Hu",
                "arxiv_comment": "8 Pages, 7 figures. Accepted by RA-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09780v2",
                "updated": "2025-06-26T02:41:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    41,
                    53,
                    3,
                    177,
                    0
                ],
                "published": "2024-05-16T03:00:08Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    3,
                    0,
                    8,
                    3,
                    137,
                    0
                ],
                "title": "EFEAR-4D: Ego-Velocity Filtering for Efficient and Accurate 4D radar\n  Odometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFEAR-4D: Ego-Velocity Filtering for Efficient and Accurate 4D radar\n  Odometry"
                },
                "summary": "Odometry is a crucial component for successfully implementing autonomous\nnavigation, relying on sensors such as cameras, LiDARs and IMUs. However, these\nsensors may encounter challenges in extreme weather conditions, such as\nsnowfall and fog. The emergence of FMCW radar technology offers the potential\nfor robust perception in adverse conditions. As the latest generation of FWCW\nradars, the 4D mmWave radar provides point cloud with range, azimuth,\nelevation, and Doppler velocity information, despite inherent sparsity and\nnoises in the point cloud. In this paper, we propose EFEAR-4D, an accurate,\nhighly efficient, and learning-free method for large-scale 4D radar odometry\nestimation. EFEAR-4D exploits Doppler velocity information delicately for\nrobust ego-velocity estimation, resulting in a highly accurate prior guess.\nEFEAR-4D maintains robustness against point-cloud sparsity and noises across\ndiverse environments through dynamic object removal and effective region-wise\nfeature extraction. Extensive experiments on two publicly available 4D radar\ndatasets demonstrate state-of-the-art reliability and localization accuracy of\nEFEAR-4D under various conditions. Furthermore, we have collected a dataset\nfollowing the same route but varying installation heights of the 4D radar,\nemphasizing the significant impact of radar height on point cloud quality - a\ncrucial consideration for real-world deployments. Our algorithm and dataset\nwill be available soon at https://github.com/CLASS-Lab/EFEAR-4D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odometry is a crucial component for successfully implementing autonomous\nnavigation, relying on sensors such as cameras, LiDARs and IMUs. However, these\nsensors may encounter challenges in extreme weather conditions, such as\nsnowfall and fog. The emergence of FMCW radar technology offers the potential\nfor robust perception in adverse conditions. As the latest generation of FWCW\nradars, the 4D mmWave radar provides point cloud with range, azimuth,\nelevation, and Doppler velocity information, despite inherent sparsity and\nnoises in the point cloud. In this paper, we propose EFEAR-4D, an accurate,\nhighly efficient, and learning-free method for large-scale 4D radar odometry\nestimation. EFEAR-4D exploits Doppler velocity information delicately for\nrobust ego-velocity estimation, resulting in a highly accurate prior guess.\nEFEAR-4D maintains robustness against point-cloud sparsity and noises across\ndiverse environments through dynamic object removal and effective region-wise\nfeature extraction. Extensive experiments on two publicly available 4D radar\ndatasets demonstrate state-of-the-art reliability and localization accuracy of\nEFEAR-4D under various conditions. Furthermore, we have collected a dataset\nfollowing the same route but varying installation heights of the 4D radar,\nemphasizing the significant impact of radar height on point cloud quality - a\ncrucial consideration for real-world deployments. Our algorithm and dataset\nwill be available soon at https://github.com/CLASS-Lab/EFEAR-4D."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Wu"
                    },
                    {
                        "name": "Yushuai Chen"
                    },
                    {
                        "name": "Zhan Li"
                    },
                    {
                        "name": "Ziyang Hong"
                    },
                    {
                        "name": "Liang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Hu"
                },
                "author": "Liang Hu",
                "arxiv_doi": "10.1109/LRA.2024.3466071",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3466071",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.09780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20949v1",
                "updated": "2025-06-26T02:28:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    28,
                    58,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T02:28:58Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    28,
                    58,
                    3,
                    177,
                    0
                ],
                "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon\n  Simulation"
                },
                "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents."
                },
                "authors": [
                    {
                        "name": "Chenkai Sun"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "ChengXiang Zhai"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]