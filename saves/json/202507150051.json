[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristóbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "José María Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muñoz"
                    },
                    {
                        "name": "Manuel Gil Pérez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Kühn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Kühn"
                },
                "author": "Martin J. Kühn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Søren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Søren Stobbe"
                },
                "author": "Søren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11767v2",
                "updated": "2025-07-11T17:59:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    56,
                    4,
                    192,
                    0
                ],
                "published": "2024-11-18T17:46:32Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    46,
                    32,
                    0,
                    323,
                    0
                ],
                "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drowning in Documents: Consequences of Scaling Reranker Inference"
                },
                "summary": "Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking."
                },
                "authors": [
                    {
                        "name": "Mathew Jacob"
                    },
                    {
                        "name": "Erik Lindgren"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Michael Carbin"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Andrew Drozdov"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Drozdov"
                },
                "author": "Andrew Drozdov",
                "arxiv_comment": "Accepted to ReNeuIR 2025 Workshop at SIGIR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08801v1",
                "updated": "2025-07-11T17:59:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    42,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:42Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    42,
                    4,
                    192,
                    0
                ],
                "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective"
                },
                "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos."
                },
                "authors": [
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Weihua Chen"
                    },
                    {
                        "name": "Jun Cen"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jingyun Liang"
                    },
                    {
                        "name": "Shuning Chang"
                    },
                    {
                        "name": "Zhihui Lin"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Pengwei Liu"
                    },
                    {
                        "name": "Jiazheng Xing"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Code and Models: https://github.com/alibaba-damo-academy/Lumos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08798v1",
                "updated": "2025-07-11T17:59:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "The Atacama Cosmology Telescope: High-redshift measurement of structure\n  growth from the cross-correlation of Quaia quasars and CMB lensing from ACT\n  DR6 and $\\textit{Planck}$ PR4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atacama Cosmology Telescope: High-redshift measurement of structure\n  growth from the cross-correlation of Quaia quasars and CMB lensing from ACT\n  DR6 and $\\textit{Planck}$ PR4"
                },
                "summary": "We measure the amplitude of matter fluctuations over a wide range of\nredshifts by combining CMB lensing observations from ACT DR6 and\n$\\textit{Planck}$ PR4 with the overdensity of quasars from Quaia, a\n$\\textit{Gaia}$ and $\\textit{unWISE}$ quasar catalog. Our analysis includes the\nCMB lensing power spectrum from ACT DR6, the auto-correlation of two Quaia\nquasar samples centered at $z \\simeq 1.0$ and $z \\simeq 2.1$, and their\ncross-correlations with CMB lensing from both ACT DR6 and $\\textit{Planck}$\nPR4. By performing a series of contamination and systematic null tests, we find\nno evidence for contamination in the lensing maps, contrary to what was\nsuggested in previous Quaia cross-correlation analyses using $\\textit{Planck}$\nPR4 CMB lensing data. From the joint analysis of the quasar auto- and\ncross-correlations with CMB lensing, and including BOSS BAO data to break the\ndegeneracy between $\\Omega_m$ and $\\sigma_8$, we obtain $\\sigma_8 =\n0.802^{+0.045}_{-0.057}$, consistent with $\\Lambda$CDM predictions from\n$\\textit{Planck}$ primary CMB measurements. Combining the CMB lensing\nauto-spectrum with the cross-correlation measurement improves the constraint on\n$\\sigma_8$ by $12\\%$ relative to the lensing auto-spectrum alone, yielding\n$\\sigma_8 = 0.804 \\pm 0.013$. This dataset combination also enables a\nreconstruction of structure growth across redshifts. We infer a $12\\%$\nconstraint on the amplitude of matter fluctuations at $z > 3$, with a\nmeasurement at the median redshift of the signal of $\\sigma_8(\\tilde{z}=5.1) =\n0.146^{+0.021}_{-0.014}$, consistent with $\\textit{Planck}$ at the $1.4\\sigma$\nlevel. These results provide one of the highest redshift constraints on the\ngrowth of structure to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We measure the amplitude of matter fluctuations over a wide range of\nredshifts by combining CMB lensing observations from ACT DR6 and\n$\\textit{Planck}$ PR4 with the overdensity of quasars from Quaia, a\n$\\textit{Gaia}$ and $\\textit{unWISE}$ quasar catalog. Our analysis includes the\nCMB lensing power spectrum from ACT DR6, the auto-correlation of two Quaia\nquasar samples centered at $z \\simeq 1.0$ and $z \\simeq 2.1$, and their\ncross-correlations with CMB lensing from both ACT DR6 and $\\textit{Planck}$\nPR4. By performing a series of contamination and systematic null tests, we find\nno evidence for contamination in the lensing maps, contrary to what was\nsuggested in previous Quaia cross-correlation analyses using $\\textit{Planck}$\nPR4 CMB lensing data. From the joint analysis of the quasar auto- and\ncross-correlations with CMB lensing, and including BOSS BAO data to break the\ndegeneracy between $\\Omega_m$ and $\\sigma_8$, we obtain $\\sigma_8 =\n0.802^{+0.045}_{-0.057}$, consistent with $\\Lambda$CDM predictions from\n$\\textit{Planck}$ primary CMB measurements. Combining the CMB lensing\nauto-spectrum with the cross-correlation measurement improves the constraint on\n$\\sigma_8$ by $12\\%$ relative to the lensing auto-spectrum alone, yielding\n$\\sigma_8 = 0.804 \\pm 0.013$. This dataset combination also enables a\nreconstruction of structure growth across redshifts. We infer a $12\\%$\nconstraint on the amplitude of matter fluctuations at $z > 3$, with a\nmeasurement at the median redshift of the signal of $\\sigma_8(\\tilde{z}=5.1) =\n0.146^{+0.021}_{-0.014}$, consistent with $\\textit{Planck}$ at the $1.4\\sigma$\nlevel. These results provide one of the highest redshift constraints on the\ngrowth of structure to date."
                },
                "authors": [
                    {
                        "name": "Carmen Embil Villagra"
                    },
                    {
                        "name": "Gerrit Farren"
                    },
                    {
                        "name": "Giulio Fabbian"
                    },
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "Irene Abril-Cabezas"
                    },
                    {
                        "name": "David Alonso"
                    },
                    {
                        "name": "Anthony Challinor"
                    },
                    {
                        "name": "Jo Dunkley"
                    },
                    {
                        "name": "Joshua Kim"
                    },
                    {
                        "name": "Niall MacCrann"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Kavilan Moodley"
                    },
                    {
                        "name": "Frank J. Qu"
                    },
                    {
                        "name": "Blake Sherwin"
                    },
                    {
                        "name": "Cristobal Sifon"
                    },
                    {
                        "name": "Alexander van Engelen"
                    },
                    {
                        "name": "Edward J. Wollack"
                    }
                ],
                "author_detail": {
                    "name": "Edward J. Wollack"
                },
                "author": "Edward J. Wollack",
                "arxiv_comment": "30 pages, 15 figures, to be submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08794v1",
                "updated": "2025-07-11T17:55:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    55,
                    22,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:55:22Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    55,
                    22,
                    4,
                    192,
                    0
                ],
                "title": "One Token to Fool LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Token to Fool LLM-as-a-Judge"
                },
                "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM."
                },
                "authors": [
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "S. Y. Kung"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12667v2",
                "updated": "2025-07-11T17:29:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    29,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-02-18T09:18:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    18,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "The Preference for Evolving Dark Energy from Cosmological Distance\n  Measurements and Possible Signatures in the Growth Rate of Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Preference for Evolving Dark Energy from Cosmological Distance\n  Measurements and Possible Signatures in the Growth Rate of Perturbations"
                },
                "summary": "In this study, we use a flexible parametrization of the equation of state of\ndark energy to explore its possible evolution with datasets from the Dark\nEnergy Spectroscopic Instrument (DESI), Planck cosmic microwave background, and\neither the 5-year Dark Energy Survey (DES) or the Pantheon+ (PP) supernova (SN)\ncompilation. This parametrization, called transitional dark energy, allows for\nrapid changes in the equation of state but also changes like that in the\nChevallier-Polarski-Linder parametrization. We find a 3.8{\\sigma} preference\nfor evolving dark energy over {\\Lambda}CDM with the DES dataset and a weaker\n2.4{\\sigma} preference when using the PP dataset. This corroborates the finding\nof the DESI Collaboration, who found that their baryon acoustic oscillation\ndata preferred evolving dark energy when fit with the CPL parametrization of\nthe equation of state. Our analysis reveals no significant outliers in the DESI\ndata around the TDE best-fit, while the data is asymmetrically distributed\naround the {\\Lambda}CDM best-fit model such that the measured distances are on\naverage smaller. The DESI and SN data both prefer an expansion history that\nimplies a higher dark energy density around z=0.5 than in the\nPlanck-{\\Lambda}CDM model, with the inferred equation of state being greater\nthan -1 around z=0 and close to or below -1 at z>0.5. We show that when the\nexpansion rate is greater than that in the Planck-{\\Lambda}CDM model (around\nz=0.5), the growth rate calculated assuming General Relativity is suppressed\nrelative to the Planck-{\\Lambda}CDM model, and it rebounds as the expansion\nrate differences between the models become smaller closer to the present time.\nThe resulting flattening of the $f\\sigma_8(z)$ curve compared to the\n{\\Lambda}CDM model could be an independent signature of the temporal evolution\nof dark energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we use a flexible parametrization of the equation of state of\ndark energy to explore its possible evolution with datasets from the Dark\nEnergy Spectroscopic Instrument (DESI), Planck cosmic microwave background, and\neither the 5-year Dark Energy Survey (DES) or the Pantheon+ (PP) supernova (SN)\ncompilation. This parametrization, called transitional dark energy, allows for\nrapid changes in the equation of state but also changes like that in the\nChevallier-Polarski-Linder parametrization. We find a 3.8{\\sigma} preference\nfor evolving dark energy over {\\Lambda}CDM with the DES dataset and a weaker\n2.4{\\sigma} preference when using the PP dataset. This corroborates the finding\nof the DESI Collaboration, who found that their baryon acoustic oscillation\ndata preferred evolving dark energy when fit with the CPL parametrization of\nthe equation of state. Our analysis reveals no significant outliers in the DESI\ndata around the TDE best-fit, while the data is asymmetrically distributed\naround the {\\Lambda}CDM best-fit model such that the measured distances are on\naverage smaller. The DESI and SN data both prefer an expansion history that\nimplies a higher dark energy density around z=0.5 than in the\nPlanck-{\\Lambda}CDM model, with the inferred equation of state being greater\nthan -1 around z=0 and close to or below -1 at z>0.5. We show that when the\nexpansion rate is greater than that in the Planck-{\\Lambda}CDM model (around\nz=0.5), the growth rate calculated assuming General Relativity is suppressed\nrelative to the Planck-{\\Lambda}CDM model, and it rebounds as the expansion\nrate differences between the models become smaller closer to the present time.\nThe resulting flattening of the $f\\sigma_8(z)$ curve compared to the\n{\\Lambda}CDM model could be an independent signature of the temporal evolution\nof dark energy."
                },
                "authors": [
                    {
                        "name": "Ryan E. Keeley"
                    },
                    {
                        "name": "Kevork N. Abazajian"
                    },
                    {
                        "name": "Manoj Kaplinghat"
                    },
                    {
                        "name": "Arman Shafieloo"
                    }
                ],
                "author_detail": {
                    "name": "Arman Shafieloo"
                },
                "author": "Arman Shafieloo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08771v1",
                "updated": "2025-07-11T17:28:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    28,
                    56,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:28:56Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    28,
                    56,
                    4,
                    192,
                    0
                ],
                "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity"
                },
                "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN)."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "21 pages, 7 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08765v1",
                "updated": "2025-07-11T17:21:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    21,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:21:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    21,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Compress Any Segment Anything Model (SAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress Any Segment Anything Model (SAM)"
                },
                "summary": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models."
                },
                "authors": [
                    {
                        "name": "Juntong Fan"
                    },
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianqiang Shen"
                    },
                    {
                        "name": "Shang-Ling Jui"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Jing-Xiao Liao"
                    },
                    {
                        "name": "Feng-Lei Fan"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Lei Fan"
                },
                "author": "Feng-Lei Fan",
                "arxiv_comment": "13 pages, 6 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08764v1",
                "updated": "2025-07-11T17:20:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    20,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:20:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    20,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "Propensity score with factor loadings: the effect of the Paris Agreement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propensity score with factor loadings: the effect of the Paris Agreement"
                },
                "summary": "Factor models for longitudinal data, where policy adoption is unconfounded\nwith respect to a low-dimensional set of latent factor loadings, have become\nincreasingly popular for causal inference. Most existing approaches, however,\nrely on a causal finite-sample approach or computationally intensive methods,\nlimiting their applicability and external validity. In this paper, we propose a\nnovel causal inference method for panel data based on inverse propensity score\nweighting where the propensity score is a function of latent factor loadings\nwithin a framework of causal inference from super-population. The approach\nrelaxes the traditional restrictive assumptions of causal panel methods, while\noffering advantages in terms of causal interpretability, policy relevance, and\ncomputational efficiency. Under standard assumptions, we outline a three-step\nestimation procedure for the ATT and derive its large-sample properties using\nMestimation theory. We apply the method to assess the causal effect of the\nParis Agreement, a policy aimed at fostering the transition to a low-carbon\neconomy, on European stock returns. Our empirical results suggest a\nstatistically significant and negative short-run effect on the stock returns of\nfirms that issued green bonds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factor models for longitudinal data, where policy adoption is unconfounded\nwith respect to a low-dimensional set of latent factor loadings, have become\nincreasingly popular for causal inference. Most existing approaches, however,\nrely on a causal finite-sample approach or computationally intensive methods,\nlimiting their applicability and external validity. In this paper, we propose a\nnovel causal inference method for panel data based on inverse propensity score\nweighting where the propensity score is a function of latent factor loadings\nwithin a framework of causal inference from super-population. The approach\nrelaxes the traditional restrictive assumptions of causal panel methods, while\noffering advantages in terms of causal interpretability, policy relevance, and\ncomputational efficiency. Under standard assumptions, we outline a three-step\nestimation procedure for the ATT and derive its large-sample properties using\nMestimation theory. We apply the method to assess the causal effect of the\nParis Agreement, a policy aimed at fostering the transition to a low-carbon\neconomy, on European stock returns. Our empirical results suggest a\nstatistically significant and negative short-run effect on the stock returns of\nfirms that issued green bonds."
                },
                "authors": [
                    {
                        "name": "Angelo Forino"
                    },
                    {
                        "name": "Andrea Mercatanti"
                    },
                    {
                        "name": "Giacomo Morelli"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Morelli"
                },
                "author": "Giacomo Morelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08742v1",
                "updated": "2025-07-11T16:44:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    44,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:44:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    44,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Influence of river incision on landslides triggered in Nepal by the\n  Gorkha earthquake: Results from a pixel-based susceptibility model using\n  inlabru",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence of river incision on landslides triggered in Nepal by the\n  Gorkha earthquake: Results from a pixel-based susceptibility model using\n  inlabru"
                },
                "summary": "This study presents a comprehensive framework for modelling\nearthquake-induced landslides (EQILs) through a channel-based analysis of\nlandslide centroid distributions. A key innovation is the incorporation of the\nnormalised channel steepness index ($k_{sn}$) as a physically meaningful and\nnovel covariate, inferring hillslope erosion and fluvial incision processes.\nUsed within spatial point process models, $k_{sn}$ supports the generation of\nlandslide susceptibility maps with quantified uncertainty. To address spatial\ndata misalignment between covariates and landslide observations, we leverage\nthe inlabru framework, which enables coherent integration through mesh-based\ndisaggregation, thereby overcoming challenges associated with spatially\nmisaligned data integration. Our modelling strategy explicitly prioritises\nprospective transferability to unseen geographical regions, provided that\nexplanatory variable data are available. By modelling both landslide locations\nand sizes, we find that elevated $k_{sn}$ is strongly associated with increased\nlandslide susceptibility but not with landslide magnitude. The best-fitting\nBayesian model, validated through cross-validation, offers a scalable and\ninterpretable solution for predicting earthquake-induced landslides in complex\nterrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive framework for modelling\nearthquake-induced landslides (EQILs) through a channel-based analysis of\nlandslide centroid distributions. A key innovation is the incorporation of the\nnormalised channel steepness index ($k_{sn}$) as a physically meaningful and\nnovel covariate, inferring hillslope erosion and fluvial incision processes.\nUsed within spatial point process models, $k_{sn}$ supports the generation of\nlandslide susceptibility maps with quantified uncertainty. To address spatial\ndata misalignment between covariates and landslide observations, we leverage\nthe inlabru framework, which enables coherent integration through mesh-based\ndisaggregation, thereby overcoming challenges associated with spatially\nmisaligned data integration. Our modelling strategy explicitly prioritises\nprospective transferability to unseen geographical regions, provided that\nexplanatory variable data are available. By modelling both landslide locations\nand sizes, we find that elevated $k_{sn}$ is strongly associated with increased\nlandslide susceptibility but not with landslide magnitude. The best-fitting\nBayesian model, validated through cross-validation, offers a scalable and\ninterpretable solution for predicting earthquake-induced landslides in complex\nterrain."
                },
                "authors": [
                    {
                        "name": "Man Ho Suen"
                    },
                    {
                        "name": "Mark Naylor"
                    },
                    {
                        "name": "Simon Mudd"
                    },
                    {
                        "name": "Finn Lindgren"
                    }
                ],
                "author_detail": {
                    "name": "Finn Lindgren"
                },
                "author": "Finn Lindgren",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02984v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02984v3",
                "updated": "2025-07-11T16:38:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    38,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2024-01-01T17:35:52Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    17,
                    35,
                    52,
                    0,
                    1,
                    0
                ],
                "title": "Large Language Models in Mental Health Care: a Scoping Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Mental Health Care: a Scoping Review"
                },
                "summary": "Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations."
                },
                "authors": [
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Yi-han Sheu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Lauren V. Moran"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Andrew Beam"
                    },
                    {
                        "name": "John Torous"
                    }
                ],
                "author_detail": {
                    "name": "John Torous"
                },
                "author": "John Torous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02984v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02984v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08734v1",
                "updated": "2025-07-11T16:37:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    37,
                    15,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:37:15Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    37,
                    15,
                    4,
                    192,
                    0
                ],
                "title": "Estimating Marginal Likelihoods in Likelihood-Free Inference via Neural\n  Density Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Marginal Likelihoods in Likelihood-Free Inference via Neural\n  Density Estimation"
                },
                "summary": "The marginal likelihood, or evidence, plays a central role in Bayesian model\nselection, yet remains notoriously challenging to compute in likelihood-free\nsettings. While Simulation-Based Inference (SBI) techniques such as Sequential\nNeural Likelihood Estimation (SNLE) offer powerful tools to approximate\nposteriors using neural density estimators, they typically do not provide\nestimates of the evidence. In this technical report presented at BayesComp\n2025, we present a simple and general methodology to estimate the marginal\nlikelihood using the output of SNLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The marginal likelihood, or evidence, plays a central role in Bayesian model\nselection, yet remains notoriously challenging to compute in likelihood-free\nsettings. While Simulation-Based Inference (SBI) techniques such as Sequential\nNeural Likelihood Estimation (SNLE) offer powerful tools to approximate\nposteriors using neural density estimators, they typically do not provide\nestimates of the evidence. In this technical report presented at BayesComp\n2025, we present a simple and general methodology to estimate the marginal\nlikelihood using the output of SNLE."
                },
                "authors": [
                    {
                        "name": "Paul Bastide"
                    },
                    {
                        "name": "Arnaud Estoup"
                    },
                    {
                        "name": "Jean-Michel Marin"
                    },
                    {
                        "name": "Julien Stoehr"
                    }
                ],
                "author_detail": {
                    "name": "Julien Stoehr"
                },
                "author": "Julien Stoehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17256v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17256v4",
                "updated": "2025-07-11T16:36:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    36,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2024-01-30T18:48:37Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    18,
                    48,
                    37,
                    1,
                    30,
                    0
                ],
                "title": "Weak-to-Strong Jailbreaking on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Jailbreaking on Large Language Models"
                },
                "summary": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong"
                },
                "authors": [
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yu-Xiang Wang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17256v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17256v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10110v2",
                "updated": "2025-07-11T16:32:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    32,
                    0,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-14T11:18:02Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    18,
                    2,
                    0,
                    104,
                    0
                ],
                "title": "Eigengap Sparsity for Covariance Parsimony",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigengap Sparsity for Covariance Parsimony"
                },
                "summary": "Covariance estimation is a central problem in statistics. An important issue\nis that there are rarely enough samples $n$ to accurately estimate the $p (p+1)\n/ 2$ coefficients in dimension $p$. Parsimonious covariance models are\ntherefore preferred, but the discrete nature of model selection makes inference\ncomputationally challenging. In this paper, we propose a relaxation of\ncovariance parsimony termed \"eigengap sparsity\" and motivated by the good\naccuracy-parsimony tradeoffs of eigenvalue-equalization in covariance matrices.\nThis penalty can be included in a penalized-likelihood framework that we\npropose to solve with a projected gradient descent on a monotone cone. The\nalgorithm turns out to resemble an isotonic regression of mutually-attracted\nsample eigenvalues, drawing an interesting link between covariance parsimony\nand shrinkage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariance estimation is a central problem in statistics. An important issue\nis that there are rarely enough samples $n$ to accurately estimate the $p (p+1)\n/ 2$ coefficients in dimension $p$. Parsimonious covariance models are\ntherefore preferred, but the discrete nature of model selection makes inference\ncomputationally challenging. In this paper, we propose a relaxation of\ncovariance parsimony termed \"eigengap sparsity\" and motivated by the good\naccuracy-parsimony tradeoffs of eigenvalue-equalization in covariance matrices.\nThis penalty can be included in a penalized-likelihood framework that we\npropose to solve with a projected gradient descent on a monotone cone. The\nalgorithm turns out to resemble an isotonic regression of mutually-attracted\nsample eigenvalues, drawing an interesting link between covariance parsimony\nand shrinkage."
                },
                "authors": [
                    {
                        "name": "Tom Szwagier"
                    },
                    {
                        "name": "Guillaume Olikier"
                    },
                    {
                        "name": "Xavier Pennec"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Pennec"
                },
                "author": "Xavier Pennec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08719v1",
                "updated": "2025-07-11T16:19:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    19,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:19:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    19,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Multilingual Multimodal Software Developer for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Multimodal Software Developer for Code Generation"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs."
                },
                "authors": [
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Liran Wang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Congnan Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08716v1",
                "updated": "2025-07-11T16:16:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    16,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:16:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    16,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine"
                },
                "summary": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD."
                },
                "authors": [
                    {
                        "name": "Kongwu Huang"
                    },
                    {
                        "name": "Shiyi Mu"
                    },
                    {
                        "name": "Jun Jiang"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Shugong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shugong Xu"
                },
                "author": "Shugong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19034v2",
                "updated": "2025-07-11T16:07:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    7,
                    37,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-26T22:00:42Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    22,
                    0,
                    42,
                    5,
                    116,
                    0
                ],
                "title": "On learning functions over biological sequence space: relating Gaussian\n  process priors, regularization, and gauge fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On learning functions over biological sequence space: relating Gaussian\n  process priors, regularization, and gauge fixing"
                },
                "summary": "Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships."
                },
                "authors": [
                    {
                        "name": "Samantha Petti"
                    },
                    {
                        "name": "Carlos Martí-Gómez"
                    },
                    {
                        "name": "Justin B. Kinney"
                    },
                    {
                        "name": "Juannan Zhou"
                    },
                    {
                        "name": "David M. McCandlish"
                    }
                ],
                "author_detail": {
                    "name": "David M. McCandlish"
                },
                "author": "David M. McCandlish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03366v2",
                "updated": "2025-07-11T16:06:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    6,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-02-05T17:03:49Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    3,
                    49,
                    2,
                    36,
                    0
                ],
                "title": "Rethinking Approximate Gaussian Inference in Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Approximate Gaussian Inference in Classification"
                },
                "summary": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling."
                },
                "authors": [
                    {
                        "name": "Bálint Mucsányi"
                    },
                    {
                        "name": "Nathaël Da Costa"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08705v1",
                "updated": "2025-07-11T16:02:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    2,
                    24,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:02:24Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    2,
                    24,
                    4,
                    192,
                    0
                ],
                "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings"
                },
                "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery."
                },
                "authors": [
                    {
                        "name": "Philip Osborne"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP\n  2025 Call for System Demonstrations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.5; I.2.1; I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08704v1",
                "updated": "2025-07-11T15:57:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:57:37Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation"
                },
                "summary": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA."
                },
                "authors": [
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yuan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Meng"
                },
                "author": "Yuan Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08700v1",
                "updated": "2025-07-11T15:51:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    51,
                    50,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:51:50Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    51,
                    50,
                    4,
                    192,
                    0
                ],
                "title": "The breakdown scale of pionless effective field theory in the\n  three-nucleon sector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The breakdown scale of pionless effective field theory in the\n  three-nucleon sector"
                },
                "summary": "We make order-by-order predictions of neutron-deuteron total cross sections\nup to next-to-next-to-leading order in pionless effective field theory. Using\nBayesian methods, we infer a posterior distribution for the breakdown scale.\nThe result shows a mode near 100 MeV, and a combined analysis with\nneutron-proton scattering further sharpens the inference, placing the mode\nclose to the pion mass scale, consistent with the expected range of pionless\neffective field theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We make order-by-order predictions of neutron-deuteron total cross sections\nup to next-to-next-to-leading order in pionless effective field theory. Using\nBayesian methods, we infer a posterior distribution for the breakdown scale.\nThe result shows a mode near 100 MeV, and a combined analysis with\nneutron-proton scattering further sharpens the inference, placing the mode\nclose to the pion mass scale, consistent with the expected range of pionless\neffective field theory."
                },
                "authors": [
                    {
                        "name": "Andreas Ekström"
                    },
                    {
                        "name": "Lucas Platter"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Platter"
                },
                "author": "Lucas Platter",
                "arxiv_comment": "6 pages, 5 figured",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00927v2",
                "updated": "2025-07-11T15:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    49,
                    30,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-01T15:59:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    59,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Multi-Token Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Token Attention"
                },
                "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial."
                },
                "authors": [
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    }
                ],
                "author_detail": {
                    "name": "Sainbayar Sukhbaatar"
                },
                "author": "Sainbayar Sukhbaatar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08686v1",
                "updated": "2025-07-11T15:37:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    37,
                    24,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:37:24Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    37,
                    24,
                    4,
                    192,
                    0
                ],
                "title": "Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and\n  Distillation"
                },
                "summary": "Overfitting in deep neural networks occurs less frequently than expected.\nThis is a puzzling observation, as theory predicts that greater model capacity\nshould eventually lead to overfitting -- yet this is rarely seen in practice.\nBut what if overfitting does occur, not globally, but in specific sub-regions\nof the data space? In this work, we introduce a novel score that measures the\nforgetting rate of deep models on validation data, capturing what we term local\noverfitting: a performance degradation confined to certain regions of the input\nspace. We demonstrate that local overfitting can arise even without\nconventional overfitting, and is closely linked to the double descent\nphenomenon.\n  Building on these insights, we introduce a two-stage approach that leverages\nthe training history of a single model to recover and retain forgotten\nknowledge: first, by aggregating checkpoints into an ensemble, and then by\ndistilling it into a single model of the original size, thus enhancing\nperformance without added inference cost.\n  Extensive experiments across multiple datasets, modern architectures, and\ntraining regimes validate the effectiveness of our approach. Notably, in the\npresence of label noise, our method -- Knowledge Fusion followed by Knowledge\nDistillation -- outperforms both the original model and independently trained\nensembles, achieving a rare win-win scenario: reduced training and inference\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overfitting in deep neural networks occurs less frequently than expected.\nThis is a puzzling observation, as theory predicts that greater model capacity\nshould eventually lead to overfitting -- yet this is rarely seen in practice.\nBut what if overfitting does occur, not globally, but in specific sub-regions\nof the data space? In this work, we introduce a novel score that measures the\nforgetting rate of deep models on validation data, capturing what we term local\noverfitting: a performance degradation confined to certain regions of the input\nspace. We demonstrate that local overfitting can arise even without\nconventional overfitting, and is closely linked to the double descent\nphenomenon.\n  Building on these insights, we introduce a two-stage approach that leverages\nthe training history of a single model to recover and retain forgotten\nknowledge: first, by aggregating checkpoints into an ensemble, and then by\ndistilling it into a single model of the original size, thus enhancing\nperformance without added inference cost.\n  Extensive experiments across multiple datasets, modern architectures, and\ntraining regimes validate the effectiveness of our approach. Notably, in the\npresence of label noise, our method -- Knowledge Fusion followed by Knowledge\nDistillation -- outperforms both the original model and independently trained\nensembles, achieving a rare win-win scenario: reduced training and inference\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Uri Stern"
                    },
                    {
                        "name": "Eli Corn"
                    },
                    {
                        "name": "Daphna Weinshall"
                    }
                ],
                "author_detail": {
                    "name": "Daphna Weinshall"
                },
                "author": "Daphna Weinshall",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2412.12968",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08679v1",
                "updated": "2025-07-11T15:21:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:21:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way"
                },
                "summary": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting."
                },
                "authors": [
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Ankesh Banerjee"
                    },
                    {
                        "name": "Arjya Bhattacharjee"
                    },
                    {
                        "name": "Kousik Dasgupta"
                    },
                    {
                        "name": "Subarna Tripathi"
                    }
                ],
                "author_detail": {
                    "name": "Subarna Tripathi"
                },
                "author": "Subarna Tripathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02827v2",
                "updated": "2025-07-11T15:13:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    13,
                    39,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-03T17:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "title": "USAD: End-to-End Human Activity Recognition via Diffusion Model with\n  Spatiotemporal Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USAD: End-to-End Human Activity Recognition via Diffusion Model with\n  Spatiotemporal Attention"
                },
                "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Hang Xiao"
                    },
                    {
                        "name": "Ying Yu"
                    },
                    {
                        "name": "Jiarui Li"
                    },
                    {
                        "name": "Zhifan Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08671v1",
                "updated": "2025-07-11T15:11:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    11,
                    27,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:11:27Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    11,
                    27,
                    4,
                    192,
                    0
                ],
                "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCup: Ranking-Enhanced Comment Updating with LLMs"
                },
                "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment."
                },
                "authors": [
                    {
                        "name": "Hua Ge"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Minxue Pan"
                    },
                    {
                        "name": "Fusen He"
                    },
                    {
                        "name": "Ziyue Tan"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Tan"
                },
                "author": "Ziyue Tan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; D.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08665v1",
                "updated": "2025-07-11T15:05:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    5,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:05:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    5,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment"
                },
                "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Jiyao Zhang"
                    },
                    {
                        "name": "Chengli Zhong"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Qige Li"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08664v1",
                "updated": "2025-07-11T15:03:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    3,
                    17,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:03:17Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    3,
                    17,
                    4,
                    192,
                    0
                ],
                "title": "Introspection of Thought Helps AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introspection of Thought Helps AI Agents"
                },
                "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shaoning Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Shaoning Zeng"
                },
                "author": "Shaoning Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08650v1",
                "updated": "2025-07-11T14:53:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    19,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:53:19Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    19,
                    4,
                    192,
                    0
                ],
                "title": "Robust inference under Benford's law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust inference under Benford's law"
                },
                "summary": "We address the task of identifying anomalous observations by analyzing digits\nunder the lens of Benford's law. Motivated by the crucial objective of\nproviding reliable statistical analysis of customs declarations, we answer one\nmajor and still open question: How can we detect the behavior of operators who\nare aware of the prevalence of the Benford's pattern in the digits of regular\nobservations and try to manipulate their data in such a way that the same\npattern also holds after data fabrication? This challenge arises from the\nability of highly skilled and strategically minded manipulators in key\norganizational positions or criminal networks to exploit statistical knowledge\nand evade detection. For this purpose, we write a specific contamination model\nfor digits, obtain new relevant distributional results and derive appropriate\ngoodness-of-fit statistics for the considered adversarial testing problem.\nAlong our path, we also unveil the peculiar relationship between two simple\nconformance tests based on the distribution of the first digit. We show the\nempirical properties of the proposed tests through a simulation exercise and\napplication to data from international trade transactions. Although we cannot\nclaim that our results are able to anticipate data fabrication with certainty,\nthey surely point to situations where more substantial controls are needed.\nFurthermore, our work can reinforce trust in data integrity in many critical\ndomains where mathematically informed misconduct is suspected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of identifying anomalous observations by analyzing digits\nunder the lens of Benford's law. Motivated by the crucial objective of\nproviding reliable statistical analysis of customs declarations, we answer one\nmajor and still open question: How can we detect the behavior of operators who\nare aware of the prevalence of the Benford's pattern in the digits of regular\nobservations and try to manipulate their data in such a way that the same\npattern also holds after data fabrication? This challenge arises from the\nability of highly skilled and strategically minded manipulators in key\norganizational positions or criminal networks to exploit statistical knowledge\nand evade detection. For this purpose, we write a specific contamination model\nfor digits, obtain new relevant distributional results and derive appropriate\ngoodness-of-fit statistics for the considered adversarial testing problem.\nAlong our path, we also unveil the peculiar relationship between two simple\nconformance tests based on the distribution of the first digit. We show the\nempirical properties of the proposed tests through a simulation exercise and\napplication to data from international trade transactions. Although we cannot\nclaim that our results are able to anticipate data fabrication with certainty,\nthey surely point to situations where more substantial controls are needed.\nFurthermore, our work can reinforce trust in data integrity in many critical\ndomains where mathematically informed misconduct is suspected."
                },
                "authors": [
                    {
                        "name": "Lucio Barabesi"
                    },
                    {
                        "name": "Andrea Cerioli"
                    },
                    {
                        "name": "Andrea Cerasa"
                    },
                    {
                        "name": "Domenico Perrotta"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Perrotta"
                },
                "author": "Domenico Perrotta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08649v1",
                "updated": "2025-07-11T14:53:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    14,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:53:14Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    14,
                    4,
                    192,
                    0
                ],
                "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning"
                },
                "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2."
                },
                "authors": [
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Chenxi Sun"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07257v2",
                "updated": "2025-07-11T14:43:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    43,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T20:03:30Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    20,
                    3,
                    30,
                    2,
                    190,
                    0
                ],
                "title": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery"
                },
                "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud."
                },
                "authors": [
                    {
                        "name": "Licong Xu"
                    },
                    {
                        "name": "Milind Sarkar"
                    },
                    {
                        "name": "Anto I. Lonappan"
                    },
                    {
                        "name": "Íñigo Zubeldia"
                    },
                    {
                        "name": "Pablo Villanueva-Domingo"
                    },
                    {
                        "name": "Santiago Casas"
                    },
                    {
                        "name": "Christian Fidler"
                    },
                    {
                        "name": "Chetana Amancharla"
                    },
                    {
                        "name": "Ujjwal Tiwari"
                    },
                    {
                        "name": "Adrian Bayer"
                    },
                    {
                        "name": "Chadi Ait Ekioui"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Adrian Dimitrov"
                    },
                    {
                        "name": "James Fergusson"
                    },
                    {
                        "name": "Kahaan Gandhi"
                    },
                    {
                        "name": "Sven Krippendorf"
                    },
                    {
                        "name": "Andrew Laverick"
                    },
                    {
                        "name": "Julien Lesgourgues"
                    },
                    {
                        "name": "Antony Lewis"
                    },
                    {
                        "name": "Thomas Meier"
                    },
                    {
                        "name": "Blake Sherwin"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Xueqing Xu"
                    },
                    {
                        "name": "Boris Bolliet"
                    }
                ],
                "author_detail": {
                    "name": "Boris Bolliet"
                },
                "author": "Boris Bolliet",
                "arxiv_comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent Videos:\n  https://www.youtube.com/@cmbagent HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent Cloud:\n  https://cmbagent.cloud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08054v2",
                "updated": "2025-07-11T14:40:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    40,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2024-08-15T09:48:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework"
                },
                "summary": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM"
                },
                "authors": [
                    {
                        "name": "Changyu Du"
                    },
                    {
                        "name": "Sebastian Esser"
                    },
                    {
                        "name": "Stavros Nousias"
                    },
                    {
                        "name": "André Borrmann"
                    }
                ],
                "author_detail": {
                    "name": "André Borrmann"
                },
                "author": "André Borrmann",
                "arxiv_comment": "Journal of Computing in Civil Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07346v2",
                "updated": "2025-07-11T14:40:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    40,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T00:19:11Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    0,
                    19,
                    11,
                    3,
                    191,
                    0
                ],
                "title": "CLASS_SZ II: Notes and Examples of Fast and Accurate Calculations of\n  Halo Model, Large Scale Structure and Cosmic Microwave Background Observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLASS_SZ II: Notes and Examples of Fast and Accurate Calculations of\n  Halo Model, Large Scale Structure and Cosmic Microwave Background Observables"
                },
                "summary": "These notes are very much work-in-progress and simply intended to showcase,\nin various degrees of details (and rigour), some of the cosmology calculations\nthat class_sz can do. We describe the class_sz code in C, Python and Jax. Based\non the Boltzmann code class, it can compute a wide range of observables\nrelevant to current and forthcoming CMB and Large Scale Structure surveys. This\nincludes galaxy shear and clustering, CMB lensing, thermal and kinetic Sunyaev\nand Zeldovich observables, Cosmic Infrared Background, cross-correlations and\nthree-point statistics. Calculations can be done either within the halo model\nor the linear bias model. For standard $\\Lambda$CDM cosmology and extensions,\nclass_sz uses high-accuracy cosmopower emulators of the CMB and matter power\nspectrum to accelerate calculations. With this, along with efficient numerical\nintegration routines, most class_sz output can be obtained in less than 500 ms\n(CMB $C_\\ell$'s or matter $P(k)$ take $\\mathcal{O}(1\\mathrm{ms})$), allowing\nfor fast or ultra-fast parameter inference analyses. Parts of the calculations\nare \"jaxified\", so the software can be integrated into differentiable\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "These notes are very much work-in-progress and simply intended to showcase,\nin various degrees of details (and rigour), some of the cosmology calculations\nthat class_sz can do. We describe the class_sz code in C, Python and Jax. Based\non the Boltzmann code class, it can compute a wide range of observables\nrelevant to current and forthcoming CMB and Large Scale Structure surveys. This\nincludes galaxy shear and clustering, CMB lensing, thermal and kinetic Sunyaev\nand Zeldovich observables, Cosmic Infrared Background, cross-correlations and\nthree-point statistics. Calculations can be done either within the halo model\nor the linear bias model. For standard $\\Lambda$CDM cosmology and extensions,\nclass_sz uses high-accuracy cosmopower emulators of the CMB and matter power\nspectrum to accelerate calculations. With this, along with efficient numerical\nintegration routines, most class_sz output can be obtained in less than 500 ms\n(CMB $C_\\ell$'s or matter $P(k)$ take $\\mathcal{O}(1\\mathrm{ms})$), allowing\nfor fast or ultra-fast parameter inference analyses. Parts of the calculations\nare \"jaxified\", so the software can be integrated into differentiable\npipelines."
                },
                "authors": [
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "Aleksandra Kusiak"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Alina Sabyr"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Jens Chluba"
                    },
                    {
                        "name": "Carmen Embil Villagra"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Boryana Hadzhiyska"
                    },
                    {
                        "name": "Dongwon Han"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Juan Francisco Macías-Pérez"
                    },
                    {
                        "name": "Mathew Madhavacheril"
                    },
                    {
                        "name": "Abhishek Maniyar"
                    },
                    {
                        "name": "Yogesh Mehta"
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Emmanuel Schaan"
                    },
                    {
                        "name": "Blake Sherwin"
                    },
                    {
                        "name": "Alessio Spurio Mancini"
                    },
                    {
                        "name": "Íñigo Zubeldia"
                    }
                ],
                "author_detail": {
                    "name": "Íñigo Zubeldia"
                },
                "author": "Íñigo Zubeldia",
                "arxiv_comment": "Code: https://github.com/CLASS-SZ. arXiv admin note: text overlap\n  with arXiv:2208.07847 (from appendix of that paper). Fixed affiliation and\n  minor details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00467v2",
                "updated": "2025-07-11T14:39:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    39,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-05-01T11:43:27Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    11,
                    43,
                    27,
                    3,
                    121,
                    0
                ],
                "title": "Red Teaming Large Language Models for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Teaming Large Language Models for Healthcare"
                },
                "summary": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided."
                },
                "authors": [
                    {
                        "name": "Vahid Balazadeh"
                    },
                    {
                        "name": "Michael Cooper"
                    },
                    {
                        "name": "David Pellow"
                    },
                    {
                        "name": "Atousa Assadi"
                    },
                    {
                        "name": "Jennifer Bell"
                    },
                    {
                        "name": "Mark Coatsworth"
                    },
                    {
                        "name": "Kaivalya Deshpande"
                    },
                    {
                        "name": "Jim Fackler"
                    },
                    {
                        "name": "Gabriel Funingana"
                    },
                    {
                        "name": "Spencer Gable-Cook"
                    },
                    {
                        "name": "Anirudh Gangadhar"
                    },
                    {
                        "name": "Abhishek Jaiswal"
                    },
                    {
                        "name": "Sumanth Kaja"
                    },
                    {
                        "name": "Christopher Khoury"
                    },
                    {
                        "name": "Amrit Krishnan"
                    },
                    {
                        "name": "Randy Lin"
                    },
                    {
                        "name": "Kaden McKeen"
                    },
                    {
                        "name": "Sara Naimimohasses"
                    },
                    {
                        "name": "Khashayar Namdar"
                    },
                    {
                        "name": "Aviraj Newatia"
                    },
                    {
                        "name": "Allan Pang"
                    },
                    {
                        "name": "Anshul Pattoo"
                    },
                    {
                        "name": "Sameer Peesapati"
                    },
                    {
                        "name": "Diana Prepelita"
                    },
                    {
                        "name": "Bogdana Rakova"
                    },
                    {
                        "name": "Saba Sadatamin"
                    },
                    {
                        "name": "Rafael Schulman"
                    },
                    {
                        "name": "Ajay Shah"
                    },
                    {
                        "name": "Syed Azhar Shah"
                    },
                    {
                        "name": "Syed Ahmar Shah"
                    },
                    {
                        "name": "Babak Taati"
                    },
                    {
                        "name": "Balagopal Unnikrishnan"
                    },
                    {
                        "name": "Iñigo Urteaga"
                    },
                    {
                        "name": "Stephanie Williams"
                    },
                    {
                        "name": "Rahul G Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G Krishnan"
                },
                "author": "Rahul G Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08629v1",
                "updated": "2025-07-11T14:31:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:31:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Nonparametric predictive inference for discrete data via\n  Metropolis-adjusted Dirichlet sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric predictive inference for discrete data via\n  Metropolis-adjusted Dirichlet sequences"
                },
                "summary": "This article is motivated by challenges in conducting Bayesian inferences on\nunknown discrete distributions, with a particular focus on count data. To avoid\nthe computational disadvantages of traditional mixture models, we develop a\nnovel Bayesian predictive approach. In particular, our Metropolis-adjusted\nDirichlet (MAD) sequence model characterizes the predictive measure as a\nmixture of a base measure and Metropolis-Hastings kernels centered on previous\ndata points. The resulting MAD sequence is asymptotically exchangeable and the\nposterior on the data generator takes the form of a martingale posterior. This\nstructure leads to straightforward algorithms for inference on count\ndistributions, with easy extensions to multivariate, regression, and binary\ndata cases. We obtain a useful asymptotic Gaussian approximation and illustrate\nthe methodology on a variety of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article is motivated by challenges in conducting Bayesian inferences on\nunknown discrete distributions, with a particular focus on count data. To avoid\nthe computational disadvantages of traditional mixture models, we develop a\nnovel Bayesian predictive approach. In particular, our Metropolis-adjusted\nDirichlet (MAD) sequence model characterizes the predictive measure as a\nmixture of a base measure and Metropolis-Hastings kernels centered on previous\ndata points. The resulting MAD sequence is asymptotically exchangeable and the\nposterior on the data generator takes the form of a martingale posterior. This\nstructure leads to straightforward algorithms for inference on count\ndistributions, with easy extensions to multivariate, regression, and binary\ndata cases. We obtain a useful asymptotic Gaussian approximation and illustrate\nthe methodology on a variety of applications."
                },
                "authors": [
                    {
                        "name": "Davide Agnoletto"
                    },
                    {
                        "name": "Tommaso Rigon"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08627v1",
                "updated": "2025-07-11T14:29:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations"
                },
                "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt."
                },
                "authors": [
                    {
                        "name": "Chi-en Amy Tai"
                    },
                    {
                        "name": "Pengyu Nie"
                    },
                    {
                        "name": "Lukasz Golab"
                    },
                    {
                        "name": "Alexander Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wong"
                },
                "author": "Alexander Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08626v1",
                "updated": "2025-07-11T14:27:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:27:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection"
                },
                "summary": "Recent advances in generative AI have made the creation of speech deepfakes\nwidely accessible, posing serious challenges to digital trust. To counter this,\nvarious speech deepfake detection strategies have been proposed, including\nPerson-of-Interest (POI) approaches, which focus on identifying impersonations\nof specific individuals by modeling and analyzing their unique vocal traits.\nDespite their excellent performance, the existing methods offer limited\ngranularity and lack interpretability. In this work, we propose a POI-based\nspeech deepfake detection method that operates at the phoneme level. Our\napproach decomposes reference audio into phonemes to construct a detailed\nspeaker profile. In inference, phonemes from a test sample are individually\ncompared against this profile, enabling fine-grained detection of synthetic\nartifacts. The proposed method achieves comparable accuracy to traditional\napproaches while offering superior robustness and interpretability, key aspects\nin multimedia forensics. By focusing on phoneme analysis, this work explores a\nnovel direction for explainable, speaker-centric deepfake detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have made the creation of speech deepfakes\nwidely accessible, posing serious challenges to digital trust. To counter this,\nvarious speech deepfake detection strategies have been proposed, including\nPerson-of-Interest (POI) approaches, which focus on identifying impersonations\nof specific individuals by modeling and analyzing their unique vocal traits.\nDespite their excellent performance, the existing methods offer limited\ngranularity and lack interpretability. In this work, we propose a POI-based\nspeech deepfake detection method that operates at the phoneme level. Our\napproach decomposes reference audio into phonemes to construct a detailed\nspeaker profile. In inference, phonemes from a test sample are individually\ncompared against this profile, enabling fine-grained detection of synthetic\nartifacts. The proposed method achieves comparable accuracy to traditional\napproaches while offering superior robustness and interpretability, key aspects\nin multimedia forensics. By focusing on phoneme analysis, this work explores a\nnovel direction for explainable, speaker-centric deepfake detection."
                },
                "authors": [
                    {
                        "name": "Davide Salvi"
                    },
                    {
                        "name": "Viola Negroni"
                    },
                    {
                        "name": "Sara Mandelli"
                    },
                    {
                        "name": "Paolo Bestagini"
                    },
                    {
                        "name": "Stefano Tubaro"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Tubaro"
                },
                "author": "Stefano Tubaro",
                "arxiv_comment": "Accepted at ICCV Workshop - Authenticity & Provenance in the age of\n  Generative AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08623v1",
                "updated": "2025-07-11T14:25:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    25,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:25:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    25,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine\n  Learning Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine\n  Learning Security"
                },
                "summary": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning."
                },
                "authors": [
                    {
                        "name": "Pascal Debus"
                    },
                    {
                        "name": "Maximilian Wendlinger"
                    },
                    {
                        "name": "Kilian Tscharke"
                    },
                    {
                        "name": "Daniel Herr"
                    },
                    {
                        "name": "Cedric Brügmann"
                    },
                    {
                        "name": "Daniel Ohl de Mello"
                    },
                    {
                        "name": "Juris Ulmanis"
                    },
                    {
                        "name": "Alexander Erhard"
                    },
                    {
                        "name": "Arthur Schmidt"
                    },
                    {
                        "name": "Fabian Petsch"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Petsch"
                },
                "author": "Fabian Petsch",
                "arxiv_comment": "Accepted for publication at IEEE International Conference on Quantum\n  Computing and Engineering (QCE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08621v1",
                "updated": "2025-07-11T14:23:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1"
                },
                "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings."
                },
                "authors": [
                    {
                        "name": "Marcin Pietroń"
                    },
                    {
                        "name": "Rafał Olszowski"
                    },
                    {
                        "name": "Jakub Gomułka"
                    },
                    {
                        "name": "Filip Gampel"
                    },
                    {
                        "name": "Andrzej Tomski"
                    }
                ],
                "author_detail": {
                    "name": "Andrzej Tomski"
                },
                "author": "Andrzej Tomski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08311v2",
                "updated": "2025-07-11T14:23:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    16,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-11T11:21:35Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    21,
                    35,
                    1,
                    70,
                    0
                ],
                "title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference"
                },
                "summary": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap."
                },
                "authors": [
                    {
                        "name": "Pol G. Recasens"
                    },
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Pol G. Recasens, Ferran Agullo: equal contribution. Paper accepted at\n  IEEE CLOUD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08619v1",
                "updated": "2025-07-11T14:19:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:19:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design"
                },
                "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted."
                },
                "authors": [
                    {
                        "name": "Soheyl Massoudi"
                    },
                    {
                        "name": "Mark Fuge"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fuge"
                },
                "author": "Mark Fuge",
                "arxiv_comment": "32 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08616v1",
                "updated": "2025-07-11T14:13:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    13,
                    22,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:13:22Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    13,
                    22,
                    4,
                    192,
                    0
                ],
                "title": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs"
                },
                "summary": "Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents."
                },
                "authors": [
                    {
                        "name": "Florian Grötschla"
                    },
                    {
                        "name": "Luis Müller"
                    },
                    {
                        "name": "Jan Tönshoff"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Bryan Perozzi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Perozzi"
                },
                "author": "Bryan Perozzi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08610v1",
                "updated": "2025-07-11T14:08:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    8,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:08:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    8,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data"
                },
                "summary": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods."
                },
                "authors": [
                    {
                        "name": "Parag Dutta"
                    },
                    {
                        "name": "Ambedkar Dukkipati"
                    }
                ],
                "author_detail": {
                    "name": "Ambedkar Dukkipati"
                },
                "author": "Ambedkar Dukkipati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08603v1",
                "updated": "2025-07-11T13:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    55,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    55,
                    45,
                    4,
                    192,
                    0
                ],
                "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Speech Instruction Data Potential with Query Rewriting"
                },
                "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities."
                },
                "authors": [
                    {
                        "name": "Yonghua Hei"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Huiyu Zhou"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09287v2",
                "updated": "2025-07-11T13:48:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    48,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-12T17:14:47Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    17,
                    14,
                    47,
                    5,
                    102,
                    0
                ],
                "title": "Looking into the Jet Cone of the Neutrino-Associated Very High Energy\n  Blazar PKS 1424+240",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looking into the Jet Cone of the Neutrino-Associated Very High Energy\n  Blazar PKS 1424+240"
                },
                "summary": "The acceleration process of massive particles as well as the production of\nvery high energy (VHE) photons and neutrinos remains a fundamental challenge in\nastrophysics. We investigate the parsec-scale jet structure and magnetic field\nof the blazar PKS 1424+240, that has been selected on the basis of strong VHE\ngamma-ray emission and identified with one of the highest peaks in the IceCube\n9-year neutrino sky. We analyze 15 GHz VLBA observations of this BL Lac object,\nstacking 42 polarization-sensitive images collected in 2009-2025 to enhance the\nsignal and reveal persistent parsec-scale structure. Our observations uncover a\nrare scenario. The object is viewed inside the jet cone, very close to the axis\nof its relativistic jet, with a viewing angle of <0.6 deg. This effectively\nmaximizes Doppler boosting to values ~30 and enhances both electromagnetic and\nneutrino emission in the observer's direction. Based on polarimetric\nobservations, we unambiguously detect a net toroidal component of the jet's\nmagnetic field, indicating a current carrying jet flowing almost directly\ntowards our line of sight. Blazars with very small jet viewing angles offer a\nsolution to the longstanding mismatch between Doppler factors inferred from low\nVLBI apparent jet speed and those derived from VHE observations -- the\nso-called \"Doppler factor crisis\". We show that relativistic beaming plays the\ncritical role in the gamma-ray and neutrino emission of blazars, with direct\nimplications for models of their multi-messenger emission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The acceleration process of massive particles as well as the production of\nvery high energy (VHE) photons and neutrinos remains a fundamental challenge in\nastrophysics. We investigate the parsec-scale jet structure and magnetic field\nof the blazar PKS 1424+240, that has been selected on the basis of strong VHE\ngamma-ray emission and identified with one of the highest peaks in the IceCube\n9-year neutrino sky. We analyze 15 GHz VLBA observations of this BL Lac object,\nstacking 42 polarization-sensitive images collected in 2009-2025 to enhance the\nsignal and reveal persistent parsec-scale structure. Our observations uncover a\nrare scenario. The object is viewed inside the jet cone, very close to the axis\nof its relativistic jet, with a viewing angle of <0.6 deg. This effectively\nmaximizes Doppler boosting to values ~30 and enhances both electromagnetic and\nneutrino emission in the observer's direction. Based on polarimetric\nobservations, we unambiguously detect a net toroidal component of the jet's\nmagnetic field, indicating a current carrying jet flowing almost directly\ntowards our line of sight. Blazars with very small jet viewing angles offer a\nsolution to the longstanding mismatch between Doppler factors inferred from low\nVLBI apparent jet speed and those derived from VHE observations -- the\nso-called \"Doppler factor crisis\". We show that relativistic beaming plays the\ncritical role in the gamma-ray and neutrino emission of blazars, with direct\nimplications for models of their multi-messenger emission."
                },
                "authors": [
                    {
                        "name": "Y. Y. Kovalev"
                    },
                    {
                        "name": "A. B. Pushkarev"
                    },
                    {
                        "name": "J. L. Gomez"
                    },
                    {
                        "name": "D. C. Homan"
                    },
                    {
                        "name": "M. L. Lister"
                    },
                    {
                        "name": "J. D. Livingston"
                    },
                    {
                        "name": "I. N. Pashchenko"
                    },
                    {
                        "name": "A. V. Plavin"
                    },
                    {
                        "name": "T. Savolainen"
                    },
                    {
                        "name": "S. V. Troitsky"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Troitsky"
                },
                "arxiv_affiliation": "INR, MSU",
                "author": "S. V. Troitsky",
                "arxiv_comment": "5 pages, 5 figures; accepted by A&A Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07248v2",
                "updated": "2025-07-11T13:39:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    39,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T19:38:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    38,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings"
                },
                "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains."
                },
                "authors": [
                    {
                        "name": "Jean-Philippe Corbeil"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Francois Beaulieu"
                    },
                    {
                        "name": "Paul Vozila"
                    }
                ],
                "author_detail": {
                    "name": "Paul Vozila"
                },
                "author": "Paul Vozila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07531v2",
                "updated": "2025-07-11T13:32:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    32,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-10T07:54:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure"
                },
                "summary": "Epistemic injustice related to AI is a growing concern. In relation to\nmachine learning models, epistemic injustice can have a diverse range of\nsources, ranging from epistemic opacity, the discriminatory automation of\ntestimonial prejudice, and the distortion of human beliefs via generative AI's\nhallucinations to the exclusion of the global South in global AI governance,\nthe execution of bureaucratic violence via algorithmic systems, and\ninteractions with conversational artificial agents. Based on a proposed general\ntaxonomy of epistemic injustice, this paper first sketches a taxonomy of the\ntypes of epistemic injustice in the context of AI, relying on the work of\nscholars from the fields of philosophy of technology, political philosophy and\nsocial epistemology. Secondly, an additional conceptualization on epistemic\ninjustice in the context of AI is provided: generative hermeneutical erasure. I\nargue that this injustice the automation of 'epistemicide', the injustice done\nto epistemic agents in their capacity for collective sense-making through the\nsuppression of difference in epistemology and conceptualization by LLMs. AI\nsystems' 'view from nowhere' epistemically inferiorizes non-Western\nepistemologies and thereby contributes to the erosion of their epistemic\nparticulars, gradually contributing to hermeneutical erasure. This work's\nrelevance lies in proposal of a taxonomy that allows epistemic injustices to be\nmapped in the AI domain and the proposal of a novel form of AI-related\nepistemic injustice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic injustice related to AI is a growing concern. In relation to\nmachine learning models, epistemic injustice can have a diverse range of\nsources, ranging from epistemic opacity, the discriminatory automation of\ntestimonial prejudice, and the distortion of human beliefs via generative AI's\nhallucinations to the exclusion of the global South in global AI governance,\nthe execution of bureaucratic violence via algorithmic systems, and\ninteractions with conversational artificial agents. Based on a proposed general\ntaxonomy of epistemic injustice, this paper first sketches a taxonomy of the\ntypes of epistemic injustice in the context of AI, relying on the work of\nscholars from the fields of philosophy of technology, political philosophy and\nsocial epistemology. Secondly, an additional conceptualization on epistemic\ninjustice in the context of AI is provided: generative hermeneutical erasure. I\nargue that this injustice the automation of 'epistemicide', the injustice done\nto epistemic agents in their capacity for collective sense-making through the\nsuppression of difference in epistemology and conceptualization by LLMs. AI\nsystems' 'view from nowhere' epistemically inferiorizes non-Western\nepistemologies and thereby contributes to the erosion of their epistemic\nparticulars, gradually contributing to hermeneutical erasure. This work's\nrelevance lies in proposal of a taxonomy that allows epistemic injustices to be\nmapped in the AI domain and the proposal of a novel form of AI-related\nepistemic injustice."
                },
                "authors": [
                    {
                        "name": "Warmhold Jan Thomas Mollema"
                    }
                ],
                "author_detail": {
                    "name": "Warmhold Jan Thomas Mollema"
                },
                "author": "Warmhold Jan Thomas Mollema",
                "arxiv_comment": "33 pages; 3 figures; 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08584v1",
                "updated": "2025-07-11T13:29:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    29,
                    32,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:29:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    29,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions."
                },
                "authors": [
                    {
                        "name": "Dimitrios Emmanoulopoulos"
                    },
                    {
                        "name": "Ollie Olby"
                    },
                    {
                        "name": "Justin Lyon"
                    },
                    {
                        "name": "Namid R. Stillman"
                    }
                ],
                "author_detail": {
                    "name": "Namid R. Stillman"
                },
                "author": "Namid R. Stillman",
                "arxiv_comment": "31 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 65C05, 68T01, 60H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.0; I.2.1; I.2.3; I.2.4; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08567v1",
                "updated": "2025-07-11T13:11:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    11,
                    11,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:11:11Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    11,
                    11,
                    4,
                    192,
                    0
                ],
                "title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling"
                },
                "summary": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters."
                },
                "authors": [
                    {
                        "name": "Preslav Aleksandrov"
                    },
                    {
                        "name": "Meghdad Kurmanji"
                    },
                    {
                        "name": "Fernando Garcia Redondo"
                    },
                    {
                        "name": "David O'Shea"
                    },
                    {
                        "name": "William Shen"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "14 pages and 6 figures. Submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08564v1",
                "updated": "2025-07-11T13:07:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    7,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:07:04Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    7,
                    4,
                    4,
                    192,
                    0
                ],
                "title": "A composition-informed search for large-scale anisotropy with the Pierre\n  Auger Observatory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A composition-informed search for large-scale anisotropy with the Pierre\n  Auger Observatory"
                },
                "summary": "The large-scale dipolar structure in the arrival directions of\nultra-high-energy cosmic rays with energies above $8\\,$EeV observed by the\nPierre Auger Collaboration is a well-established anisotropy measurement. This\nanisotropy is understood to be of extragalactic origin, as the maximum of the\ndipolar component is located ${\\sim}115^\\circ$ away from the Galactic Center.\nCosmic rays interact with background radiation and magnetized regions on their\npath from their sources to Earth. These interactions, which depend on the\ncosmic-ray energy, charge and mass composition, give rise to different horizons\nand deflections that are expected to lead to different anisotropies in the\narrival directions of cosmic rays at Earth. The Auger Collaboration has\ndetermined that the mass composition of cosmic rays at ultra-high energies is\nmixed, becoming increasingly heavier as a function of energy. Thus, different\ndipole amplitudes are expected to be measured at a given energy when separating\nthe data into composition-distinct subsets of lighter and heavier elements.\n  In this contribution, we investigate the composition signature on the\nlarge-scale anisotropy taking advantage of composition estimators obtained from\nthe data gathered with the surface detector. A way of probing for composition\nsignatures in anisotropy patterns is then to divide the data into subsets of\n``lighter'' and ``heavier'' elements per energy bin. In a simulation library,\nwe evaluate the possibility of measuring a separation in total dipole amplitude\nbetween two such populations of the measured dataset under a source-agnostic\nmodel. We present the results using two different composition estimators, one\nbased on air-shower universality and one inferred with deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale dipolar structure in the arrival directions of\nultra-high-energy cosmic rays with energies above $8\\,$EeV observed by the\nPierre Auger Collaboration is a well-established anisotropy measurement. This\nanisotropy is understood to be of extragalactic origin, as the maximum of the\ndipolar component is located ${\\sim}115^\\circ$ away from the Galactic Center.\nCosmic rays interact with background radiation and magnetized regions on their\npath from their sources to Earth. These interactions, which depend on the\ncosmic-ray energy, charge and mass composition, give rise to different horizons\nand deflections that are expected to lead to different anisotropies in the\narrival directions of cosmic rays at Earth. The Auger Collaboration has\ndetermined that the mass composition of cosmic rays at ultra-high energies is\nmixed, becoming increasingly heavier as a function of energy. Thus, different\ndipole amplitudes are expected to be measured at a given energy when separating\nthe data into composition-distinct subsets of lighter and heavier elements.\n  In this contribution, we investigate the composition signature on the\nlarge-scale anisotropy taking advantage of composition estimators obtained from\nthe data gathered with the surface detector. A way of probing for composition\nsignatures in anisotropy patterns is then to divide the data into subsets of\n``lighter'' and ``heavier'' elements per energy bin. In a simulation library,\nwe evaluate the possibility of measuring a separation in total dipole amplitude\nbetween two such populations of the measured dataset under a source-agnostic\nmodel. We present the results using two different composition estimators, one\nbased on air-shower universality and one inferred with deep learning."
                },
                "authors": [
                    {
                        "name": "G. Golup"
                    }
                ],
                "author_detail": {
                    "name": "G. Golup"
                },
                "arxiv_affiliation": "for the Pierre Auger Collaboration",
                "author": "G. Golup",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.01619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.01619v2",
                "updated": "2025-07-11T13:04:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    4,
                    40,
                    4,
                    192,
                    0
                ],
                "published": "2022-09-04T13:40:33Z",
                "published_parsed": [
                    2022,
                    9,
                    4,
                    13,
                    40,
                    33,
                    6,
                    247,
                    0
                ],
                "title": "Interpreting systems as solving POMDPs: a step towards a formal\n  understanding of agency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting systems as solving POMDPs: a step towards a formal\n  understanding of agency"
                },
                "summary": "Under what circumstances can a system be said to have beliefs and goals, and\nhow do such agency-related features relate to its physical state? Recent work\nhas proposed a notion of interpretation map, a function that maps the state of\na system to a probability distribution representing its beliefs about an\nexternal world. Such a map is not completely arbitrary, as the beliefs it\nattributes to the system must evolve over time in a manner that is consistent\nwith Bayes' theorem, and consequently the dynamics of a system constrain its\npossible interpretations. Here we build on this approach, proposing a notion of\ninterpretation not just in terms of beliefs but in terms of goals and actions.\nTo do this we make use of the existing theory of partially observable Markov\nprocesses (POMDPs): we say that a system can be interpreted as a solution to a\nPOMDP if it not only admits an interpretation map describing its beliefs about\nthe hidden state of a POMDP but also takes actions that are optimal according\nto its belief state. An agent is then a system together with an interpretation\nof this system as a POMDP solution. Although POMDPs are not the only possible\nformulation of what it means to have a goal, this nevertheless represents a\nstep towards a more general formal definition of what it means for a system to\nbe an agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under what circumstances can a system be said to have beliefs and goals, and\nhow do such agency-related features relate to its physical state? Recent work\nhas proposed a notion of interpretation map, a function that maps the state of\na system to a probability distribution representing its beliefs about an\nexternal world. Such a map is not completely arbitrary, as the beliefs it\nattributes to the system must evolve over time in a manner that is consistent\nwith Bayes' theorem, and consequently the dynamics of a system constrain its\npossible interpretations. Here we build on this approach, proposing a notion of\ninterpretation not just in terms of beliefs but in terms of goals and actions.\nTo do this we make use of the existing theory of partially observable Markov\nprocesses (POMDPs): we say that a system can be interpreted as a solution to a\nPOMDP if it not only admits an interpretation map describing its beliefs about\nthe hidden state of a POMDP but also takes actions that are optimal according\nto its belief state. An agent is then a system together with an interpretation\nof this system as a POMDP solution. Although POMDPs are not the only possible\nformulation of what it means to have a goal, this nevertheless represents a\nstep towards a more general formal definition of what it means for a system to\nbe an agent."
                },
                "authors": [
                    {
                        "name": "Martin Biehl"
                    },
                    {
                        "name": "Nathaniel Virgo"
                    }
                ],
                "author_detail": {
                    "name": "Nathaniel Virgo"
                },
                "author": "Nathaniel Virgo",
                "arxiv_doi": "10.1007/978-3-031-28719-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-28719-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2209.01619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.01619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, no figures, published in Proceedings of 3rd International\n  Workshop on Active Inference 2022",
                "arxiv_journal_ref": "Active Inference. IWAI 2022. Communications in Computer and\n  Information Science, vol 1721. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v3",
                "updated": "2025-07-11T12:58:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    58,
                    59,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experiments demonstrate\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation,\nwhile maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation,\nexhibiting robust feature transfer capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experiments demonstrate\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation,\nwhile maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation,\nexhibiting robust feature transfer capabilities."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Yugui Lin"
                    },
                    {
                        "name": "Pietro Lio"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Code and data will be made available on request. V3 Update: Add\n  Ablation Study and Discussion; Improve Introduction; Optimize Figures; Add\n  references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08557v1",
                "updated": "2025-07-11T12:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:57:51Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation"
                },
                "summary": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/"
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Zehua Chen"
                    },
                    {
                        "name": "Zeqian Ju"
                    },
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Weibei Dou"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v1",
                "updated": "2025-07-11T12:39:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08538v1",
                "updated": "2025-07-11T12:38:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    38,
                    2,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:38:02Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    38,
                    2,
                    4,
                    192,
                    0
                ],
                "title": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on\n  Multilingual Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on\n  Multilingual Benchmarks"
                },
                "summary": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language."
                },
                "authors": [
                    {
                        "name": "David Pomerenke"
                    },
                    {
                        "name": "Jonas Nothnagel"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03873v2",
                "updated": "2025-07-11T12:27:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    27,
                    37,
                    4,
                    192,
                    0
                ],
                "published": "2024-11-06T12:40:59Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    40,
                    59,
                    2,
                    311,
                    0
                ],
                "title": "Biomechanics-Aware Trajectory Optimization for Online Navigation during\n  Robotic Physiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomechanics-Aware Trajectory Optimization for Online Navigation during\n  Robotic Physiotherapy"
                },
                "summary": "Robotic devices provide a great opportunity to assist in delivering physical\ntherapy and rehabilitation movements, yet current robot-assisted methods\nstruggle to incorporate biomechanical metrics essential for safe and effective\ntherapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization\napproach to online robotic Navigation of human musculoskeletal loads for\nrotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the\nhuman shoulder into an optimal control framework, generating strain-minimizing\ntrajectories for real-time control of therapeutic movements. \\addedText{Its\ncore strength lies in the ability to adapt biomechanics-informed trajectories\nonline to unpredictable volitional human actions or reflexive reactions during\nphysical human-robot interaction based on robot-sensed motion and forces.\nBATON's adaptability is enabled by a real-time, model-based estimator that\ninfers changes in muscle activity via a rapid redundancy solver driven by robot\npose and force/torque sensor data. We validated BATON through physical\nhuman-robot interaction experiments, assessing response speed, motion\nsmoothness, and interaction forces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic devices provide a great opportunity to assist in delivering physical\ntherapy and rehabilitation movements, yet current robot-assisted methods\nstruggle to incorporate biomechanical metrics essential for safe and effective\ntherapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization\napproach to online robotic Navigation of human musculoskeletal loads for\nrotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the\nhuman shoulder into an optimal control framework, generating strain-minimizing\ntrajectories for real-time control of therapeutic movements. \\addedText{Its\ncore strength lies in the ability to adapt biomechanics-informed trajectories\nonline to unpredictable volitional human actions or reflexive reactions during\nphysical human-robot interaction based on robot-sensed motion and forces.\nBATON's adaptability is enabled by a real-time, model-based estimator that\ninfers changes in muscle activity via a rapid redundancy solver driven by robot\npose and force/torque sensor data. We validated BATON through physical\nhuman-robot interaction experiments, assessing response speed, motion\nsmoothness, and interaction forces."
                },
                "authors": [
                    {
                        "name": "Italo Belli"
                    },
                    {
                        "name": "Florian van Melis"
                    },
                    {
                        "name": "J. Micah Prendergast"
                    },
                    {
                        "name": "Ajay Seth"
                    },
                    {
                        "name": "Luka Peternel"
                    }
                ],
                "author_detail": {
                    "name": "Luka Peternel"
                },
                "author": "Luka Peternel",
                "arxiv_comment": "15 pages, 9 figures, under review. Major changes: title, use of\n  biomechanical model for online estimation of human muscle activation (leading\n  to revision in abstract, methods, results, figures, discussion, and\n  conclusion), broader review of related work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08526v1",
                "updated": "2025-07-11T12:25:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    25,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:25:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    25,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Anisotropic Diffusion of $e^\\pm$ in Pulsar Halos over Multiple Coherence\n  of Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anisotropic Diffusion of $e^\\pm$ in Pulsar Halos over Multiple Coherence\n  of Magnetic Field"
                },
                "summary": "The slow particle diffusion in pulsar halos, inferred from TeV gamma-ray\nsurface brightness profiles, is attributed to cross-field diffusion under the\nanisotropic diffusion model. This model assumes sub-Alfv\\'enic interstellar\nturbulence in the surrounding medium of the pulsar and a rough alignment of the\nline-of-sight of observers towards the pulsars with the local mean magnetic\nfield direction in the halo. In this model, the expected morphology of a pulsar\nhalo is highly dependent on the properties of the interstellar magnetic field.\nWe investigate the anisotropic diffusion of electron-positron pairs across\nmultiple coherence of magnetic fields in pulsar halos in this work. We focus\nparticularly on their influences on the predicted gamma-ray surface brightness\nprofile and the asymmetry of the halo's morphology, as well as the\nobservational expectations by the Large High Altitude Air Shower Observatory\n(LHAASO). Our results indicate that the requirement of a specific magnetic\nfield geometry can be alleviated when accounting for a limited (and realistic)\ncoherence length of the magnetic field in the model. Also, the halo's\nmorphology may appear less asymmetric, especially after being smoothed by the\npoint spread function of instruments. It largely relaxes the tension between\nthe asymmetric morphology of halos predicted by the model and lack of apparent\nasymmetric halos detected so far. Our findings demonstrate the important\ninfluence of the coherence length of interstellar magnetic field on the\ndistribution of particles around their accelerators, and the consequence on the\nmeasured source morphology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The slow particle diffusion in pulsar halos, inferred from TeV gamma-ray\nsurface brightness profiles, is attributed to cross-field diffusion under the\nanisotropic diffusion model. This model assumes sub-Alfv\\'enic interstellar\nturbulence in the surrounding medium of the pulsar and a rough alignment of the\nline-of-sight of observers towards the pulsars with the local mean magnetic\nfield direction in the halo. In this model, the expected morphology of a pulsar\nhalo is highly dependent on the properties of the interstellar magnetic field.\nWe investigate the anisotropic diffusion of electron-positron pairs across\nmultiple coherence of magnetic fields in pulsar halos in this work. We focus\nparticularly on their influences on the predicted gamma-ray surface brightness\nprofile and the asymmetry of the halo's morphology, as well as the\nobservational expectations by the Large High Altitude Air Shower Observatory\n(LHAASO). Our results indicate that the requirement of a specific magnetic\nfield geometry can be alleviated when accounting for a limited (and realistic)\ncoherence length of the magnetic field in the model. Also, the halo's\nmorphology may appear less asymmetric, especially after being smoothed by the\npoint spread function of instruments. It largely relaxes the tension between\nthe asymmetric morphology of halos predicted by the model and lack of apparent\nasymmetric halos detected so far. Our findings demonstrate the important\ninfluence of the coherence length of interstellar magnetic field on the\ndistribution of particles around their accelerators, and the consequence on the\nmeasured source morphology."
                },
                "authors": [
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Sha Wu"
                    },
                    {
                        "name": "Ruo-Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ruo-Yu Liu"
                },
                "author": "Ruo-Yu Liu",
                "arxiv_doi": "10.3847/1538-4357/add6a4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/add6a4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published by The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10657v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10657v4",
                "updated": "2025-07-11T12:24:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    24,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-15T12:16:33Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    12,
                    16,
                    33,
                    0,
                    197,
                    0
                ],
                "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Validating Synthetic Data for Formula Generation"
                },
                "summary": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data."
                },
                "authors": [
                    {
                        "name": "Usneek Singh"
                    },
                    {
                        "name": "José Cambronero"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Aditya Kanade"
                    },
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Vu Le"
                    },
                    {
                        "name": "Mukul Singh"
                    },
                    {
                        "name": "Gust Verbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Gust Verbruggen"
                },
                "author": "Gust Verbruggen",
                "arxiv_comment": "Accepted at Findings of NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10657v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10657v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17546v3",
                "updated": "2025-07-11T12:23:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    23,
                    34,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-21T21:41:48Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    41,
                    48,
                    4,
                    80,
                    0
                ],
                "title": "Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures"
                },
                "summary": "The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity, which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges cortical connectivity. Existing methods for inferring\nstructural connections from observed dynamics, such as correlation-based or\nspectral techniques, may fail to fully capture complex relationships in\nhigh-dimensional time series in an interpretable way. Here, we propose the use\nof path signatures, a mathematical framework that encodes geometric and\ntemporal properties of continuous paths, to address this problem. Path\nsignatures provide a reparametrization-invariant characterization of dynamical\ndata and can be used to compute the lead matrix, which reveals lead-lag\nphenomena. We showcase our approach on time series from coupled oscillators in\nthe Kuramoto model defined on a stochastic block model graph, termed the\nKuramoto Stochastic Block Model (KSBM). Using mean-field theory and Gaussian\napproximations, we analytically derive reduced models of KSBM dynamics in\ndifferent temporal regimes and theoretically characterize the lead matrix in\nthese settings. Leveraging these insights, we propose a novel signature-based\ncommunity detection algorithm, achieving exact recovery of structural\ncommunities from observed time series in multiple KSBM instances. We also\nexplored the performance of our community detection on a stochastic variant of\nthe KSBM as well as on real neuropixels of cortical recordings to demonstrate\napplicability on real-world data. Our results demonstrate that path signatures\nprovide a novel perspective on analyzing complex neural data and other\nhigh-dimensional systems, explicitly exploiting temporal functional\nrelationships to infer underlying structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity, which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges cortical connectivity. Existing methods for inferring\nstructural connections from observed dynamics, such as correlation-based or\nspectral techniques, may fail to fully capture complex relationships in\nhigh-dimensional time series in an interpretable way. Here, we propose the use\nof path signatures, a mathematical framework that encodes geometric and\ntemporal properties of continuous paths, to address this problem. Path\nsignatures provide a reparametrization-invariant characterization of dynamical\ndata and can be used to compute the lead matrix, which reveals lead-lag\nphenomena. We showcase our approach on time series from coupled oscillators in\nthe Kuramoto model defined on a stochastic block model graph, termed the\nKuramoto Stochastic Block Model (KSBM). Using mean-field theory and Gaussian\napproximations, we analytically derive reduced models of KSBM dynamics in\ndifferent temporal regimes and theoretically characterize the lead matrix in\nthese settings. Leveraging these insights, we propose a novel signature-based\ncommunity detection algorithm, achieving exact recovery of structural\ncommunities from observed time series in multiple KSBM instances. We also\nexplored the performance of our community detection on a stochastic variant of\nthe KSBM as well as on real neuropixels of cortical recordings to demonstrate\napplicability on real-world data. Our results demonstrate that path signatures\nprovide a novel perspective on analyzing complex neural data and other\nhigh-dimensional systems, explicitly exploiting temporal functional\nrelationships to infer underlying structure."
                },
                "authors": [
                    {
                        "name": "Tâm Johan Nguyên"
                    },
                    {
                        "name": "Darrick Lee"
                    },
                    {
                        "name": "Bernadette Jana Stolz"
                    }
                ],
                "author_detail": {
                    "name": "Bernadette Jana Stolz"
                },
                "author": "Bernadette Jana Stolz",
                "arxiv_comment": "56 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11924v2",
                "updated": "2025-07-11T12:13:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    13,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-14T23:47:46Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    23,
                    47,
                    46,
                    4,
                    73,
                    0
                ],
                "title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives"
                },
                "summary": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models."
                },
                "authors": [
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Krishna Sayana"
                    },
                    {
                        "name": "Hubert Pham"
                    },
                    {
                        "name": "James Pine"
                    },
                    {
                        "name": "Yuri Vasilevski"
                    },
                    {
                        "name": "Raghavendra Vasudeva"
                    },
                    {
                        "name": "Marialena Kyriakidi"
                    },
                    {
                        "name": "Liam Hebert"
                    },
                    {
                        "name": "Ambarish Jash"
                    },
                    {
                        "name": "Anushya Subbiah"
                    },
                    {
                        "name": "Sukhdeep Sodhi"
                    }
                ],
                "author_detail": {
                    "name": "Sukhdeep Sodhi"
                },
                "author": "Sukhdeep Sodhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04957v2",
                "updated": "2025-07-11T12:04:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    4,
                    38,
                    4,
                    192,
                    0
                ],
                "published": "2024-11-07T18:32:42Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    32,
                    42,
                    3,
                    312,
                    0
                ],
                "title": "Belief propagation for general graphical models with loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief propagation for general graphical models with loops"
                },
                "summary": "There is an increasing interest in scaling tensor network methods through\nbelief propagation (BP), as well as increasing the accuracy of BP through\ntensor network methods. We develop a unification framework that takes an\narbitrary graphical model with loops and provides message passing update rules\nand inference equations. We show that recent state-of-the-art methods regarding\ntensors and BP, like block belief propagation and tensor network message\npassing, are special instances of our framework. From a practical perspective,\nwe discuss how our framework can be useful in order to understand the benefits\nof scheduling in BP, and show how it can be used for decoding purposes in\nquantum error correction. Lastly, to numerically support our claims, we show\nthat our framework can achieve an accuracy improvement of more than ten orders\nof magnitude over tensor network BP when computing the trace of a tensor\nnetwork consisting of a chain of connected triangles with periodic boundary\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing interest in scaling tensor network methods through\nbelief propagation (BP), as well as increasing the accuracy of BP through\ntensor network methods. We develop a unification framework that takes an\narbitrary graphical model with loops and provides message passing update rules\nand inference equations. We show that recent state-of-the-art methods regarding\ntensors and BP, like block belief propagation and tensor network message\npassing, are special instances of our framework. From a practical perspective,\nwe discuss how our framework can be useful in order to understand the benefits\nof scheduling in BP, and show how it can be used for decoding purposes in\nquantum error correction. Lastly, to numerically support our claims, we show\nthat our framework can achieve an accuracy improvement of more than ten orders\nof magnitude over tensor network BP when computing the trace of a tensor\nnetwork consisting of a chain of connected triangles with periodic boundary\nconditions."
                },
                "authors": [
                    {
                        "name": "Pedro Hack"
                    },
                    {
                        "name": "Christian B. Mendl"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08513v1",
                "updated": "2025-07-11T12:00:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    0,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:00:10Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    0,
                    10,
                    4,
                    192,
                    0
                ],
                "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications."
                },
                "authors": [
                    {
                        "name": "Liu He"
                    },
                    {
                        "name": "Xiao Zeng"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Albert Y. C. Chen"
                    },
                    {
                        "name": "Lu Xia"
                    },
                    {
                        "name": "Shashwat Verma"
                    },
                    {
                        "name": "Sankalp Dayal"
                    },
                    {
                        "name": "Min Sun"
                    },
                    {
                        "name": "Cheng-Hao Kuo"
                    },
                    {
                        "name": "Daniel Aliaga"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Aliaga"
                },
                "author": "Daniel Aliaga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18246v2",
                "updated": "2025-07-11T11:43:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    43,
                    8,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-25T10:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "title": "One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason)."
                },
                "authors": [
                    {
                        "name": "Ritesh Goru"
                    },
                    {
                        "name": "Shanay Mehta"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07445v2",
                "updated": "2025-07-11T11:40:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    40,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T05:48:28Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    5,
                    48,
                    28,
                    3,
                    191,
                    0
                ],
                "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs\n  in Production-Living Simulations with Stardew Valley",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs\n  in Production-Living Simulations with Stardew Valley"
                },
                "summary": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments."
                },
                "authors": [
                    {
                        "name": "Weihao Tan"
                    },
                    {
                        "name": "Changjiu Jiang"
                    },
                    {
                        "name": "Yu Duan"
                    },
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Jiageng Li"
                    },
                    {
                        "name": "Yitian Hong"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "Project website: https://weihaotan.github.io/StarDojo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08505v1",
                "updated": "2025-07-11T11:30:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:30:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R"
                },
                "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks."
                },
                "authors": [
                    {
                        "name": "Pablo Robin Guerrero"
                    },
                    {
                        "name": "Yueyang Pan"
                    },
                    {
                        "name": "Sanidhya Kashyap"
                    }
                ],
                "author_detail": {
                    "name": "Sanidhya Kashyap"
                },
                "author": "Sanidhya Kashyap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08960v2",
                "updated": "2025-07-11T11:29:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    29,
                    20,
                    4,
                    192,
                    0
                ],
                "published": "2025-05-13T20:57:16Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    57,
                    16,
                    1,
                    133,
                    0
                ],
                "title": "Modern causal inference approaches to improve power for subgroup\n  analysis in randomized controlled trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern causal inference approaches to improve power for subgroup\n  analysis in randomized controlled trials"
                },
                "summary": "Randomized controlled trials (RCTs) often include subgroup analyses to assess\nwhether treatment effects vary across pre-specified patient populations.\nHowever, these analyses frequently suffer from small sample sizes which limit\nthe power to detect heterogeneous effects. Power can be improved by leveraging\npredictors of the outcome -- i.e., through covariate adjustment -- as well as\nby borrowing external data from similar RCTs or observational studies. The\nbenefits of covariate adjustment may be limited when the trial sample is small.\nBorrowing external data can increase the effective sample size and improve\npower, but it introduces two key challenges: (i) integrating data across\nsources can lead to model misspecification, and (ii) practical violations of\nthe positivity assumption -- where the probability of receiving the target\ntreatment is near-zero for some covariate profiles in the external data -- can\nlead to extreme inverse-probability weights and unstable inferences, ultimately\nnegating potential power gains. To account for these shortcomings, we present\nan approach to improving power in pre-planned subgroup analyses of small RCTs\nthat leverages both baseline predictors and external data. We propose debiased\nestimators that accommodate parametric, machine learning, and nonparametric\nBayesian methods. To address practical positivity violations, we introduce\nthree estimators: a covariate-balancing approach, an automated debiased machine\nlearning (DML) estimator, and a calibrated DML estimator. We show improved\npower in various simulations and offer practical recommendations for the\napplication of the proposed methods. Finally, we apply them to evaluate the\neffectiveness of citalopram for negative symptoms in first-episode\nschizophrenia patients across subgroups defined by duration of untreated\npsychosis, using data from two small RCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized controlled trials (RCTs) often include subgroup analyses to assess\nwhether treatment effects vary across pre-specified patient populations.\nHowever, these analyses frequently suffer from small sample sizes which limit\nthe power to detect heterogeneous effects. Power can be improved by leveraging\npredictors of the outcome -- i.e., through covariate adjustment -- as well as\nby borrowing external data from similar RCTs or observational studies. The\nbenefits of covariate adjustment may be limited when the trial sample is small.\nBorrowing external data can increase the effective sample size and improve\npower, but it introduces two key challenges: (i) integrating data across\nsources can lead to model misspecification, and (ii) practical violations of\nthe positivity assumption -- where the probability of receiving the target\ntreatment is near-zero for some covariate profiles in the external data -- can\nlead to extreme inverse-probability weights and unstable inferences, ultimately\nnegating potential power gains. To account for these shortcomings, we present\nan approach to improving power in pre-planned subgroup analyses of small RCTs\nthat leverages both baseline predictors and external data. We propose debiased\nestimators that accommodate parametric, machine learning, and nonparametric\nBayesian methods. To address practical positivity violations, we introduce\nthree estimators: a covariate-balancing approach, an automated debiased machine\nlearning (DML) estimator, and a calibrated DML estimator. We show improved\npower in various simulations and offer practical recommendations for the\napplication of the proposed methods. Finally, we apply them to evaluate the\neffectiveness of citalopram for negative symptoms in first-episode\nschizophrenia patients across subgroups defined by duration of untreated\npsychosis, using data from two small RCTs."
                },
                "authors": [
                    {
                        "name": "Antonio D'Alessandro"
                    },
                    {
                        "name": "Jiyu Kim"
                    },
                    {
                        "name": "Samrachana Adhikari"
                    },
                    {
                        "name": "Donald Goff"
                    },
                    {
                        "name": "Falco J. Bargagli Stoffi"
                    },
                    {
                        "name": "Michele Santacatterina"
                    }
                ],
                "author_detail": {
                    "name": "Michele Santacatterina"
                },
                "author": "Michele Santacatterina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08501v1",
                "updated": "2025-07-11T11:24:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    24,
                    9,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:24:09Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    24,
                    9,
                    4,
                    192,
                    0
                ],
                "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Language to Logic: A Bi-Level Framework for Structured Reasoning"
                },
                "summary": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs."
                },
                "authors": [
                    {
                        "name": "Keying Yang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08498v1",
                "updated": "2025-07-11T11:20:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    20,
                    39,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:20:39Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    20,
                    39,
                    4,
                    192,
                    0
                ],
                "title": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop"
                },
                "summary": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08496v1",
                "updated": "2025-07-11T11:18:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    18,
                    49,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:18:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    18,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware\n  Procedural Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware\n  Procedural Planning"
                },
                "summary": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa."
                },
                "authors": [
                    {
                        "name": "Shibo Sun"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Mingjie Wei"
                    },
                    {
                        "name": "Lanshun Nie"
                    },
                    {
                        "name": "Wei-Nan Zhang"
                    },
                    {
                        "name": "Dechen Zhan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Lei Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lei Fan"
                },
                "author": "Lei Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08491v1",
                "updated": "2025-07-11T11:16:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    16,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:16:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    16,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation\n  using clembench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation\n  using clembench"
                },
                "summary": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests."
                },
                "authors": [
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Jonathan Jordan"
                    },
                    {
                        "name": "Philipp Sadler"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Sadler"
                },
                "author": "Philipp Sadler",
                "arxiv_comment": "All code required to run the benchmark, as well as extensive\n  documentation, is available at https://github.com/clembench/clembench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08490v1",
                "updated": "2025-07-11T11:14:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    14,
                    28,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:14:28Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    14,
                    28,
                    4,
                    192,
                    0
                ],
                "title": "Onboard Neuromorphic Split Computing via Optical Links for LEO Remote\n  Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Onboard Neuromorphic Split Computing via Optical Links for LEO Remote\n  Sensing"
                },
                "summary": "Low Earth orbit (LEO) satellite constellations increasingly require onboard\nintelligence under strict power and communication constraints. This paper\nproposes a neuromorphic split computing framework tailored for hierarchical LEO\nsystems, where edge satellites perform event-driven sensing using dynamic\nvision sensors (DVS) and lightweight spiking neural network (SNN) encoders,\nwhile core satellites conduct inference using a powerful SNN decoder. A learned\nspike mapping scheme enables direct transmission over optical inter-satellite\nlinks (OISLs) without conventional modulation overhead. Experimental results on\nsynthetic aerial scene classification demonstrate that the proposed\narchitecture achieves accuracy on par with modern large vision-based pipelines,\nwhile offering energy efficiency comparable to that of existing lightweight\nimplementations. These findings highlight the potential of neuromorphic\ncomputing for energy-efficient inter-satellite split computing in LEO remote\nsensing missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth orbit (LEO) satellite constellations increasingly require onboard\nintelligence under strict power and communication constraints. This paper\nproposes a neuromorphic split computing framework tailored for hierarchical LEO\nsystems, where edge satellites perform event-driven sensing using dynamic\nvision sensors (DVS) and lightweight spiking neural network (SNN) encoders,\nwhile core satellites conduct inference using a powerful SNN decoder. A learned\nspike mapping scheme enables direct transmission over optical inter-satellite\nlinks (OISLs) without conventional modulation overhead. Experimental results on\nsynthetic aerial scene classification demonstrate that the proposed\narchitecture achieves accuracy on par with modern large vision-based pipelines,\nwhile offering energy efficiency comparable to that of existing lightweight\nimplementations. These findings highlight the potential of neuromorphic\ncomputing for energy-efficient inter-satellite split computing in LEO remote\nsensing missions."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08479v1",
                "updated": "2025-07-11T10:42:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    42,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:42:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    42,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Beyond LambdaCDM: How the Hubble tension challenges early universe\n  physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond LambdaCDM: How the Hubble tension challenges early universe\n  physics"
                },
                "summary": "Differences in the values of the Hubble constant obtained from the local\nuniverse and the early universe have resulted in a significant tension. This\ntension signifies that our understanding of cosmology (physical processes\nand/or cosmological data) is incomplete. Some of the suggested solutions\ninclude physics of the early Universe. In this paper we aim to investigate\ncommon features of various early universe solutions to the Hubble constant\ntension. The physics of the early universe affects the size of the sound\nhorizon which is probed with the Cosmic Microwave Background (CMB) data. Within\nthe standard model, the size of the horizon (within limits of current\nmeasurements) is affected by processes that could occur between (approximately)\n1 day after the Big Bang and the last scattering instant. We focus on simple\nextensions incorporating Early Dark Energy (EDE) and show how such a model\naffects the inferred values of the Hubble constant. We compare this model to\nLambdaCDM models using MCMC analysis, likelihoods over the parameter space and\nBayesian evidence. The MCMC analysis shows that EDE leads to a decrease in the\nsize of the sound horizon that is consistent with H0 = 73.56 km/s/Mpc but we\nalso show that MCMC analysis favours increasing redshift and proportion of EDE.\nThe Bayesian evidence favours our EDE model for very narrow, finely-tuned\nparameter space. The LambdaCDM model used for comparison has good evidence\nacross a wide parameter space. We interpret this as an indication that more\nsophisticated models are required. We conclude that if the Hubble tension were\nto be related to the physics of the early universe, EDE could be used as a\nwindow to explore conditions of the early universe and extend our understanding\nof that era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differences in the values of the Hubble constant obtained from the local\nuniverse and the early universe have resulted in a significant tension. This\ntension signifies that our understanding of cosmology (physical processes\nand/or cosmological data) is incomplete. Some of the suggested solutions\ninclude physics of the early Universe. In this paper we aim to investigate\ncommon features of various early universe solutions to the Hubble constant\ntension. The physics of the early universe affects the size of the sound\nhorizon which is probed with the Cosmic Microwave Background (CMB) data. Within\nthe standard model, the size of the horizon (within limits of current\nmeasurements) is affected by processes that could occur between (approximately)\n1 day after the Big Bang and the last scattering instant. We focus on simple\nextensions incorporating Early Dark Energy (EDE) and show how such a model\naffects the inferred values of the Hubble constant. We compare this model to\nLambdaCDM models using MCMC analysis, likelihoods over the parameter space and\nBayesian evidence. The MCMC analysis shows that EDE leads to a decrease in the\nsize of the sound horizon that is consistent with H0 = 73.56 km/s/Mpc but we\nalso show that MCMC analysis favours increasing redshift and proportion of EDE.\nThe Bayesian evidence favours our EDE model for very narrow, finely-tuned\nparameter space. The LambdaCDM model used for comparison has good evidence\nacross a wide parameter space. We interpret this as an indication that more\nsophisticated models are required. We conclude that if the Hubble tension were\nto be related to the physics of the early universe, EDE could be used as a\nwindow to explore conditions of the early universe and extend our understanding\nof that era."
                },
                "authors": [
                    {
                        "name": "Gawain Simpson"
                    },
                    {
                        "name": "Krzysztof Bolejko"
                    },
                    {
                        "name": "Stephen Walters"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Walters"
                },
                "author": "Stephen Walters",
                "arxiv_comment": "22 pages, 13 figures. Accepted for publishing in Classical and\n  Quantum Gravity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06892v3",
                "updated": "2025-07-11T10:32:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    32,
                    34,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T14:29:45Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    29,
                    45,
                    2,
                    190,
                    0
                ],
                "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc."
                },
                "authors": [
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "Preliminary version, v3, added the missing name of x-axis in the left\n  part of Fig.1 and corrected a wrong number in Fig.3. Project page:\n  https://anitaleungxx.github.io/ReMix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08473v1",
                "updated": "2025-07-11T10:31:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:31:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Evaluating SAE interpretability without explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating SAE interpretability without explanations"
                },
                "summary": "Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques."
                },
                "authors": [
                    {
                        "name": "Gonçalo Paulo"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08472v1",
                "updated": "2025-07-11T10:29:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    29,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:29:04Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    29,
                    4,
                    4,
                    192,
                    0
                ],
                "title": "Pre-Training LLMs on a budget: A comparison of three optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Training LLMs on a budget: A comparison of three optimizers"
                },
                "summary": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results."
                },
                "authors": [
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Christian Kroos"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Küch"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Küch"
                },
                "author": "Fabian Küch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08468v1",
                "updated": "2025-07-11T10:19:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    19,
                    56,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:19:56Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    19,
                    56,
                    4,
                    192,
                    0
                ],
                "title": "Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study"
                },
                "summary": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information."
                },
                "authors": [
                    {
                        "name": "Marina Luketina"
                    },
                    {
                        "name": "Andrea Benkel"
                    },
                    {
                        "name": "Christoph G. Schuetz"
                    }
                ],
                "author_detail": {
                    "name": "Christoph G. Schuetz"
                },
                "author": "Christoph G. Schuetz",
                "arxiv_comment": "26 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08459v1",
                "updated": "2025-07-11T10:02:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    2,
                    21,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:02:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    2,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method."
                },
                "authors": [
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Shuyi Xie"
                    },
                    {
                        "name": "Qingsong Lv"
                    },
                    {
                        "name": "Shupei Xiao"
                    },
                    {
                        "name": "Linlin Song"
                    },
                    {
                        "name": "Sui Wenjuan"
                    },
                    {
                        "name": "Fan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lin"
                },
                "author": "Fan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14255v2",
                "updated": "2025-07-11T09:58:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    58,
                    28,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-19T10:16:06Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    10,
                    16,
                    6,
                    5,
                    109,
                    0
                ],
                "title": "Inferring Dense Confined Circumstellar Medium around Supernova\n  Progenitors via Long-term Hydrodynamical Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Dense Confined Circumstellar Medium around Supernova\n  Progenitors via Long-term Hydrodynamical Evolution"
                },
                "summary": "Circumstellar interaction of supernova (SN) ejecta is an essential process in\nits evolution and observations of SNe have found the signature of circumstellar\ninteraction both in the early and late evolutionary phase of SNe. In this\nLetter, we show that if the SN forward shock plunges into tenuous stellar wind\nfrom dense circumstellar medium (CSM) in the vicinity of the progenitor (i.e.,\nconfined CSM), the subsequent time evolutions of the SN-CSM interaction system\ndeviates from the prediction of self-similar solution. In this case, after all\nof the confined CSM is swept up by the SN forward shock (roughly $10$ days\nafter the explosion), the propagation of the shocked shell will be driven by\nthe freely expanding ram pressure of the confined CSM component, instead of the\nSN ejecta. Meanwhile, the forward shock decelerates faster than the prediction\nof thin-shell approximation once the confined CSM component reaches homologous\nexpansion. This lasts until the reverse shock in the confined CSM component\nreaches the head of the SN ejecta, leading to the restoration of the system\ninto the evolutionary model without confined CSM, where the SN ejecta drives\nthe expansion of the system. We also show that this peculiar evolution will be\nreflected in observational signatures originating from SN-CSM interaction,\ntaking rapid decline and rebrightening of radio emission as examples. Our\nresults shed light on the importance of taking into account the effect of\ninitial SN-CSM interaction even when we focus on observational properties of\nSNe a few years after the explosion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circumstellar interaction of supernova (SN) ejecta is an essential process in\nits evolution and observations of SNe have found the signature of circumstellar\ninteraction both in the early and late evolutionary phase of SNe. In this\nLetter, we show that if the SN forward shock plunges into tenuous stellar wind\nfrom dense circumstellar medium (CSM) in the vicinity of the progenitor (i.e.,\nconfined CSM), the subsequent time evolutions of the SN-CSM interaction system\ndeviates from the prediction of self-similar solution. In this case, after all\nof the confined CSM is swept up by the SN forward shock (roughly $10$ days\nafter the explosion), the propagation of the shocked shell will be driven by\nthe freely expanding ram pressure of the confined CSM component, instead of the\nSN ejecta. Meanwhile, the forward shock decelerates faster than the prediction\nof thin-shell approximation once the confined CSM component reaches homologous\nexpansion. This lasts until the reverse shock in the confined CSM component\nreaches the head of the SN ejecta, leading to the restoration of the system\ninto the evolutionary model without confined CSM, where the SN ejecta drives\nthe expansion of the system. We also show that this peculiar evolution will be\nreflected in observational signatures originating from SN-CSM interaction,\ntaking rapid decline and rebrightening of radio emission as examples. Our\nresults shed light on the importance of taking into account the effect of\ninitial SN-CSM interaction even when we focus on observational properties of\nSNe a few years after the explosion."
                },
                "authors": [
                    {
                        "name": "Tomoki Matsuoka"
                    },
                    {
                        "name": "Keiichi Maeda"
                    },
                    {
                        "name": "Ke-Jung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ke-Jung Chen"
                },
                "author": "Ke-Jung Chen",
                "arxiv_comment": "11 pages, 5 figures, 1 table. Accepted for publication in The\n  Astrophysical Journal Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v3",
                "updated": "2025-07-11T09:50:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    50,
                    2,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08448v1",
                "updated": "2025-07-11T09:41:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    41,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:41:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    41,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT"
                },
                "summary": "3D reconstruction, which aims to recover the dense three-dimensional\nstructure of a scene, is a cornerstone technology for numerous applications,\nincluding augmented/virtual reality, autonomous driving, and robotics. While\ntraditional pipelines like Structure from Motion (SfM) and Multi-View Stereo\n(MVS) achieve high precision through iterative optimization, they are limited\nby complex workflows, high computational cost, and poor robustness in\nchallenging scenarios like texture-less regions. Recently, deep learning has\ncatalyzed a paradigm shift in 3D reconstruction. A new family of models,\nexemplified by DUSt3R, has pioneered a feed-forward approach. These models\nemploy a unified deep network to jointly infer camera poses and dense geometry\ndirectly from an Unconstrained set of images in a single forward pass. This\nsurvey provides a systematic review of this emerging domain. We begin by\ndissecting the technical framework of these feed-forward models, including\ntheir Transformer-based correspondence modeling, joint pose and geometry\nregression mechanisms, and strategies for scaling from two-view to multi-view\nscenarios. To highlight the disruptive nature of this new paradigm, we contrast\nit with both traditional pipelines and earlier learning-based methods like\nMVSNet. Furthermore, we provide an overview of relevant datasets and evaluation\nmetrics. Finally, we discuss the technology's broad application prospects and\nidentify key future challenges and opportunities, such as model accuracy and\nscalability, and handling dynamic scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction, which aims to recover the dense three-dimensional\nstructure of a scene, is a cornerstone technology for numerous applications,\nincluding augmented/virtual reality, autonomous driving, and robotics. While\ntraditional pipelines like Structure from Motion (SfM) and Multi-View Stereo\n(MVS) achieve high precision through iterative optimization, they are limited\nby complex workflows, high computational cost, and poor robustness in\nchallenging scenarios like texture-less regions. Recently, deep learning has\ncatalyzed a paradigm shift in 3D reconstruction. A new family of models,\nexemplified by DUSt3R, has pioneered a feed-forward approach. These models\nemploy a unified deep network to jointly infer camera poses and dense geometry\ndirectly from an Unconstrained set of images in a single forward pass. This\nsurvey provides a systematic review of this emerging domain. We begin by\ndissecting the technical framework of these feed-forward models, including\ntheir Transformer-based correspondence modeling, joint pose and geometry\nregression mechanisms, and strategies for scaling from two-view to multi-view\nscenarios. To highlight the disruptive nature of this new paradigm, we contrast\nit with both traditional pipelines and earlier learning-based methods like\nMVSNet. Furthermore, we provide an overview of relevant datasets and evaluation\nmetrics. Finally, we discuss the technology's broad application prospects and\nidentify key future challenges and opportunities, such as model accuracy and\nscalability, and handling dynamic scenes."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yihang Wu"
                    },
                    {
                        "name": "Songhua Li"
                    },
                    {
                        "name": "Wenjie Ma"
                    },
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Qi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wang"
                },
                "author": "Qi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08445v1",
                "updated": "2025-07-11T09:36:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    36,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:36:45Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    36,
                    45,
                    4,
                    192,
                    0
                ],
                "title": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval"
                },
                "summary": "Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems."
                },
                "authors": [
                    {
                        "name": "Yaodong Su"
                    },
                    {
                        "name": "Yixiang Fang"
                    },
                    {
                        "name": "Yingli Zhou"
                    },
                    {
                        "name": "Quanqing Xu"
                    },
                    {
                        "name": "Chuanhui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chuanhui Yang"
                },
                "author": "Chuanhui Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08440v1",
                "updated": "2025-07-11T09:31:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    31,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:31:10Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    31,
                    10,
                    4,
                    192,
                    0
                ],
                "title": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences"
                },
                "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains."
                },
                "authors": [
                    {
                        "name": "Selina Heller"
                    },
                    {
                        "name": "Mohamed Ibrahim"
                    },
                    {
                        "name": "David Antony Selby"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08427v1",
                "updated": "2025-07-11T09:13:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    13,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:13:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    13,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains"
                },
                "summary": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing."
                },
                "authors": [
                    {
                        "name": "Zilu Dong"
                    },
                    {
                        "name": "Xiangqing Shen"
                    },
                    {
                        "name": "Zinong Yang"
                    },
                    {
                        "name": "Rui Xia"
                    }
                ],
                "author_detail": {
                    "name": "Rui Xia"
                },
                "author": "Rui Xia",
                "arxiv_comment": "Accepted to ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08425v1",
                "updated": "2025-07-11T09:11:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    11,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:11:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    11,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies."
                },
                "authors": [
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Yaping Zhang"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_doi": "10.24846/v34i1y202501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24846/v34i1y202501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Lu XIANG, Yang ZHAO, Yaping ZHANG, Chengqing ZONG, \"A Survey of\n  Large Language Models in Discipline-specific Research: Challenges, Methods\n  and Opportunities\", Studies in Informatics and Control, ISSN 1220-1766, vol.\n  34(1), pp. 5-24, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08424v1",
                "updated": "2025-07-11T09:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    9,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    9,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "RTNinja: a generalized machine learning framework for analyzing random\n  telegraph noise signals in nanoelectronic devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTNinja: a generalized machine learning framework for analyzing random\n  telegraph noise signals in nanoelectronic devices"
                },
                "summary": "Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Anirudh Varanasi"
                    },
                    {
                        "name": "Robin Degraeve"
                    },
                    {
                        "name": "Philippe Roussel"
                    },
                    {
                        "name": "Clement Merckling"
                    }
                ],
                "author_detail": {
                    "name": "Clement Merckling"
                },
                "author": "Clement Merckling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09626v2",
                "updated": "2025-07-11T09:08:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    8,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2024-12-12T18:59:59Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    59,
                    3,
                    347,
                    0
                ],
                "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free\n  Scale Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free\n  Scale Fusion"
                },
                "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. To tackle this challenge, we propose FreeScale, a\ntuning-free inference paradigm to enable higher-resolution visual generation\nvia scale fusion. Specifically, FreeScale processes information from different\nreceptive scales and then fuses it by extracting desired frequency components.\nExtensive experiments validate the superiority of our paradigm in extending the\ncapabilities of higher-resolution visual generation for both image and video\nmodels. Notably, compared with previous best-performing methods, FreeScale\nunlocks the 8k-resolution text-to-image generation for the first time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. To tackle this challenge, we propose FreeScale, a\ntuning-free inference paradigm to enable higher-resolution visual generation\nvia scale fusion. Specifically, FreeScale processes information from different\nreceptive scales and then fuses it by extracting desired frequency components.\nExtensive experiments validate the superiority of our paradigm in extending the\ncapabilities of higher-resolution visual generation for both image and video\nmodels. Notably, compared with previous best-performing methods, FreeScale\nunlocks the 8k-resolution text-to-image generation for the first time."
                },
                "authors": [
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "ICCV 2025, Project Page:\n  http://haonanqiu.com/projects/FreeScale.html, Code Repo:\n  https://github.com/ali-vilab/FreeScale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13857v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13857v4",
                "updated": "2025-07-11T08:59:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    59,
                    48,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-18T03:14:23Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    3,
                    14,
                    23,
                    1,
                    77,
                    0
                ],
                "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations"
                },
                "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiayi Tong"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Ziyang Hu"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Christopher J. Lindsell"
                    },
                    {
                        "name": "Michael J. Pencina"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Chuan Hong"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Hong"
                },
                "author": "Chuan Hong",
                "arxiv_comment": "30 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13857v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13857v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08525v2",
                "updated": "2025-07-11T08:36:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    36,
                    40,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-11T15:17:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training"
                },
                "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes."
                },
                "authors": [
                    {
                        "name": "Tong Wei"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Zongqing Lu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08402v1",
                "updated": "2025-07-11T08:20:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    20,
                    19,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:20:19Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    20,
                    19,
                    4,
                    192,
                    0
                ],
                "title": "SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent\n  Intracortical Motor Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent\n  Intracortical Motor Decoding"
                },
                "summary": "Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from\nneural population activity, enabling individuals with motor impairments to\nregain motor functions and communication abilities. A key challenge in\nlong-term iBCI is the nonstationarity of neural recordings, where the\ncomposition and tuning profiles of the recorded populations are unstable across\nrecording sessions. Existing methods attempt to address this issue by explicit\nalignment techniques; however, they rely on fixed neural identities and require\ntest-time labels or parameter updates, limiting their generalization across\nsessions and imposing additional computational burden during deployment. In\nthis work, we introduce SPINT - a Spatial Permutation-Invariant Neural\nTransformer framework for behavioral decoding that operates directly on\nunordered sets of neural units. Central to our approach is a novel\ncontext-dependent positional embedding scheme that dynamically infers\nunit-specific identities, enabling flexible generalization across recording\nsessions. SPINT supports inference on variable-size populations and allows\nfew-shot, gradient-free adaptation using a small amount of unlabeled data from\nthe test session. To further promote model robustness to population\nvariability, we introduce dynamic channel dropout, a regularization method for\niBCI that simulates shifts in population composition during training. We\nevaluate SPINT on three multi-session datasets from the FALCON Benchmark,\ncovering continuous motor decoding tasks in human and non-human primates. SPINT\ndemonstrates robust cross-session generalization, outperforming existing\nzero-shot and few-shot unsupervised baselines while eliminating the need for\ntest-time alignment and fine-tuning. Our work contributes an initial step\ntoward a robust and scalable neural decoding framework for long-term iBCI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from\nneural population activity, enabling individuals with motor impairments to\nregain motor functions and communication abilities. A key challenge in\nlong-term iBCI is the nonstationarity of neural recordings, where the\ncomposition and tuning profiles of the recorded populations are unstable across\nrecording sessions. Existing methods attempt to address this issue by explicit\nalignment techniques; however, they rely on fixed neural identities and require\ntest-time labels or parameter updates, limiting their generalization across\nsessions and imposing additional computational burden during deployment. In\nthis work, we introduce SPINT - a Spatial Permutation-Invariant Neural\nTransformer framework for behavioral decoding that operates directly on\nunordered sets of neural units. Central to our approach is a novel\ncontext-dependent positional embedding scheme that dynamically infers\nunit-specific identities, enabling flexible generalization across recording\nsessions. SPINT supports inference on variable-size populations and allows\nfew-shot, gradient-free adaptation using a small amount of unlabeled data from\nthe test session. To further promote model robustness to population\nvariability, we introduce dynamic channel dropout, a regularization method for\niBCI that simulates shifts in population composition during training. We\nevaluate SPINT on three multi-session datasets from the FALCON Benchmark,\ncovering continuous motor decoding tasks in human and non-human primates. SPINT\ndemonstrates robust cross-session generalization, outperforming existing\nzero-shot and few-shot unsupervised baselines while eliminating the need for\ntest-time alignment and fine-tuning. Our work contributes an initial step\ntoward a robust and scalable neural decoding framework for long-term iBCI\napplications."
                },
                "authors": [
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Jingyuan Li"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Lu Mi"
                    },
                    {
                        "name": "Amy Orsborn"
                    },
                    {
                        "name": "Uygar Sümbül"
                    },
                    {
                        "name": "Eli Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shlizerman"
                },
                "author": "Eli Shlizerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08392v1",
                "updated": "2025-07-11T08:04:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    4,
                    32,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:04:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    4,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems"
                },
                "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05788v2",
                "updated": "2025-07-11T08:00:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    0,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-08T08:50:47Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    50,
                    47,
                    1,
                    189,
                    0
                ],
                "title": "Flippi: End To End GenAI Assistant for E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flippi: End To End GenAI Assistant for E-Commerce"
                },
                "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace."
                },
                "authors": [
                    {
                        "name": "Anand A. Rajasekar"
                    },
                    {
                        "name": "Praveen Tangarajan"
                    },
                    {
                        "name": "Anjali Nainani"
                    },
                    {
                        "name": "Amogh Batwal"
                    },
                    {
                        "name": "Vinay Rao Dandin"
                    },
                    {
                        "name": "Anusua Trivedi"
                    },
                    {
                        "name": "Ozan Ersoy"
                    }
                ],
                "author_detail": {
                    "name": "Ozan Ersoy"
                },
                "author": "Ozan Ersoy",
                "arxiv_comment": "10 pages, 2 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08390v1",
                "updated": "2025-07-11T08:00:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    0,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:00:47Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    0,
                    47,
                    4,
                    192,
                    0
                ],
                "title": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs\n  Sampling"
                },
                "summary": "Discrete diffusion models have emerged as a powerful paradigm for language\nmodeling, rivaling auto-regressive models by training-time scaling. However,\ninference-time scaling in discrete diffusion models remains relatively\nunder-explored. In this work, we study sampling-based approaches for achieving\nhigh-quality text generation from discrete diffusion models in reward-guided\nsettings. We introduce a novel inference-time scaling approach based on\nparticle Gibbs sampling for discrete diffusion models. The particle Gibbs\nsampling algorithm iteratively refines full diffusion trajectories using\nconditional Sequential Monte Carlo as its transition mechanism. This process\nensures that the updated samples progressively improve and move closer to the\nreward-weighted target distribution. Unlike existing inference-time scaling\nmethods, which are often limited to single diffusion trajectories, our approach\nleverages iterative refinement across multiple trajectories. Within this\nframework, we further analyze the trade-offs between four key axes for\ninference-time scaling under fixed compute budgets: particle Gibbs iterations,\nparticle count, denoising steps, and reward estimation cost. Empirically, our\nmethod consistently outperforms prior inference-time strategies on\nreward-guided text generation tasks, achieving significant improvement in\naccuracy under varying compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models have emerged as a powerful paradigm for language\nmodeling, rivaling auto-regressive models by training-time scaling. However,\ninference-time scaling in discrete diffusion models remains relatively\nunder-explored. In this work, we study sampling-based approaches for achieving\nhigh-quality text generation from discrete diffusion models in reward-guided\nsettings. We introduce a novel inference-time scaling approach based on\nparticle Gibbs sampling for discrete diffusion models. The particle Gibbs\nsampling algorithm iteratively refines full diffusion trajectories using\nconditional Sequential Monte Carlo as its transition mechanism. This process\nensures that the updated samples progressively improve and move closer to the\nreward-weighted target distribution. Unlike existing inference-time scaling\nmethods, which are often limited to single diffusion trajectories, our approach\nleverages iterative refinement across multiple trajectories. Within this\nframework, we further analyze the trade-offs between four key axes for\ninference-time scaling under fixed compute budgets: particle Gibbs iterations,\nparticle count, denoising steps, and reward estimation cost. Empirically, our\nmethod consistently outperforms prior inference-time strategies on\nreward-guided text generation tasks, achieving significant improvement in\naccuracy under varying compute budgets."
                },
                "authors": [
                    {
                        "name": "Meihua Dang"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Minkai Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02651v3",
                "updated": "2025-07-11T08:00:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    0,
                    17,
                    4,
                    192,
                    0
                ],
                "published": "2024-12-03T18:24:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    24,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: a Review of Acceleration Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: a Review of Acceleration Methods"
                },
                "summary": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence (CBC) events every year with\nsubstantially higher SNR and longer signal duration, presenting significant\ncomputational challenges. In this study, we systematically evaluate the\ncomputational costs of CBC source parameter estimation (PE) in the 3G era by\nmodeling the PE time cost as a function of SNR and signal duration. We examine\nthe standard PE method alongside acceleration methods including relative\nbinning, multibanding, and reduced order quadrature. We predict that PE for a\none-month-observation catalog with 3G detectors could require at least billions\nof CPU core hours with the standard PE method, whereas acceleration techniques\ncan reduce this demand to less than millions of core hours, which is as high as\nthe cost of analyzing GW events in the past 10 years. These findings highlight\nthe necessity for more efficient PE methods to enable cost-effective and\nenvironmentally sustainable data analysis for 3G detectors. In addition, we\nassess the accuracy of accelerated PE methods, emphasizing the need for careful\ntreatment in high-SNR scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence (CBC) events every year with\nsubstantially higher SNR and longer signal duration, presenting significant\ncomputational challenges. In this study, we systematically evaluate the\ncomputational costs of CBC source parameter estimation (PE) in the 3G era by\nmodeling the PE time cost as a function of SNR and signal duration. We examine\nthe standard PE method alongside acceleration methods including relative\nbinning, multibanding, and reduced order quadrature. We predict that PE for a\none-month-observation catalog with 3G detectors could require at least billions\nof CPU core hours with the standard PE method, whereas acceleration techniques\ncan reduce this demand to less than millions of core hours, which is as high as\nthe cost of analyzing GW events in the past 10 years. These findings highlight\nthe necessity for more efficient PE methods to enable cost-effective and\nenvironmentally sustainable data analysis for 3G detectors. In addition, we\nassess the accuracy of accelerated PE methods, emphasizing the need for careful\ntreatment in high-SNR scenarios."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "15 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08386v1",
                "updated": "2025-07-11T07:59:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    59,
                    23,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T07:59:23Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    59,
                    23,
                    4,
                    192,
                    0
                ],
                "title": "Detecting Evolutionary Change-Points with Branch-Specific Substitution\n  Models and Shrinkage Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Evolutionary Change-Points with Branch-Specific Substitution\n  Models and Shrinkage Priors"
                },
                "summary": "Branch-specific substitution models are popular for detecting evolutionary\nchange-points, such as shifts in selective pressure. However, applying such\nmodels typically requires prior knowledge of change-point locations on the\nphylogeny or faces scalability issues with large data sets. To address both\nlimitations, we integrate branch-specific substitution models with shrinkage\npriors to automatically identify change-points without prior knowledge, while\nsimultaneously estimating distinct substitution parameters for each branch. To\nenable tractable inference under this high-dimensional model, we develop an\nanalytical gradient algorithm for the branch-specific substitution parameters\nwhere the computation time is linear in the number of parameters. We apply this\ngradient algorithm to infer selection pressure dynamics in the evolution of the\nBRCA1 gene in primates and mutational dynamics in viral sequences from the\nrecent mpox epidemic. Our novel algorithm enhances inference efficiency,\nachieving up to a 90-fold speedup per iteration in maximum-likelihood\noptimization when compared to central difference numerical gradient method and\nup to a 360-fold improvement in computational performance within a Bayesian\nframework using Hamiltonian Monte Carlo sampler compared to conventional\nunivariate random walk sampler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branch-specific substitution models are popular for detecting evolutionary\nchange-points, such as shifts in selective pressure. However, applying such\nmodels typically requires prior knowledge of change-point locations on the\nphylogeny or faces scalability issues with large data sets. To address both\nlimitations, we integrate branch-specific substitution models with shrinkage\npriors to automatically identify change-points without prior knowledge, while\nsimultaneously estimating distinct substitution parameters for each branch. To\nenable tractable inference under this high-dimensional model, we develop an\nanalytical gradient algorithm for the branch-specific substitution parameters\nwhere the computation time is linear in the number of parameters. We apply this\ngradient algorithm to infer selection pressure dynamics in the evolution of the\nBRCA1 gene in primates and mutational dynamics in viral sequences from the\nrecent mpox epidemic. Our novel algorithm enhances inference efficiency,\nachieving up to a 90-fold speedup per iteration in maximum-likelihood\noptimization when compared to central difference numerical gradient method and\nup to a 360-fold improvement in computational performance within a Bayesian\nframework using Hamiltonian Monte Carlo sampler compared to conventional\nunivariate random walk sampler."
                },
                "authors": [
                    {
                        "name": "Xiang Ji"
                    },
                    {
                        "name": "Benjamin Redelings"
                    },
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Hongcun Bao"
                    },
                    {
                        "name": "Wu-Min Deng"
                    },
                    {
                        "name": "Samuel L. Hong"
                    },
                    {
                        "name": "Guy Baele"
                    },
                    {
                        "name": "Philippe Lemey"
                    },
                    {
                        "name": "Marc A. Suchard"
                    }
                ],
                "author_detail": {
                    "name": "Marc A. Suchard"
                },
                "author": "Marc A. Suchard",
                "arxiv_comment": "1 table, 4 figures, 29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14123v2",
                "updated": "2025-07-11T07:43:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    43,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-06-17T02:37:04Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    37,
                    4,
                    1,
                    168,
                    0
                ],
                "title": "Sampling from Your Language Model One Byte at a Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from Your Language Model One Byte at a Time"
                },
                "summary": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations, an issue known as the Prompt Boundary Problem (PBP).\nFor example, users are often advised not to end their prompts with a space\nbecause it prevents the model from including the space as part of the next\ntoken. While this heuristic is effective in English, the underlying PBP\ncontinues to affect languages such as Chinese as well as code generation, where\ntokens often do not line up with word and syntactic boundaries. In this work,\nwe present an inference-time method to convert any autoregressive LM with a BPE\ntokenizer into a character-level or byte-level LM. Our method efficiently\nsolves the PBP and is also able to unify the vocabularies of language models\nwith different tokenizers, allowing one to ensemble LMs with different\ntokenizers at inference time or transfer the post-training from one model to\nanother using proxy-tuning. We demonstrate in experiments that the ensemble and\nproxy-tuned models outperform their constituents on downstream evals. Code is\navailable at https://github.com/SewoongLab/byte-sampler .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations, an issue known as the Prompt Boundary Problem (PBP).\nFor example, users are often advised not to end their prompts with a space\nbecause it prevents the model from including the space as part of the next\ntoken. While this heuristic is effective in English, the underlying PBP\ncontinues to affect languages such as Chinese as well as code generation, where\ntokens often do not line up with word and syntactic boundaries. In this work,\nwe present an inference-time method to convert any autoregressive LM with a BPE\ntokenizer into a character-level or byte-level LM. Our method efficiently\nsolves the PBP and is also able to unify the vocabularies of language models\nwith different tokenizers, allowing one to ensemble LMs with different\ntokenizers at inference time or transfer the post-training from one model to\nanother using proxy-tuning. We demonstrate in experiments that the ensemble and\nproxy-tuned models outperform their constituents on downstream evals. Code is\navailable at https://github.com/SewoongLab/byte-sampler ."
                },
                "authors": [
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Alisa Liu"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Sewoong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Sewoong Oh"
                },
                "author": "Sewoong Oh",
                "arxiv_comment": "23 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07668v2",
                "updated": "2025-07-11T07:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    41,
                    35,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T11:49:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    11,
                    49,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Learning Pole Structures of Hadronic States using Predictive Uncertainty\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Pole Structures of Hadronic States using Predictive Uncertainty\n  Estimation"
                },
                "summary": "Matching theoretical predictions to experimental data remains a central\nchallenge in hadron spectroscopy. In particular, the identification of new\nhadronic states is difficult, as exotic signals near threshold can arise from a\nvariety of physical mechanisms. A key diagnostic in this context is the pole\nstructure of the scattering amplitude, but different configurations can produce\nsimilar signatures. The mapping between pole configurations and line shapes is\nespecially ambiguous near the mass threshold, where analytic control is\nlimited. In this work, we introduce an uncertainty-aware machine learning\napproach for classifying pole structures in $S$-matrix elements. Our method is\nbased on an ensemble of classifier chains that provide both epistemic and\naleatoric uncertainty estimates. We apply a rejection criterion based on\npredictive uncertainty, achieving a validation accuracy of nearly $95\\%$ while\ndiscarding only a small fraction of high-uncertainty predictions. Trained on\nsynthetic data with known pole structures, the model generalizes to previously\nunseen experimental data, including enhancements associated with the\n$P_{c\\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole\nstructure, representing the presence of a genuine compact pentaquark in the\npresence of a higher channel virtual state pole with non-vanishing width. While\nevaluated on this particular state, our framework is broadly applicable to\nother candidate hadronic states and offers a scalable tool for pole structure\ninference in scattering amplitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching theoretical predictions to experimental data remains a central\nchallenge in hadron spectroscopy. In particular, the identification of new\nhadronic states is difficult, as exotic signals near threshold can arise from a\nvariety of physical mechanisms. A key diagnostic in this context is the pole\nstructure of the scattering amplitude, but different configurations can produce\nsimilar signatures. The mapping between pole configurations and line shapes is\nespecially ambiguous near the mass threshold, where analytic control is\nlimited. In this work, we introduce an uncertainty-aware machine learning\napproach for classifying pole structures in $S$-matrix elements. Our method is\nbased on an ensemble of classifier chains that provide both epistemic and\naleatoric uncertainty estimates. We apply a rejection criterion based on\npredictive uncertainty, achieving a validation accuracy of nearly $95\\%$ while\ndiscarding only a small fraction of high-uncertainty predictions. Trained on\nsynthetic data with known pole structures, the model generalizes to previously\nunseen experimental data, including enhancements associated with the\n$P_{c\\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole\nstructure, representing the presence of a genuine compact pentaquark in the\npresence of a higher channel virtual state pole with non-vanishing width. While\nevaluated on this particular state, our framework is broadly applicable to\nother candidate hadronic states and offers a scalable tool for pole structure\ninference in scattering amplitudes."
                },
                "authors": [
                    {
                        "name": "Felix Frohnert"
                    },
                    {
                        "name": "Denny Lane B. Sombillo"
                    },
                    {
                        "name": "Evert van Nieuwenburg"
                    },
                    {
                        "name": "Patrick Emonts"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Emonts"
                },
                "author": "Patrick Emonts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.08801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08801v1",
                "updated": "2025-07-11T17:59:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    42,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:42Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    42,
                    4,
                    192,
                    0
                ],
                "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective"
                },
                "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos."
                },
                "authors": [
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Weihua Chen"
                    },
                    {
                        "name": "Jun Cen"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jingyun Liang"
                    },
                    {
                        "name": "Shuning Chang"
                    },
                    {
                        "name": "Zhihui Lin"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Pengwei Liu"
                    },
                    {
                        "name": "Jiazheng Xing"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Code and Models: https://github.com/alibaba-damo-academy/Lumos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08794v1",
                "updated": "2025-07-11T17:55:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    55,
                    22,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:55:22Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    55,
                    22,
                    4,
                    192,
                    0
                ],
                "title": "One Token to Fool LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Token to Fool LLM-as-a-Judge"
                },
                "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM."
                },
                "authors": [
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "S. Y. Kung"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12003v2",
                "updated": "2025-07-11T17:44:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    44,
                    32,
                    4,
                    192,
                    0
                ],
                "published": "2025-06-13T17:55:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    55,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Upgrade or Switch: Do We Need a Next-Gen Trusted Architecture for the\n  Internet of AI Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade or Switch: Do We Need a Next-Gen Trusted Architecture for the\n  Internet of AI Agents?"
                },
                "summary": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built index\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid index/registries. Drawing parallels to\ndialup-to-broadband transitions, we find that agent requirements constitute\nqualitative, and not incremental, changes. While upgrades offer compatibility\nand faster deployment, clean-slate solutions provide better performance but\nrequire longer for adoption. Our analysis suggests hybrid approaches will\nemerge, with centralized indexes for critical agents and federated meshes for\nspecialized use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built index\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid index/registries. Drawing parallels to\ndialup-to-broadband transitions, we find that agent requirements constitute\nqualitative, and not incremental, changes. While upgrades offer compatibility\nand faster deployment, clean-slate solutions provide better performance but\nrequire longer for adoption. Our analysis suggests hybrid approaches will\nemerge, with centralized indexes for critical agents and federated meshes for\nspecialized use cases."
                },
                "authors": [
                    {
                        "name": "Ramesh Raskar"
                    },
                    {
                        "name": "Pradyumna Chari"
                    },
                    {
                        "name": "Jared James Grogan"
                    },
                    {
                        "name": "Mahesh Lambe"
                    },
                    {
                        "name": "Robert Lincourt"
                    },
                    {
                        "name": "Raghu Bala"
                    },
                    {
                        "name": "Aditi Joshi"
                    },
                    {
                        "name": "Abhishek Singh"
                    },
                    {
                        "name": "Ayush Chopra"
                    },
                    {
                        "name": "Rajesh Ranjan"
                    },
                    {
                        "name": "Shailja Gupta"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Maria Gorskikh"
                    },
                    {
                        "name": "Sichao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sichao Wang"
                },
                "author": "Sichao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08771v1",
                "updated": "2025-07-11T17:28:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    28,
                    56,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:28:56Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    28,
                    56,
                    4,
                    192,
                    0
                ],
                "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity"
                },
                "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN)."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "21 pages, 7 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08765v1",
                "updated": "2025-07-11T17:21:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    21,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:21:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    21,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Compress Any Segment Anything Model (SAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress Any Segment Anything Model (SAM)"
                },
                "summary": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models."
                },
                "authors": [
                    {
                        "name": "Juntong Fan"
                    },
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianqiang Shen"
                    },
                    {
                        "name": "Shang-Ling Jui"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Jing-Xiao Liao"
                    },
                    {
                        "name": "Feng-Lei Fan"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Lei Fan"
                },
                "author": "Feng-Lei Fan",
                "arxiv_comment": "13 pages, 6 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08743v1",
                "updated": "2025-07-11T16:45:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    45,
                    59,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:45:59Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    45,
                    59,
                    4,
                    192,
                    0
                ],
                "title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane\n  Geometry Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane\n  Geometry Detection"
                },
                "summary": "Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git."
                },
                "authors": [
                    {
                        "name": "Rei Tamaru"
                    },
                    {
                        "name": "Pei Li"
                    },
                    {
                        "name": "Bin Ran"
                    }
                ],
                "author_detail": {
                    "name": "Bin Ran"
                },
                "author": "Bin Ran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02984v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02984v3",
                "updated": "2025-07-11T16:38:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    38,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2024-01-01T17:35:52Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    17,
                    35,
                    52,
                    0,
                    1,
                    0
                ],
                "title": "Large Language Models in Mental Health Care: a Scoping Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Mental Health Care: a Scoping Review"
                },
                "summary": "Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations."
                },
                "authors": [
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Yi-han Sheu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Lauren V. Moran"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Andrew Beam"
                    },
                    {
                        "name": "John Torous"
                    }
                ],
                "author_detail": {
                    "name": "John Torous"
                },
                "author": "John Torous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02984v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02984v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17256v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17256v4",
                "updated": "2025-07-11T16:36:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    36,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2024-01-30T18:48:37Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    18,
                    48,
                    37,
                    1,
                    30,
                    0
                ],
                "title": "Weak-to-Strong Jailbreaking on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Jailbreaking on Large Language Models"
                },
                "summary": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong"
                },
                "authors": [
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yu-Xiang Wang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17256v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17256v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14679v3",
                "updated": "2025-07-11T16:23:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    23,
                    12,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-23T03:01:50Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    3,
                    1,
                    50,
                    0,
                    267,
                    0
                ],
                "title": "Quantifying Context Bias in Domain Adaptation for Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Context Bias in Domain Adaptation for Object Detection"
                },
                "summary": "Domain adaptation for object detection (DAOD) has become essential to counter\nperformance degradation caused by distribution shifts between training and\ndeployment domains. However, a critical factor influencing DAOD - context bias\nresulting from learned foreground-background (FG-BG) associations - has\nremained underexplored. We address three key questions regarding FG BG\nassociations in object detection: are FG-BG associations encoded during the\ntraining, is there a causal relationship between FG-BG associations and\ndetection performance, and is there an effect of FG-BG association on DAOD. To\nexamine how models capture FG BG associations, we analyze class-wise and\nfeature-wise performance degradation using background masking and feature\nperturbation, measured via change in accuracies (defined as drop rate). To\nexplore the causal role of FG-BG associations, we apply do-calculus on FG-BG\npairs guided by class activation mapping (CAM). To quantify the causal\ninfluence of FG-BG associations across domains, we propose a novel metric -\ndomain association gradient - defined as the ratio of drop rate to maximum mean\ndiscrepancy (MMD). Through systematic experiments involving background masking,\nfeature-level perturbations, and CAM, we reveal that convolution-based object\ndetection models encode FG-BG associations. Our results demonstrate that\ncontext bias not only exists but causally undermines the generalization\ncapabilities of object detection models across domains. Furthermore, we\nvalidate these findings across multiple models and datasets, including\nstate-of-the-art architectures such as ALDI++. This study highlights the\nnecessity of addressing context bias explicitly in DAOD frameworks, providing\ninsights that pave the way for developing more robust and generalizable object\ndetection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain adaptation for object detection (DAOD) has become essential to counter\nperformance degradation caused by distribution shifts between training and\ndeployment domains. However, a critical factor influencing DAOD - context bias\nresulting from learned foreground-background (FG-BG) associations - has\nremained underexplored. We address three key questions regarding FG BG\nassociations in object detection: are FG-BG associations encoded during the\ntraining, is there a causal relationship between FG-BG associations and\ndetection performance, and is there an effect of FG-BG association on DAOD. To\nexamine how models capture FG BG associations, we analyze class-wise and\nfeature-wise performance degradation using background masking and feature\nperturbation, measured via change in accuracies (defined as drop rate). To\nexplore the causal role of FG-BG associations, we apply do-calculus on FG-BG\npairs guided by class activation mapping (CAM). To quantify the causal\ninfluence of FG-BG associations across domains, we propose a novel metric -\ndomain association gradient - defined as the ratio of drop rate to maximum mean\ndiscrepancy (MMD). Through systematic experiments involving background masking,\nfeature-level perturbations, and CAM, we reveal that convolution-based object\ndetection models encode FG-BG associations. Our results demonstrate that\ncontext bias not only exists but causally undermines the generalization\ncapabilities of object detection models across domains. Furthermore, we\nvalidate these findings across multiple models and datasets, including\nstate-of-the-art architectures such as ALDI++. This study highlights the\nnecessity of addressing context bias explicitly in DAOD frameworks, providing\ninsights that pave the way for developing more robust and generalizable object\ndetection systems."
                },
                "authors": [
                    {
                        "name": "Hojun Son"
                    },
                    {
                        "name": "Asma Almutairi"
                    },
                    {
                        "name": "Arpan Kusari"
                    }
                ],
                "author_detail": {
                    "name": "Arpan Kusari"
                },
                "author": "Arpan Kusari",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08719v1",
                "updated": "2025-07-11T16:19:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    19,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:19:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    19,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Multilingual Multimodal Software Developer for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Multimodal Software Developer for Code Generation"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs."
                },
                "authors": [
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Liran Wang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Congnan Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08716v1",
                "updated": "2025-07-11T16:16:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    16,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:16:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    16,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine"
                },
                "summary": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD."
                },
                "authors": [
                    {
                        "name": "Kongwu Huang"
                    },
                    {
                        "name": "Shiyi Mu"
                    },
                    {
                        "name": "Jun Jiang"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Shugong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shugong Xu"
                },
                "author": "Shugong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08705v1",
                "updated": "2025-07-11T16:02:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    2,
                    24,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:02:24Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    2,
                    24,
                    4,
                    192,
                    0
                ],
                "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings"
                },
                "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery."
                },
                "authors": [
                    {
                        "name": "Philip Osborne"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP\n  2025 Call for System Demonstrations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.5; I.2.1; I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08704v1",
                "updated": "2025-07-11T15:57:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:57:37Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation"
                },
                "summary": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA."
                },
                "authors": [
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yuan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Meng"
                },
                "author": "Yuan Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00927v2",
                "updated": "2025-07-11T15:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    49,
                    30,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-01T15:59:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    59,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Multi-Token Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Token Attention"
                },
                "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial."
                },
                "authors": [
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    }
                ],
                "author_detail": {
                    "name": "Sainbayar Sukhbaatar"
                },
                "author": "Sainbayar Sukhbaatar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08679v1",
                "updated": "2025-07-11T15:21:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:21:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    21,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way"
                },
                "summary": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting."
                },
                "authors": [
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Ankesh Banerjee"
                    },
                    {
                        "name": "Arjya Bhattacharjee"
                    },
                    {
                        "name": "Kousik Dasgupta"
                    },
                    {
                        "name": "Subarna Tripathi"
                    }
                ],
                "author_detail": {
                    "name": "Subarna Tripathi"
                },
                "author": "Subarna Tripathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02827v2",
                "updated": "2025-07-11T15:13:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    13,
                    39,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-03T17:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    38,
                    44,
                    3,
                    184,
                    0
                ],
                "title": "USAD: End-to-End Human Activity Recognition via Diffusion Model with\n  Spatiotemporal Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USAD: End-to-End Human Activity Recognition via Diffusion Model with\n  Spatiotemporal Attention"
                },
                "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Hang Xiao"
                    },
                    {
                        "name": "Ying Yu"
                    },
                    {
                        "name": "Jiarui Li"
                    },
                    {
                        "name": "Zhifan Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08671v1",
                "updated": "2025-07-11T15:11:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    11,
                    27,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:11:27Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    11,
                    27,
                    4,
                    192,
                    0
                ],
                "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCup: Ranking-Enhanced Comment Updating with LLMs"
                },
                "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment."
                },
                "authors": [
                    {
                        "name": "Hua Ge"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Minxue Pan"
                    },
                    {
                        "name": "Fusen He"
                    },
                    {
                        "name": "Ziyue Tan"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Tan"
                },
                "author": "Ziyue Tan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; D.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08665v1",
                "updated": "2025-07-11T15:05:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    5,
                    6,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:05:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    5,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment"
                },
                "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Jiyao Zhang"
                    },
                    {
                        "name": "Chengli Zhong"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Qige Li"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08664v1",
                "updated": "2025-07-11T15:03:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    3,
                    17,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T15:03:17Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    3,
                    17,
                    4,
                    192,
                    0
                ],
                "title": "Introspection of Thought Helps AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introspection of Thought Helps AI Agents"
                },
                "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shaoning Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Shaoning Zeng"
                },
                "author": "Shaoning Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08649v1",
                "updated": "2025-07-11T14:53:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    14,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:53:14Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    53,
                    14,
                    4,
                    192,
                    0
                ],
                "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning"
                },
                "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2."
                },
                "authors": [
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Chenxi Sun"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07257v2",
                "updated": "2025-07-11T14:43:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    43,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T20:03:30Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    20,
                    3,
                    30,
                    2,
                    190,
                    0
                ],
                "title": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery"
                },
                "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud."
                },
                "authors": [
                    {
                        "name": "Licong Xu"
                    },
                    {
                        "name": "Milind Sarkar"
                    },
                    {
                        "name": "Anto I. Lonappan"
                    },
                    {
                        "name": "Íñigo Zubeldia"
                    },
                    {
                        "name": "Pablo Villanueva-Domingo"
                    },
                    {
                        "name": "Santiago Casas"
                    },
                    {
                        "name": "Christian Fidler"
                    },
                    {
                        "name": "Chetana Amancharla"
                    },
                    {
                        "name": "Ujjwal Tiwari"
                    },
                    {
                        "name": "Adrian Bayer"
                    },
                    {
                        "name": "Chadi Ait Ekioui"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Adrian Dimitrov"
                    },
                    {
                        "name": "James Fergusson"
                    },
                    {
                        "name": "Kahaan Gandhi"
                    },
                    {
                        "name": "Sven Krippendorf"
                    },
                    {
                        "name": "Andrew Laverick"
                    },
                    {
                        "name": "Julien Lesgourgues"
                    },
                    {
                        "name": "Antony Lewis"
                    },
                    {
                        "name": "Thomas Meier"
                    },
                    {
                        "name": "Blake Sherwin"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Xueqing Xu"
                    },
                    {
                        "name": "Boris Bolliet"
                    }
                ],
                "author_detail": {
                    "name": "Boris Bolliet"
                },
                "author": "Boris Bolliet",
                "arxiv_comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent Videos:\n  https://www.youtube.com/@cmbagent HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent Cloud:\n  https://cmbagent.cloud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08054v2",
                "updated": "2025-07-11T14:40:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    40,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2024-08-15T09:48:45Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    48,
                    45,
                    3,
                    228,
                    0
                ],
                "title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework"
                },
                "summary": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM"
                },
                "authors": [
                    {
                        "name": "Changyu Du"
                    },
                    {
                        "name": "Sebastian Esser"
                    },
                    {
                        "name": "Stavros Nousias"
                    },
                    {
                        "name": "André Borrmann"
                    }
                ],
                "author_detail": {
                    "name": "André Borrmann"
                },
                "author": "André Borrmann",
                "arxiv_comment": "Journal of Computing in Civil Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00467v2",
                "updated": "2025-07-11T14:39:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    39,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-05-01T11:43:27Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    11,
                    43,
                    27,
                    3,
                    121,
                    0
                ],
                "title": "Red Teaming Large Language Models for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Teaming Large Language Models for Healthcare"
                },
                "summary": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided."
                },
                "authors": [
                    {
                        "name": "Vahid Balazadeh"
                    },
                    {
                        "name": "Michael Cooper"
                    },
                    {
                        "name": "David Pellow"
                    },
                    {
                        "name": "Atousa Assadi"
                    },
                    {
                        "name": "Jennifer Bell"
                    },
                    {
                        "name": "Mark Coatsworth"
                    },
                    {
                        "name": "Kaivalya Deshpande"
                    },
                    {
                        "name": "Jim Fackler"
                    },
                    {
                        "name": "Gabriel Funingana"
                    },
                    {
                        "name": "Spencer Gable-Cook"
                    },
                    {
                        "name": "Anirudh Gangadhar"
                    },
                    {
                        "name": "Abhishek Jaiswal"
                    },
                    {
                        "name": "Sumanth Kaja"
                    },
                    {
                        "name": "Christopher Khoury"
                    },
                    {
                        "name": "Amrit Krishnan"
                    },
                    {
                        "name": "Randy Lin"
                    },
                    {
                        "name": "Kaden McKeen"
                    },
                    {
                        "name": "Sara Naimimohasses"
                    },
                    {
                        "name": "Khashayar Namdar"
                    },
                    {
                        "name": "Aviraj Newatia"
                    },
                    {
                        "name": "Allan Pang"
                    },
                    {
                        "name": "Anshul Pattoo"
                    },
                    {
                        "name": "Sameer Peesapati"
                    },
                    {
                        "name": "Diana Prepelita"
                    },
                    {
                        "name": "Bogdana Rakova"
                    },
                    {
                        "name": "Saba Sadatamin"
                    },
                    {
                        "name": "Rafael Schulman"
                    },
                    {
                        "name": "Ajay Shah"
                    },
                    {
                        "name": "Syed Azhar Shah"
                    },
                    {
                        "name": "Syed Ahmar Shah"
                    },
                    {
                        "name": "Babak Taati"
                    },
                    {
                        "name": "Balagopal Unnikrishnan"
                    },
                    {
                        "name": "Iñigo Urteaga"
                    },
                    {
                        "name": "Stephanie Williams"
                    },
                    {
                        "name": "Rahul G Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G Krishnan"
                },
                "author": "Rahul G Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08627v1",
                "updated": "2025-07-11T14:29:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    29,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations"
                },
                "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt."
                },
                "authors": [
                    {
                        "name": "Chi-en Amy Tai"
                    },
                    {
                        "name": "Pengyu Nie"
                    },
                    {
                        "name": "Lukasz Golab"
                    },
                    {
                        "name": "Alexander Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wong"
                },
                "author": "Alexander Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08621v1",
                "updated": "2025-07-11T14:23:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1"
                },
                "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings."
                },
                "authors": [
                    {
                        "name": "Marcin Pietroń"
                    },
                    {
                        "name": "Rafał Olszowski"
                    },
                    {
                        "name": "Jakub Gomułka"
                    },
                    {
                        "name": "Filip Gampel"
                    },
                    {
                        "name": "Andrzej Tomski"
                    }
                ],
                "author_detail": {
                    "name": "Andrzej Tomski"
                },
                "author": "Andrzej Tomski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08311v2",
                "updated": "2025-07-11T14:23:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    16,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-11T11:21:35Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    21,
                    35,
                    1,
                    70,
                    0
                ],
                "title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference"
                },
                "summary": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap."
                },
                "authors": [
                    {
                        "name": "Pol G. Recasens"
                    },
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Pol G. Recasens, Ferran Agullo: equal contribution. Paper accepted at\n  IEEE CLOUD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08619v1",
                "updated": "2025-07-11T14:19:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:19:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design"
                },
                "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted."
                },
                "authors": [
                    {
                        "name": "Soheyl Massoudi"
                    },
                    {
                        "name": "Mark Fuge"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fuge"
                },
                "author": "Mark Fuge",
                "arxiv_comment": "32 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08616v1",
                "updated": "2025-07-11T14:13:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    13,
                    22,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:13:22Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    13,
                    22,
                    4,
                    192,
                    0
                ],
                "title": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs"
                },
                "summary": "Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents."
                },
                "authors": [
                    {
                        "name": "Florian Grötschla"
                    },
                    {
                        "name": "Luis Müller"
                    },
                    {
                        "name": "Jan Tönshoff"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Bryan Perozzi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Perozzi"
                },
                "author": "Bryan Perozzi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08610v1",
                "updated": "2025-07-11T14:08:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    8,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:08:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    8,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data"
                },
                "summary": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods."
                },
                "authors": [
                    {
                        "name": "Parag Dutta"
                    },
                    {
                        "name": "Ambedkar Dukkipati"
                    }
                ],
                "author_detail": {
                    "name": "Ambedkar Dukkipati"
                },
                "author": "Ambedkar Dukkipati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08603v1",
                "updated": "2025-07-11T13:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    55,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    55,
                    45,
                    4,
                    192,
                    0
                ],
                "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Speech Instruction Data Potential with Query Rewriting"
                },
                "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities."
                },
                "authors": [
                    {
                        "name": "Yonghua Hei"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Huiyu Zhou"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07879v2",
                "updated": "2025-07-11T13:50:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    50,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T16:02:50Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    2,
                    50,
                    3,
                    191,
                    0
                ],
                "title": "LISTEN: Lightweight Industrial Sound-representable Transformer for Edge\n  Notification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LISTEN: Lightweight Industrial Sound-representable Transformer for Edge\n  Notification"
                },
                "summary": "Deep learning-based machine listening is broadening the scope of industrial\nacoustic analysis for applications like anomaly detection and predictive\nmaintenance, thereby improving manufacturing efficiency and reliability.\nNevertheless, its reliance on large, task-specific annotated datasets for every\nnew task limits widespread implementation on shop floors. While emerging sound\nfoundation models aim to alleviate data dependency, they are too large and\ncomputationally expensive, requiring cloud infrastructure or high-end hardware\nthat is impractical for on-site, real-time deployment. We address this gap with\nLISTEN (Lightweight Industrial Sound-representable Transformer for Edge\nNotification), a kilobyte-sized industrial sound foundation model. Using\nknowledge distillation, LISTEN runs in real-time on low-cost edge devices. On\nbenchmark downstream tasks, it performs nearly identically to its much larger\nparent model, even when fine-tuned with minimal datasets and training resource.\nBeyond the model itself, we demonstrate its real-world utility by integrating\nLISTEN into a complete machine monitoring framework on an edge device with an\nIndustrial Internet of Things (IIoT) sensor and system, validating its\nperformance and generalization capabilities on a live manufacturing shop floor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based machine listening is broadening the scope of industrial\nacoustic analysis for applications like anomaly detection and predictive\nmaintenance, thereby improving manufacturing efficiency and reliability.\nNevertheless, its reliance on large, task-specific annotated datasets for every\nnew task limits widespread implementation on shop floors. While emerging sound\nfoundation models aim to alleviate data dependency, they are too large and\ncomputationally expensive, requiring cloud infrastructure or high-end hardware\nthat is impractical for on-site, real-time deployment. We address this gap with\nLISTEN (Lightweight Industrial Sound-representable Transformer for Edge\nNotification), a kilobyte-sized industrial sound foundation model. Using\nknowledge distillation, LISTEN runs in real-time on low-cost edge devices. On\nbenchmark downstream tasks, it performs nearly identically to its much larger\nparent model, even when fine-tuned with minimal datasets and training resource.\nBeyond the model itself, we demonstrate its real-world utility by integrating\nLISTEN into a complete machine monitoring framework on an edge device with an\nIndustrial Internet of Things (IIoT) sensor and system, validating its\nperformance and generalization capabilities on a live manufacturing shop floor."
                },
                "authors": [
                    {
                        "name": "Changheon Han"
                    },
                    {
                        "name": "Yun Seok Kang"
                    },
                    {
                        "name": "Yuseop Sim"
                    },
                    {
                        "name": "Hyung Wook Park"
                    },
                    {
                        "name": "Martin Byung-Guk Jun"
                    }
                ],
                "author_detail": {
                    "name": "Martin Byung-Guk Jun"
                },
                "author": "Martin Byung-Guk Jun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07248v2",
                "updated": "2025-07-11T13:39:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    39,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T19:38:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    38,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings"
                },
                "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains."
                },
                "authors": [
                    {
                        "name": "Jean-Philippe Corbeil"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Francois Beaulieu"
                    },
                    {
                        "name": "Paul Vozila"
                    }
                ],
                "author_detail": {
                    "name": "Paul Vozila"
                },
                "author": "Paul Vozila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07531v2",
                "updated": "2025-07-11T13:32:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    32,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-10T07:54:47Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    54,
                    47,
                    3,
                    100,
                    0
                ],
                "title": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A taxonomy of epistemic injustice in the context of AI and the case for\n  generative hermeneutical erasure"
                },
                "summary": "Epistemic injustice related to AI is a growing concern. In relation to\nmachine learning models, epistemic injustice can have a diverse range of\nsources, ranging from epistemic opacity, the discriminatory automation of\ntestimonial prejudice, and the distortion of human beliefs via generative AI's\nhallucinations to the exclusion of the global South in global AI governance,\nthe execution of bureaucratic violence via algorithmic systems, and\ninteractions with conversational artificial agents. Based on a proposed general\ntaxonomy of epistemic injustice, this paper first sketches a taxonomy of the\ntypes of epistemic injustice in the context of AI, relying on the work of\nscholars from the fields of philosophy of technology, political philosophy and\nsocial epistemology. Secondly, an additional conceptualization on epistemic\ninjustice in the context of AI is provided: generative hermeneutical erasure. I\nargue that this injustice the automation of 'epistemicide', the injustice done\nto epistemic agents in their capacity for collective sense-making through the\nsuppression of difference in epistemology and conceptualization by LLMs. AI\nsystems' 'view from nowhere' epistemically inferiorizes non-Western\nepistemologies and thereby contributes to the erosion of their epistemic\nparticulars, gradually contributing to hermeneutical erasure. This work's\nrelevance lies in proposal of a taxonomy that allows epistemic injustices to be\nmapped in the AI domain and the proposal of a novel form of AI-related\nepistemic injustice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic injustice related to AI is a growing concern. In relation to\nmachine learning models, epistemic injustice can have a diverse range of\nsources, ranging from epistemic opacity, the discriminatory automation of\ntestimonial prejudice, and the distortion of human beliefs via generative AI's\nhallucinations to the exclusion of the global South in global AI governance,\nthe execution of bureaucratic violence via algorithmic systems, and\ninteractions with conversational artificial agents. Based on a proposed general\ntaxonomy of epistemic injustice, this paper first sketches a taxonomy of the\ntypes of epistemic injustice in the context of AI, relying on the work of\nscholars from the fields of philosophy of technology, political philosophy and\nsocial epistemology. Secondly, an additional conceptualization on epistemic\ninjustice in the context of AI is provided: generative hermeneutical erasure. I\nargue that this injustice the automation of 'epistemicide', the injustice done\nto epistemic agents in their capacity for collective sense-making through the\nsuppression of difference in epistemology and conceptualization by LLMs. AI\nsystems' 'view from nowhere' epistemically inferiorizes non-Western\nepistemologies and thereby contributes to the erosion of their epistemic\nparticulars, gradually contributing to hermeneutical erasure. This work's\nrelevance lies in proposal of a taxonomy that allows epistemic injustices to be\nmapped in the AI domain and the proposal of a novel form of AI-related\nepistemic injustice."
                },
                "authors": [
                    {
                        "name": "Warmhold Jan Thomas Mollema"
                    }
                ],
                "author_detail": {
                    "name": "Warmhold Jan Thomas Mollema"
                },
                "author": "Warmhold Jan Thomas Mollema",
                "arxiv_comment": "33 pages; 3 figures; 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08584v1",
                "updated": "2025-07-11T13:29:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    29,
                    32,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:29:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    29,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions."
                },
                "authors": [
                    {
                        "name": "Dimitrios Emmanoulopoulos"
                    },
                    {
                        "name": "Ollie Olby"
                    },
                    {
                        "name": "Justin Lyon"
                    },
                    {
                        "name": "Namid R. Stillman"
                    }
                ],
                "author_detail": {
                    "name": "Namid R. Stillman"
                },
                "author": "Namid R. Stillman",
                "arxiv_comment": "31 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 65C05, 68T01, 60H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.0; I.2.1; I.2.3; I.2.4; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08570v1",
                "updated": "2025-07-11T13:18:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    18,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:18:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    18,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Photonic processor benchmarking for variational quantum process\n  tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic processor benchmarking for variational quantum process\n  tomography"
                },
                "summary": "We present a quantum-analogous experimental demonstration of variational\nquantum process tomography using an optical processor. This approach leverages\nclassical one-hot encoding and unitary decomposition to perform the variational\nquantum algorithm on a photonic platform. We create the first benchmark for\nvariational quantum process tomography evaluating the performance of the\nquantum-analogous experiment on the optical processor against several publicly\naccessible quantum computing platforms, including IBM's 127-qubit Sherbrooke\nprocessor, QuTech's 5-qubit Tuna-5 processor, and Quandela's 12-mode Ascella\nquantum optical processor. We evaluate each method using process fidelity, cost\nfunction convergence, and processing time per iteration for variational quantum\ncircuit depths of $d=3$ and $d=6$. Our results indicate that the optical\nprocessors outperform their superconducting counterparts in terms of fidelity\nand convergence behavior reaching fidelities of $0.8$ after $9$ iterations,\nparticularly at higher depths, where the noise of decoherence and dephasing\naffect the superconducting processors significantly.\n  We further investigate the influence of any additional quantum optical\neffects in our platform relative to the classical one-hot encoding. From the\nprocess fidelity results it shows that the (classical) thermal noise in the\nphase-shifters dominates over other optical imperfections, such as mode\nmismatch and dark counts from single-photon sources.\n  The benchmarking framework and experimental results demonstrate that photonic\nprocessors are strong contenders for near-term quantum algorithm deployment,\nparticularly in hybrid variational contexts. This analysis is valuable not only\nfor state and process tomography but also for a wide range of applications\ninvolving variational quantum circuit based algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a quantum-analogous experimental demonstration of variational\nquantum process tomography using an optical processor. This approach leverages\nclassical one-hot encoding and unitary decomposition to perform the variational\nquantum algorithm on a photonic platform. We create the first benchmark for\nvariational quantum process tomography evaluating the performance of the\nquantum-analogous experiment on the optical processor against several publicly\naccessible quantum computing platforms, including IBM's 127-qubit Sherbrooke\nprocessor, QuTech's 5-qubit Tuna-5 processor, and Quandela's 12-mode Ascella\nquantum optical processor. We evaluate each method using process fidelity, cost\nfunction convergence, and processing time per iteration for variational quantum\ncircuit depths of $d=3$ and $d=6$. Our results indicate that the optical\nprocessors outperform their superconducting counterparts in terms of fidelity\nand convergence behavior reaching fidelities of $0.8$ after $9$ iterations,\nparticularly at higher depths, where the noise of decoherence and dephasing\naffect the superconducting processors significantly.\n  We further investigate the influence of any additional quantum optical\neffects in our platform relative to the classical one-hot encoding. From the\nprocess fidelity results it shows that the (classical) thermal noise in the\nphase-shifters dominates over other optical imperfections, such as mode\nmismatch and dark counts from single-photon sources.\n  The benchmarking framework and experimental results demonstrate that photonic\nprocessors are strong contenders for near-term quantum algorithm deployment,\nparticularly in hybrid variational contexts. This analysis is valuable not only\nfor state and process tomography but also for a wide range of applications\ninvolving variational quantum circuit based algorithms."
                },
                "authors": [
                    {
                        "name": "Vladlen Galetsky"
                    },
                    {
                        "name": "Paul Kohl"
                    },
                    {
                        "name": "Janis Nötzel"
                    }
                ],
                "author_detail": {
                    "name": "Janis Nötzel"
                },
                "author": "Janis Nötzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08567v1",
                "updated": "2025-07-11T13:11:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    11,
                    11,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T13:11:11Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    11,
                    11,
                    4,
                    192,
                    0
                ],
                "title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling"
                },
                "summary": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters."
                },
                "authors": [
                    {
                        "name": "Preslav Aleksandrov"
                    },
                    {
                        "name": "Meghdad Kurmanji"
                    },
                    {
                        "name": "Fernando Garcia Redondo"
                    },
                    {
                        "name": "David O'Shea"
                    },
                    {
                        "name": "William Shen"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "14 pages and 6 figures. Submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v3",
                "updated": "2025-07-11T12:58:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    58,
                    59,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experiments demonstrate\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation,\nwhile maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation,\nexhibiting robust feature transfer capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experiments demonstrate\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation,\nwhile maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation,\nexhibiting robust feature transfer capabilities."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Yugui Lin"
                    },
                    {
                        "name": "Pietro Lio"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Code and data will be made available on request. V3 Update: Add\n  Ablation Study and Discussion; Improve Introduction; Optimize Figures; Add\n  references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08557v1",
                "updated": "2025-07-11T12:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:57:51Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation"
                },
                "summary": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/"
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Zehua Chen"
                    },
                    {
                        "name": "Zeqian Ju"
                    },
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Weibei Dou"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v1",
                "updated": "2025-07-11T12:39:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08538v1",
                "updated": "2025-07-11T12:38:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    38,
                    2,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:38:02Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    38,
                    2,
                    4,
                    192,
                    0
                ],
                "title": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on\n  Multilingual Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on\n  Multilingual Benchmarks"
                },
                "summary": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language."
                },
                "authors": [
                    {
                        "name": "David Pomerenke"
                    },
                    {
                        "name": "Jonas Nothnagel"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10657v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10657v4",
                "updated": "2025-07-11T12:24:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    24,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-15T12:16:33Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    12,
                    16,
                    33,
                    0,
                    197,
                    0
                ],
                "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Validating Synthetic Data for Formula Generation"
                },
                "summary": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data."
                },
                "authors": [
                    {
                        "name": "Usneek Singh"
                    },
                    {
                        "name": "José Cambronero"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Aditya Kanade"
                    },
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Vu Le"
                    },
                    {
                        "name": "Mukul Singh"
                    },
                    {
                        "name": "Gust Verbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Gust Verbruggen"
                },
                "author": "Gust Verbruggen",
                "arxiv_comment": "Accepted at Findings of NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10657v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10657v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11924v2",
                "updated": "2025-07-11T12:13:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    13,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-14T23:47:46Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    23,
                    47,
                    46,
                    4,
                    73,
                    0
                ],
                "title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives"
                },
                "summary": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models."
                },
                "authors": [
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Krishna Sayana"
                    },
                    {
                        "name": "Hubert Pham"
                    },
                    {
                        "name": "James Pine"
                    },
                    {
                        "name": "Yuri Vasilevski"
                    },
                    {
                        "name": "Raghavendra Vasudeva"
                    },
                    {
                        "name": "Marialena Kyriakidi"
                    },
                    {
                        "name": "Liam Hebert"
                    },
                    {
                        "name": "Ambarish Jash"
                    },
                    {
                        "name": "Anushya Subbiah"
                    },
                    {
                        "name": "Sukhdeep Sodhi"
                    }
                ],
                "author_detail": {
                    "name": "Sukhdeep Sodhi"
                },
                "author": "Sukhdeep Sodhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08513v1",
                "updated": "2025-07-11T12:00:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    0,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:00:10Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    0,
                    10,
                    4,
                    192,
                    0
                ],
                "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications."
                },
                "authors": [
                    {
                        "name": "Liu He"
                    },
                    {
                        "name": "Xiao Zeng"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Albert Y. C. Chen"
                    },
                    {
                        "name": "Lu Xia"
                    },
                    {
                        "name": "Shashwat Verma"
                    },
                    {
                        "name": "Sankalp Dayal"
                    },
                    {
                        "name": "Min Sun"
                    },
                    {
                        "name": "Cheng-Hao Kuo"
                    },
                    {
                        "name": "Daniel Aliaga"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Aliaga"
                },
                "author": "Daniel Aliaga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08708v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08708v6",
                "updated": "2025-07-11T11:45:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    45,
                    52,
                    4,
                    192,
                    0
                ],
                "published": "2023-06-14T19:20:43Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    19,
                    20,
                    43,
                    2,
                    165,
                    0
                ],
                "title": "Naeural AI OS -- Decentralized ubiquitous computing MLOps execution\n  engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Naeural AI OS -- Decentralized ubiquitous computing MLOps execution\n  engine"
                },
                "summary": "Over the past few years, ubiquitous, or pervasive computing has gained\npopularity as the primary approach for a wide range of applications, including\nenterprise-grade systems, consumer applications, and gaming systems. Ubiquitous\ncomputing refers to the integration of computing technologies into everyday\nobjects and environments, creating a network of interconnected devices that can\ncommunicate with each other and with humans. By using ubiquitous computing\ntechnologies, communities can become more connected and efficient, with members\nable to communicate and collaborate more easily. This enabled\ninterconnectedness and collaboration can lead to a more successful and\nsustainable community. The spread of ubiquitous computing, however, has\nemphasized the importance of automated learning and smart applications in\ngeneral. Even though there have been significant strides in Artificial\nIntelligence and Deep Learning, large scale adoption has been hesitant due to\nmounting pressure on expensive and highly complex cloud numerical-compute\ninfrastructures. Adopting, and even developing, practical machine learning\nsystems can come with prohibitive costs, not only in terms of complex\ninfrastructures but also of solid expertise in Data Science and Machine\nLearning. In this paper we present an innovative approach for low-code\ndevelopment and deployment of end-to-end AI cooperative application pipelines.\nWe address infrastructure allocation, costs, and secure job distribution in a\nfully decentralized global cooperative community based on tokenized economics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, ubiquitous, or pervasive computing has gained\npopularity as the primary approach for a wide range of applications, including\nenterprise-grade systems, consumer applications, and gaming systems. Ubiquitous\ncomputing refers to the integration of computing technologies into everyday\nobjects and environments, creating a network of interconnected devices that can\ncommunicate with each other and with humans. By using ubiquitous computing\ntechnologies, communities can become more connected and efficient, with members\nable to communicate and collaborate more easily. This enabled\ninterconnectedness and collaboration can lead to a more successful and\nsustainable community. The spread of ubiquitous computing, however, has\nemphasized the importance of automated learning and smart applications in\ngeneral. Even though there have been significant strides in Artificial\nIntelligence and Deep Learning, large scale adoption has been hesitant due to\nmounting pressure on expensive and highly complex cloud numerical-compute\ninfrastructures. Adopting, and even developing, practical machine learning\nsystems can come with prohibitive costs, not only in terms of complex\ninfrastructures but also of solid expertise in Data Science and Machine\nLearning. In this paper we present an innovative approach for low-code\ndevelopment and deployment of end-to-end AI cooperative application pipelines.\nWe address infrastructure allocation, costs, and secure job distribution in a\nfully decentralized global cooperative community based on tokenized economics."
                },
                "authors": [
                    {
                        "name": "Cristian Bleotiu"
                    },
                    {
                        "name": "Stefan Saraev"
                    },
                    {
                        "name": "Bogdan Hobeanu"
                    },
                    {
                        "name": "Andrei Ionut Damian"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Ionut Damian"
                },
                "author": "Andrei Ionut Damian",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08708v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08708v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.5; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18246v2",
                "updated": "2025-07-11T11:43:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    43,
                    8,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-25T10:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "title": "One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason)."
                },
                "authors": [
                    {
                        "name": "Ritesh Goru"
                    },
                    {
                        "name": "Shanay Mehta"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07445v2",
                "updated": "2025-07-11T11:40:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    40,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T05:48:28Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    5,
                    48,
                    28,
                    3,
                    191,
                    0
                ],
                "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs\n  in Production-Living Simulations with Stardew Valley",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs\n  in Production-Living Simulations with Stardew Valley"
                },
                "summary": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments."
                },
                "authors": [
                    {
                        "name": "Weihao Tan"
                    },
                    {
                        "name": "Changjiu Jiang"
                    },
                    {
                        "name": "Yu Duan"
                    },
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Jiageng Li"
                    },
                    {
                        "name": "Yitian Hong"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "Project website: https://weihaotan.github.io/StarDojo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08505v1",
                "updated": "2025-07-11T11:30:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:30:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    30,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R"
                },
                "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks."
                },
                "authors": [
                    {
                        "name": "Pablo Robin Guerrero"
                    },
                    {
                        "name": "Yueyang Pan"
                    },
                    {
                        "name": "Sanidhya Kashyap"
                    }
                ],
                "author_detail": {
                    "name": "Sanidhya Kashyap"
                },
                "author": "Sanidhya Kashyap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08501v1",
                "updated": "2025-07-11T11:24:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    24,
                    9,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:24:09Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    24,
                    9,
                    4,
                    192,
                    0
                ],
                "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Language to Logic: A Bi-Level Framework for Structured Reasoning"
                },
                "summary": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs."
                },
                "authors": [
                    {
                        "name": "Keying Yang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08498v1",
                "updated": "2025-07-11T11:20:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    20,
                    39,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:20:39Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    20,
                    39,
                    4,
                    192,
                    0
                ],
                "title": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop"
                },
                "summary": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08496v1",
                "updated": "2025-07-11T11:18:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    18,
                    49,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:18:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    18,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware\n  Procedural Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware\n  Procedural Planning"
                },
                "summary": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa."
                },
                "authors": [
                    {
                        "name": "Shibo Sun"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Mingjie Wei"
                    },
                    {
                        "name": "Lanshun Nie"
                    },
                    {
                        "name": "Wei-Nan Zhang"
                    },
                    {
                        "name": "Dechen Zhan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Lei Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lei Fan"
                },
                "author": "Lei Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08491v1",
                "updated": "2025-07-11T11:16:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    16,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T11:16:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    11,
                    16,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation\n  using clembench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation\n  using clembench"
                },
                "summary": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests."
                },
                "authors": [
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Jonathan Jordan"
                    },
                    {
                        "name": "Philipp Sadler"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Sadler"
                },
                "author": "Philipp Sadler",
                "arxiv_comment": "All code required to run the benchmark, as well as extensive\n  documentation, is available at https://github.com/clembench/clembench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06892v3",
                "updated": "2025-07-11T10:32:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    32,
                    34,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T14:29:45Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    29,
                    45,
                    2,
                    190,
                    0
                ],
                "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc."
                },
                "authors": [
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "Preliminary version, v3, added the missing name of x-axis in the left\n  part of Fig.1 and corrected a wrong number in Fig.3. Project page:\n  https://anitaleungxx.github.io/ReMix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08473v1",
                "updated": "2025-07-11T10:31:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:31:53Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    31,
                    53,
                    4,
                    192,
                    0
                ],
                "title": "Evaluating SAE interpretability without explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating SAE interpretability without explanations"
                },
                "summary": "Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques."
                },
                "authors": [
                    {
                        "name": "Gonçalo Paulo"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08472v1",
                "updated": "2025-07-11T10:29:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    29,
                    4,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:29:04Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    29,
                    4,
                    4,
                    192,
                    0
                ],
                "title": "Pre-Training LLMs on a budget: A comparison of three optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Training LLMs on a budget: A comparison of three optimizers"
                },
                "summary": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results."
                },
                "authors": [
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Christian Kroos"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Küch"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Küch"
                },
                "author": "Fabian Küch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08468v1",
                "updated": "2025-07-11T10:19:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    19,
                    56,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:19:56Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    19,
                    56,
                    4,
                    192,
                    0
                ],
                "title": "Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study"
                },
                "summary": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information."
                },
                "authors": [
                    {
                        "name": "Marina Luketina"
                    },
                    {
                        "name": "Andrea Benkel"
                    },
                    {
                        "name": "Christoph G. Schuetz"
                    }
                ],
                "author_detail": {
                    "name": "Christoph G. Schuetz"
                },
                "author": "Christoph G. Schuetz",
                "arxiv_comment": "26 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08460v1",
                "updated": "2025-07-11T10:03:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    3,
                    23,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:03:23Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    3,
                    23,
                    4,
                    192,
                    0
                ],
                "title": "F3-Net: Foundation Model for Full Abnormality Segmentation of Medical\n  Images with Flexible Input Modality Requirement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F3-Net: Foundation Model for Full Abnormality Segmentation of Medical\n  Images with Flexible Input Modality Requirement"
                },
                "summary": "F3-Net is a foundation model designed to overcome persistent challenges in\nclinical medical image segmentation, including reliance on complete multimodal\ninputs, limited generalizability, and narrow task specificity. Through flexible\nsynthetic modality training, F3-Net maintains robust performance even in the\npresence of missing MRI sequences, leveraging a zero-image strategy to\nsubstitute absent modalities without relying on explicit synthesis networks,\nthereby enhancing real-world applicability. Its unified architecture supports\nmulti-pathology segmentation across glioma, metastasis, stroke, and white\nmatter lesions without retraining, outperforming CNN-based and\ntransformer-based models that typically require disease-specific fine-tuning.\nEvaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,\nF3-Net demonstrates strong resilience to domain shifts and clinical\nheterogeneity. On the whole pathology dataset, F3-Net achieves average Dice\nSimilarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET\n2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a\nversatile, scalable solution bridging the gap between deep learning research\nand practical clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F3-Net is a foundation model designed to overcome persistent challenges in\nclinical medical image segmentation, including reliance on complete multimodal\ninputs, limited generalizability, and narrow task specificity. Through flexible\nsynthetic modality training, F3-Net maintains robust performance even in the\npresence of missing MRI sequences, leveraging a zero-image strategy to\nsubstitute absent modalities without relying on explicit synthesis networks,\nthereby enhancing real-world applicability. Its unified architecture supports\nmulti-pathology segmentation across glioma, metastasis, stroke, and white\nmatter lesions without retraining, outperforming CNN-based and\ntransformer-based models that typically require disease-specific fine-tuning.\nEvaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,\nF3-Net demonstrates strong resilience to domain shifts and clinical\nheterogeneity. On the whole pathology dataset, F3-Net achieves average Dice\nSimilarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET\n2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a\nversatile, scalable solution bridging the gap between deep learning research\nand practical clinical deployment."
                },
                "authors": [
                    {
                        "name": "Seyedeh Sahar Taheri Otaghsara"
                    },
                    {
                        "name": "Reza Rahmanzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahmanzadeh"
                },
                "author": "Reza Rahmanzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08459v1",
                "updated": "2025-07-11T10:02:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    2,
                    21,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:02:21Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    2,
                    21,
                    4,
                    192,
                    0
                ],
                "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method."
                },
                "authors": [
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Shuyi Xie"
                    },
                    {
                        "name": "Qingsong Lv"
                    },
                    {
                        "name": "Shupei Xiao"
                    },
                    {
                        "name": "Linlin Song"
                    },
                    {
                        "name": "Sui Wenjuan"
                    },
                    {
                        "name": "Fan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lin"
                },
                "author": "Fan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v3",
                "updated": "2025-07-11T09:50:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    50,
                    2,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08445v1",
                "updated": "2025-07-11T09:36:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    36,
                    45,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:36:45Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    36,
                    45,
                    4,
                    192,
                    0
                ],
                "title": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval"
                },
                "summary": "Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems."
                },
                "authors": [
                    {
                        "name": "Yaodong Su"
                    },
                    {
                        "name": "Yixiang Fang"
                    },
                    {
                        "name": "Yingli Zhou"
                    },
                    {
                        "name": "Quanqing Xu"
                    },
                    {
                        "name": "Chuanhui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chuanhui Yang"
                },
                "author": "Chuanhui Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08440v1",
                "updated": "2025-07-11T09:31:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    31,
                    10,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:31:10Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    31,
                    10,
                    4,
                    192,
                    0
                ],
                "title": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences"
                },
                "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains."
                },
                "authors": [
                    {
                        "name": "Selina Heller"
                    },
                    {
                        "name": "Mohamed Ibrahim"
                    },
                    {
                        "name": "David Antony Selby"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08264v2",
                "updated": "2025-07-11T09:23:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    23,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-05-13T06:26:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    26,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and\n  Efficient Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and\n  Efficient Reinforcement Learning"
                },
                "summary": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9% in low traffic density, +21% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9% in low traffic density, +21% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents."
                },
                "authors": [
                    {
                        "name": "Ahmed Abouelazm"
                    },
                    {
                        "name": "Tim Weinstein"
                    },
                    {
                        "name": "Tim Joseph"
                    },
                    {
                        "name": "Philip Schörner"
                    },
                    {
                        "name": "J. Marius Zöllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zöllner"
                },
                "author": "J. Marius Zöllner",
                "arxiv_comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08430v1",
                "updated": "2025-07-11T09:17:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    17,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:17:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    17,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "A co-deployed dust-logging instrument for the IceCube Upgrade and\n  IceCube-Gen2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A co-deployed dust-logging instrument for the IceCube Upgrade and\n  IceCube-Gen2"
                },
                "summary": "A precise understanding of the optical properties of the instrumented\nAntarctic ice sheet is crucial to the performance of optical Cherenkov\ntelescopes such as the IceCube Neutrino Observatory and its planned successor,\nIceCube-Gen2. One complication arising from the large envisioned footprint of\nIceCube-Gen2 is the larger impact of the so-called ice tilt. It describes the\nundulation of ice layers of constant optical properties within the detector. In\nthis contribution, we will describe the project to build a co-deployed laser\ndust logger. This is a device to measure the stratigraphy of impurities in the\nice to derive the ice tilt. It consists of a light source that will be\nco-deployed with the photosensor modules, meaning it is part of the deployment\nstring and operated during the deployment of the detector. The newly developed\ndevice will be tested during the deployment of the IceCube Upgrade in the\n2025/26 austral summer to pave the way for IceCube-Gen2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A precise understanding of the optical properties of the instrumented\nAntarctic ice sheet is crucial to the performance of optical Cherenkov\ntelescopes such as the IceCube Neutrino Observatory and its planned successor,\nIceCube-Gen2. One complication arising from the large envisioned footprint of\nIceCube-Gen2 is the larger impact of the so-called ice tilt. It describes the\nundulation of ice layers of constant optical properties within the detector. In\nthis contribution, we will describe the project to build a co-deployed laser\ndust logger. This is a device to measure the stratigraphy of impurities in the\nice to derive the ice tilt. It consists of a light source that will be\nco-deployed with the photosensor modules, meaning it is part of the deployment\nstring and operated during the deployment of the detector. The newly developed\ndevice will be tested during the deployment of the IceCube Upgrade in the\n2025/26 austral summer to pave the way for IceCube-Gen2."
                },
                "authors": [
                    {
                        "name": "Anna Eimer"
                    },
                    {
                        "name": "Martin Rongen"
                    }
                ],
                "author_detail": {
                    "name": "Martin Rongen"
                },
                "arxiv_affiliation": "for the IceCube Collaboration",
                "author": "Martin Rongen",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08427v1",
                "updated": "2025-07-11T09:13:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    13,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:13:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    13,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains"
                },
                "summary": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing."
                },
                "authors": [
                    {
                        "name": "Zilu Dong"
                    },
                    {
                        "name": "Xiangqing Shen"
                    },
                    {
                        "name": "Zinong Yang"
                    },
                    {
                        "name": "Rui Xia"
                    }
                ],
                "author_detail": {
                    "name": "Rui Xia"
                },
                "author": "Rui Xia",
                "arxiv_comment": "Accepted to ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08425v1",
                "updated": "2025-07-11T09:11:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    11,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:11:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    11,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies."
                },
                "authors": [
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Yaping Zhang"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_doi": "10.24846/v34i1y202501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24846/v34i1y202501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Lu XIANG, Yang ZHAO, Yaping ZHANG, Chengqing ZONG, \"A Survey of\n  Large Language Models in Discipline-specific Research: Challenges, Methods\n  and Opportunities\", Studies in Informatics and Control, ISSN 1220-1766, vol.\n  34(1), pp. 5-24, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13857v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13857v4",
                "updated": "2025-07-11T08:59:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    59,
                    48,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-18T03:14:23Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    3,
                    14,
                    23,
                    1,
                    77,
                    0
                ],
                "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations"
                },
                "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiayi Tong"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Ziyang Hu"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Christopher J. Lindsell"
                    },
                    {
                        "name": "Michael J. Pencina"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Chuan Hong"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Hong"
                },
                "author": "Chuan Hong",
                "arxiv_comment": "30 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13857v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13857v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08525v2",
                "updated": "2025-07-11T08:36:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    36,
                    40,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-11T15:17:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training"
                },
                "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes."
                },
                "authors": [
                    {
                        "name": "Tong Wei"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Zongqing Lu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08403v1",
                "updated": "2025-07-11T08:21:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    21,
                    8,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:21:08Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    21,
                    8,
                    4,
                    192,
                    0
                ],
                "title": "Towards AI-Native RAN: An Operator's Perspective of 6G Day 1\n  Standardization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards AI-Native RAN: An Operator's Perspective of 6G Day 1\n  Standardization"
                },
                "summary": "Artificial Intelligence/Machine Learning (AI/ML) has become the most certain\nand prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not\nnatively integrated but rather an add-on feature over existing architecture, 6G\nshall incorporate AI from the onset to address its complexity and support\nubiquitous AI applications. Based on our extensive mobile network operation and\nstandardization experience from 2G to 5G, this paper explores the design and\nstandardization principles of AI-Native radio access networks (RAN) for 6G,\nwith a particular focus on its critical Day 1 architecture, functionalities and\ncapabilities. We investigate the framework of AI-Native RAN and present its\nthree essential capabilities to shed some light on the standardization\ndirection; namely, AI-driven RAN processing/optimization/automation, reliable\nAI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The\nstandardization of AI-Native RAN, in particular the Day 1 features, including\nan AI-Native 6G RAN architecture, were proposed. For validation, a large-scale\nfield trial with over 5000 5G-A base stations have been built and delivered\nsignificant improvements in average air interface latency, root cause\nidentification, and network energy consumption with the proposed architecture\nand the supporting AI functions. This paper aims to provide a Day 1 framework\nfor 6G AI-Native RAN standardization design, balancing technical innovation\nwith practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence/Machine Learning (AI/ML) has become the most certain\nand prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not\nnatively integrated but rather an add-on feature over existing architecture, 6G\nshall incorporate AI from the onset to address its complexity and support\nubiquitous AI applications. Based on our extensive mobile network operation and\nstandardization experience from 2G to 5G, this paper explores the design and\nstandardization principles of AI-Native radio access networks (RAN) for 6G,\nwith a particular focus on its critical Day 1 architecture, functionalities and\ncapabilities. We investigate the framework of AI-Native RAN and present its\nthree essential capabilities to shed some light on the standardization\ndirection; namely, AI-driven RAN processing/optimization/automation, reliable\nAI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The\nstandardization of AI-Native RAN, in particular the Day 1 features, including\nan AI-Native 6G RAN architecture, were proposed. For validation, a large-scale\nfield trial with over 5000 5G-A base stations have been built and delivered\nsignificant improvements in average air interface latency, root cause\nidentification, and network energy consumption with the proposed architecture\nand the supporting AI functions. This paper aims to provide a Day 1 framework\nfor 6G AI-Native RAN standardization design, balancing technical innovation\nwith practical deployment."
                },
                "authors": [
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Lehan Wang"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Jinri Huang"
                    },
                    {
                        "name": "Chunhui Liu"
                    },
                    {
                        "name": "Jing Gao"
                    },
                    {
                        "name": "Yuhong Huang"
                    },
                    {
                        "name": "Chih-Lin I"
                    }
                ],
                "author_detail": {
                    "name": "Chih-Lin I"
                },
                "author": "Chih-Lin I",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08402v1",
                "updated": "2025-07-11T08:20:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    20,
                    19,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:20:19Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    20,
                    19,
                    4,
                    192,
                    0
                ],
                "title": "SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent\n  Intracortical Motor Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent\n  Intracortical Motor Decoding"
                },
                "summary": "Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from\nneural population activity, enabling individuals with motor impairments to\nregain motor functions and communication abilities. A key challenge in\nlong-term iBCI is the nonstationarity of neural recordings, where the\ncomposition and tuning profiles of the recorded populations are unstable across\nrecording sessions. Existing methods attempt to address this issue by explicit\nalignment techniques; however, they rely on fixed neural identities and require\ntest-time labels or parameter updates, limiting their generalization across\nsessions and imposing additional computational burden during deployment. In\nthis work, we introduce SPINT - a Spatial Permutation-Invariant Neural\nTransformer framework for behavioral decoding that operates directly on\nunordered sets of neural units. Central to our approach is a novel\ncontext-dependent positional embedding scheme that dynamically infers\nunit-specific identities, enabling flexible generalization across recording\nsessions. SPINT supports inference on variable-size populations and allows\nfew-shot, gradient-free adaptation using a small amount of unlabeled data from\nthe test session. To further promote model robustness to population\nvariability, we introduce dynamic channel dropout, a regularization method for\niBCI that simulates shifts in population composition during training. We\nevaluate SPINT on three multi-session datasets from the FALCON Benchmark,\ncovering continuous motor decoding tasks in human and non-human primates. SPINT\ndemonstrates robust cross-session generalization, outperforming existing\nzero-shot and few-shot unsupervised baselines while eliminating the need for\ntest-time alignment and fine-tuning. Our work contributes an initial step\ntoward a robust and scalable neural decoding framework for long-term iBCI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from\nneural population activity, enabling individuals with motor impairments to\nregain motor functions and communication abilities. A key challenge in\nlong-term iBCI is the nonstationarity of neural recordings, where the\ncomposition and tuning profiles of the recorded populations are unstable across\nrecording sessions. Existing methods attempt to address this issue by explicit\nalignment techniques; however, they rely on fixed neural identities and require\ntest-time labels or parameter updates, limiting their generalization across\nsessions and imposing additional computational burden during deployment. In\nthis work, we introduce SPINT - a Spatial Permutation-Invariant Neural\nTransformer framework for behavioral decoding that operates directly on\nunordered sets of neural units. Central to our approach is a novel\ncontext-dependent positional embedding scheme that dynamically infers\nunit-specific identities, enabling flexible generalization across recording\nsessions. SPINT supports inference on variable-size populations and allows\nfew-shot, gradient-free adaptation using a small amount of unlabeled data from\nthe test session. To further promote model robustness to population\nvariability, we introduce dynamic channel dropout, a regularization method for\niBCI that simulates shifts in population composition during training. We\nevaluate SPINT on three multi-session datasets from the FALCON Benchmark,\ncovering continuous motor decoding tasks in human and non-human primates. SPINT\ndemonstrates robust cross-session generalization, outperforming existing\nzero-shot and few-shot unsupervised baselines while eliminating the need for\ntest-time alignment and fine-tuning. Our work contributes an initial step\ntoward a robust and scalable neural decoding framework for long-term iBCI\napplications."
                },
                "authors": [
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Jingyuan Li"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Lu Mi"
                    },
                    {
                        "name": "Amy Orsborn"
                    },
                    {
                        "name": "Uygar Sümbül"
                    },
                    {
                        "name": "Eli Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shlizerman"
                },
                "author": "Eli Shlizerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08392v1",
                "updated": "2025-07-11T08:04:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    4,
                    32,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T08:04:32Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    4,
                    32,
                    4,
                    192,
                    0
                ],
                "title": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems"
                },
                "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05788v2",
                "updated": "2025-07-11T08:00:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    8,
                    0,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-08T08:50:47Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    50,
                    47,
                    1,
                    189,
                    0
                ],
                "title": "Flippi: End To End GenAI Assistant for E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flippi: End To End GenAI Assistant for E-Commerce"
                },
                "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace."
                },
                "authors": [
                    {
                        "name": "Anand A. Rajasekar"
                    },
                    {
                        "name": "Praveen Tangarajan"
                    },
                    {
                        "name": "Anjali Nainani"
                    },
                    {
                        "name": "Amogh Batwal"
                    },
                    {
                        "name": "Vinay Rao Dandin"
                    },
                    {
                        "name": "Anusua Trivedi"
                    },
                    {
                        "name": "Ozan Ersoy"
                    }
                ],
                "author_detail": {
                    "name": "Ozan Ersoy"
                },
                "author": "Ozan Ersoy",
                "arxiv_comment": "10 pages, 2 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08382v1",
                "updated": "2025-07-11T07:54:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    54,
                    16,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T07:54:16Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    54,
                    16,
                    4,
                    192,
                    0
                ],
                "title": "Two-cluster test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-cluster test"
                },
                "summary": "Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering."
                },
                "authors": [
                    {
                        "name": "Xinying Liu"
                    },
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Mudi Jiang"
                    },
                    {
                        "name": "Simen Zhang"
                    },
                    {
                        "name": "Jun Lou"
                    },
                    {
                        "name": "Zengyou He"
                    }
                ],
                "author_detail": {
                    "name": "Zengyou He"
                },
                "author": "Zengyou He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08379v1",
                "updated": "2025-07-11T07:47:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    47,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T07:47:47Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    47,
                    47,
                    4,
                    192,
                    0
                ],
                "title": "Advances in Machine Learning: Where Can Quantum Techniques Help?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Machine Learning: Where Can Quantum Techniques Help?"
                },
                "summary": "Quantum Machine Learning (QML) represents a promising frontier at the\nintersection of quantum computing and artificial intelligence, aiming to\nleverage quantum computational advantages to enhance data-driven tasks. This\nreview explores the potential of QML to address the computational bottlenecks\nof classical machine learning, particularly in processing complex datasets. We\nintroduce the theoretical foundations of QML, including quantum data encoding,\nquantum learning theory and optimization techniques, while categorizing QML\napproaches based on data type and computational architecture. It is\nwell-established that quantum computational advantages are problem-dependent,\nand so potentially useful directions for QML need to be systematically\nidentified. Key developments, such as Quantum Principal Component Analysis,\nquantum-enhanced sensing and applications in material science, are critically\nevaluated for their theoretical speed-ups and practical limitations. The\nchallenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including\nhardware noise, scalability constraints and data encoding overheads, are\ndiscussed in detail. We also outline future directions, emphasizing the need\nfor quantum-native algorithms, improved error correction, and realistic\nbenchmarks to bridge the gap between theoretical promise and practical\ndeployment. This comprehensive analysis underscores that while QML has\nsignificant potential for specific applications such as quantum chemistry and\nsensing, its broader utility in real-world scenarios remains contingent on\novercoming technological and methodological hurdles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Machine Learning (QML) represents a promising frontier at the\nintersection of quantum computing and artificial intelligence, aiming to\nleverage quantum computational advantages to enhance data-driven tasks. This\nreview explores the potential of QML to address the computational bottlenecks\nof classical machine learning, particularly in processing complex datasets. We\nintroduce the theoretical foundations of QML, including quantum data encoding,\nquantum learning theory and optimization techniques, while categorizing QML\napproaches based on data type and computational architecture. It is\nwell-established that quantum computational advantages are problem-dependent,\nand so potentially useful directions for QML need to be systematically\nidentified. Key developments, such as Quantum Principal Component Analysis,\nquantum-enhanced sensing and applications in material science, are critically\nevaluated for their theoretical speed-ups and practical limitations. The\nchallenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including\nhardware noise, scalability constraints and data encoding overheads, are\ndiscussed in detail. We also outline future directions, emphasizing the need\nfor quantum-native algorithms, improved error correction, and realistic\nbenchmarks to bridge the gap between theoretical promise and practical\ndeployment. This comprehensive analysis underscores that while QML has\nsignificant potential for specific applications such as quantum chemistry and\nsensing, its broader utility in real-world scenarios remains contingent on\novercoming technological and methodological hurdles."
                },
                "authors": [
                    {
                        "name": "Samarth Kashyap"
                    },
                    {
                        "name": "Rohit K Ramakrishnan"
                    },
                    {
                        "name": "Kumari Jyoti"
                    },
                    {
                        "name": "Apoorva D Patel"
                    }
                ],
                "author_detail": {
                    "name": "Apoorva D Patel"
                },
                "author": "Apoorva D Patel",
                "arxiv_comment": "28 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03897v3",
                "updated": "2025-07-11T07:40:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    40,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-06-06T09:36:14Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    9,
                    36,
                    14,
                    3,
                    158,
                    0
                ],
                "title": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew"
                },
                "summary": "While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general."
                },
                "authors": [
                    {
                        "name": "Tzuf Paz-Argaman"
                    },
                    {
                        "name": "Itai Mondshine"
                    },
                    {
                        "name": "Asaf Achi Mordechai"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    }
                ],
                "author_detail": {
                    "name": "Reut Tsarfaty"
                },
                "author": "Reut Tsarfaty",
                "arxiv_journal_ref": "ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18865v3",
                "updated": "2025-07-11T07:37:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    37,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2024-04-29T16:52:57Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    16,
                    52,
                    57,
                    0,
                    120,
                    0
                ],
                "title": "Truth-value judgment in language models: 'truth directions' are context\n  sensitive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth-value judgment in language models: 'truth directions' are context\n  sensitive"
                },
                "summary": "Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as\nuncovering a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon,\nlooking closely at the impact of context on the probes. Our experiments\nestablish where in the LLM the probe's predictions are (most) sensitive to the\npresence of related sentences, and how to best characterize this kind of\nsensitivity. We do so by measuring different types of consistency errors that\noccur after probing an LLM whose inputs consist of hypotheses preceded by\n(negated) supporting and contradicting sentences. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these truth-value directions influences the position of an\nentailed or contradicted sentence along that same direction. We find that the\nprobes we test are generally context sensitive, but that contexts which should\nnot affect the truth often still impact the probe outputs. Our experiments show\nthat the type of errors depend on the layer, the model, and the kind of data.\nFinally, our results suggest that truth-value directions are causal mediators\nin the inference process that incorporates in-context information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as\nuncovering a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon,\nlooking closely at the impact of context on the probes. Our experiments\nestablish where in the LLM the probe's predictions are (most) sensitive to the\npresence of related sentences, and how to best characterize this kind of\nsensitivity. We do so by measuring different types of consistency errors that\noccur after probing an LLM whose inputs consist of hypotheses preceded by\n(negated) supporting and contradicting sentences. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these truth-value directions influences the position of an\nentailed or contradicted sentence along that same direction. We find that the\nprobes we test are generally context sensitive, but that contexts which should\nnot affect the truth often still impact the probe outputs. Our experiments show\nthat the type of errors depend on the layer, the model, and the kind of data.\nFinally, our results suggest that truth-value directions are causal mediators\nin the inference process that incorporates in-context information."
                },
                "authors": [
                    {
                        "name": "Stefan F. Schouten"
                    },
                    {
                        "name": "Peter Bloem"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Piek Vossen"
                    }
                ],
                "author_detail": {
                    "name": "Piek Vossen"
                },
                "author": "Piek Vossen",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08367v1",
                "updated": "2025-07-11T07:28:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    28,
                    49,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T07:28:49Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    7,
                    28,
                    49,
                    4,
                    192,
                    0
                ],
                "title": "Understanding Driving Risks using Large Language Models: Toward Elderly\n  Driver Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Driving Risks using Large Language Models: Toward Elderly\n  Driver Assessment"
                },
                "summary": "This study investigates the potential of a multimodal large language model\n(LLM), specifically ChatGPT-4o, to perform human-like interpretations of\ntraffic scenes using static dashcam images. Herein, we focus on three judgment\ntasks relevant to elderly driver assessments: evaluating traffic density,\nassessing intersection visibility, and recognizing stop signs recognition.\nThese tasks require contextual reasoning rather than simple object detection.\nUsing zero-shot, few-shot, and multi-shot prompting strategies, we evaluated\nthe performance of the model with human annotations serving as the reference\nstandard. Evaluation metrics included precision, recall, and F1-score. Results\nindicate that prompt design considerably affects performance, with recall for\nintersection visibility increasing from 21.7% (zero-shot) to 57.0%\n(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In\nstop-sign detection, the model demonstrated high precision (up to 86.3%) but a\nlower recall (approximately 76.7%), indicating a conservative response\ntendency. Output stability analysis revealed that humans and the model faced\ndifficulties interpreting structurally ambiguous scenes. However, the model's\nexplanatory texts corresponded with its predictions, enhancing\ninterpretability. These findings suggest that, with well-designed prompts, LLMs\nhold promise as supportive tools for scene-level driving risk assessments.\nFuture studies should explore scalability using larger datasets, diverse\nannotators, and next-generation model architectures for elderly driver\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential of a multimodal large language model\n(LLM), specifically ChatGPT-4o, to perform human-like interpretations of\ntraffic scenes using static dashcam images. Herein, we focus on three judgment\ntasks relevant to elderly driver assessments: evaluating traffic density,\nassessing intersection visibility, and recognizing stop signs recognition.\nThese tasks require contextual reasoning rather than simple object detection.\nUsing zero-shot, few-shot, and multi-shot prompting strategies, we evaluated\nthe performance of the model with human annotations serving as the reference\nstandard. Evaluation metrics included precision, recall, and F1-score. Results\nindicate that prompt design considerably affects performance, with recall for\nintersection visibility increasing from 21.7% (zero-shot) to 57.0%\n(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In\nstop-sign detection, the model demonstrated high precision (up to 86.3%) but a\nlower recall (approximately 76.7%), indicating a conservative response\ntendency. Output stability analysis revealed that humans and the model faced\ndifficulties interpreting structurally ambiguous scenes. However, the model's\nexplanatory texts corresponded with its predictions, enhancing\ninterpretability. These findings suggest that, with well-designed prompts, LLMs\nhold promise as supportive tools for scene-level driving risk assessments.\nFuture studies should explore scalability using larger datasets, diverse\nannotators, and next-generation model architectures for elderly driver\nassessments."
                },
                "authors": [
                    {
                        "name": "Yuki Yoshihara"
                    },
                    {
                        "name": "Linjing Jiang"
                    },
                    {
                        "name": "Nihan Karatas"
                    },
                    {
                        "name": "Hitoshi Kanamori"
                    },
                    {
                        "name": "Asuka Harada"
                    },
                    {
                        "name": "Takahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Tanaka"
                },
                "author": "Takahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08350v1",
                "updated": "2025-07-11T06:53:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    53,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T06:53:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    53,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation"
                },
                "summary": "Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator."
                },
                "authors": [
                    {
                        "name": "Keisuke Ueda"
                    },
                    {
                        "name": "Wataru Hirota"
                    },
                    {
                        "name": "Takuto Asakura"
                    },
                    {
                        "name": "Takahiro Omi"
                    },
                    {
                        "name": "Kosuke Takahashi"
                    },
                    {
                        "name": "Kosuke Arima"
                    },
                    {
                        "name": "Tatsuya Ishigaki"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Ishigaki"
                },
                "author": "Tatsuya Ishigaki",
                "arxiv_comment": "16 pages, 1 figure, appendix. Accepted to SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08339v1",
                "updated": "2025-07-11T06:37:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    37,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T06:37:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    37,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "What Factors Affect LLMs and RLLMs in Financial Question Answering?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Factors Affect LLMs and RLLMs in Financial Question Answering?"
                },
                "summary": "Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering."
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xuesi Hu"
                    },
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Yuntao Zou"
                    },
                    {
                        "name": "Qiancheng Zhang"
                    },
                    {
                        "name": "Dagang Li"
                    }
                ],
                "author_detail": {
                    "name": "Dagang Li"
                },
                "author": "Dagang Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16521v2",
                "updated": "2025-07-11T06:31:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    31,
                    58,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-17T02:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    2,
                    16,
                    41,
                    0,
                    76,
                    0
                ],
                "title": "Conversational Self-Play for Discovering and Understanding Psychotherapy\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Self-Play for Discovering and Understanding Psychotherapy\n  Approaches"
                },
                "summary": "This paper explores conversational self-play with LLMs as a scalable approach\nfor analyzing and exploring psychotherapy approaches, evaluating how well\nAI-generated therapeutic dialogues align with established modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores conversational self-play with LLMs as a scalable approach\nfor analyzing and exploring psychotherapy approaches, evaluating how well\nAI-generated therapeutic dialogues align with established modalities."
                },
                "authors": [
                    {
                        "name": "Onno P Kampman"
                    },
                    {
                        "name": "Michael Xing"
                    },
                    {
                        "name": "Charmaine Lim"
                    },
                    {
                        "name": "Ahmad Ishqi Jabir"
                    },
                    {
                        "name": "Ryan Louie"
                    },
                    {
                        "name": "Jimmy Lee"
                    },
                    {
                        "name": "Robert JT Morris"
                    }
                ],
                "author_detail": {
                    "name": "Robert JT Morris"
                },
                "author": "Robert JT Morris",
                "arxiv_comment": "Improve writing, add multiple techniques extraction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02356v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02356v3",
                "updated": "2025-07-11T06:20:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    6,
                    20,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-04T07:27:41Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    7,
                    27,
                    41,
                    1,
                    63,
                    0
                ],
                "title": "Efficient Long Context Fine-tuning with Chunk Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long Context Fine-tuning with Chunk Flow"
                },
                "summary": "Long context fine-tuning of large language models(LLMs) involves training on\ndatasets that are predominantly composed of short sequences and a small\nproportion of longer sequences. However, existing approaches overlook this\nlong-tail distribution and employ training strategies designed specifically for\nlong sequences. Moreover, these approaches also fail to address the challenges\nposed by variable sequence lengths during distributed training, such as load\nimbalance in data parallelism and severe pipeline bubbles in pipeline\nparallelism. These issues lead to suboptimal training performance and poor GPU\nresource utilization. To tackle these problems, we propose a chunk-centric\ntraining method named ChunkFlow. ChunkFlow reorganizes input sequences into\nuniformly sized chunks by consolidating short sequences and splitting longer\nones. This approach achieves optimal computational efficiency and balance among\ntraining inputs. Additionally, ChunkFlow incorporates a state-aware chunk\nscheduling mechanism to ensure that the peak memory usage during training is\nprimarily determined by the chunk size rather than the maximum sequence length\nin the dataset. Integrating this scheduling mechanism with existing pipeline\nscheduling algorithms further enhances the performance of distributed training.\nExperimental results demonstrate that, compared with Megatron-LM, ChunkFlow can\nbe up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we\nbelieve that ChunkFlow serves as an effective solution for a broader range of\nscenarios, such as long context continual pre-training, where datasets contain\nvariable-length sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context fine-tuning of large language models(LLMs) involves training on\ndatasets that are predominantly composed of short sequences and a small\nproportion of longer sequences. However, existing approaches overlook this\nlong-tail distribution and employ training strategies designed specifically for\nlong sequences. Moreover, these approaches also fail to address the challenges\nposed by variable sequence lengths during distributed training, such as load\nimbalance in data parallelism and severe pipeline bubbles in pipeline\nparallelism. These issues lead to suboptimal training performance and poor GPU\nresource utilization. To tackle these problems, we propose a chunk-centric\ntraining method named ChunkFlow. ChunkFlow reorganizes input sequences into\nuniformly sized chunks by consolidating short sequences and splitting longer\nones. This approach achieves optimal computational efficiency and balance among\ntraining inputs. Additionally, ChunkFlow incorporates a state-aware chunk\nscheduling mechanism to ensure that the peak memory usage during training is\nprimarily determined by the chunk size rather than the maximum sequence length\nin the dataset. Integrating this scheduling mechanism with existing pipeline\nscheduling algorithms further enhances the performance of distributed training.\nExperimental results demonstrate that, compared with Megatron-LM, ChunkFlow can\nbe up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we\nbelieve that ChunkFlow serves as an effective solution for a broader range of\nscenarios, such as long context continual pre-training, where datasets contain\nvariable-length sequences."
                },
                "authors": [
                    {
                        "name": "Xiulong Yuan"
                    },
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Wenting Shen"
                    },
                    {
                        "name": "Ang Wang"
                    },
                    {
                        "name": "Xiafei Qiu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02356v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02356v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08330v1",
                "updated": "2025-07-11T05:58:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    58,
                    22,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T05:58:22Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    58,
                    22,
                    4,
                    192,
                    0
                ],
                "title": "Interpretability-Aware Pruning for Efficient Medical Image Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability-Aware Pruning for Efficient Medical Image Analysis"
                },
                "summary": "Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Nikita Malik"
                    },
                    {
                        "name": "Pratinav Seth"
                    },
                    {
                        "name": "Neeraj Kumar Singh"
                    },
                    {
                        "name": "Chintan Chitroda"
                    },
                    {
                        "name": "Vinay Kumar Sankarapu"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kumar Sankarapu"
                },
                "author": "Vinay Kumar Sankarapu",
                "arxiv_comment": "Pre-Print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01077v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01077v4",
                "updated": "2025-07-11T05:39:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    39,
                    8,
                    4,
                    192,
                    0
                ],
                "published": "2024-11-01T23:18:32Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    18,
                    32,
                    4,
                    306,
                    0
                ],
                "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection"
                },
                "summary": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "N. Benjamin Erichson"
                    }
                ],
                "author_detail": {
                    "name": "N. Benjamin Erichson"
                },
                "author": "N. Benjamin Erichson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01077v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01077v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08325v1",
                "updated": "2025-07-11T05:31:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    31,
                    35,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T05:31:35Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    31,
                    35,
                    4,
                    192,
                    0
                ],
                "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation"
                },
                "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics."
                },
                "authors": [
                    {
                        "name": "Yinzhu Quan"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02899v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02899v3",
                "updated": "2025-07-11T05:01:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    1,
                    20,
                    4,
                    192,
                    0
                ],
                "published": "2025-06-23T04:29:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    4,
                    29,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Learning to Generate Vectorized Maps at Intersections with Multiple\n  Roadside Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Vectorized Maps at Intersections with Multiple\n  Roadside Cameras"
                },
                "summary": "Vectorized maps are indispensable for precise navigation and the safe\noperation of autonomous vehicles. Traditional methods for constructing these\nmaps fall into two categories: offline techniques, which rely on expensive,\nlabor-intensive LiDAR data collection and manual annotation, and online\napproaches that use onboard cameras to reduce costs but suffer from limited\nperformance, especially at complex intersections. To bridge this gap, we\nintroduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network\ndesigned to generate high-definition vectorized maps directly at intersections.\nLeveraging existing roadside surveillance cameras, MRC-VMap directly converts\ntime-aligned, multi-directional images into vectorized map representations.\nThis integrated solution lowers the need for additional intermediate\nmodules--such as separate feature extraction and Bird's-Eye View (BEV)\nconversion steps--thus reducing both computational overhead and error\npropagation. Moreover, the use of multiple camera views enhances mapping\ncompleteness, mitigates occlusions, and provides robust performance under\npractical deployment constraints. Extensive experiments conducted on 4,000\nintersections across 4 major metropolitan areas in China demonstrate that\nMRC-VMap not only outperforms state-of-the-art online methods but also achieves\naccuracy comparable to high-cost LiDAR-based approaches, thereby offering a\nscalable and efficient solution for modern autonomous navigation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vectorized maps are indispensable for precise navigation and the safe\noperation of autonomous vehicles. Traditional methods for constructing these\nmaps fall into two categories: offline techniques, which rely on expensive,\nlabor-intensive LiDAR data collection and manual annotation, and online\napproaches that use onboard cameras to reduce costs but suffer from limited\nperformance, especially at complex intersections. To bridge this gap, we\nintroduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network\ndesigned to generate high-definition vectorized maps directly at intersections.\nLeveraging existing roadside surveillance cameras, MRC-VMap directly converts\ntime-aligned, multi-directional images into vectorized map representations.\nThis integrated solution lowers the need for additional intermediate\nmodules--such as separate feature extraction and Bird's-Eye View (BEV)\nconversion steps--thus reducing both computational overhead and error\npropagation. Moreover, the use of multiple camera views enhances mapping\ncompleteness, mitigates occlusions, and provides robust performance under\npractical deployment constraints. Extensive experiments conducted on 4,000\nintersections across 4 major metropolitan areas in China demonstrate that\nMRC-VMap not only outperforms state-of-the-art online methods but also achieves\naccuracy comparable to high-cost LiDAR-based approaches, thereby offering a\nscalable and efficient solution for modern autonomous navigation systems."
                },
                "authors": [
                    {
                        "name": "Quanxin Zheng"
                    },
                    {
                        "name": "Miao Fan"
                    },
                    {
                        "name": "Shengtong Xu"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong",
                "arxiv_comment": "Accepted by IROS'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02899v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02899v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07787v2",
                "updated": "2025-07-11T04:57:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    4,
                    57,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T14:09:53Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    14,
                    9,
                    53,
                    3,
                    191,
                    0
                ],
                "title": "Measuring AI Alignment with Human Flourishing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring AI Alignment with Human Flourishing"
                },
                "summary": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation."
                },
                "authors": [
                    {
                        "name": "Elizabeth Hilliard"
                    },
                    {
                        "name": "Akshaya Jagadeesh"
                    },
                    {
                        "name": "Alex Cook"
                    },
                    {
                        "name": "Steele Billings"
                    },
                    {
                        "name": "Nicholas Skytland"
                    },
                    {
                        "name": "Alicia Llewellyn"
                    },
                    {
                        "name": "Jackson Paull"
                    },
                    {
                        "name": "Nathan Paull"
                    },
                    {
                        "name": "Nolan Kurylo"
                    },
                    {
                        "name": "Keatra Nesbitt"
                    },
                    {
                        "name": "Robert Gruenewald"
                    },
                    {
                        "name": "Anthony Jantzi"
                    },
                    {
                        "name": "Omar Chavez"
                    }
                ],
                "author_detail": {
                    "name": "Omar Chavez"
                },
                "author": "Omar Chavez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14023v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14023v5",
                "updated": "2025-07-11T04:26:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    4,
                    26,
                    11,
                    4,
                    192,
                    0
                ],
                "published": "2024-06-20T06:42:08Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    42,
                    8,
                    3,
                    172,
                    0
                ],
                "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective"
                },
                "summary": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/."
                },
                "authors": [
                    {
                        "name": "Yuchen Wen"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14023v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14023v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01163v2",
                "updated": "2025-07-11T03:56:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    56,
                    5,
                    4,
                    192,
                    0
                ],
                "published": "2025-03-03T04:24:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    4,
                    24,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers"
                },
                "summary": "Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS ."
                },
                "authors": [
                    {
                        "name": "Rin Ashizawa"
                    },
                    {
                        "name": "Yoichi Hirose"
                    },
                    {
                        "name": "Nozomu Yoshinari"
                    },
                    {
                        "name": "Kento Uchida"
                    },
                    {
                        "name": "Shinichi Shirakawa"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Shirakawa"
                },
                "author": "Shinichi Shirakawa",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09918v3",
                "updated": "2025-07-11T03:52:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    52,
                    42,
                    4,
                    192,
                    0
                ],
                "published": "2024-10-13T16:53:02Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    16,
                    53,
                    2,
                    6,
                    287,
                    0
                ],
                "title": "Dualformer: Controllable Fast and Slow Thinking by Learning with\n  Randomized Reasoning Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dualformer: Controllable Fast and Slow Thinking by Learning with\n  Randomized Reasoning Traces"
                },
                "summary": "In cognition theory, human thinking is governed by two systems: the fast and\nintuitive System 1 and the slower but more deliberative System 2. Analogously,\nLarge Language Models (LLMs) can operate in two reasoning modes: outputting\nonly the solutions (\\emph{fast mode}) or both the reasoning chain and the final\nsolution (\\emph{slow mode}). We present \\dualformer, a single Transformer model\nthat seamlessly integrates both the fast and slow reasoning modes by training\non randomized reasoning traces, where different parts of the traces are\nstrategically dropped during training. At inference time, \\dualformer can be\neasily configured to execute in either fast or slow mode, or automatically\ndecide which mode to engage (\\emph{auto mode}). It outperforms baselines in\nboth performance and computational efficiency across all three modes: (1) in\nslow mode, \\dualformer achieves $97.6\\%$ optimal rate on unseen $30 \\times 30$\nmaze tasks, surpassing the \\searchformer baseline ($93.3\\%$) trained on data\nwith complete reasoning traces, with $45.5\\%$ fewer reasoning steps; (2) in\nfast mode, \\dualformer achieves $80\\%$ optimal rate, significantly\noutperforming the Solution-Only model trained on solution-only data, which has\nan optimal rate of only $30\\%$; (3) in auto mode, \\dualformer achieves $96.6\\%$\noptimal rate with $59.9\\%$ fewer steps than \\searchformer. Moreover,\n\\dualformer produces more diverse reasoning traces than \\searchformer{}. For\nmath reasoning problems, our techniques have also achieved improved performance\nwith LLM fine-tuning, demonstrating its generalization beyond task-specific\nmodels. We open source our code at\nhttps://github.com/facebookresearch/dualformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cognition theory, human thinking is governed by two systems: the fast and\nintuitive System 1 and the slower but more deliberative System 2. Analogously,\nLarge Language Models (LLMs) can operate in two reasoning modes: outputting\nonly the solutions (\\emph{fast mode}) or both the reasoning chain and the final\nsolution (\\emph{slow mode}). We present \\dualformer, a single Transformer model\nthat seamlessly integrates both the fast and slow reasoning modes by training\non randomized reasoning traces, where different parts of the traces are\nstrategically dropped during training. At inference time, \\dualformer can be\neasily configured to execute in either fast or slow mode, or automatically\ndecide which mode to engage (\\emph{auto mode}). It outperforms baselines in\nboth performance and computational efficiency across all three modes: (1) in\nslow mode, \\dualformer achieves $97.6\\%$ optimal rate on unseen $30 \\times 30$\nmaze tasks, surpassing the \\searchformer baseline ($93.3\\%$) trained on data\nwith complete reasoning traces, with $45.5\\%$ fewer reasoning steps; (2) in\nfast mode, \\dualformer achieves $80\\%$ optimal rate, significantly\noutperforming the Solution-Only model trained on solution-only data, which has\nan optimal rate of only $30\\%$; (3) in auto mode, \\dualformer achieves $96.6\\%$\noptimal rate with $59.9\\%$ fewer steps than \\searchformer. Moreover,\n\\dualformer produces more diverse reasoning traces than \\searchformer{}. For\nmath reasoning problems, our techniques have also achieved improved performance\nwith LLM fine-tuning, demonstrating its generalization beyond task-specific\nmodels. We open source our code at\nhttps://github.com/facebookresearch/dualformer."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Michael Rabbat"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Qinqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Qinqing Zheng"
                },
                "author": "Qinqing Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06515v2",
                "updated": "2025-07-11T03:50:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    50,
                    12,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T03:30:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    30,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "QUEST: Query Optimization in Unstructured Document Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUEST: Query Optimization in Unstructured Document Analysis"
                },
                "summary": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Zhaoze Sun"
                    },
                    {
                        "name": "Qiyan Deng"
                    },
                    {
                        "name": "Chengliang Chai"
                    },
                    {
                        "name": "Kaisen Jin"
                    },
                    {
                        "name": "Xinyu Guo"
                    },
                    {
                        "name": "Han Han"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Guoren Wang"
                    },
                    {
                        "name": "Lei Cao"
                    }
                ],
                "author_detail": {
                    "name": "Lei Cao"
                },
                "author": "Lei Cao",
                "arxiv_comment": "Accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07104v2",
                "updated": "2025-07-11T03:43:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    43,
                    50,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-09T17:59:04Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    4,
                    2,
                    190,
                    0
                ],
                "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models"
                },
                "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD."
                },
                "authors": [
                    {
                        "name": "Tiezheng Zhang"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Yu-cheng Chou"
                    },
                    {
                        "name": "Jieneng Chen"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Junfei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Junfei Xiao"
                },
                "author": "Junfei Xiao",
                "arxiv_comment": "Project Page: https://lambert-x.github.io/Vision-Language-Vision/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08288v1",
                "updated": "2025-07-11T03:24:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    24,
                    47,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T03:24:47Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    24,
                    47,
                    4,
                    192,
                    0
                ],
                "title": "Invariant-based Robust Weights Watermark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant-based Robust Weights Watermark for Large Language Models"
                },
                "summary": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks)."
                },
                "authors": [
                    {
                        "name": "Qingxiao Guo"
                    },
                    {
                        "name": "Xinjie Zhu"
                    },
                    {
                        "name": "Yilong Ma"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Weifeng Zhang"
                    },
                    {
                        "name": "Xiaobing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobing Guo"
                },
                "author": "Xiaobing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08284v1",
                "updated": "2025-07-11T03:17:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    17,
                    58,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T03:17:58Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    17,
                    58,
                    4,
                    192,
                    0
                ],
                "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided\n  Adversarial Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Safety Guardrails via Synthetic Data and RL-guided\n  Adversarial Training"
                },
                "summary": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems."
                },
                "authors": [
                    {
                        "name": "Aleksei Ilin"
                    },
                    {
                        "name": "Gor Matevosyan"
                    },
                    {
                        "name": "Xueying Ma"
                    },
                    {
                        "name": "Vladimir Eremin"
                    },
                    {
                        "name": "Suhaa Dada"
                    },
                    {
                        "name": "Muqun Li"
                    },
                    {
                        "name": "Riyaaz Shaik"
                    },
                    {
                        "name": "Haluk Noyan Tokgozoglu"
                    }
                ],
                "author_detail": {
                    "name": "Haluk Noyan Tokgozoglu"
                },
                "author": "Haluk Noyan Tokgozoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08283v1",
                "updated": "2025-07-11T03:16:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    16,
                    55,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T03:16:55Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    16,
                    55,
                    4,
                    192,
                    0
                ],
                "title": "TableCopilot: A Table Assistant Empowered by Natural Language\n  Conditional Table Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableCopilot: A Table Assistant Empowered by Natural Language\n  Conditional Table Discovery"
                },
                "summary": "The rise of LLM has enabled natural language-based table assistants, but\nexisting systems assume users already have a well-formed table, neglecting the\nchallenge of table discovery in large-scale table pools. To address this, we\nintroduce TableCopilot, an LLM-powered assistant for interactive, precise, and\npersonalized table discovery and analysis. We define a novel scenario, nlcTD,\nwhere users provide both a natural language condition and a query table,\nenabling intuitive and flexible table discovery for users of all expertise\nlevels. To handle this, we propose Crofuma, a cross-fusion-based approach that\nlearns and aggregates single-modal and cross-modal matching scores.\nExperimental results show Crofuma outperforms SOTA single-input methods by at\nleast 12% on NDCG@5. We also release an instructional video, codebase,\ndatasets, and other resources on GitHub to encourage community contributions.\nTableCopilot sets a new standard for interactive table assistants, making\nadvanced table discovery accessible and integrated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLM has enabled natural language-based table assistants, but\nexisting systems assume users already have a well-formed table, neglecting the\nchallenge of table discovery in large-scale table pools. To address this, we\nintroduce TableCopilot, an LLM-powered assistant for interactive, precise, and\npersonalized table discovery and analysis. We define a novel scenario, nlcTD,\nwhere users provide both a natural language condition and a query table,\nenabling intuitive and flexible table discovery for users of all expertise\nlevels. To handle this, we propose Crofuma, a cross-fusion-based approach that\nlearns and aggregates single-modal and cross-modal matching scores.\nExperimental results show Crofuma outperforms SOTA single-input methods by at\nleast 12% on NDCG@5. We also release an instructional video, codebase,\ndatasets, and other resources on GitHub to encourage community contributions.\nTableCopilot sets a new standard for interactive table assistants, making\nadvanced table discovery accessible and integrated."
                },
                "authors": [
                    {
                        "name": "Lingxi Cui"
                    },
                    {
                        "name": "Guanyu Jiang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    },
                    {
                        "name": "Gang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Gang Chen"
                },
                "author": "Gang Chen",
                "arxiv_comment": "Accepted by VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07901v2",
                "updated": "2025-07-11T03:11:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    3,
                    11,
                    21,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-10T16:33:06Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    33,
                    6,
                    3,
                    191,
                    0
                ],
                "title": "The Trust Fabric: Decentralized Interoperability and Economic\n  Coordination for the Agentic Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trust Fabric: Decentralized Interoperability and Economic\n  Coordination for the Agentic Web"
                },
                "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems."
                },
                "authors": [
                    {
                        "name": "Sree Bhargavi Balija"
                    },
                    {
                        "name": "Rekha Singal"
                    },
                    {
                        "name": "Abhishek Singh"
                    },
                    {
                        "name": "Ramesh Raskar"
                    },
                    {
                        "name": "Erfan Darzi"
                    },
                    {
                        "name": "Raghu Bala"
                    },
                    {
                        "name": "Thomas Hardjono"
                    },
                    {
                        "name": "Ken Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ken Huang"
                },
                "author": "Ken Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01403v2",
                "updated": "2025-07-11T02:49:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    49,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-04-02T06:40:09Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    6,
                    40,
                    9,
                    2,
                    92,
                    0
                ],
                "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval"
                },
                "summary": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Ming Pang"
                    },
                    {
                        "name": "Chunyuan Yuan"
                    },
                    {
                        "name": "Xiaoyu He"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Donghao Xie"
                    },
                    {
                        "name": "Fanyi Qu"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Changping Peng"
                    },
                    {
                        "name": "Zhangang Lin"
                    },
                    {
                        "name": "Ching Law"
                    },
                    {
                        "name": "Jingping Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jingping Shao"
                },
                "author": "Jingping Shao",
                "arxiv_comment": "Accepted by WWW2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08270v1",
                "updated": "2025-07-11T02:34:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    34,
                    16,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:34:16Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    34,
                    16,
                    4,
                    192,
                    0
                ],
                "title": "Agent Safety Alignment via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Safety Alignment via Reinforcement Learning"
                },
                "summary": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents."
                },
                "authors": [
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Hanling Tian"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Shiwen Cui"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Weiqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Wang"
                },
                "author": "Weiqiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08267v1",
                "updated": "2025-07-11T02:26:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    26,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:26:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    26,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning"
                },
                "summary": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1."
                },
                "authors": [
                    {
                        "name": "Hiroshi Yoshihara"
                    },
                    {
                        "name": "Taiki Yamaguchi"
                    },
                    {
                        "name": "Yuichi Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Yuichi Inoue"
                },
                "author": "Yuichi Inoue",
                "arxiv_comment": "Presented at ICML 2025 Workshop on The second AI for MATH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17621v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17621v3",
                "updated": "2025-07-11T02:23:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    23,
                    39,
                    4,
                    192,
                    0
                ],
                "published": "2025-05-23T08:30:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    30,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation\n  Guided Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation\n  Guided Exploration"
                },
                "summary": "Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Rui Zhong"
                    },
                    {
                        "name": "Chi Lu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17621v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17621v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]