[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.05787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v1",
                "updated": "2024-11-08T18:57:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "Recycled Attention: Efficient inference for long-context language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recycled Attention: Efficient inference for long-context language models"
                },
                "summary": "Generating long sequences of tokens given a long-context input imposes a\nheavy computational burden for large language models (LLMs). One of the\ncomputational bottleneck comes from computing attention over a long sequence of\ninput at each generation step. In this paper, we propose Recycled Attention, an\ninference-time method which alternates between full context attention and\nattention over a subset of input tokens. When performing partial attention, we\nrecycle the attention pattern of a previous token that has performed full\nattention and attend only to the top K most attended tokens, reducing the cost\nof data movement and attention computation. Compared to previously proposed\ninference-time acceleration method which attends only to local context or\ntokens with high accumulative attention scores, our approach flexibly chooses\ntokens that are relevant to the current decoding step. We evaluate our methods\non RULER, a suite of tasks designed to comprehensively evaluate long-context\nabilities, and long-context language modeling tasks. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to baselines which only consider\nlocal context while improving the performance by 2x. We further explore two\nideas to improve performance-efficiency trade-offs: (1) dynamically decide when\nto perform recycled or full attention step based on the query similarities and\n(2) continued pre-training the model with Recycled Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input imposes a\nheavy computational burden for large language models (LLMs). One of the\ncomputational bottleneck comes from computing attention over a long sequence of\ninput at each generation step. In this paper, we propose Recycled Attention, an\ninference-time method which alternates between full context attention and\nattention over a subset of input tokens. When performing partial attention, we\nrecycle the attention pattern of a previous token that has performed full\nattention and attend only to the top K most attended tokens, reducing the cost\nof data movement and attention computation. Compared to previously proposed\ninference-time acceleration method which attends only to local context or\ntokens with high accumulative attention scores, our approach flexibly chooses\ntokens that are relevant to the current decoding step. We evaluate our methods\non RULER, a suite of tasks designed to comprehensively evaluate long-context\nabilities, and long-context language modeling tasks. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to baselines which only consider\nlocal context while improving the performance by 2x. We further explore two\nideas to improve performance-efficiency trade-offs: (1) dynamically decide when\nto perform recycled or full attention step based on the query similarities and\n(2) continued pre-training the model with Recycled Attention."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08627v2",
                "updated": "2024-11-08T18:56:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    56,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-04-12T17:41:05Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    17,
                    41,
                    5,
                    4,
                    103,
                    0
                ],
                "title": "Is ChatGPT Transforming Academics' Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT Transforming Academics' Writing Style?"
                },
                "summary": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style."
                },
                "authors": [
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09539v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09539v3",
                "updated": "2024-11-08T18:56:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    56,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-14T16:27:49Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    16,
                    27,
                    49,
                    3,
                    74,
                    0
                ],
                "title": "Logits of API-Protected LLMs Leak Proprietary Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logits of API-Protected LLMs Leak Proprietary Information"
                },
                "summary": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability."
                },
                "authors": [
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09539v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09539v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05781v1",
                "updated": "2024-11-08T18:48:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    48,
                    57,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:48:57Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    48,
                    57,
                    4,
                    313,
                    0
                ],
                "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Language Models to Disambiguate Lexical Choices in Translation"
                },
                "summary": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4."
                },
                "authors": [
                    {
                        "name": "Josh Barua"
                    },
                    {
                        "name": "Sanjay Subramanian"
                    },
                    {
                        "name": "Kayo Yin"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05778v1",
                "updated": "2024-11-08T18:45:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    45,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:45:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    45,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture"
                },
                "summary": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%."
                },
                "authors": [
                    {
                        "name": "Colin Doyle"
                    }
                ],
                "author_detail": {
                    "name": "Colin Doyle"
                },
                "author": "Colin Doyle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05777v1",
                "updated": "2024-11-08T18:43:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:43:15Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding"
                },
                "summary": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results."
                },
                "authors": [
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Ondrej Sotolar"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Sotolar"
                },
                "author": "Ondrej Sotolar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01378v2",
                "updated": "2024-11-08T18:40:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    40,
                    34,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-02T23:03:04Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    23,
                    3,
                    4,
                    5,
                    307,
                    0
                ],
                "title": "New Cold Subdwarf Discoveries from Backyard Worlds and a Metallicity\n  Classification System for T Subdwarfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Cold Subdwarf Discoveries from Backyard Worlds and a Metallicity\n  Classification System for T Subdwarfs"
                },
                "summary": "We report the results of a spectroscopic survey of candidate T subdwarfs\nidentified by the Backyard Worlds: Planet 9 program. Near-infrared spectra of\n31 sources with red $J-W2$ colors and large $J$-band reduced proper motions\nshow varying signatures of subsolar metallicity, including strong\ncollision-induced H$_2$ absorption, obscured methane and water features, and\nweak K I absorption. These metallicity signatures are supported by spectral\nmodel fits and 3D velocities, indicating thick disk and halo population\nmembership for several sources. We identify three new metal-poor T subdwarfs\n([M/H] $\\lesssim$ $-$0.5), CWISE J062316.19+071505.6, WISEA\nJ152443.14$-$262001.8, and CWISE J211250.11-052925.2; and 19 new \"mild\"\nsubdwarfs with modest metal deficiency ([M/H] $\\lesssim$ $-$0.25). We also\nidentify three metal-rich brown dwarfs with thick disk kinematics. We provide\nkinematic evidence that the extreme L subdwarf 2MASS J053253.46+824646.5 and\nthe mild T subdwarf CWISE J113010.07+313944.7 may be part of the Thamnos\npopulation, while the T subdwarf CWISE J155349.96+693355.2 may be part of the\nHelmi stream. We define a metallicity classification system for T dwarfs that\nadds mild subdwarfs (d/sdT), subdwarfs (sdT), and extreme subdwarfs (esdT) to\nthe existing dwarf sequence. We also define a metallicity spectral index that\ncorrelates with metallicities inferred from spectral model fits and iron\nabundances from stellar primaries of benchmark T dwarf companions. This\nexpansion of the T dwarf classification system supports investigations of\nancient, metal-poor brown dwarfs now being uncovered in deep imaging and\nspectroscopic surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the results of a spectroscopic survey of candidate T subdwarfs\nidentified by the Backyard Worlds: Planet 9 program. Near-infrared spectra of\n31 sources with red $J-W2$ colors and large $J$-band reduced proper motions\nshow varying signatures of subsolar metallicity, including strong\ncollision-induced H$_2$ absorption, obscured methane and water features, and\nweak K I absorption. These metallicity signatures are supported by spectral\nmodel fits and 3D velocities, indicating thick disk and halo population\nmembership for several sources. We identify three new metal-poor T subdwarfs\n([M/H] $\\lesssim$ $-$0.5), CWISE J062316.19+071505.6, WISEA\nJ152443.14$-$262001.8, and CWISE J211250.11-052925.2; and 19 new \"mild\"\nsubdwarfs with modest metal deficiency ([M/H] $\\lesssim$ $-$0.25). We also\nidentify three metal-rich brown dwarfs with thick disk kinematics. We provide\nkinematic evidence that the extreme L subdwarf 2MASS J053253.46+824646.5 and\nthe mild T subdwarf CWISE J113010.07+313944.7 may be part of the Thamnos\npopulation, while the T subdwarf CWISE J155349.96+693355.2 may be part of the\nHelmi stream. We define a metallicity classification system for T dwarfs that\nadds mild subdwarfs (d/sdT), subdwarfs (sdT), and extreme subdwarfs (esdT) to\nthe existing dwarf sequence. We also define a metallicity spectral index that\ncorrelates with metallicities inferred from spectral model fits and iron\nabundances from stellar primaries of benchmark T dwarf companions. This\nexpansion of the T dwarf classification system supports investigations of\nancient, metal-poor brown dwarfs now being uncovered in deep imaging and\nspectroscopic surveys."
                },
                "authors": [
                    {
                        "name": "Adam J. Burgasser"
                    },
                    {
                        "name": "Adam C. Schneider"
                    },
                    {
                        "name": "Aaron M. Meisner"
                    },
                    {
                        "name": "Dan Caselden"
                    },
                    {
                        "name": "Chih-Chun Hsu"
                    },
                    {
                        "name": "Roman Gerasimov"
                    },
                    {
                        "name": "Christian Aganze"
                    },
                    {
                        "name": "Emma Softich"
                    },
                    {
                        "name": "Preethi Karpoor"
                    },
                    {
                        "name": "Christopher A. Theissen"
                    },
                    {
                        "name": "Hunter Brooks"
                    },
                    {
                        "name": "Thomas P. Bickle"
                    },
                    {
                        "name": "Jonathan Gagn"
                    },
                    {
                        "name": "tienne Artigau"
                    },
                    {
                        "name": "Michal Marsset"
                    },
                    {
                        "name": "Austin Rothermich"
                    },
                    {
                        "name": "Jacqueline K. Faherty"
                    },
                    {
                        "name": "J. Davy Kirkpatrick"
                    },
                    {
                        "name": "Marc J. Kuchner"
                    },
                    {
                        "name": "Nikolaj Stevnbak Andersen"
                    },
                    {
                        "name": "Paul Beaulieu"
                    },
                    {
                        "name": "Guillaume Colin"
                    },
                    {
                        "name": "Jean Marc Gantier"
                    },
                    {
                        "name": "Leopold Gramaize"
                    },
                    {
                        "name": "Les Hamlet"
                    },
                    {
                        "name": "Ken Hinckley"
                    },
                    {
                        "name": "Martin Kabatnik"
                    },
                    {
                        "name": "Frank Kiwy"
                    },
                    {
                        "name": "David W. Martin"
                    },
                    {
                        "name": "Diego H. Massat"
                    },
                    {
                        "name": "William Pendrill"
                    },
                    {
                        "name": "Arttu Sainio"
                    },
                    {
                        "name": "Jrg Schmann"
                    },
                    {
                        "name": "Melina Thvenot"
                    },
                    {
                        "name": "Jim Walla"
                    },
                    {
                        "name": "Zbigniew Wdracki"
                    },
                    {
                        "name": "the Backyard Worlds"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Planet 9 Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "Planet 9 Collaboration"
                },
                "arxiv_affiliation": "Backyard Worlds",
                "author": "Planet 9 Collaboration",
                "arxiv_comment": "82 pages, 19 figures, accepted to ApJS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05775v1",
                "updated": "2024-11-08T18:36:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    36,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:36:33Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    36,
                    33,
                    4,
                    313,
                    0
                ],
                "title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?"
                },
                "summary": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media."
                },
                "authors": [
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Shaina Raza"
                    }
                ],
                "author_detail": {
                    "name": "Shaina Raza"
                },
                "author": "Shaina Raza",
                "arxiv_comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04264v4",
                "updated": "2024-11-08T18:35:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    35,
                    49,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-17T17:01:45Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    17,
                    1,
                    45,
                    6,
                    77,
                    0
                ],
                "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs"
                },
                "summary": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT."
                },
                "authors": [
                    {
                        "name": "Lihui Liu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Eunice Chan"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05007v2",
                "updated": "2024-11-08T18:32:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    32,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T18:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models"
                },
                "summary": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced."
                },
                "authors": [
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Quantization Library: https://github.com/mit-han-lab/deepcompressor\n  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:\n  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:\n  https://hanlab.mit.edu/blog/svdquant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05764v1",
                "updated": "2024-11-08T18:26:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    26,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    26,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents"
                },
                "summary": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents."
                },
                "authors": [
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yitao Long"
                    },
                    {
                        "name": "Yuru Jiang"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Weiyuan Chen"
                    },
                    {
                        "name": "Hongjun Liu"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05762v1",
                "updated": "2024-11-08T18:25:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    25,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:25:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    25,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024"
                },
                "summary": "Separating disinformation from fact on the web has long challenged both the\nsearch and the reasoning powers of humans. We show that the reasoning power of\nlarge language models (LLMs) and the retrieval power of modern search engines\ncan be combined to automate this process and explainably verify claims. We\nintegrate LLMs and search under a multi-hop evidence pursuit strategy. This\nstrategy generates an initial question based on an input claim using a sequence\nto sequence model, searches and formulates an answer to the question, and\niteratively generates follow-up questions to pursue the evidence that is\nmissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)\nshared task. Compared to a strategy of generating all the questions at once,\nour method obtains .045 higher label accuracy and .155 higher AVeriTeC score\n(evaluating the adequacy of the evidence). Through ablations, we show the\nimportance of various design choices, such as the question generation method,\nmedium-sized context, reasoning with one document at a time, adding metadata,\nparaphrasing, reducing the problem to two classes, and reconsidering the final\nverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and\n.477 AVeriTeC score on the test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating disinformation from fact on the web has long challenged both the\nsearch and the reasoning powers of humans. We show that the reasoning power of\nlarge language models (LLMs) and the retrieval power of modern search engines\ncan be combined to automate this process and explainably verify claims. We\nintegrate LLMs and search under a multi-hop evidence pursuit strategy. This\nstrategy generates an initial question based on an input claim using a sequence\nto sequence model, searches and formulates an answer to the question, and\niteratively generates follow-up questions to pursue the evidence that is\nmissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)\nshared task. Compared to a strategy of generating all the questions at once,\nour method obtains .045 higher label accuracy and .155 higher AVeriTeC score\n(evaluating the adequacy of the evidence). Through ablations, we show the\nimportance of various design choices, such as the question generation method,\nmedium-sized context, reasoning with one document at a time, adding metadata,\nparaphrasing, reducing the problem to two classes, and reconsidering the final\nverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and\n.477 AVeriTeC score on the test set."
                },
                "authors": [
                    {
                        "name": "Christopher Malon"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Malon"
                },
                "author": "Christopher Malon",
                "arxiv_comment": "To appear in the Seventh FEVER Workshop at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05743v1",
                "updated": "2024-11-08T18:04:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    4,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:04:41Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    4,
                    41,
                    4,
                    313,
                    0
                ],
                "title": "Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods"
                },
                "summary": "Membership inference attacks (MIAs) are widely used to empirically assess the\nprivacy risks of samples used to train a target machine learning model.\nState-of-the-art methods however require training hundreds of shadow models,\nwith the same size and architecture of the target model, solely to evaluate the\nprivacy risk. While one might be able to afford this for small models, the cost\noften becomes prohibitive for medium and large models.\n  We here instead propose a novel approach to identify the at-risk samples\nusing only artifacts available during training, with little to no additional\ncomputational overhead. Our method analyzes individual per-sample loss traces\nand uses them to identify the vulnerable data samples. We demonstrate the\neffectiveness of our artifact-based approach through experiments on the CIFAR10\ndataset, showing high precision in identifying vulnerable samples as determined\nby a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the\nsame precision as another SOTA MIA when measured against LiRA, despite it being\norders of magnitude cheaper. We then show LT-IQR to outperform alternative loss\naggregation methods, perform ablation studies on hyperparameters, and validate\nthe robustness of our method to the target metric. Finally, we study the\nevolution of the vulnerability score distribution throughout training as a\nmetric for model-level risk assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) are widely used to empirically assess the\nprivacy risks of samples used to train a target machine learning model.\nState-of-the-art methods however require training hundreds of shadow models,\nwith the same size and architecture of the target model, solely to evaluate the\nprivacy risk. While one might be able to afford this for small models, the cost\noften becomes prohibitive for medium and large models.\n  We here instead propose a novel approach to identify the at-risk samples\nusing only artifacts available during training, with little to no additional\ncomputational overhead. Our method analyzes individual per-sample loss traces\nand uses them to identify the vulnerable data samples. We demonstrate the\neffectiveness of our artifact-based approach through experiments on the CIFAR10\ndataset, showing high precision in identifying vulnerable samples as determined\nby a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the\nsame precision as another SOTA MIA when measured against LiRA, despite it being\norders of magnitude cheaper. We then show LT-IQR to outperform alternative loss\naggregation methods, perform ablation studies on hyperparameters, and validate\nthe robustness of our method to the target metric. Finally, we study the\nevolution of the vulnerability score distribution throughout training as a\nmetric for model-level risk assessment."
                },
                "authors": [
                    {
                        "name": "Joseph Pollock"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Euodia Dodd"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05729v1",
                "updated": "2024-11-08T17:40:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    40,
                    43,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T17:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    40,
                    43,
                    4,
                    313,
                    0
                ],
                "title": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate\n  Data"
                },
                "summary": "Representing and exploiting multivariate signals require capturing complex\nrelations between variables. We define a novel Graph-Dictionary signal model,\nwhere a finite set of graphs characterizes relationships in data distribution\nthrough a weighted sum of their Laplacians. We propose a framework to infer the\ngraph dictionary representation from observed data, along with a bilinear\ngeneralization of the primal-dual splitting algorithm to solve the learning\nproblem. Our new formulation allows to include a priori knowledge on signal\nproperties, as well as on underlying graphs and their coefficients. We show the\ncapability of our method to reconstruct graphs from signals in multiple\nsynthetic settings, where our model outperforms previous baselines. Then, we\nexploit graph-dictionary representations in a motor imagery decoding task on\nbrain activity data, where we classify imagined motion better than standard\nmethods relying on many more features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing and exploiting multivariate signals require capturing complex\nrelations between variables. We define a novel Graph-Dictionary signal model,\nwhere a finite set of graphs characterizes relationships in data distribution\nthrough a weighted sum of their Laplacians. We propose a framework to infer the\ngraph dictionary representation from observed data, along with a bilinear\ngeneralization of the primal-dual splitting algorithm to solve the learning\nproblem. Our new formulation allows to include a priori knowledge on signal\nproperties, as well as on underlying graphs and their coefficients. We show the\ncapability of our method to reconstruct graphs from signals in multiple\nsynthetic settings, where our model outperforms previous baselines. Then, we\nexploit graph-dictionary representations in a motor imagery decoding task on\nbrain activity data, where we classify imagined motion better than standard\nmethods relying on many more features."
                },
                "authors": [
                    {
                        "name": "William Cappelletti"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04242v2",
                "updated": "2024-11-08T17:16:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    16,
                    29,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-06T20:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    20,
                    11,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "Multimodal Structure-Aware Quantum Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Structure-Aware Quantum Data Processing"
                },
                "summary": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured."
                },
                "authors": [
                    {
                        "name": "Hala Hawashin"
                    },
                    {
                        "name": "Mehrnoosh Sadrzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrnoosh Sadrzadeh"
                },
                "author": "Mehrnoosh Sadrzadeh",
                "arxiv_comment": "10 Pages, 16 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10025v2",
                "updated": "2024-11-08T17:09:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    9,
                    35,
                    4,
                    313,
                    0
                ],
                "published": "2024-07-13T23:03:13Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    23,
                    3,
                    13,
                    5,
                    195,
                    0
                ],
                "title": "Badminton Birdie-Like Aerodynamic Alignment of Drifting Dust Grains by\n  Subsonic Gaseous Flows in Protoplanetary Disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Badminton Birdie-Like Aerodynamic Alignment of Drifting Dust Grains by\n  Subsonic Gaseous Flows in Protoplanetary Disks"
                },
                "summary": "Recent (sub)millimeter polarization observations of protoplanetary disks\nreveal toroidally aligned, effectively prolate dust grains large enough (at\nleast ~100 $\\mu$m) to efficiently scatter millimeter light. The alignment\nmechanism for these grains remains unclear. We explore the possibility that gas\ndrag aligns grains through gas-dust relative motion when the grain's center of\nmass is offset from its geometric center, analogous to a badminton birdie's\nalignment in flight. A simple grain model of two non-identical spheres\nillustrates how a grain undergoes damped oscillations from flow-induced\nrestoring torques which align its geometric center in the flow direction\nrelative to its center of mass. Assuming specular reflection and subsonic flow,\nwe derive an analytical equation of motion for spheroids where the center of\nmass can be shifted away from the spheroid's geometric center. We show that a\nprolate or an oblate grain can be aligned with the long axis parallel to the\ngas flow when the center of mass is shifted along that axis. Both scenarios can\nexplain the required effectively prolate grains inferred from observations.\nApplication to a simple disk model shows that the alignment timescales are\nshorter than or comparable to the orbital time. The grain alignment direction\nin a disk depends on the disk (sub-)structure and grain Stokes number (St) with\nazimuthal alignment for large St grains in sub-Keplerian smooth gas disks and\nfor small St grains near the gas pressure extrema, such as rings and gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent (sub)millimeter polarization observations of protoplanetary disks\nreveal toroidally aligned, effectively prolate dust grains large enough (at\nleast ~100 $\\mu$m) to efficiently scatter millimeter light. The alignment\nmechanism for these grains remains unclear. We explore the possibility that gas\ndrag aligns grains through gas-dust relative motion when the grain's center of\nmass is offset from its geometric center, analogous to a badminton birdie's\nalignment in flight. A simple grain model of two non-identical spheres\nillustrates how a grain undergoes damped oscillations from flow-induced\nrestoring torques which align its geometric center in the flow direction\nrelative to its center of mass. Assuming specular reflection and subsonic flow,\nwe derive an analytical equation of motion for spheroids where the center of\nmass can be shifted away from the spheroid's geometric center. We show that a\nprolate or an oblate grain can be aligned with the long axis parallel to the\ngas flow when the center of mass is shifted along that axis. Both scenarios can\nexplain the required effectively prolate grains inferred from observations.\nApplication to a simple disk model shows that the alignment timescales are\nshorter than or comparable to the orbital time. The grain alignment direction\nin a disk depends on the disk (sub-)structure and grain Stokes number (St) with\nazimuthal alignment for large St grains in sub-Keplerian smooth gas disks and\nfor small St grains near the gas pressure extrema, such as rings and gaps."
                },
                "authors": [
                    {
                        "name": "Zhe-Yu Daniel Lin"
                    },
                    {
                        "name": "Zhi-Yun Li"
                    },
                    {
                        "name": "Haifeng Yang"
                    },
                    {
                        "name": "Leslie W. Looney"
                    },
                    {
                        "name": "Ian W. Stephens"
                    },
                    {
                        "name": "Manuel Fernndez-Lpez"
                    },
                    {
                        "name": "Rachel E. Harrison"
                    }
                ],
                "author_detail": {
                    "name": "Rachel E. Harrison"
                },
                "author": "Rachel E. Harrison",
                "arxiv_comment": "21 pages, 12 figures, accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16749v3",
                "updated": "2024-11-08T17:07:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    7,
                    23,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-24T15:57:49Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    57,
                    49,
                    0,
                    176,
                    0
                ],
                "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stochastic low-rank recurrent neural networks from neural data"
                },
                "summary": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability."
                },
                "authors": [
                    {
                        "name": "Matthijs Pals"
                    },
                    {
                        "name": "A Erdem Satekin"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Jakob H Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H Macke"
                },
                "author": "Jakob H Macke",
                "arxiv_journal_ref": "The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16651v2",
                "updated": "2024-11-08T16:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    52,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-01-30T00:48:55Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    0,
                    48,
                    55,
                    1,
                    30,
                    0
                ],
                "title": "A constructive approach to selective risk control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A constructive approach to selective risk control"
                },
                "summary": "Many modern applications require using data to select the statistical tasks\nand make valid inference after selection. In this article, we provide a\nunifying approach to control for a class of selective risks. Our method is\nmotivated by a reformulation of the celebrated Benjamini-Hochberg (BH)\nprocedure for multiple hypothesis testing as the fixed point iteration of the\nBenjamini-Yekutieli (BY) procedure for constructing post-selection confidence\nintervals. Building on this observation, we propose a constructive approach to\ncontrol extra-selection risk (where selection is made after decision) by\niterating decision strategies that control the post-selection risk (where\ndecision is made after selection). We show that many previous methods and\nresults are special cases of this general framework, and we further extend this\napproach to problems with multiple selective risks. Our development leads to\ntwo surprising results about the BH procedure: (1) in the context of one-sided\nlocation testing, the BH procedure not only controls the false discovery rate\nat the null but also at other locations for free; (2) in the context of\npermutation tests, the BH procedure with exact permutation p-values can be well\napproximated by a procedure which only requires a total number of permutations\nthat is almost linear in the total number of hypotheses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications require using data to select the statistical tasks\nand make valid inference after selection. In this article, we provide a\nunifying approach to control for a class of selective risks. Our method is\nmotivated by a reformulation of the celebrated Benjamini-Hochberg (BH)\nprocedure for multiple hypothesis testing as the fixed point iteration of the\nBenjamini-Yekutieli (BY) procedure for constructing post-selection confidence\nintervals. Building on this observation, we propose a constructive approach to\ncontrol extra-selection risk (where selection is made after decision) by\niterating decision strategies that control the post-selection risk (where\ndecision is made after selection). We show that many previous methods and\nresults are special cases of this general framework, and we further extend this\napproach to problems with multiple selective risks. Our development leads to\ntwo surprising results about the BH procedure: (1) in the context of one-sided\nlocation testing, the BH procedure not only controls the false discovery rate\nat the null but also at other locations for free; (2) in the context of\npermutation tests, the BH procedure with exact permutation p-values can be well\napproximated by a procedure which only requires a total number of permutations\nthat is almost linear in the total number of hypotheses."
                },
                "authors": [
                    {
                        "name": "Zijun Gao"
                    },
                    {
                        "name": "Wenjie Hu"
                    },
                    {
                        "name": "Qingyuan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qingyuan Zhao"
                },
                "author": "Qingyuan Zhao",
                "arxiv_comment": "9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02791v2",
                "updated": "2024-11-08T16:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    50,
                    24,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-04T21:29:56Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    21,
                    29,
                    56,
                    1,
                    156,
                    0
                ],
                "title": "Language Models can Infer Action Semantics for Symbolic Planners from\n  Environment Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Infer Action Semantics for Symbolic Planners from\n  Environment Feedback"
                },
                "summary": "Symbolic planners can discover a sequence of actions from initial to goal\nstates given expert-defined, domain-specific logical action semantics. Large\nLanguage Models (LLMs) can directly generate such sequences, but limitations in\nreasoning and state-tracking often result in plans that are insufficient or\nunexecutable. We propose Predicting Semantics of Actions with Language Models\n(PSALM), which automatically learns action semantics by leveraging the\nstrengths of both symbolic planners and LLMs. PSALM repeatedly proposes and\nexecutes plans, using the LLM to partially generate plans and to infer\ndomain-specific action semantics based on execution outcomes. PSALM maintains a\nbelief over possible action semantics that is iteratively updated until a goal\nstate is reached. Experiments on 7 environments show that when learning just\nfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to\n100%, and explores the environment more efficiently than prior work to infer\nground truth domain action semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic planners can discover a sequence of actions from initial to goal\nstates given expert-defined, domain-specific logical action semantics. Large\nLanguage Models (LLMs) can directly generate such sequences, but limitations in\nreasoning and state-tracking often result in plans that are insufficient or\nunexecutable. We propose Predicting Semantics of Actions with Language Models\n(PSALM), which automatically learns action semantics by leveraging the\nstrengths of both symbolic planners and LLMs. PSALM repeatedly proposes and\nexecutes plans, using the LLM to partially generate plans and to infer\ndomain-specific action semantics based on execution outcomes. PSALM maintains a\nbelief over possible action semantics that is iteratively updated until a goal\nstate is reached. Experiments on 7 environments show that when learning just\nfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to\n100%, and explores the environment more efficiently than prior work to infer\nground truth domain action semantics."
                },
                "authors": [
                    {
                        "name": "Wang Zhu"
                    },
                    {
                        "name": "Ishika Singh"
                    },
                    {
                        "name": "Robin Jia"
                    },
                    {
                        "name": "Jesse Thomason"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Thomason"
                },
                "author": "Jesse Thomason",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v2",
                "updated": "2024-11-08T16:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    50,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v3",
                "updated": "2024-11-08T16:42:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    42,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4"
                },
                "summary": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00177v2",
                "updated": "2024-11-08T16:42:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    42,
                    18,
                    4,
                    313,
                    0
                ],
                "published": "2024-10-31T19:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    48,
                    12,
                    3,
                    305,
                    0
                ],
                "title": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction"
                },
                "summary": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction."
                },
                "authors": [
                    {
                        "name": "Andre Niyongabo Rubungo"
                    },
                    {
                        "name": "Kangming Li"
                    },
                    {
                        "name": "Jason Hattrick-Simpers"
                    },
                    {
                        "name": "Adji Bousso Dieng"
                    }
                ],
                "author_detail": {
                    "name": "Adji Bousso Dieng"
                },
                "author": "Adji Bousso Dieng",
                "arxiv_comment": "Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can\n  be found at: https://github.com/vertaix/LLM4Mat-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05681v1",
                "updated": "2024-11-08T16:31:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    31,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:31:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    31,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "A Survey of AI-Related Cyber Security Risks and Countermeasures in\n  Mobility-as-a-Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of AI-Related Cyber Security Risks and Countermeasures in\n  Mobility-as-a-Service"
                },
                "summary": "Mobility-as-a-Service (MaaS) integrates different transport modalities and\ncan support more personalisation of travellers' journey planning based on their\nindividual preferences, behaviours and wishes. To fully achieve the potential\nof MaaS, a range of AI (including machine learning and data mining) algorithms\nare needed to learn personal requirements and needs, to optimise journey\nplanning of each traveller and all travellers as a whole, to help transport\nservice operators and relevant governmental bodies to operate and plan their\nservices, and to detect and prevent cyber attacks from various threat actors\nincluding dishonest and malicious travellers and transport operators. The\nincreasing use of different AI and data processing algorithms in both\ncentralised and distributed settings opens the MaaS ecosystem up to diverse\ncyber and privacy attacks at both the AI algorithm level and the connectivity\nsurfaces. In this paper, we present the first comprehensive review on the\ncoupling between AI-driven MaaS design and the diverse cyber security\nchallenges related to cyber attacks and countermeasures. In particular, we\nfocus on how current and emerging AI-facilitated privacy risks (profiling,\ninference, and third-party threats) and adversarial AI attacks (evasion,\nextraction, and gamification) may impact the MaaS ecosystem. These risks often\ncombine novel attacks (e.g., inverse learning) with traditional attack vectors\n(e.g., man-in-the-middle attacks), exacerbating the risks for the wider\nparticipation actors and the emergence of new business models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility-as-a-Service (MaaS) integrates different transport modalities and\ncan support more personalisation of travellers' journey planning based on their\nindividual preferences, behaviours and wishes. To fully achieve the potential\nof MaaS, a range of AI (including machine learning and data mining) algorithms\nare needed to learn personal requirements and needs, to optimise journey\nplanning of each traveller and all travellers as a whole, to help transport\nservice operators and relevant governmental bodies to operate and plan their\nservices, and to detect and prevent cyber attacks from various threat actors\nincluding dishonest and malicious travellers and transport operators. The\nincreasing use of different AI and data processing algorithms in both\ncentralised and distributed settings opens the MaaS ecosystem up to diverse\ncyber and privacy attacks at both the AI algorithm level and the connectivity\nsurfaces. In this paper, we present the first comprehensive review on the\ncoupling between AI-driven MaaS design and the diverse cyber security\nchallenges related to cyber attacks and countermeasures. In particular, we\nfocus on how current and emerging AI-facilitated privacy risks (profiling,\ninference, and third-party threats) and adversarial AI attacks (evasion,\nextraction, and gamification) may impact the MaaS ecosystem. These risks often\ncombine novel attacks (e.g., inverse learning) with traditional attack vectors\n(e.g., man-in-the-middle attacks), exacerbating the risks for the wider\nparticipation actors and the emergence of new business models."
                },
                "authors": [
                    {
                        "name": "Kai-Fung Chu"
                    },
                    {
                        "name": "Haiyue Yuan"
                    },
                    {
                        "name": "Jinsheng Yuan"
                    },
                    {
                        "name": "Weisi Guo"
                    },
                    {
                        "name": "Nazmiye Balta-Ozkan"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_doi": "10.1109/MITS.2024.3427655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MITS.2024.3427655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Intelligent Transportation Systems Magazine (Volume: 16,\n  Issue: 6, Nov.-Dec. 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05679v1",
                "updated": "2024-11-08T16:29:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:29:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text"
                },
                "summary": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning."
                },
                "authors": [
                    {
                        "name": "Xiulong Liu"
                    },
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Eli Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shlizerman"
                },
                "author": "Eli Shlizerman",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05665v1",
                "updated": "2024-11-08T16:07:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    7,
                    47,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:07:47Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    7,
                    47,
                    4,
                    313,
                    0
                ],
                "title": "Unmasking the Limits of Large Language Models: A Systematic Evaluation\n  of Masked Text Processing Ability through MskQA and MskCal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Limits of Large Language Models: A Systematic Evaluation\n  of Masked Text Processing Ability through MskQA and MskCal"
                },
                "summary": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities."
                },
                "authors": [
                    {
                        "name": "Fuka Matsuzaki"
                    },
                    {
                        "name": "Haru-Tada Sato"
                    }
                ],
                "author_detail": {
                    "name": "Haru-Tada Sato"
                },
                "author": "Haru-Tada Sato",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v2",
                "updated": "2024-11-08T16:06:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    6,
                    9,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Building Very Large Knowledge Bases from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Building Very Large Knowledge Bases from Language Models"
                },
                "summary": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at http://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at http://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "11 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05653v1",
                "updated": "2024-11-08T15:49:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    49,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:49:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    49,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "The influence of persona and conversational task on social interactions\n  with a LLM-controlled embodied conversational agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of persona and conversational task on social interactions\n  with a LLM-controlled embodied conversational agent"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment."
                },
                "authors": [
                    {
                        "name": "Leon O. H. Kroczek"
                    },
                    {
                        "name": "Alexander May"
                    },
                    {
                        "name": "Selina Hettenkofer"
                    },
                    {
                        "name": "Andreas Ruider"
                    },
                    {
                        "name": "Bernd Ludwig"
                    },
                    {
                        "name": "Andreas Mhlberger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Mhlberger"
                },
                "author": "Andreas Mhlberger",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05651v1",
                "updated": "2024-11-08T15:46:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    46,
                    10,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:46:10Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    46,
                    10,
                    4,
                    313,
                    0
                ],
                "title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning\n  and Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning\n  and Execution"
                },
                "summary": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks\nbased on observations and execute tasks by creating visualizations and\ninteractive exploration to gain insights. This process demands skills in\nprogramming, data processing, and visualization tools, highlighting the need\nfor a more intelligent, streamlined VA approach. Large language models (LLMs)\nhave recently been developed as agents to handle various tasks with dynamic\nplanning and tool-using capabilities, offering the potential to enhance the\nefficiency and versatility of VA. We propose LightVA, a lightweight VA\nframework that supports task decomposition, data analysis, and interactive\nexploration through human-agent collaboration. Our method is designed to help\nusers progressively translate high-level analytical goals into low-level tasks,\nproducing visualizations and deriving insights. Specifically, we introduce an\nLLM agent-based task planning and execution strategy, employing a recursive\nprocess involving a planner, executor, and controller. The planner is\nresponsible for recommending and decomposing tasks, the executor handles task\nexecution, including data analysis, visualization generation and multi-view\ncomposition, and the controller coordinates the interaction between the planner\nand executor. Building on the framework, we develop a system with a hybrid user\ninterface that includes a task flow diagram for monitoring and managing the\ntask planning process, a visualization panel for interactive data exploration,\nand a chat view for guiding the model through natural language instructions. We\nexamine the effectiveness of our method through a usage scenario and an expert\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks\nbased on observations and execute tasks by creating visualizations and\ninteractive exploration to gain insights. This process demands skills in\nprogramming, data processing, and visualization tools, highlighting the need\nfor a more intelligent, streamlined VA approach. Large language models (LLMs)\nhave recently been developed as agents to handle various tasks with dynamic\nplanning and tool-using capabilities, offering the potential to enhance the\nefficiency and versatility of VA. We propose LightVA, a lightweight VA\nframework that supports task decomposition, data analysis, and interactive\nexploration through human-agent collaboration. Our method is designed to help\nusers progressively translate high-level analytical goals into low-level tasks,\nproducing visualizations and deriving insights. Specifically, we introduce an\nLLM agent-based task planning and execution strategy, employing a recursive\nprocess involving a planner, executor, and controller. The planner is\nresponsible for recommending and decomposing tasks, the executor handles task\nexecution, including data analysis, visualization generation and multi-view\ncomposition, and the controller coordinates the interaction between the planner\nand executor. Building on the framework, we develop a system with a hybrid user\ninterface that includes a task flow diagram for monitoring and managing the\ntask planning process, a visualization panel for interactive data exploration,\nand a chat view for guiding the model through natural language instructions. We\nexamine the effectiveness of our method through a usage scenario and an expert\nstudy."
                },
                "authors": [
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Linbin Xiang"
                    },
                    {
                        "name": "Xiaowen Zhang"
                    },
                    {
                        "name": "Zifei Guo"
                    },
                    {
                        "name": "Cagatay Turkay"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05641v1",
                "updated": "2024-11-08T15:35:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    35,
                    43,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:35:43Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    35,
                    43,
                    4,
                    313,
                    0
                ],
                "title": "Evaluating Large Language Model Capability in Vietnamese Fact-Checking\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Model Capability in Vietnamese Fact-Checking\n  Data Generation"
                },
                "summary": "Large Language Models (LLMs), with gradually improving reading comprehension\nand reasoning capabilities, are being applied to a range of complex language\ntasks, including the automatic generation of language data for various\npurposes. However, research on applying LLMs for automatic data generation in\nlow-resource languages like Vietnamese is still underdeveloped and lacks\ncomprehensive evaluation. In this paper, we explore the use of LLMs for\nautomatic data generation for the Vietnamese fact-checking task, which faces\nsignificant data limitations. Specifically, we focus on fact-checking data\nwhere claims are synthesized from multiple evidence sentences to assess the\ninformation synthesis capabilities of LLMs. We develop an automatic data\nconstruction process using simple prompt techniques on LLMs and explore several\nmethods to improve the quality of the generated data. To evaluate the quality\nof the data generated by LLMs, we conduct both manual quality assessments and\nperformance evaluations using language models. Experimental results and manual\nevaluations illustrate that while the quality of the generated data has\nsignificantly improved through fine-tuning techniques, LLMs still cannot match\nthe data quality produced by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with gradually improving reading comprehension\nand reasoning capabilities, are being applied to a range of complex language\ntasks, including the automatic generation of language data for various\npurposes. However, research on applying LLMs for automatic data generation in\nlow-resource languages like Vietnamese is still underdeveloped and lacks\ncomprehensive evaluation. In this paper, we explore the use of LLMs for\nautomatic data generation for the Vietnamese fact-checking task, which faces\nsignificant data limitations. Specifically, we focus on fact-checking data\nwhere claims are synthesized from multiple evidence sentences to assess the\ninformation synthesis capabilities of LLMs. We develop an automatic data\nconstruction process using simple prompt techniques on LLMs and explore several\nmethods to improve the quality of the generated data. To evaluate the quality\nof the data generated by LLMs, we conduct both manual quality assessments and\nperformance evaluations using language models. Experimental results and manual\nevaluations illustrate that while the quality of the generated data has\nsignificantly improved through fine-tuning techniques, LLMs still cannot match\nthe data quality produced by humans."
                },
                "authors": [
                    {
                        "name": "Long Truong To"
                    },
                    {
                        "name": "Hung Tuan Le"
                    },
                    {
                        "name": "Dat Van-Thanh Nguyen"
                    },
                    {
                        "name": "Manh Trong Nguyen"
                    },
                    {
                        "name": "Tri Thien Nguyen"
                    },
                    {
                        "name": "Tin Van Huynh"
                    },
                    {
                        "name": "Kiet Van Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Kiet Van Nguyen"
                },
                "author": "Kiet Van Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05639v1",
                "updated": "2024-11-08T15:34:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    34,
                    8,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:34:08Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    34,
                    8,
                    4,
                    313,
                    0
                ],
                "title": "Assessing Open-Source Large Language Models on Argumentation Mining\n  Subtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Open-Source Large Language Models on Argumentation Mining\n  Subtasks"
                },
                "summary": "We explore the capability of four open-sourcelarge language models (LLMs) in\nargumentation mining (AM). We conduct experiments on three different corpora;\npersuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based\non two argumentation mining sub-tasks: (i) argumentative discourse units\nclassifications (ADUC), and (ii) argumentative relation classification (ARC).\nThis work aims to assess the argumentation capability of open-source LLMs,\nincluding Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot\nand few-shot scenarios. Our analysis contributes to further assessing\ncomputational argumentation with open-source LLMs in future research efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the capability of four open-sourcelarge language models (LLMs) in\nargumentation mining (AM). We conduct experiments on three different corpora;\npersuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based\non two argumentation mining sub-tasks: (i) argumentative discourse units\nclassifications (ADUC), and (ii) argumentative relation classification (ARC).\nThis work aims to assess the argumentation capability of open-source LLMs,\nincluding Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot\nand few-shot scenarios. Our analysis contributes to further assessing\ncomputational argumentation with open-source LLMs in future research efforts."
                },
                "authors": [
                    {
                        "name": "Mohammad Yeghaneh Abkenar"
                    },
                    {
                        "name": "Weixing Wang"
                    },
                    {
                        "name": "Hendrik Graupner"
                    },
                    {
                        "name": "Manfred Stede"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Stede"
                },
                "author": "Manfred Stede",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05625v1",
                "updated": "2024-11-08T15:15:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    15,
                    34,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:15:34Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    15,
                    34,
                    4,
                    313,
                    0
                ],
                "title": "Cross-validating causal discovery via Leave-One-Variable-Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-validating causal discovery via Leave-One-Variable-Out"
                },
                "summary": "We propose a new approach to falsify causal discovery algorithms without\nground truth, which is based on testing the causal model on a pair of variables\nthat has been dropped when learning the causal model. To this end, we use the\n\"Leave-One-Variable-Out (LOVO)\" prediction where $Y$ is inferred from $X$\nwithout any joint observations of $X$ and $Y$, given only training data from\n$X,Z_1,\\dots,Z_k$ and from $Z_1,\\dots,Z_k,Y$. We demonstrate that causal models\non the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often\nentail conclusions on the dependencies between $X$ and $Y$, enabling this type\nof prediction. The prediction error can then be estimated since the joint\ndistribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only\nbeen omitted for the purpose of falsification. After presenting this graphical\nmethod, which is applicable to general causal discovery algorithms, we\nillustrate how to construct a LOVO predictor tailored towards algorithms\nrelying on specific a priori assumptions, such as linear additive noise models.\nSimulations indicate that the LOVO prediction error is indeed correlated with\nthe accuracy of the causal outputs, affirming the method's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new approach to falsify causal discovery algorithms without\nground truth, which is based on testing the causal model on a pair of variables\nthat has been dropped when learning the causal model. To this end, we use the\n\"Leave-One-Variable-Out (LOVO)\" prediction where $Y$ is inferred from $X$\nwithout any joint observations of $X$ and $Y$, given only training data from\n$X,Z_1,\\dots,Z_k$ and from $Z_1,\\dots,Z_k,Y$. We demonstrate that causal models\non the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often\nentail conclusions on the dependencies between $X$ and $Y$, enabling this type\nof prediction. The prediction error can then be estimated since the joint\ndistribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only\nbeen omitted for the purpose of falsification. After presenting this graphical\nmethod, which is applicable to general causal discovery algorithms, we\nillustrate how to construct a LOVO predictor tailored towards algorithms\nrelying on specific a priori assumptions, such as linear additive noise models.\nSimulations indicate that the LOVO prediction error is indeed correlated with\nthe accuracy of the causal outputs, affirming the method's effectiveness."
                },
                "authors": [
                    {
                        "name": "Daniela Schkoda"
                    },
                    {
                        "name": "Philipp Faller"
                    },
                    {
                        "name": "Patrick Blbaum"
                    },
                    {
                        "name": "Dominik Janzing"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Janzing"
                },
                "author": "Dominik Janzing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05623v1",
                "updated": "2024-11-08T15:12:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    12,
                    38,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:12:38Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    12,
                    38,
                    4,
                    313,
                    0
                ],
                "title": "The rush to the poles and the role of magnetic buoyancy in the solar\n  dynamo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rush to the poles and the role of magnetic buoyancy in the solar\n  dynamo"
                },
                "summary": "The butterfly diagram of the solar cycle exhibits a poleward migration of the\ndiffuse magnetic field resulting from the decay of trailing sunspots. It is one\ncomponent of what is sometimes referred to as the \"rush to the poles\". We\ninvestigate under which conditions the rush to the poles can be reproduced in\nflux-transport Babcock-Leighton dynamo models. We identify three main ways to\nreproduce it: a flux emergence probability that decreases rapidly with\nlatitude; a threshold in subsurface toroidal field strength between slow and\nfast emergence; and an emergence rate based on magnetic buoyancy. We find that\nall three mechanisms lead to solar-like butterfly diagrams, but which present\nnotable differences between them. The shape of the butterfly diagram is very\nsensitive to model parameters for the threshold prescription, while most models\nincorporating magnetic buoyancy converge to very similar butterfly diagrams,\nwith butterfly wings widths of $\\lesssim\\pm 30^\\circ$, in very good agreement\nwith observations. With turbulent diffusivities above $35~\\text{km}^2/\\text{s}$\nbut below about $40~\\text{km}^2/\\text{s}$, buoyancy models are strikingly\nsolar-like. The threshold and magnetic buoyancy prescriptions make the models\nnon-linear and as such can saturate the dynamo through latitudinal quenching.\nThe period of the models involving buoyancy is independent of the source term\namplitude, but emergence loss increases it by $\\simeq 60\\%$. For the rush to\nthe poles to be visible, a mechanism suppressing (enhancing) emergences at high\n(low) latitudes must operate. It is not sufficient that the toroidal field be\nstored at low latitudes for emergences to be limited to low latitudes. From\nthese models we infer that the Sun is not in the advection-dominated regime,\nbut also not in the diffusion-dominated regime. The cycle period is set through\na balance between advection, diffusion and flux emergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The butterfly diagram of the solar cycle exhibits a poleward migration of the\ndiffuse magnetic field resulting from the decay of trailing sunspots. It is one\ncomponent of what is sometimes referred to as the \"rush to the poles\". We\ninvestigate under which conditions the rush to the poles can be reproduced in\nflux-transport Babcock-Leighton dynamo models. We identify three main ways to\nreproduce it: a flux emergence probability that decreases rapidly with\nlatitude; a threshold in subsurface toroidal field strength between slow and\nfast emergence; and an emergence rate based on magnetic buoyancy. We find that\nall three mechanisms lead to solar-like butterfly diagrams, but which present\nnotable differences between them. The shape of the butterfly diagram is very\nsensitive to model parameters for the threshold prescription, while most models\nincorporating magnetic buoyancy converge to very similar butterfly diagrams,\nwith butterfly wings widths of $\\lesssim\\pm 30^\\circ$, in very good agreement\nwith observations. With turbulent diffusivities above $35~\\text{km}^2/\\text{s}$\nbut below about $40~\\text{km}^2/\\text{s}$, buoyancy models are strikingly\nsolar-like. The threshold and magnetic buoyancy prescriptions make the models\nnon-linear and as such can saturate the dynamo through latitudinal quenching.\nThe period of the models involving buoyancy is independent of the source term\namplitude, but emergence loss increases it by $\\simeq 60\\%$. For the rush to\nthe poles to be visible, a mechanism suppressing (enhancing) emergences at high\n(low) latitudes must operate. It is not sufficient that the toroidal field be\nstored at low latitudes for emergences to be limited to low latitudes. From\nthese models we infer that the Sun is not in the advection-dominated regime,\nbut also not in the diffusion-dominated regime. The cycle period is set through\na balance between advection, diffusion and flux emergence."
                },
                "authors": [
                    {
                        "name": "Simon Cloutier"
                    },
                    {
                        "name": "Robert H. Cameron"
                    },
                    {
                        "name": "Laurent Gizon"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Gizon"
                },
                "author": "Laurent Gizon",
                "arxiv_comment": "12 pages, 17 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11167v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11167v4",
                "updated": "2024-11-08T14:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    58,
                    31,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-20T19:58:44Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    19,
                    58,
                    44,
                    1,
                    233,
                    0
                ],
                "title": "Multi-time small-area estimation of oil and gas production capacity by\n  Bayesian multilevel modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-time small-area estimation of oil and gas production capacity by\n  Bayesian multilevel modeling"
                },
                "summary": "This paper presents a Bayesian multilevel modeling approach for estimating\nwell-level oil and gas production capacities across small geographic areas over\nmultiple time periods. Focusing on a basin, which is a geologically and\neconomically distinct drilling region, we model the production level of wells\ngrouped by area and time, using priors as regulators of inferences. Our model\naccounts for area-level and time-level variations as well as well-level\nvariations, incorporating lateral length, water usage, and sand usage. The\nMaidenhead Coordinate System is used to define uniform (small) geographic\nareas, many of which contain only a small number of wells in a given time\nperiod. The Bayesian small-area model is first built and checked, using data\nfrom the Bakken region, covering from 21 February 2012 to 12 June 2024. The\nmodel is expanded to accommodate temporal dynamics by introducing time-effect\ncomponents, allowing for the analysis of production trends over times. We\nexplore the impact of technological advancements by modeling water-sand\nintensity as a proxy for production efficiency. The Bayesian multilevel\nmodeling approach provides a robust and flexible tool for modeling oil or/and\ngas production at area and time levels, informing the energy production\nprediction with uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Bayesian multilevel modeling approach for estimating\nwell-level oil and gas production capacities across small geographic areas over\nmultiple time periods. Focusing on a basin, which is a geologically and\neconomically distinct drilling region, we model the production level of wells\ngrouped by area and time, using priors as regulators of inferences. Our model\naccounts for area-level and time-level variations as well as well-level\nvariations, incorporating lateral length, water usage, and sand usage. The\nMaidenhead Coordinate System is used to define uniform (small) geographic\nareas, many of which contain only a small number of wells in a given time\nperiod. The Bayesian small-area model is first built and checked, using data\nfrom the Bakken region, covering from 21 February 2012 to 12 June 2024. The\nmodel is expanded to accommodate temporal dynamics by introducing time-effect\ncomponents, allowing for the analysis of production trends over times. We\nexplore the impact of technological advancements by modeling water-sand\nintensity as a proxy for production efficiency. The Bayesian multilevel\nmodeling approach provides a robust and flexible tool for modeling oil or/and\ngas production at area and time levels, informing the energy production\nprediction with uncertainties."
                },
                "authors": [
                    {
                        "name": "Hiroaki Minato"
                    }
                ],
                "author_detail": {
                    "name": "Hiroaki Minato"
                },
                "author": "Hiroaki Minato",
                "arxiv_comment": "16 pages, 4 charts, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11167v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11167v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.01404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.01404v3",
                "updated": "2024-11-08T14:54:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    54,
                    51,
                    4,
                    313,
                    0
                ],
                "published": "2023-09-04T07:20:51Z",
                "published_parsed": [
                    2023,
                    9,
                    4,
                    7,
                    20,
                    51,
                    0,
                    247,
                    0
                ],
                "title": "Hierarchical Regression Discontinuity Design: Pursuing Subgroup\n  Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Regression Discontinuity Design: Pursuing Subgroup\n  Treatment Effects"
                },
                "summary": "Regression discontinuity design (RDD) is widely adopted for causal inference\nunder intervention determined by a continuous variable. While one is interested\nin treatment effect heterogeneity by subgroups in many applications, RDD\ntypically suffers from small subgroup-wise sample sizes, which makes the\nestimation results highly instable. To solve this issue, we introduce\nhierarchical RDD (HRDD), a hierarchical Bayes approach for pursuing treatment\neffect heterogeneity in RDD. A key feature of HRDD is to employ a pseudo-model\nbased on a loss function to estimate subgroup-level parameters of treatment\neffects under RDD, and assign a hierarchical prior distribution to ''borrow\nstrength'' from other subgroups. The posterior computation can be easily done\nby a simple Gibbs sampling, and the optimal bandwidth can be automatically\nselected by the Hyv\\\"{a}rinen scores for unnormalized models. We demonstrate\nthe proposed HRDD through simulation and real data analysis, and show that HRDD\nprovides much more stable point and interval estimation than separately\napplying the standard RDD method to each subgroup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression discontinuity design (RDD) is widely adopted for causal inference\nunder intervention determined by a continuous variable. While one is interested\nin treatment effect heterogeneity by subgroups in many applications, RDD\ntypically suffers from small subgroup-wise sample sizes, which makes the\nestimation results highly instable. To solve this issue, we introduce\nhierarchical RDD (HRDD), a hierarchical Bayes approach for pursuing treatment\neffect heterogeneity in RDD. A key feature of HRDD is to employ a pseudo-model\nbased on a loss function to estimate subgroup-level parameters of treatment\neffects under RDD, and assign a hierarchical prior distribution to ''borrow\nstrength'' from other subgroups. The posterior computation can be easily done\nby a simple Gibbs sampling, and the optimal bandwidth can be automatically\nselected by the Hyv\\\"{a}rinen scores for unnormalized models. We demonstrate\nthe proposed HRDD through simulation and real data analysis, and show that HRDD\nprovides much more stable point and interval estimation than separately\napplying the standard RDD method to each subgroup."
                },
                "authors": [
                    {
                        "name": "Shonosuke Sugasawa"
                    },
                    {
                        "name": "Takuya Ishihara"
                    },
                    {
                        "name": "Daisuke Kurisu"
                    }
                ],
                "author_detail": {
                    "name": "Daisuke Kurisu"
                },
                "author": "Daisuke Kurisu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.01404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.01404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05609v1",
                "updated": "2024-11-08T14:52:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:52:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis"
                },
                "summary": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and a Large Language\nModel (LLM) to generate disease diagnoses based on the predicted concepts. We\nvalidate our approach on three skin lesion datasets, demonstrating that it\noutperforms traditional CBMs and state-of-the-art explainable methods, all\nwithout requiring any training and utilizing only a few annotated examples. The\ncode is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and a Large Language\nModel (LLM) to generate disease diagnoses based on the predicted concepts. We\nvalidate our approach on three skin lesion datasets, demonstrating that it\noutperforms traditional CBMs and state-of-the-art explainable methods, all\nwithout requiring any training and utilizing only a few annotated examples. The\ncode is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis."
                },
                "authors": [
                    {
                        "name": "Cristiano Patrcio"
                    },
                    {
                        "name": "Lus F. Teixeira"
                    },
                    {
                        "name": "Joo C. Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joo C. Neves"
                },
                "author": "Joo C. Neves",
                "arxiv_comment": "Preprint submitted for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v3",
                "updated": "2024-11-08T14:46:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    46,
                    40,
                    4,
                    313,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01286v2",
                "updated": "2024-11-08T14:28:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    28,
                    32,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-02T14:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    14,
                    30,
                    43,
                    0,
                    246,
                    0
                ],
                "title": "Ionising properties of galaxies in JADES for a stellar mass complete\n  sample: resolving the cosmic ionising photon budget crisis at the Epoch of\n  Reionisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ionising properties of galaxies in JADES for a stellar mass complete\n  sample: resolving the cosmic ionising photon budget crisis at the Epoch of\n  Reionisation"
                },
                "summary": "We use NIRCam imaging from the JWST Advanced Deep Extragalactic Survey\n(JADES) to study the ionising properties of a sample of 15721 galaxies at $3\n\\leq z_{\\rm{phot}} \\leq 9$, 90\\% complete in stellar mass down to\nlog(M$_{\\star}$/[M$_{\\odot}$])$\\approx 7.5$. Out of the full sample, 1620 of\nthe galaxies have spectroscopic redshift measurements from the literature. We\nuse the spectral energy distribution fitting code \\texttt{Prospector} to fit\nall available photometry and infer galaxy properties. We find a significantly\nmilder evolution of the ionising photon production efficiency (\\xion\\/) with\nredshift and UV magnitude than previously reported. Interestingly, we observe\ntwo distinct populations in \\xion\\/, distinguished by their burstiness (given\nby SFR$_{10}$/SFR$_{100}$). Both populations show the same evolution with $z$\nand M$_{\\rm{UV}}$, but have a different \\xion\\/ normalisation. We convolve the\nmore representative $\\log(\\xi_{\\rm{ion}} (z,\\text{M}_{\\rm{UV}}))$ relations\n(accounting for $\\sim96$\\% of the sample), with luminosity functions from\nliterature, to place constraints on the cosmic ionising photon budget. By\ncombining our results, we find that one of our models can match the\nobservational constraints from the \\lya\\/ forest at $z\\lesssim6$. We conclude\nthat galaxies with M$_{\\rm{UV}}$ between $-16$ and $-20$, adopting a reasonable\nescape fraction, can produce enough ionising photons to ionise the Universe,\nwithout exceeding the required ionising photon budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use NIRCam imaging from the JWST Advanced Deep Extragalactic Survey\n(JADES) to study the ionising properties of a sample of 15721 galaxies at $3\n\\leq z_{\\rm{phot}} \\leq 9$, 90\\% complete in stellar mass down to\nlog(M$_{\\star}$/[M$_{\\odot}$])$\\approx 7.5$. Out of the full sample, 1620 of\nthe galaxies have spectroscopic redshift measurements from the literature. We\nuse the spectral energy distribution fitting code \\texttt{Prospector} to fit\nall available photometry and infer galaxy properties. We find a significantly\nmilder evolution of the ionising photon production efficiency (\\xion\\/) with\nredshift and UV magnitude than previously reported. Interestingly, we observe\ntwo distinct populations in \\xion\\/, distinguished by their burstiness (given\nby SFR$_{10}$/SFR$_{100}$). Both populations show the same evolution with $z$\nand M$_{\\rm{UV}}$, but have a different \\xion\\/ normalisation. We convolve the\nmore representative $\\log(\\xi_{\\rm{ion}} (z,\\text{M}_{\\rm{UV}}))$ relations\n(accounting for $\\sim96$\\% of the sample), with luminosity functions from\nliterature, to place constraints on the cosmic ionising photon budget. By\ncombining our results, we find that one of our models can match the\nobservational constraints from the \\lya\\/ forest at $z\\lesssim6$. We conclude\nthat galaxies with M$_{\\rm{UV}}$ between $-16$ and $-20$, adopting a reasonable\nescape fraction, can produce enough ionising photons to ionise the Universe,\nwithout exceeding the required ionising photon budget."
                },
                "authors": [
                    {
                        "name": "C. Simmonds"
                    },
                    {
                        "name": "S. Tacchella"
                    },
                    {
                        "name": "K. Hainline"
                    },
                    {
                        "name": "B. D. Johnson"
                    },
                    {
                        "name": "D. Pusks"
                    },
                    {
                        "name": "B. Robertson"
                    },
                    {
                        "name": "W. M. Baker"
                    },
                    {
                        "name": "R. Bhatawdekar"
                    },
                    {
                        "name": "K. Boyett"
                    },
                    {
                        "name": "A. J. Bunker"
                    },
                    {
                        "name": "P. A. Cargile"
                    },
                    {
                        "name": "S. Carniani"
                    },
                    {
                        "name": "J. Chevallard"
                    },
                    {
                        "name": "M. Curti"
                    },
                    {
                        "name": "E. Curtis-Lake"
                    },
                    {
                        "name": "Z. Ji"
                    },
                    {
                        "name": "G. C. Jones"
                    },
                    {
                        "name": "N. Kumari"
                    },
                    {
                        "name": "I. Laseter"
                    },
                    {
                        "name": "R. Maiolino"
                    },
                    {
                        "name": "M. V. Maseda"
                    },
                    {
                        "name": "P. Rinaldi"
                    },
                    {
                        "name": "A. Stoffers"
                    },
                    {
                        "name": "H. bler"
                    },
                    {
                        "name": "N. C. Villanueva"
                    },
                    {
                        "name": "C. C. Williams"
                    },
                    {
                        "name": "C. Willot"
                    },
                    {
                        "name": "J. Witstok"
                    },
                    {
                        "name": "Y. Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Y. Zhu"
                },
                "author": "Y. Zhu",
                "arxiv_comment": "Accepted in MNRAS. 23 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05593v1",
                "updated": "2024-11-08T14:26:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    26,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:26:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    26,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "Evaluating and Adapting Large Language Models to Represent Folktales in\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Adapting Large Language Models to Represent Folktales in\n  Low-Resource Languages"
                },
                "summary": "Folktales are a rich resource of knowledge about the society and culture of a\ncivilisation. Digital folklore research aims to use automated techniques to\nbetter understand these folktales, and it relies on abstract representations of\nthe textual data. Although a number of large language models (LLMs) claim to be\nable to represent low-resource langauges such as Irish and Gaelic, we present\ntwo classification tasks to explore how useful these representations are, and\nthree adaptations to improve the performance of these models. We find that\nadapting the models to work with longer sequences, and continuing pre-training\non the domain of folktales improves classification performance, although these\nfindings are tempered by the impressive performance of a baseline SVM with\nnon-contextual features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Folktales are a rich resource of knowledge about the society and culture of a\ncivilisation. Digital folklore research aims to use automated techniques to\nbetter understand these folktales, and it relies on abstract representations of\nthe textual data. Although a number of large language models (LLMs) claim to be\nable to represent low-resource langauges such as Irish and Gaelic, we present\ntwo classification tasks to explore how useful these representations are, and\nthree adaptations to improve the performance of these models. We find that\nadapting the models to work with longer sequences, and continuing pre-training\non the domain of folktales improves classification performance, although these\nfindings are tempered by the impressive performance of a baseline SVM with\nnon-contextual features."
                },
                "authors": [
                    {
                        "name": "JA Meaney"
                    },
                    {
                        "name": "Beatrice Alex"
                    },
                    {
                        "name": "William Lamb"
                    }
                ],
                "author_detail": {
                    "name": "William Lamb"
                },
                "author": "William Lamb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05589v1",
                "updated": "2024-11-08T14:23:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    23,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:23:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    23,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "Predicting Resistive Pulse Signatures in Nanopores by Accurately\n  Modeling Access Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Resistive Pulse Signatures in Nanopores by Accurately\n  Modeling Access Regions"
                },
                "summary": "Resistive pulse sensing has been widely used to characterize and count single\nparticles in solution moving through channels under an electric bias, with\nnanoscale pores more recently providing enough spatial resolution for nucleic\nacid sequencing at the single-molecule level. At its core, this technique\nrelies on measuring the drop in ionic current through the pore induced by the\npassage of a molecule and, through conductance models, translating the blockage\nsignal to molecular dimensions. However, there exists no model considering the\nresistive contributions of the pore exterior, i.e. the access regions, when\nobstructed by a molecule. This is becoming increasingly important for low\naspect ratio pores, with the advent of 2D materials and ultrathin membranes. In\nthis work, a general method by which to model the resistance of the access\nregions of a pore in the presence of an insulating obstruction is presented.\nThin oblate spheroidal slices are used to partition access regions and infer\ntheir conductance when blocked by differently shaped objects. We show that our\nmodel accurately estimates the blocked-state conductance of 2D and\nfinite-length pores as a function of the distance from the pore in the presence\nof simple obstructions geometries (e.g. cylindrical and spherical objects) or\ncomplex structures (i.e. sequence of simple obstruction sub-units). The model\nis further shown to capture off-axis effects by predicting deeper blockages for\nobstructions offset from the pore's central axis. A web-based tool is created\nto predict the electrical signatures of a wide range of molecule geometries\ntranslocating through differently shaped pores. The introduced model will help\nguide experimental designs and thus presents a straightforward way to extend\nthe quantification of the resistive pulse technique at the nanoscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resistive pulse sensing has been widely used to characterize and count single\nparticles in solution moving through channels under an electric bias, with\nnanoscale pores more recently providing enough spatial resolution for nucleic\nacid sequencing at the single-molecule level. At its core, this technique\nrelies on measuring the drop in ionic current through the pore induced by the\npassage of a molecule and, through conductance models, translating the blockage\nsignal to molecular dimensions. However, there exists no model considering the\nresistive contributions of the pore exterior, i.e. the access regions, when\nobstructed by a molecule. This is becoming increasingly important for low\naspect ratio pores, with the advent of 2D materials and ultrathin membranes. In\nthis work, a general method by which to model the resistance of the access\nregions of a pore in the presence of an insulating obstruction is presented.\nThin oblate spheroidal slices are used to partition access regions and infer\ntheir conductance when blocked by differently shaped objects. We show that our\nmodel accurately estimates the blocked-state conductance of 2D and\nfinite-length pores as a function of the distance from the pore in the presence\nof simple obstructions geometries (e.g. cylindrical and spherical objects) or\ncomplex structures (i.e. sequence of simple obstruction sub-units). The model\nis further shown to capture off-axis effects by predicting deeper blockages for\nobstructions offset from the pore's central axis. A web-based tool is created\nto predict the electrical signatures of a wide range of molecule geometries\ntranslocating through differently shaped pores. The introduced model will help\nguide experimental designs and thus presents a straightforward way to extend\nthe quantification of the resistive pulse technique at the nanoscale."
                },
                "authors": [
                    {
                        "name": "Martin Charron"
                    },
                    {
                        "name": "Zachary Roelen"
                    },
                    {
                        "name": "Deekshant Wadhwa"
                    },
                    {
                        "name": "Vincent Tabard-Cossa"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Tabard-Cossa"
                },
                "author": "Vincent Tabard-Cossa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02355v3",
                "updated": "2024-11-08T14:17:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    17,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-05-03T02:48:55Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    2,
                    48,
                    55,
                    4,
                    124,
                    0
                ],
                "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation"
                },
                "summary": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/."
                },
                "authors": [
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Jizheng Chen"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Lingyue Fu"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05577v1",
                "updated": "2024-11-08T14:07:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    7,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:07:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    7,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "Exploring Relationships Between Cryptocurrency News Outlets and\n  Influencers' Twitter Activity and Market Prices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Relationships Between Cryptocurrency News Outlets and\n  Influencers' Twitter Activity and Market Prices"
                },
                "summary": "Academics increasingly acknowledge the predictive power of social media for a\nwide variety of events and, more specifically, for financial markets. Anecdotal\nand empirical findings show that cryptocurrencies are among the financial\nassets that have been affected by news and influencers' activities on Twitter.\nHowever, the extent to which Twitter crypto influencer's posts about trading\nsignals and their effect on market prices is mostly unexplored. In this paper,\nwe use LLMs to uncover buy and not-buy signals from influencers and news\noutlets' Twitter posts and use a VAR analysis with Granger Causality tests and\ncross-correlation analysis to understand how these trading signals are\ntemporally correlated with the top nine major cryptocurrencies' prices.\nOverall, the results show a mixed pattern across cryptocurrencies and temporal\nperiods. However, we found that for the top three cryptocurrencies with the\nhighest presence within news and influencer posts, their aggregated\nLLM-detected trading signal over the preceding 24 hours granger-causes\nfluctuations in their market prices, exhibiting a lag of at least 6 hours. In\naddition, the results reveal fundamental differences in how influencers and\nnews outlets cover cryptocurrencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academics increasingly acknowledge the predictive power of social media for a\nwide variety of events and, more specifically, for financial markets. Anecdotal\nand empirical findings show that cryptocurrencies are among the financial\nassets that have been affected by news and influencers' activities on Twitter.\nHowever, the extent to which Twitter crypto influencer's posts about trading\nsignals and their effect on market prices is mostly unexplored. In this paper,\nwe use LLMs to uncover buy and not-buy signals from influencers and news\noutlets' Twitter posts and use a VAR analysis with Granger Causality tests and\ncross-correlation analysis to understand how these trading signals are\ntemporally correlated with the top nine major cryptocurrencies' prices.\nOverall, the results show a mixed pattern across cryptocurrencies and temporal\nperiods. However, we found that for the top three cryptocurrencies with the\nhighest presence within news and influencer posts, their aggregated\nLLM-detected trading signal over the preceding 24 hours granger-causes\nfluctuations in their market prices, exhibiting a lag of at least 6 hours. In\naddition, the results reveal fundamental differences in how influencers and\nnews outlets cover cryptocurrencies."
                },
                "authors": [
                    {
                        "name": "Meysam Alizadeh"
                    },
                    {
                        "name": "Yasaman Asgari"
                    },
                    {
                        "name": "Zeynab Samei"
                    },
                    {
                        "name": "Sara Yari"
                    },
                    {
                        "name": "Shirin Dehghani"
                    },
                    {
                        "name": "Mael Kubli"
                    },
                    {
                        "name": "Darya Zare"
                    },
                    {
                        "name": "Juan Diego Bermeo"
                    },
                    {
                        "name": "Veronika Batzdorfer"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Gilardi"
                },
                "author": "Fabrizio Gilardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15267v2",
                "updated": "2024-11-08T14:02:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    2,
                    13,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-21T16:03:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    16,
                    3,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Evaluating Diversity in Automatic Poetry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Diversity in Automatic Poetry Generation"
                },
                "summary": "Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation -- can humans distinguish between automatic and\nhuman generated poetry -- we evaluate the diversity of automatically generated\npoetry (with a focus on quatrains), by comparing distributions of generated\npoetry to distributions of human poetry along structural, lexical, semantic and\nstylistic dimensions, assessing different model types (word vs.\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\nunconditioned). We find that current automatic poetry systems are considerably\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\nare semantically too uniform and even do not match the length distribution of\nhuman poetry. Our experiments reveal, however, that style-conditioning and\ncharacter-level modeling clearly increases diversity across virtually all\ndimensions we explore. Our identified limitations may serve as the basis for\nmore genuinely diverse future poetry generation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation -- can humans distinguish between automatic and\nhuman generated poetry -- we evaluate the diversity of automatically generated\npoetry (with a focus on quatrains), by comparing distributions of generated\npoetry to distributions of human poetry along structural, lexical, semantic and\nstylistic dimensions, assessing different model types (word vs.\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\nunconditioned). We find that current automatic poetry systems are considerably\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\nare semantically too uniform and even do not match the length distribution of\nhuman poetry. Our experiments reveal, however, that style-conditioning and\ncharacter-level modeling clearly increases diversity across virtually all\ndimensions we explore. Our identified limitations may serve as the basis for\nmore genuinely diverse future poetry generation models."
                },
                "authors": [
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Hannes Grner"
                    },
                    {
                        "name": "Sina Zarrie"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "EMNLP 2024 main; camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05572v1",
                "updated": "2024-11-08T13:51:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    51,
                    37,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:51:37Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    51,
                    37,
                    4,
                    313,
                    0
                ],
                "title": "Why These Documents? Explainable Generative Retrieval with Hierarchical\n  Category Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why These Documents? Explainable Generative Retrieval with Hierarchical\n  Category Paths"
                },
                "summary": "Generative retrieval has recently emerged as a new alternative of traditional\ninformation retrieval approaches. However, existing generative retrieval\nmethods directly decode docid when a query is given, making it impossible to\nprovide users with explanations as an answer for \"Why this document is\nretrieved?\". To address this limitation, we propose Hierarchical Category\nPath-Enhanced Generative Retrieval(HyPE), which enhances explainability by\ngenerating hierarchical category paths step-by-step before decoding docid. HyPE\nleverages hierarchical category paths as explanation, progressing from broad to\nspecific semantic categories. This approach enables diverse explanations for\nthe same document depending on the query by using shared category paths between\nthe query and the document, and provides reasonable explanation by reflecting\nthe document's semantic structure through a coarse-to-fine manner. HyPE\nconstructs category paths with external high-quality semantic hierarchy,\nleverages LLM to select appropriate candidate paths for each document, and\noptimizes the generative retrieval model with path-augmented dataset. During\ninference, HyPE utilizes path-aware reranking strategy to aggregate diverse\ntopic information, allowing the most relevant documents to be prioritized in\nthe final ranked list of docids. Our extensive experiments demonstrate that\nHyPE not only offers a high level of explainability but also improves the\nretrieval performance in the document retrieval task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval has recently emerged as a new alternative of traditional\ninformation retrieval approaches. However, existing generative retrieval\nmethods directly decode docid when a query is given, making it impossible to\nprovide users with explanations as an answer for \"Why this document is\nretrieved?\". To address this limitation, we propose Hierarchical Category\nPath-Enhanced Generative Retrieval(HyPE), which enhances explainability by\ngenerating hierarchical category paths step-by-step before decoding docid. HyPE\nleverages hierarchical category paths as explanation, progressing from broad to\nspecific semantic categories. This approach enables diverse explanations for\nthe same document depending on the query by using shared category paths between\nthe query and the document, and provides reasonable explanation by reflecting\nthe document's semantic structure through a coarse-to-fine manner. HyPE\nconstructs category paths with external high-quality semantic hierarchy,\nleverages LLM to select appropriate candidate paths for each document, and\noptimizes the generative retrieval model with path-augmented dataset. During\ninference, HyPE utilizes path-aware reranking strategy to aggregate diverse\ntopic information, allowing the most relevant documents to be prioritized in\nthe final ranked list of docids. Our extensive experiments demonstrate that\nHyPE not only offers a high level of explainability but also improves the\nretrieval performance in the document retrieval task."
                },
                "authors": [
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Susik Yoon"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00686v3",
                "updated": "2024-11-08T13:40:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    40,
                    37,
                    4,
                    313,
                    0
                ],
                "published": "2024-02-01T15:48:40Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    48,
                    40,
                    3,
                    32,
                    0
                ],
                "title": "Maximum a posteriori testing in statistical inverse problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum a posteriori testing in statistical inverse problems"
                },
                "summary": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations."
                },
                "authors": [
                    {
                        "name": "Remo Kretschmann"
                    },
                    {
                        "name": "Frank Werner"
                    }
                ],
                "author_detail": {
                    "name": "Frank Werner"
                },
                "author": "Frank Werner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "47A52, 62F15, 62G10, 62G20, 65J20, 65J22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05724v2",
                "updated": "2024-11-08T13:30:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    30,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-04-08T17:59:02Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    17,
                    59,
                    2,
                    0,
                    99,
                    0
                ],
                "title": "JADES: Primaeval Lyman-$\\mathrm$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES: Primaeval Lyman-$\\mathrm$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$"
                },
                "summary": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation."
                },
                "authors": [
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Kristan Boyett"
                    },
                    {
                        "name": "Alex J. Cameron"
                    },
                    {
                        "name": "Phillip A. Cargile"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stphane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Ryan Hausen"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Isaac Laseter"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Chris Willott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Willott"
                },
                "author": "Chris Willott",
                "arxiv_comment": "25 pages, 13 figures, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05556v1",
                "updated": "2024-11-08T13:24:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs"
                },
                "summary": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict . Greta\nbuilds on Tensorflow, making it comparatively easy to take advantage of the\nsignificant gain in speed offered by GPUs. In these complex spatio-temporal\nmodels, we show a reduction of up to 70\\% in computational time relative to\nfitting the same models on CPUs. We show how the choice of covariance kernel\nimpacts the ability to infer spread and extrapolate to unobserved spatial and\ntemporal units. The inference pipeline is applied to weekly incidence data on\ntuberculosis in the East and West Midlands regions of England over a period of\ntwo years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict . Greta\nbuilds on Tensorflow, making it comparatively easy to take advantage of the\nsignificant gain in speed offered by GPUs. In these complex spatio-temporal\nmodels, we show a reduction of up to 70\\% in computational time relative to\nfitting the same models on CPUs. We show how the choice of covariance kernel\nimpacts the ability to infer spread and extrapolate to unobserved spatial and\ntemporal units. The inference pipeline is applied to weekly incidence data on\ntuberculosis in the East and West Midlands regions of England over a period of\ntwo years."
                },
                "authors": [
                    {
                        "name": "Eva Gunn"
                    },
                    {
                        "name": "Nikhil Sengupta"
                    },
                    {
                        "name": "Ben Swallow"
                    }
                ],
                "author_detail": {
                    "name": "Ben Swallow"
                },
                "author": "Ben Swallow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21333v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21333v3",
                "updated": "2024-11-08T13:11:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    11,
                    58,
                    4,
                    313,
                    0
                ],
                "published": "2024-10-27T18:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    18,
                    30,
                    41,
                    6,
                    301,
                    0
                ],
                "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse"
                },
                "summary": "Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning."
                },
                "authors": [
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Addison J. Wu"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Tania Lombrozo"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21333v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21333v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05547v1",
                "updated": "2024-11-08T13:09:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "title": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance."
                },
                "authors": [
                    {
                        "name": "Geonmin Kim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Hancheol Park"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05540v1",
                "updated": "2024-11-08T12:55:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    55,
                    4,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:55:04Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    55,
                    4,
                    4,
                    313,
                    0
                ],
                "title": "CRepair: CVAE-based Automatic Vulnerability Repair Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRepair: CVAE-based Automatic Vulnerability Repair Technology"
                },
                "summary": "Software vulnerabilities are flaws in computer software systems that pose\nsignificant threats to the integrity, security, and reliability of modern\nsoftware and its application data. These vulnerabilities can lead to\nsubstantial economic losses across various industries. Manual vulnerability\nrepair is not only time-consuming but also prone to errors. To address the\nchallenges of vulnerability repair, researchers have proposed various\nsolutions, with learning-based automatic vulnerability repair techniques\ngaining widespread attention. However, existing methods often focus on learning\nmore vulnerability data to improve repair outcomes, while neglecting the\ndiverse characteristics of vulnerable code, and suffer from imprecise\nvulnerability localization.To address these shortcomings, this paper proposes\nCRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing\nsecurity vulnerabilities in system code. We first preprocess the vulnerability\ndata using a prompt-based method to serve as input to the model. Then, we apply\ncausal inference techniques to map the vulnerability feature data to\nprobability distributions. By employing multi-sample feature fusion, we capture\ndiverse vulnerability feature information. Finally, conditional control is used\nto guide the model in repairing the vulnerabilities.Experimental results\ndemonstrate that the proposed method significantly outperforms other benchmark\nmodels, achieving a perfect repair rate of 52%. The effectiveness of the\napproach is validated from multiple perspectives, advancing AI-driven code\nvulnerability repair and showing promising applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities are flaws in computer software systems that pose\nsignificant threats to the integrity, security, and reliability of modern\nsoftware and its application data. These vulnerabilities can lead to\nsubstantial economic losses across various industries. Manual vulnerability\nrepair is not only time-consuming but also prone to errors. To address the\nchallenges of vulnerability repair, researchers have proposed various\nsolutions, with learning-based automatic vulnerability repair techniques\ngaining widespread attention. However, existing methods often focus on learning\nmore vulnerability data to improve repair outcomes, while neglecting the\ndiverse characteristics of vulnerable code, and suffer from imprecise\nvulnerability localization.To address these shortcomings, this paper proposes\nCRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing\nsecurity vulnerabilities in system code. We first preprocess the vulnerability\ndata using a prompt-based method to serve as input to the model. Then, we apply\ncausal inference techniques to map the vulnerability feature data to\nprobability distributions. By employing multi-sample feature fusion, we capture\ndiverse vulnerability feature information. Finally, conditional control is used\nto guide the model in repairing the vulnerabilities.Experimental results\ndemonstrate that the proposed method significantly outperforms other benchmark\nmodels, achieving a perfect repair rate of 52%. The effectiveness of the\napproach is validated from multiple perspectives, advancing AI-driven code\nvulnerability repair and showing promising applications."
                },
                "authors": [
                    {
                        "name": "Penghui Liu"
                    },
                    {
                        "name": "Yingzhou Bi"
                    },
                    {
                        "name": "Jiangtao Huang"
                    },
                    {
                        "name": "Xinxin Jiang"
                    },
                    {
                        "name": "Lianmei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lianmei Wang"
                },
                "author": "Lianmei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16254v2",
                "updated": "2024-11-08T12:54:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    54,
                    16,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-24T01:31:03Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    1,
                    31,
                    3,
                    0,
                    176,
                    0
                ],
                "title": "Confidence Regulation Neurons in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Regulation Neurons in Language Models"
                },
                "summary": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences."
                },
                "authors": [
                    {
                        "name": "Alessandro Stolfo"
                    },
                    {
                        "name": "Ben Wu"
                    },
                    {
                        "name": "Wes Gurnee"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Xingyi Song"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17044v2",
                "updated": "2024-11-08T12:44:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    44,
                    49,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-25T15:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not"
                },
                "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM."
                },
                "authors": [
                    {
                        "name": "Francesco Verdini"
                    },
                    {
                        "name": "Pierfrancesco Melucci"
                    },
                    {
                        "name": "Stefano Perna"
                    },
                    {
                        "name": "Francesco Cariaggi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Szymon Mazurek"
                    },
                    {
                        "name": "Marek Kasztelnik"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Sbastien Bratires"
                    },
                    {
                        "name": "Paolo Merialdo"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05533v1",
                "updated": "2024-11-08T12:42:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    42,
                    45,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:42:45Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    42,
                    45,
                    4,
                    313,
                    0
                ],
                "title": "Analyzing Logs of Large-Scale Software Systems using Time Curves\n  Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Logs of Large-Scale Software Systems using Time Curves\n  Visualization"
                },
                "summary": "Logs are crucial for analyzing large-scale software systems, offering\ninsights into system health, performance, security threats, potential bugs,\netc. However, their chaotic nature$\\unicode{x2013}$characterized by sheer\nvolume, lack of standards, and variability$\\unicode{x2013}$makes manual\nanalysis complex. The use of clustering algorithms can assist by grouping logs\ninto a smaller set of templates, but lose the temporal and relational context\nin doing so. On the contrary, Large Language Models (LLMs) can provide\nmeaningful explanations but struggle with processing large collections\nefficiently. Moreover, representation techniques for both approaches are\ntypically limited to either plain text or traditional charting, especially when\ndealing with large-scale systems. In this paper, we combine clustering and LLM\nsummarization with event detection and Multidimensional Scaling through the use\nof Time Curves to produce a holistic pipeline that enables efficient and\nautomatic summarization of vast collections of software system logs. The core\nof our approach is the proposal of a semimetric distance that effectively\nmeasures similarity between events, thus enabling a meaningful representation.\nWe show that our method can explain the main events of logs collected from\ndifferent applications without prior knowledge. We also show how the approach\ncan be used to detect general trends as well as outliers in parallel and\ndistributed systems by overlapping multiple projections. As a result, we expect\na significant reduction of the time required to analyze and resolve system-wide\nissues, identify performance bottlenecks and security risks, debug\napplications, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs are crucial for analyzing large-scale software systems, offering\ninsights into system health, performance, security threats, potential bugs,\netc. However, their chaotic nature$\\unicode{x2013}$characterized by sheer\nvolume, lack of standards, and variability$\\unicode{x2013}$makes manual\nanalysis complex. The use of clustering algorithms can assist by grouping logs\ninto a smaller set of templates, but lose the temporal and relational context\nin doing so. On the contrary, Large Language Models (LLMs) can provide\nmeaningful explanations but struggle with processing large collections\nefficiently. Moreover, representation techniques for both approaches are\ntypically limited to either plain text or traditional charting, especially when\ndealing with large-scale systems. In this paper, we combine clustering and LLM\nsummarization with event detection and Multidimensional Scaling through the use\nof Time Curves to produce a holistic pipeline that enables efficient and\nautomatic summarization of vast collections of software system logs. The core\nof our approach is the proposal of a semimetric distance that effectively\nmeasures similarity between events, thus enabling a meaningful representation.\nWe show that our method can explain the main events of logs collected from\ndifferent applications without prior knowledge. We also show how the approach\ncan be used to detect general trends as well as outliers in parallel and\ndistributed systems by overlapping multiple projections. As a result, we expect\na significant reduction of the time required to analyze and resolve system-wide\nissues, identify performance bottlenecks and security risks, debug\napplications, etc."
                },
                "authors": [
                    {
                        "name": "Dmytro Borysenkov"
                    },
                    {
                        "name": "Adriano Vogel"
                    },
                    {
                        "name": "Sren Henning"
                    },
                    {
                        "name": "Esteban Perez-Wohlfeil"
                    }
                ],
                "author_detail": {
                    "name": "Esteban Perez-Wohlfeil"
                },
                "author": "Esteban Perez-Wohlfeil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05521v1",
                "updated": "2024-11-08T12:27:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:27:13Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark"
                },
                "summary": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases."
                },
                "authors": [
                    {
                        "name": "Sithursan Sivasubramaniam"
                    },
                    {
                        "name": "Cedric Osei-Akoto"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    },
                    {
                        "name": "Jonathan Fuerst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fuerst"
                },
                "author": "Jonathan Fuerst",
                "arxiv_comment": "NeurIPS 2024 Track Datasets and Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05515v1",
                "updated": "2024-11-08T12:20:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    20,
                    58,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:20:58Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    20,
                    58,
                    4,
                    313,
                    0
                ],
                "title": "The robustness of inferred envelope and core rotation rates of red-giant\n  stars from asteroseismology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of inferred envelope and core rotation rates of red-giant\n  stars from asteroseismology"
                },
                "summary": "Rotation is an important, yet poorly-modelled phenomenon of stellar structure\nand evolution. Accurate estimates of internal rotation rates are therefore\nvaluable for constraining stellar evolution models. We aim to assess the\naccuracy of asteroseismic estimates of internal rotation rates and how these\ndepend on the fundamental stellar parameters. We apply the recently-developed\nmethod called extended-MOLA inversions to infer localised estimates of internal\nrotation rates of synthetic observations of red giants. We search for suitable\nreference stellar models following a grid-based approach, and assess the\nrobustness of the resulting inferences to the choice of reference model. We\nfind that matching the mixed mode pattern between the observation and the\nreference model is an important criterion to select suitable reference models.\nWe propose to i) select a set of reference models based on the correlation\nbetween the observed rotational splittings and the mode-trapping parameter ii)\ncompute rotation rates for all these models iii) use the mean value obtained\nacross the whole set as the estimate of the internal rotation rates. We find\nthat the effect of a near surface perturbation in the synthetic observations on\nthe rotation rates estimated based on the correlation between the observed\nrotational splittings and the mode-trapping parameter is negligible. We\nconclude that when using an ensemble of reference models, constructed based on\nmatching the mixed mode pattern, the input rotation rates can be recovered\nacross a range of fundamental stellar parameters like mass, mixing-length\nparameter and composition. Further, red-giant rotation rates determined in this\nway are also independent of a near surface perturbation of stellar structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotation is an important, yet poorly-modelled phenomenon of stellar structure\nand evolution. Accurate estimates of internal rotation rates are therefore\nvaluable for constraining stellar evolution models. We aim to assess the\naccuracy of asteroseismic estimates of internal rotation rates and how these\ndepend on the fundamental stellar parameters. We apply the recently-developed\nmethod called extended-MOLA inversions to infer localised estimates of internal\nrotation rates of synthetic observations of red giants. We search for suitable\nreference stellar models following a grid-based approach, and assess the\nrobustness of the resulting inferences to the choice of reference model. We\nfind that matching the mixed mode pattern between the observation and the\nreference model is an important criterion to select suitable reference models.\nWe propose to i) select a set of reference models based on the correlation\nbetween the observed rotational splittings and the mode-trapping parameter ii)\ncompute rotation rates for all these models iii) use the mean value obtained\nacross the whole set as the estimate of the internal rotation rates. We find\nthat the effect of a near surface perturbation in the synthetic observations on\nthe rotation rates estimated based on the correlation between the observed\nrotational splittings and the mode-trapping parameter is negligible. We\nconclude that when using an ensemble of reference models, constructed based on\nmatching the mixed mode pattern, the input rotation rates can be recovered\nacross a range of fundamental stellar parameters like mass, mixing-length\nparameter and composition. Further, red-giant rotation rates determined in this\nway are also independent of a near surface perturbation of stellar structure."
                },
                "authors": [
                    {
                        "name": "F. Ahlborn"
                    },
                    {
                        "name": "E. P. Bellinger"
                    },
                    {
                        "name": "S. Hekker"
                    },
                    {
                        "name": "S. Basu"
                    },
                    {
                        "name": "D. Mokrytska"
                    }
                ],
                "author_detail": {
                    "name": "D. Mokrytska"
                },
                "author": "D. Mokrytska",
                "arxiv_comment": "23 pages, 16 figures, Accepted for publication in Astronomy and\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05508v1",
                "updated": "2024-11-08T12:08:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    8,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:08:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    8,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "An Early FIRST Reproduction and Improvements to Single-Token Decoding\n  for Fast Listwise Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Early FIRST Reproduction and Improvements to Single-Token Decoding\n  for Fast Listwise Reranking"
                },
                "summary": "Recent advances have demonstrated that large language models (LLMs) excel as\nlistwise rerankers, but their high computational demands remain a barrier to\nwidespread adoption. Further, the traditional language modeling (LM) objective\nis not ideally suited for reranking tasks. FIRST is a novel approach that\naddresses these challenges by integrating a learning-to-rank objective and\nleveraging the logits of only the first generated token, thereby significantly\nreducing inference latency compared to traditional LLM rerankers. In this\nstudy, we extend the evaluation of FIRST to the TREC Deep Learning datasets\n(DL19-22), validating its robustness across diverse domains. We investigate the\ninfluence of different first-stage retrievers on FIRST rerankers, observing\ndiminishing returns and patterns consistent with traditional LLM rerankers.\nThrough applying the FIRST objective to a broader range of backbone models, we\nachieve effectiveness surpassing the original implementation. Our experiments\nconfirm that fast reranking with single-token logits does not compromise\nout-of-domain reranking quality. To better quantify the computational savings\nin the original study, we measure and compare latency to find a 21%-42% gain\nacross various models and benchmarks. Moreover, while LM training implicitly\nimproves zero-shot single-token reranking, our experiments also raise questions\nabout whether LM pre-training may hinder subsequent fine-tuning with the FIRST\nobjective. These findings pave the way for more efficient and effective\nlistwise reranking in future applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have demonstrated that large language models (LLMs) excel as\nlistwise rerankers, but their high computational demands remain a barrier to\nwidespread adoption. Further, the traditional language modeling (LM) objective\nis not ideally suited for reranking tasks. FIRST is a novel approach that\naddresses these challenges by integrating a learning-to-rank objective and\nleveraging the logits of only the first generated token, thereby significantly\nreducing inference latency compared to traditional LLM rerankers. In this\nstudy, we extend the evaluation of FIRST to the TREC Deep Learning datasets\n(DL19-22), validating its robustness across diverse domains. We investigate the\ninfluence of different first-stage retrievers on FIRST rerankers, observing\ndiminishing returns and patterns consistent with traditional LLM rerankers.\nThrough applying the FIRST objective to a broader range of backbone models, we\nachieve effectiveness surpassing the original implementation. Our experiments\nconfirm that fast reranking with single-token logits does not compromise\nout-of-domain reranking quality. To better quantify the computational savings\nin the original study, we measure and compare latency to find a 21%-42% gain\nacross various models and benchmarks. Moreover, while LM training implicitly\nimproves zero-shot single-token reranking, our experiments also raise questions\nabout whether LM pre-training may hinder subsequent fine-tuning with the FIRST\nobjective. These findings pave the way for more efficient and effective\nlistwise reranking in future applications."
                },
                "authors": [
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05504v1",
                "updated": "2024-11-08T12:03:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    36,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:03:36Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    36,
                    4,
                    313,
                    0
                ],
                "title": "LBPE: Long-token-first Tokenization to Improve Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBPE: Long-token-first Tokenization to Improve Large Language Models"
                },
                "summary": "The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)\nfacilitates robust handling of subword units and avoids issues of\nout-of-vocabulary words. Despite its success, a critical challenge persists:\nlong tokens, rich in semantic information, have fewer occurrences in tokenized\ndatasets compared to short tokens, which can result in imbalanced learning\nissue across different tokens. To address that, we propose LBPE, which\nprioritizes long tokens during the encoding process. LBPE generates tokens\naccording to their reverse ranks of token length rather than their ranks in the\nvocabulary, granting longer tokens higher priority during the encoding process.\nConsequently, LBPE smooths the frequency differences between short and long\ntokens, and thus mitigates the learning imbalance. Extensive experiments across\ndiverse language modeling tasks demonstrate that LBPE consistently outperforms\nthe original BPE, well demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)\nfacilitates robust handling of subword units and avoids issues of\nout-of-vocabulary words. Despite its success, a critical challenge persists:\nlong tokens, rich in semantic information, have fewer occurrences in tokenized\ndatasets compared to short tokens, which can result in imbalanced learning\nissue across different tokens. To address that, we propose LBPE, which\nprioritizes long tokens during the encoding process. LBPE generates tokens\naccording to their reverse ranks of token length rather than their ranks in the\nvocabulary, granting longer tokens higher priority during the encoding process.\nConsequently, LBPE smooths the frequency differences between short and long\ntokens, and thus mitigates the learning imbalance. Extensive experiments across\ndiverse language modeling tasks demonstrate that LBPE consistently outperforms\nthe original BPE, well demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jianwei Niu"
                    },
                    {
                        "name": "Shasha Mo"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.17808",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05503v1",
                "updated": "2024-11-08T12:03:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    31,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:03:31Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    31,
                    4,
                    313,
                    0
                ],
                "title": "KyrgyzNLP: Challenges, Progress, and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KyrgyzNLP: Challenges, Progress, and Future"
                },
                "summary": "Large language models (LLMs) have excelled in numerous benchmarks, advancing\nAI applications in both linguistic and non-linguistic tasks. However, this has\nprimarily benefited well-resourced languages, leaving less-resourced ones\n(LRLs) at a disadvantage. In this paper, we highlight the current state of the\nNLP field in the specific LRL: kyrgyz tili.\n  Human evaluation, including annotated datasets created by native speakers,\nremains an irreplaceable component of reliable NLP performance, especially for\nLRLs where automatic evaluations can fall short. In recent assessments of the\nresources for Turkic languages, Kyrgyz is labeled with the status 'Scraping\nBy', a severely under-resourced language spoken by millions. This is concerning\ngiven the growing importance of the language, not only in Kyrgyzstan but also\namong diaspora communities where it holds no official status.\n  We review prior efforts in the field, noting that many of the publicly\navailable resources have only recently been developed, with few exceptions\nbeyond dictionaries (the processed data used for the analysis is presented at\nhttps://kyrgyznlp.github.io/). While recent papers have made some headway, much\nmore remains to be done. Despite interest and support from both business and\ngovernment sectors in the Kyrgyz Republic, the situation for Kyrgyz language\nresources remains challenging. We stress the importance of community-driven\nefforts to build these resources, ensuring the future advancement\nsustainability. We then share our view of the most pressing challenges in\nKyrgyz NLP. Finally, we propose a roadmap for future development in terms of\nresearch topics and language resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in numerous benchmarks, advancing\nAI applications in both linguistic and non-linguistic tasks. However, this has\nprimarily benefited well-resourced languages, leaving less-resourced ones\n(LRLs) at a disadvantage. In this paper, we highlight the current state of the\nNLP field in the specific LRL: kyrgyz tili.\n  Human evaluation, including annotated datasets created by native speakers,\nremains an irreplaceable component of reliable NLP performance, especially for\nLRLs where automatic evaluations can fall short. In recent assessments of the\nresources for Turkic languages, Kyrgyz is labeled with the status 'Scraping\nBy', a severely under-resourced language spoken by millions. This is concerning\ngiven the growing importance of the language, not only in Kyrgyzstan but also\namong diaspora communities where it holds no official status.\n  We review prior efforts in the field, noting that many of the publicly\navailable resources have only recently been developed, with few exceptions\nbeyond dictionaries (the processed data used for the analysis is presented at\nhttps://kyrgyznlp.github.io/). While recent papers have made some headway, much\nmore remains to be done. Despite interest and support from both business and\ngovernment sectors in the Kyrgyz Republic, the situation for Kyrgyz language\nresources remains challenging. We stress the importance of community-driven\nefforts to build these resources, ensuring the future advancement\nsustainability. We then share our view of the most pressing challenges in\nKyrgyz NLP. Finally, we propose a roadmap for future development in terms of\nresearch topics and language resources."
                },
                "authors": [
                    {
                        "name": "Anton Alekseev"
                    },
                    {
                        "name": "Timur Turatali"
                    }
                ],
                "author_detail": {
                    "name": "Timur Turatali"
                },
                "author": "Timur Turatali",
                "arxiv_comment": "Keynote talk at the 12th International Conference on Analysis of\n  Images, Social Networks and Texts (AIST-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05487v1",
                "updated": "2024-11-08T11:35:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    35,
                    38,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T11:35:38Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    35,
                    38,
                    4,
                    313,
                    0
                ],
                "title": "Estimating location parameters of two exponential distributions with\n  ordered scale parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating location parameters of two exponential distributions with\n  ordered scale parameters"
                },
                "summary": "In the usual statistical inference problem, we estimate an unknown parameter\nof a statistical model using the information in the random sample. A priori\ninformation about the parameter is also known in several real-life situations.\nOne such information is order restriction between the parameters. This prior\nformation improves the estimation quality. In this paper, we deal with the\ncomponent-wise estimation of location parameters of two exponential\ndistributions studied with ordered scale parameters under a bowl-shaped affine\ninvariant loss function and generalized Pitman closeness criterion. We have\nshown that several benchmark estimators, such as maximum likelihood estimators\n(MLE), uniformly minimum variance unbiased estimators (UMVUE), and best affine\nequivariant estimators (BAEE), are inadmissible. We have given sufficient\nconditions under which the dominating estimators are derived. Under the\ngeneralized Pitman closeness criterion, a Stein-type improved estimator is\nproposed. As an application, we have considered special sampling schemes such\nas type-II censoring, progressive type-II censoring, and record values.\nFinally, we perform a simulation study to compare the risk performance of the\nimproved estimators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the usual statistical inference problem, we estimate an unknown parameter\nof a statistical model using the information in the random sample. A priori\ninformation about the parameter is also known in several real-life situations.\nOne such information is order restriction between the parameters. This prior\nformation improves the estimation quality. In this paper, we deal with the\ncomponent-wise estimation of location parameters of two exponential\ndistributions studied with ordered scale parameters under a bowl-shaped affine\ninvariant loss function and generalized Pitman closeness criterion. We have\nshown that several benchmark estimators, such as maximum likelihood estimators\n(MLE), uniformly minimum variance unbiased estimators (UMVUE), and best affine\nequivariant estimators (BAEE), are inadmissible. We have given sufficient\nconditions under which the dominating estimators are derived. Under the\ngeneralized Pitman closeness criterion, a Stein-type improved estimator is\nproposed. As an application, we have considered special sampling schemes such\nas type-II censoring, progressive type-II censoring, and record values.\nFinally, we perform a simulation study to compare the risk performance of the\nimproved estimators"
                },
                "authors": [
                    {
                        "name": "Lakshmi Kanta Patra"
                    },
                    {
                        "name": "Constantinos Petropoulos"
                    },
                    {
                        "name": "Shrajal Bajpai"
                    },
                    {
                        "name": "Naresh Garg"
                    }
                ],
                "author_detail": {
                    "name": "Naresh Garg"
                },
                "author": "Naresh Garg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05479v1",
                "updated": "2024-11-08T11:09:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    9,
                    45,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T11:09:45Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    9,
                    45,
                    4,
                    313,
                    0
                ],
                "title": "EUREKHA: Enhancing User Representation for Key Hackers Identification in\n  Underground Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EUREKHA: Enhancing User Representation for Key Hackers Identification in\n  Underground Forums"
                },
                "summary": "Underground forums serve as hubs for cybercriminal activities, offering a\nspace for anonymity and evasion of conventional online oversight. In these\nhidden communities, malicious actors collaborate to exchange illicit knowledge,\ntools, and tactics, driving a range of cyber threats from hacking techniques to\nthe sale of stolen data, malware, and zero-day exploits. Identifying the key\ninstigators (i.e., key hackers), behind these operations is essential but\nremains a complex challenge. This paper presents a novel method called EUREKHA\n(Enhancing User Representation for Key Hacker Identification in Underground\nForums), designed to identify these key hackers by modeling each user as a\ntextual sequence. This sequence is processed through a large language model\n(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.\nThese extracted features are then fed into a Graph Neural Network (GNN) to\nmodel user structural relationships, significantly improving identification\naccuracy. Furthermore, we employ BERTopic (Bidirectional Encoder\nRepresentations from Transformers Topic Modeling) to extract personalized\ntopics from user-generated content, enabling multiple textual representations\nper user and optimizing the selection of the most representative sequence. Our\nstudy demonstrates that fine-tuned LLMs outperform state-of-the-art methods in\nidentifying key hackers. Additionally, when combined with GNNs, our model\nachieves significant improvements, resulting in approximately 6% and 10%\nincreases in accuracy and F1-score, respectively, over existing methods.\nEUREKHA was tested on the Hack-Forums dataset, and we provide open-source\naccess to our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underground forums serve as hubs for cybercriminal activities, offering a\nspace for anonymity and evasion of conventional online oversight. In these\nhidden communities, malicious actors collaborate to exchange illicit knowledge,\ntools, and tactics, driving a range of cyber threats from hacking techniques to\nthe sale of stolen data, malware, and zero-day exploits. Identifying the key\ninstigators (i.e., key hackers), behind these operations is essential but\nremains a complex challenge. This paper presents a novel method called EUREKHA\n(Enhancing User Representation for Key Hacker Identification in Underground\nForums), designed to identify these key hackers by modeling each user as a\ntextual sequence. This sequence is processed through a large language model\n(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.\nThese extracted features are then fed into a Graph Neural Network (GNN) to\nmodel user structural relationships, significantly improving identification\naccuracy. Furthermore, we employ BERTopic (Bidirectional Encoder\nRepresentations from Transformers Topic Modeling) to extract personalized\ntopics from user-generated content, enabling multiple textual representations\nper user and optimizing the selection of the most representative sequence. Our\nstudy demonstrates that fine-tuned LLMs outperform state-of-the-art methods in\nidentifying key hackers. Additionally, when combined with GNNs, our model\nachieves significant improvements, resulting in approximately 6% and 10%\nincreases in accuracy and F1-score, respectively, over existing methods.\nEUREKHA was tested on the Hack-Forums dataset, and we provide open-source\naccess to our code."
                },
                "authors": [
                    {
                        "name": "Abdoul Nasser Hassane Amadou"
                    },
                    {
                        "name": "Anas Motii"
                    },
                    {
                        "name": "Saida Elouardi"
                    },
                    {
                        "name": "EL Houcine Bergou"
                    }
                ],
                "author_detail": {
                    "name": "EL Houcine Bergou"
                },
                "author": "EL Houcine Bergou",
                "arxiv_comment": "Accepted at IEEE Trustcom 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05474v1",
                "updated": "2024-11-08T11:00:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T11:00:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction"
                },
                "summary": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution."
                },
                "authors": [
                    {
                        "name": "miland Garrab"
                    },
                    {
                        "name": "Pierre Teixeira"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stphane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Doncieux"
                },
                "author": "Stphane Doncieux",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05472v1",
                "updated": "2024-11-08T10:53:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    53,
                    39,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T10:53:39Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    53,
                    39,
                    4,
                    313,
                    0
                ],
                "title": "Bridging the Gap between Learning and Inference for Diffusion-Based\n  Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap between Learning and Inference for Diffusion-Based\n  Molecule Generation"
                },
                "summary": "The efficacy of diffusion models in generating a spectrum of data modalities,\nincluding images, text, and videos, has spurred inquiries into their utility in\nmolecular generation, yielding significant advancements in the field. However,\nthe molecular generation process with diffusion models involves multiple\nautoregressive steps over a finite time horizon, leading to exposure bias\nissues inherently. To address the exposure bias issue, we propose a training\nframework named GapDiff. The core idea of GapDiff is to utilize model-predicted\nconformations as ground truth probabilistically during training, aiming to\nmitigate the data distributional disparity between training and inference,\nthereby enhancing the affinity of generated molecules. We conduct experiments\nusing a 3D molecular generation model on the CrossDocked2020 dataset, and the\nvina energy and diversity demonstrate the potency of our framework with\nsuperior affinity. GapDiff is available at\n\\url{https://github.com/HUGHNew/gapdiff}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of diffusion models in generating a spectrum of data modalities,\nincluding images, text, and videos, has spurred inquiries into their utility in\nmolecular generation, yielding significant advancements in the field. However,\nthe molecular generation process with diffusion models involves multiple\nautoregressive steps over a finite time horizon, leading to exposure bias\nissues inherently. To address the exposure bias issue, we propose a training\nframework named GapDiff. The core idea of GapDiff is to utilize model-predicted\nconformations as ground truth probabilistically during training, aiming to\nmitigate the data distributional disparity between training and inference,\nthereby enhancing the affinity of generated molecules. We conduct experiments\nusing a 3D molecular generation model on the CrossDocked2020 dataset, and the\nvina energy and diversity demonstrate the potency of our framework with\nsuperior affinity. GapDiff is available at\n\\url{https://github.com/HUGHNew/gapdiff}."
                },
                "authors": [
                    {
                        "name": "Peidong Liu"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Xue Zhe"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Xianggen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianggen Liu"
                },
                "author": "Xianggen Liu",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17904v2",
                "updated": "2024-11-08T10:45:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    45,
                    12,
                    4,
                    313,
                    0
                ],
                "published": "2024-01-31T15:10:29Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    15,
                    10,
                    29,
                    2,
                    31,
                    0
                ],
                "title": "Hi-SAM: Marrying Segment Anything Model for Hierarchical Text\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hi-SAM: Marrying Segment Anything Model for Hierarchical Text\n  Segmentation"
                },
                "summary": "The Segment Anything Model (SAM), a profound vision foundation model\npretrained on a large-scale dataset, breaks the boundaries of general\nsegmentation and sparks various downstream applications. This paper introduces\nHi-SAM, a unified model leveraging SAM for hierarchical text segmentation.\nHi-SAM excels in segmentation across four hierarchies, including pixel-level\ntext, word, text-line, and paragraph, while realizing layout analysis as well.\nSpecifically, we first turn SAM into a high-quality pixel-level text\nsegmentation (TS) model through a parameter-efficient fine-tuning approach. We\nuse this TS model to iteratively generate the pixel-level text labels in a\nsemi-automatical manner, unifying labels across the four text hierarchies in\nthe HierText dataset. Subsequently, with these complete labels, we launch the\nend-to-end trainable Hi-SAM based on the TS architecture with a customized\nhierarchical mask decoder. During inference, Hi-SAM offers both automatic mask\ngeneration (AMG) mode and promptable segmentation (PS) mode. In the AMG mode,\nHi-SAM segments pixel-level text foreground masks initially, then samples\nforeground points for hierarchical text mask generation and achieves layout\nanalysis in passing. As for the PS mode, Hi-SAM provides word, text-line, and\nparagraph masks with a single point click. Experimental results show the\nstate-of-the-art performance of our TS model: 84.86% fgIOU on Total-Text and\n88.96% fgIOU on TextSeg for pixel-level text segmentation. Moreover, compared\nto the previous specialist for joint hierarchical detection and layout analysis\non HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on\nthe text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout\nanalysis, requiring $20\\times$ fewer training epochs. The code is available at\nhttps://github.com/ymy-k/Hi-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM), a profound vision foundation model\npretrained on a large-scale dataset, breaks the boundaries of general\nsegmentation and sparks various downstream applications. This paper introduces\nHi-SAM, a unified model leveraging SAM for hierarchical text segmentation.\nHi-SAM excels in segmentation across four hierarchies, including pixel-level\ntext, word, text-line, and paragraph, while realizing layout analysis as well.\nSpecifically, we first turn SAM into a high-quality pixel-level text\nsegmentation (TS) model through a parameter-efficient fine-tuning approach. We\nuse this TS model to iteratively generate the pixel-level text labels in a\nsemi-automatical manner, unifying labels across the four text hierarchies in\nthe HierText dataset. Subsequently, with these complete labels, we launch the\nend-to-end trainable Hi-SAM based on the TS architecture with a customized\nhierarchical mask decoder. During inference, Hi-SAM offers both automatic mask\ngeneration (AMG) mode and promptable segmentation (PS) mode. In the AMG mode,\nHi-SAM segments pixel-level text foreground masks initially, then samples\nforeground points for hierarchical text mask generation and achieves layout\nanalysis in passing. As for the PS mode, Hi-SAM provides word, text-line, and\nparagraph masks with a single point click. Experimental results show the\nstate-of-the-art performance of our TS model: 84.86% fgIOU on Total-Text and\n88.96% fgIOU on TextSeg for pixel-level text segmentation. Moreover, compared\nto the previous specialist for joint hierarchical detection and layout analysis\non HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on\nthe text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout\nanalysis, requiring $20\\times$ fewer training epochs. The code is available at\nhttps://github.com/ymy-k/Hi-SAM."
                },
                "authors": [
                    {
                        "name": "Maoyuan Ye"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Chenyu Liu"
                    },
                    {
                        "name": "Baocai Yin"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by IEEE TPAMI. GitHub repository:\n  https://github.com/ymy-k/Hi-SAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04088v2",
                "updated": "2024-11-08T10:38:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    38,
                    26,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-06T13:58:41Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    13,
                    58,
                    41,
                    3,
                    158,
                    0
                ],
                "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic Uncertainty Propagation for Improved Model-Based Offline\n  Reinforcement Learning"
                },
                "summary": "Current approaches to model-based offline reinforcement learning often\nincorporate uncertainty-based reward penalization to address the distributional\nshift problem. These approaches, commonly known as pessimistic value iteration,\nuse Monte Carlo sampling to estimate the Bellman target to perform temporal\ndifference based policy evaluation. We find out that the randomness caused by\nthis sampling step significantly delays convergence. We present a theoretical\nresult demonstrating the strong dependency of suboptimality on the number of\nMonte Carlo samples taken per Bellman target calculation. Our main contribution\nis a deterministic approximation to the Bellman target that uses progressive\nmoment matching, a method developed originally for deterministic variational\ninference. The resulting algorithm, which we call Moment Matching Offline\nModel-Based Policy Optimization (MOMBO), propagates the uncertainty of the next\nstate through a nonlinear Q-network in a deterministic fashion by approximating\nthe distributions of hidden layer activations by a normal distribution. We show\nthat it is possible to provide tighter guarantees for the suboptimality of\nMOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO\nto converge faster than these approaches in a large set of benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current approaches to model-based offline reinforcement learning often\nincorporate uncertainty-based reward penalization to address the distributional\nshift problem. These approaches, commonly known as pessimistic value iteration,\nuse Monte Carlo sampling to estimate the Bellman target to perform temporal\ndifference based policy evaluation. We find out that the randomness caused by\nthis sampling step significantly delays convergence. We present a theoretical\nresult demonstrating the strong dependency of suboptimality on the number of\nMonte Carlo samples taken per Bellman target calculation. Our main contribution\nis a deterministic approximation to the Bellman target that uses progressive\nmoment matching, a method developed originally for deterministic variational\ninference. The resulting algorithm, which we call Moment Matching Offline\nModel-Based Policy Optimization (MOMBO), propagates the uncertainty of the next\nstate through a nonlinear Q-network in a deterministic fashion by approximating\nthe distributions of hidden layer activations by a normal distribution. We show\nthat it is possible to provide tighter guarantees for the suboptimality of\nMOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO\nto converge faster than these approaches in a large set of benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Abdullah Akgl"
                    },
                    {
                        "name": "Manuel Haumann"
                    },
                    {
                        "name": "Melih Kandemir"
                    }
                ],
                "author_detail": {
                    "name": "Melih Kandemir"
                },
                "author": "Melih Kandemir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v3",
                "updated": "2024-11-08T10:30:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    30,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05451v1",
                "updated": "2024-11-08T09:58:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    58,
                    2,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:58:02Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    58,
                    2,
                    4,
                    313,
                    0
                ],
                "title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM."
                },
                "authors": [
                    {
                        "name": "Shengda Fan"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Yuepeng Fu"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Shuyan Zhang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05442v1",
                "updated": "2024-11-08T09:40:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    40,
                    53,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:40:53Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    40,
                    53,
                    4,
                    313,
                    0
                ],
                "title": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\n  Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\n  Delivery"
                },
                "summary": "In the rapidly evolving landscape of cyber security, intelligent chatbots are\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\nLanguage Processing empower these chatbots to handle user inquiries and deliver\nthreat intelligence. This helps cyber security knowledge readily available to\nboth professionals and the public. Traditional rule-based chatbots often lack\nflexibility and struggle to adapt to user interactions. In contrast, Large\nLanguage Model-based chatbots offer contextually relevant information across\nmultiple domains and adapt to evolving conversational contexts. In this work,\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\ncutting-edge technologies like Large Language Models and Langchain alongside a\nRetrieval-Augmented Generation model to deliver superior capabilities. This\nchatbot gathers information from diverse data sources to create a comprehensive\nknowledge base covering known vulnerabilities, recent cyber attacks, and\nemerging threats. It delivers tailored responses, serving as a primary hub for\ncyber security insights. By providing instant access to relevant information\nand resources, this IntellBot enhances threat intelligence, incident response,\nand overall security posture, saving time and empowering users with knowledge\nof cyber security best practices. Moreover, we analyzed the performance of our\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\nevaluate the RAG model, and all evaluation metrics consistently produced scores\nabove 0.77, highlighting the efficacy of our system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of cyber security, intelligent chatbots are\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\nLanguage Processing empower these chatbots to handle user inquiries and deliver\nthreat intelligence. This helps cyber security knowledge readily available to\nboth professionals and the public. Traditional rule-based chatbots often lack\nflexibility and struggle to adapt to user interactions. In contrast, Large\nLanguage Model-based chatbots offer contextually relevant information across\nmultiple domains and adapt to evolving conversational contexts. In this work,\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\ncutting-edge technologies like Large Language Models and Langchain alongside a\nRetrieval-Augmented Generation model to deliver superior capabilities. This\nchatbot gathers information from diverse data sources to create a comprehensive\nknowledge base covering known vulnerabilities, recent cyber attacks, and\nemerging threats. It delivers tailored responses, serving as a primary hub for\ncyber security insights. By providing instant access to relevant information\nand resources, this IntellBot enhances threat intelligence, incident response,\nand overall security posture, saving time and empowering users with knowledge\nof cyber security best practices. Moreover, we analyzed the performance of our\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\nevaluate the RAG model, and all evaluation metrics consistently produced scores\nabove 0.77, highlighting the efficacy of our system."
                },
                "authors": [
                    {
                        "name": "Dincy R. Arikkat"
                    },
                    {
                        "name": "Abhinav M."
                    },
                    {
                        "name": "Navya Binu"
                    },
                    {
                        "name": "Parvathi M."
                    },
                    {
                        "name": "Navya Biju"
                    },
                    {
                        "name": "K. S. Arunima"
                    },
                    {
                        "name": "Vinod P."
                    },
                    {
                        "name": "Rafidha Rehiman K. A."
                    },
                    {
                        "name": "Mauro Conti"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Conti"
                },
                "author": "Mauro Conti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00720v2",
                "updated": "2024-11-08T09:34:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    34,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-31T15:27:44Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    15,
                    27,
                    44,
                    6,
                    91,
                    0
                ],
                "title": "Kicking time back in black-hole mergers: Ancestral masses, spins, birth\n  recoils and hierarchical-formation viability of GW190521",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kicking time back in black-hole mergers: Ancestral masses, spins, birth\n  recoils and hierarchical-formation viability of GW190521"
                },
                "summary": "Pair-instability supernova (PISN) prevents black-hole formation from stellar\ncollapse within the approximate mass range $M\\in [65,130]M_\\odot$. However,\nsuch black holes may form hierarchically through merging ancestral black holes,\nwhose properties determine those of the ``child'' one: mass, spin, and recoil\nvelocity. Crucially, the child will leave its host environment if its ``birth\nrecoil'' exceeds the corresponding escape velocity, preventing further mergers.\nWe exploit relations between the final recoil and spin of quasi-circular\nblack-hole mergers to obtain posterior probability distributions for the\nhypothetical ancestral masses, spins and birth recoils of the component black\nholes of GW190521. To this, we present a Bayesian framework applicable to\nexisting estimates for the components of black-hole merger observations. We\nconsider both the quasi-circular (generically spinning) analysis performed by\nthe LIGO-Virgo-KAGRA collaboration and the eccentric (aligned-spin) one\nperformed by Romero-Shaw et. al. We evaluate the probability $p_{2g}$ that the\nGW190521 components inferred by these analyses formed from the merger of\nstellar-origin black holes and were retained by their environment. For the\nprimary component, which populates the PISN gap, such scenario is strongly\nsuppressed if GW190521 happened in a Globular Cluster with $p_{2g} \\sim\n10^{-3}$ unless it was quasi-circular and its ancestors had aligned-spins,\nuncharacteristic of hierarchical formation channels, or small spins, which\nyields $p_{2g} \\simeq 10^{-2}$. If GW190521 was eccentric, we obtain $p_{2g}\n\\simeq 0.1$ for any host other than an AGN, and zero for a Globular Cluster. If\nGW190521 was quasi-circular, a Nuclear-Star Cluster origin is possible with\n$p_{2g} \\in (\\sim 0.4 \\sim ,0.8)$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pair-instability supernova (PISN) prevents black-hole formation from stellar\ncollapse within the approximate mass range $M\\in [65,130]M_\\odot$. However,\nsuch black holes may form hierarchically through merging ancestral black holes,\nwhose properties determine those of the ``child'' one: mass, spin, and recoil\nvelocity. Crucially, the child will leave its host environment if its ``birth\nrecoil'' exceeds the corresponding escape velocity, preventing further mergers.\nWe exploit relations between the final recoil and spin of quasi-circular\nblack-hole mergers to obtain posterior probability distributions for the\nhypothetical ancestral masses, spins and birth recoils of the component black\nholes of GW190521. To this, we present a Bayesian framework applicable to\nexisting estimates for the components of black-hole merger observations. We\nconsider both the quasi-circular (generically spinning) analysis performed by\nthe LIGO-Virgo-KAGRA collaboration and the eccentric (aligned-spin) one\nperformed by Romero-Shaw et. al. We evaluate the probability $p_{2g}$ that the\nGW190521 components inferred by these analyses formed from the merger of\nstellar-origin black holes and were retained by their environment. For the\nprimary component, which populates the PISN gap, such scenario is strongly\nsuppressed if GW190521 happened in a Globular Cluster with $p_{2g} \\sim\n10^{-3}$ unless it was quasi-circular and its ancestors had aligned-spins,\nuncharacteristic of hierarchical formation channels, or small spins, which\nyields $p_{2g} \\simeq 10^{-2}$. If GW190521 was eccentric, we obtain $p_{2g}\n\\simeq 0.1$ for any host other than an AGN, and zero for a Globular Cluster. If\nGW190521 was quasi-circular, a Nuclear-Star Cluster origin is possible with\n$p_{2g} \\in (\\sim 0.4 \\sim ,0.8)$"
                },
                "authors": [
                    {
                        "name": "Carlos Arajo lvarez"
                    },
                    {
                        "name": "Henry W. Y. Wong"
                    },
                    {
                        "name": "Anna Liu"
                    },
                    {
                        "name": "Juan Caldern Bustillo"
                    }
                ],
                "author_detail": {
                    "name": "Juan Caldern Bustillo"
                },
                "author": "Juan Caldern Bustillo",
                "arxiv_comment": "20 pages, 7 Figures, Version accepted for publication in The\n  Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05423v1",
                "updated": "2024-11-08T09:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    15,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:15:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    15,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "VISTA: Visual Integrated System for Tailored Automation in Math Problem\n  Generation Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA: Visual Integrated System for Tailored Automation in Math Problem\n  Generation Using LLM"
                },
                "summary": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education."
                },
                "authors": [
                    {
                        "name": "Jeongwoo Lee"
                    },
                    {
                        "name": "Kwangsuk Park"
                    },
                    {
                        "name": "Jihyeon Park"
                    }
                ],
                "author_detail": {
                    "name": "Jihyeon Park"
                },
                "author": "Jihyeon Park",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03488v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03488v4",
                "updated": "2024-11-08T09:02:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    2,
                    24,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-05T17:50:03Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    50,
                    3,
                    2,
                    157,
                    0
                ],
                "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training"
                },
                "summary": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03488v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03488v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05409v1",
                "updated": "2024-11-08T08:59:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    59,
                    40,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:59:40Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    59,
                    40,
                    4,
                    313,
                    0
                ],
                "title": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights"
                },
                "summary": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges."
                },
                "authors": [
                    {
                        "name": "Abigail Yongping Huang"
                    },
                    {
                        "name": "Ashwin Nair"
                    },
                    {
                        "name": "Zhen Rong Goh"
                    },
                    {
                        "name": "Tianrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Liu"
                },
                "author": "Tianrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07230v2",
                "updated": "2024-11-08T08:55:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    55,
                    0,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-12T00:58:19Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    0,
                    58,
                    19,
                    1,
                    72,
                    0
                ],
                "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences"
                },
                "summary": "Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique. We release the preference pairs used in\nalignment at:\nhttps://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique. We release the preference pairs used in\nalignment at:\nhttps://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences"
                },
                "authors": [
                    {
                        "name": "Pulkit Pattnaik"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "arxiv_comment": "Published at EMNLP 2024 as long (findings) conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05407v1",
                "updated": "2024-11-08T08:52:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    52,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:52:59Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    52,
                    59,
                    4,
                    313,
                    0
                ],
                "title": "Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning"
                },
                "summary": "Despite the strong performance of large language models (LLMs) in tasks like\nmathematical reasoning, their practical use is limited by high computational\ndemands and proprietary restrictions. Chain-of-thought (CoT) and\nprogram-of-thought (PoT) fine-tuning are common methods to transfer LLM\nknowledge to small language models (SLMs). However, CoT often leads to\ncalculation errors in SLMs, while PoT has shown more promise. While most\nPoT-based approaches focus on direct problem-to-code conversion or extracting\nonly the key information from questions and then providing code solution for\nit, this work emphasizes filling the gaps in the question to clearly illustrate\nthe solution path, which can be challenging for an SLM to understand when such\ninformation is not explicitly provided. Therefore, this paper introduces\nGap-Filling Prompting (GFP), a novel two-step prompting strategy designed to\nenhance the problem-solving process for SLMs. The first step identifies these\ngaps and provides hints for filling them, while the second step adds the hints\nto the question to generate a final code solution. Experimental results on two\nbenchmark datasets demonstrate that GFP significantly improves the mathematical\nreasoning abilities of SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the strong performance of large language models (LLMs) in tasks like\nmathematical reasoning, their practical use is limited by high computational\ndemands and proprietary restrictions. Chain-of-thought (CoT) and\nprogram-of-thought (PoT) fine-tuning are common methods to transfer LLM\nknowledge to small language models (SLMs). However, CoT often leads to\ncalculation errors in SLMs, while PoT has shown more promise. While most\nPoT-based approaches focus on direct problem-to-code conversion or extracting\nonly the key information from questions and then providing code solution for\nit, this work emphasizes filling the gaps in the question to clearly illustrate\nthe solution path, which can be challenging for an SLM to understand when such\ninformation is not explicitly provided. Therefore, this paper introduces\nGap-Filling Prompting (GFP), a novel two-step prompting strategy designed to\nenhance the problem-solving process for SLMs. The first step identifies these\ngaps and provides hints for filling them, while the second step adds the hints\nto the question to generate a final code solution. Experimental results on two\nbenchmark datasets demonstrate that GFP significantly improves the mathematical\nreasoning abilities of SLMs."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghiasvand Mohammadkhani"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Ghiasvand Mohammadkhani"
                },
                "author": "Mohammad Ghiasvand Mohammadkhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05403v1",
                "updated": "2024-11-08T08:41:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    41,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:41:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    41,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "Benchmarking Distributional Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Distributional Alignment of Large Language Models"
                },
                "summary": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions."
                },
                "authors": [
                    {
                        "name": "Nicole Meister"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02540v2",
                "updated": "2024-11-08T08:29:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    29,
                    10,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-04T19:21:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    21,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphXAIN: Narratives to Explain Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose interpretability challenges, especially\nfor non-expert users. Existing GNN explanation methods often yield technical\noutputs such as subgraphs and feature importance scores, which are not easily\nunderstood. Building on recent insights from social science and other\nExplainable AI (XAI) methods, we propose GraphXAIN, a natural language\nnarrative that explains individual predictions made by GNNs. We present a\nmodel-agnostic and explainer-agnostic XAI approach that complements graph\nexplainers by generating GraphXAINs, using Large Language Models (LLMs) and\nintegrating graph data, individual predictions from GNNs, explanatory\nsubgraphs, and feature importances. We define XAI Narratives and XAI\nDescriptions, highlighting their distinctions and emphasizing the importance of\nnarrative principles in effective explanations. By incorporating natural\nlanguage narratives, our approach supports graph practitioners and non-expert\nusers, aligning with social science research on explainability and enhancing\nuser understanding and trust in complex GNN models. We demonstrate GraphXAIN's\ncapabilities on a real-world graph dataset, illustrating how its generated\nnarratives can aid understanding compared to traditional graph explainer\noutputs or other descriptive explanation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose interpretability challenges, especially\nfor non-expert users. Existing GNN explanation methods often yield technical\noutputs such as subgraphs and feature importance scores, which are not easily\nunderstood. Building on recent insights from social science and other\nExplainable AI (XAI) methods, we propose GraphXAIN, a natural language\nnarrative that explains individual predictions made by GNNs. We present a\nmodel-agnostic and explainer-agnostic XAI approach that complements graph\nexplainers by generating GraphXAINs, using Large Language Models (LLMs) and\nintegrating graph data, individual predictions from GNNs, explanatory\nsubgraphs, and feature importances. We define XAI Narratives and XAI\nDescriptions, highlighting their distinctions and emphasizing the importance of\nnarrative principles in effective explanations. By incorporating natural\nlanguage narratives, our approach supports graph practitioners and non-expert\nusers, aligning with social science research on explainability and enhancing\nuser understanding and trust in complex GNN models. We demonstrate GraphXAIN's\ncapabilities on a real-world graph dataset, illustrating how its generated\nnarratives can aid understanding compared to traditional graph explainer\noutputs or other descriptive explanation methods."
                },
                "authors": [
                    {
                        "name": "Mateusz Cedro"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05399v1",
                "updated": "2024-11-08T08:26:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    26,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:26:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    26,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "Post-Hoc Robustness Enhancement in Graph Neural Networks with\n  Conditional Random Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Hoc Robustness Enhancement in Graph Neural Networks with\n  Conditional Random Fields"
                },
                "summary": "Graph Neural Networks (GNNs), which are nowadays the benchmark approach in\ngraph representation learning, have been shown to be vulnerable to adversarial\nattacks, raising concerns about their real-world applicability. While existing\ndefense techniques primarily concentrate on the training phase of GNNs,\ninvolving adjustments to message passing architectures or pre-processing\nmethods, there is a noticeable gap in methods focusing on increasing robustness\nduring inference. In this context, this study introduces RobustCRF, a post-hoc\napproach aiming to enhance the robustness of GNNs at the inference stage. Our\nproposed method, founded on statistical relational learning using a Conditional\nRandom Field, is model-agnostic and does not require prior knowledge about the\nunderlying model architecture. We validate the efficacy of this approach across\nvarious models, leveraging benchmark node classification datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs), which are nowadays the benchmark approach in\ngraph representation learning, have been shown to be vulnerable to adversarial\nattacks, raising concerns about their real-world applicability. While existing\ndefense techniques primarily concentrate on the training phase of GNNs,\ninvolving adjustments to message passing architectures or pre-processing\nmethods, there is a noticeable gap in methods focusing on increasing robustness\nduring inference. In this context, this study introduces RobustCRF, a post-hoc\napproach aiming to enhance the robustness of GNNs at the inference stage. Our\nproposed method, founded on statistical relational learning using a Conditional\nRandom Field, is model-agnostic and does not require prior knowledge about the\nunderlying model architecture. We validate the efficacy of this approach across\nvarious models, leveraging benchmark node classification datasets."
                },
                "authors": [
                    {
                        "name": "Yassine Abbahaddou"
                    },
                    {
                        "name": "Sofiane Ennadir"
                    },
                    {
                        "name": "Johannes F. Lutzeyer"
                    },
                    {
                        "name": "Fragkiskos D. Malliaros"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05382v1",
                "updated": "2024-11-08T07:33:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    33,
                    30,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T07:33:30Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    33,
                    30,
                    4,
                    313,
                    0
                ],
                "title": "Lift-and-Embed Learning Methods for Solving Scalar Hyperbolic Equations\n  with Discontinuous Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lift-and-Embed Learning Methods for Solving Scalar Hyperbolic Equations\n  with Discontinuous Solutions"
                },
                "summary": "Unlike traditional mesh-based approximations of differential operators,\nmachine learning methods, which exploit the automatic differentiation of neural\nnetworks, have attracted increasing attention for their potential to mitigate\nstability issues encountered in the numerical simulation of hyperbolic\nconservation laws. However, solutions to hyperbolic problems are often\npiecewise smooth, rendering the differential form invalid along discontinuity\ninterfaces and limiting the effectiveness of standard learning approaches. In\nthis work, we propose lift-and-embed learning methods for solving scalar\nhyperbolic equations with discontinuous solutions, which consist of (i)\nembedding the Rankine-Hugoniot jump condition within a higher-dimensional space\nthrough the inclusion of an augmented variable in the solution ansatz; (ii)\nutilizing physics-informed neural networks to manage the increased\ndimensionality and to address both linear and quasi-linear problems within a\nunified learning framework; and (iii) projecting the trained network solution\nback onto the original lower-dimensional plane to obtain the approximate\nsolution. Besides, the location of discontinuity can be parametrized as extra\nmodel parameters and inferred concurrently with the training of network\nsolution. With collocation points sampled on piecewise surfaces rather than\ndistributed over the entire lifted space, we conduct numerical experiments on\nvarious benchmark problems to demonstrate the capability of our methods in\nresolving discontinuous solutions without spurious numerical smearing and\noscillations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike traditional mesh-based approximations of differential operators,\nmachine learning methods, which exploit the automatic differentiation of neural\nnetworks, have attracted increasing attention for their potential to mitigate\nstability issues encountered in the numerical simulation of hyperbolic\nconservation laws. However, solutions to hyperbolic problems are often\npiecewise smooth, rendering the differential form invalid along discontinuity\ninterfaces and limiting the effectiveness of standard learning approaches. In\nthis work, we propose lift-and-embed learning methods for solving scalar\nhyperbolic equations with discontinuous solutions, which consist of (i)\nembedding the Rankine-Hugoniot jump condition within a higher-dimensional space\nthrough the inclusion of an augmented variable in the solution ansatz; (ii)\nutilizing physics-informed neural networks to manage the increased\ndimensionality and to address both linear and quasi-linear problems within a\nunified learning framework; and (iii) projecting the trained network solution\nback onto the original lower-dimensional plane to obtain the approximate\nsolution. Besides, the location of discontinuity can be parametrized as extra\nmodel parameters and inferred concurrently with the training of network\nsolution. With collocation points sampled on piecewise surfaces rather than\ndistributed over the entire lifted space, we conduct numerical experiments on\nvarious benchmark problems to demonstrate the capability of our methods in\nresolving discontinuous solutions without spurious numerical smearing and\noscillations."
                },
                "authors": [
                    {
                        "name": "Zhenjiang Liu"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Xuejun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xuejun Xu"
                },
                "author": "Xuejun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04358v2",
                "updated": "2024-11-08T07:31:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    31,
                    26,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T01:31:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    31,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation"
                },
                "summary": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Vaibhav Seth"
                    },
                    {
                        "name": "Arinjay Pathak"
                    },
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Sriram Gopalakrishnan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "48 pages, 10 figures, 10 tables, Code:\n  https://github.com/LCS2-IIITD/MonteCLoRA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16407v2",
                "updated": "2024-11-08T07:30:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    30,
                    25,
                    4,
                    313,
                    0
                ],
                "published": "2024-01-29T18:46:53Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    18,
                    46,
                    53,
                    0,
                    29,
                    0
                ],
                "title": "Is K-fold cross validation the best model selection method for Machine\n  Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is K-fold cross validation the best model selection method for Machine\n  Learning?"
                },
                "summary": "As a technique that can compactly represent complex patterns, machine\nlearning has significant potential for predictive inference. K-fold\ncross-validation (CV) is the most common approach to ascertaining the\nlikelihood that a machine learning outcome is generated by chance, and it\nfrequently outperforms conventional hypothesis testing. This improvement uses\nmeasures directly obtained from machine learning classifications, such as\naccuracy, that do not have a parametric description. To approach a frequentist\nanalysis within machine learning pipelines, a permutation test or simple\nstatistics from data partitions (i.e., folds) can be added to estimate\nconfidence intervals. Unfortunately, neither parametric nor non-parametric\ntests solve the inherent problems of partitioning small sample-size datasets\nand learning from heterogeneous data sources. The fact that machine learning\nstrongly depends on the learning parameters and the distribution of data across\nfolds recapitulates familiar difficulties around excess false positives and\nreplication. A novel statistical test based on K-fold CV and the Upper Bound of\nthe actual risk (K-fold CUBV) is proposed, where uncertain predictions of\nmachine learning with CV are bounded by the worst case through the evaluation\nof concentration inequalities. Probably Approximately Correct-Bayesian upper\nbounds for linear classifiers in combination with K-fold CV are derived and\nused to estimate the actual risk. The performance with simulated and\nneuroimaging datasets suggests that K-fold CUBV is a robust criterion for\ndetecting effects and validating accuracy values obtained from machine learning\nand classical CV schemes, while avoiding excess false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a technique that can compactly represent complex patterns, machine\nlearning has significant potential for predictive inference. K-fold\ncross-validation (CV) is the most common approach to ascertaining the\nlikelihood that a machine learning outcome is generated by chance, and it\nfrequently outperforms conventional hypothesis testing. This improvement uses\nmeasures directly obtained from machine learning classifications, such as\naccuracy, that do not have a parametric description. To approach a frequentist\nanalysis within machine learning pipelines, a permutation test or simple\nstatistics from data partitions (i.e., folds) can be added to estimate\nconfidence intervals. Unfortunately, neither parametric nor non-parametric\ntests solve the inherent problems of partitioning small sample-size datasets\nand learning from heterogeneous data sources. The fact that machine learning\nstrongly depends on the learning parameters and the distribution of data across\nfolds recapitulates familiar difficulties around excess false positives and\nreplication. A novel statistical test based on K-fold CV and the Upper Bound of\nthe actual risk (K-fold CUBV) is proposed, where uncertain predictions of\nmachine learning with CV are bounded by the worst case through the evaluation\nof concentration inequalities. Probably Approximately Correct-Bayesian upper\nbounds for linear classifiers in combination with K-fold CV are derived and\nused to estimate the actual risk. The performance with simulated and\nneuroimaging datasets suggests that K-fold CUBV is a robust criterion for\ndetecting effects and validating accuracy values obtained from machine learning\nand classical CV schemes, while avoiding excess false positives."
                },
                "authors": [
                    {
                        "name": "Juan M Gorriz"
                    },
                    {
                        "name": "R. Martin Clemente"
                    },
                    {
                        "name": "F Segovia"
                    },
                    {
                        "name": "J Ramirez"
                    },
                    {
                        "name": "A Ortiz"
                    },
                    {
                        "name": "J. Suckling"
                    }
                ],
                "author_detail": {
                    "name": "J. Suckling"
                },
                "author": "J. Suckling",
                "arxiv_comment": "40 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05375v1",
                "updated": "2024-11-08T07:05:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    5,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T07:05:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    5,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking"
                },
                "summary": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches."
                },
                "authors": [
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Michael Schlichtkrull"
                    },
                    {
                        "name": "Andreas Vlachos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vlachos"
                },
                "author": "Andreas Vlachos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17263v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17263v5",
                "updated": "2024-11-08T06:57:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    57,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-01-30T18:56:08Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    18,
                    56,
                    8,
                    1,
                    30,
                    0
                ],
                "title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks"
                },
                "summary": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo"
                },
                "authors": [
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "NeurIPS 2024 Spotlight; code available at\n  https://github.com/lapisrocks/rpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17263v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17263v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05371v1",
                "updated": "2024-11-08T06:49:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    49,
                    28,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:49:28Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    49,
                    28,
                    4,
                    313,
                    0
                ],
                "title": "BayesianFitForecast: A User-Friendly R Toolbox for Parameter Estimation\n  and Forecasting with Ordinary Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesianFitForecast: A User-Friendly R Toolbox for Parameter Estimation\n  and Forecasting with Ordinary Differential Equations"
                },
                "summary": "Background: Mathematical models based on ordinary differential equations\n(ODEs) are essential tools across various scientific disciplines, including\nbiology, ecology, and healthcare informatics. They are used to simulate complex\ndynamic systems and inform decision-making. In this paper, we introduce\nBayesianFitForecast, an R toolbox specifically developed to streamline Bayesian\nparameter estimation and forecasting in ODE models, making it particularly\nrelevant to health informatics and public health decision-making. The toolbox\nis available at https://github.com/gchowell/BayesianFitForecast/.\n  Results: This toolbox enables automatic generation of Stan files, allowing\nusers to configure models, define priors, and analyze results with minimal\nprogramming expertise. To demonstrate the versatility and robustness of\nBayesianFitForecast, we apply it to the analysis of the 1918 influenza pandemic\nin San Francisco, comparing Poisson and negative binomial error structures\nwithin the SEIR model. We also test it by fitting multiple time series of state\nvariables using simulated data. BayesianFitForecast provides robust tools for\nevaluating model performance, including convergence diagnostics, posterior\ndistributions, credible intervals, and performance metrics.\n  Conclusion: By improving the accessibility of advanced Bayesian methods, this\ntoolbox significantly broadens the application of Bayesian inference methods to\ndynamical systems critical for healthcare and epidemiological forecasting. A\ntutorial video demonstrating the toolbox's functionality is available at\nhttps://youtu.be/jnxMjz3V3n8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Mathematical models based on ordinary differential equations\n(ODEs) are essential tools across various scientific disciplines, including\nbiology, ecology, and healthcare informatics. They are used to simulate complex\ndynamic systems and inform decision-making. In this paper, we introduce\nBayesianFitForecast, an R toolbox specifically developed to streamline Bayesian\nparameter estimation and forecasting in ODE models, making it particularly\nrelevant to health informatics and public health decision-making. The toolbox\nis available at https://github.com/gchowell/BayesianFitForecast/.\n  Results: This toolbox enables automatic generation of Stan files, allowing\nusers to configure models, define priors, and analyze results with minimal\nprogramming expertise. To demonstrate the versatility and robustness of\nBayesianFitForecast, we apply it to the analysis of the 1918 influenza pandemic\nin San Francisco, comparing Poisson and negative binomial error structures\nwithin the SEIR model. We also test it by fitting multiple time series of state\nvariables using simulated data. BayesianFitForecast provides robust tools for\nevaluating model performance, including convergence diagnostics, posterior\ndistributions, credible intervals, and performance metrics.\n  Conclusion: By improving the accessibility of advanced Bayesian methods, this\ntoolbox significantly broadens the application of Bayesian inference methods to\ndynamical systems critical for healthcare and epidemiological forecasting. A\ntutorial video demonstrating the toolbox's functionality is available at\nhttps://youtu.be/jnxMjz3V3n8."
                },
                "authors": [
                    {
                        "name": "Hamed Karami"
                    },
                    {
                        "name": "Amanda Bleichrodt"
                    },
                    {
                        "name": "Ruiyan Luo"
                    },
                    {
                        "name": "Gerardo Chowell"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Chowell"
                },
                "author": "Gerardo Chowell",
                "arxiv_comment": "40 pages, 10 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92D25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05357v1",
                "updated": "2024-11-08T06:28:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    28,
                    2,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:28:02Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    28,
                    2,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Visual Classification using Comparative Descriptors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Classification using Comparative Descriptors"
                },
                "summary": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences."
                },
                "authors": [
                    {
                        "name": "Hankyeol Lee"
                    },
                    {
                        "name": "Gawon Seo"
                    },
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    },
                    {
                        "name": "Jiyoung Jung"
                    }
                ],
                "author_detail": {
                    "name": "Jiyoung Jung"
                },
                "author": "Jiyoung Jung",
                "arxiv_comment": "Accepted to WACV 2025. Main paper with 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05353v1",
                "updated": "2024-11-08T06:19:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    19,
                    29,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:19:29Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    19,
                    29,
                    4,
                    313,
                    0
                ],
                "title": "Controlling Grokking with Nonlinearity and Data Symmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Grokking with Nonlinearity and Data Symmetry"
                },
                "summary": "This paper demonstrates that grokking behavior in modular arithmetic with a\nmodulus P in a neural network can be controlled by modifying the profile of the\nactivation function as well as the depth and width of the model. Plotting the\neven PCA projections of the weights of the last NN layer against their odd\nprojections further yields patterns which become significantly more uniform\nwhen the nonlinearity is increased by incrementing the number of layers. These\npatterns can be employed to factor P when P is nonprime. Finally, a metric for\nthe generalization ability of the network is inferred from the entropy of the\nlayer weights while the degree of nonlinearity is related to correlations\nbetween the local entropy of the weights of the neurons in the final layer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper demonstrates that grokking behavior in modular arithmetic with a\nmodulus P in a neural network can be controlled by modifying the profile of the\nactivation function as well as the depth and width of the model. Plotting the\neven PCA projections of the weights of the last NN layer against their odd\nprojections further yields patterns which become significantly more uniform\nwhen the nonlinearity is increased by incrementing the number of layers. These\npatterns can be employed to factor P when P is nonprime. Finally, a metric for\nthe generalization ability of the network is inferred from the entropy of the\nlayer weights while the degree of nonlinearity is related to correlations\nbetween the local entropy of the weights of the neurons in the final layer."
                },
                "authors": [
                    {
                        "name": "Ahmed Salah"
                    },
                    {
                        "name": "David Yevick"
                    }
                ],
                "author_detail": {
                    "name": "David Yevick"
                },
                "author": "David Yevick",
                "arxiv_comment": "15 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05349v1",
                "updated": "2024-11-08T06:12:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    12,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:12:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    12,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\n  Cluster Diagnosis System and Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\n  Cluster Diagnosis System and Evaluation Framework"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods."
                },
                "authors": [
                    {
                        "name": "Honghao Shi"
                    },
                    {
                        "name": "Longkai Cheng"
                    },
                    {
                        "name": "Wenli Wu"
                    },
                    {
                        "name": "Yuhang Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Shaokai Nie"
                    },
                    {
                        "name": "Weixv Wang"
                    },
                    {
                        "name": "Xuebin Min"
                    },
                    {
                        "name": "Chunlei Men"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06979v2",
                "updated": "2024-11-08T06:08:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    8,
                    2,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-11T03:12:18Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    12,
                    18,
                    2,
                    255,
                    0
                ],
                "title": "A High-Performance List Decoding Algorithm for Surface Codes with\n  Erroneous Syndrome",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High-Performance List Decoding Algorithm for Surface Codes with\n  Erroneous Syndrome"
                },
                "summary": "Quantum error-correcting codes (QECCs) are necessary for fault-tolerant\nquantum computation. Surface codes are a class of topological QECCs that have\nattracted significant attention due to their exceptional error-correcting\ncapabilities and easy implementation. In the decoding process of surface codes,\nthe syndromes are crucial for error correction, however, they are not always\ncorrectly measured. Most of the existing decoding algorithms for surface codes\nneed extra measurements to correct syndromes with errors, which implies a\npotential increase in inference complexity and decoding latency. In this paper,\nwe propose a high-performance list decoding algorithm for surface codes with\nerroneous syndromes, where syndrome soft information is incorporated in the\ndecoding, allowing qubits and syndrome to be recovered without needing extra\nmeasurements. Precisely, we first use belief propagation (BP) decoding for\npre-processing with syndrome soft information, followed by ordered statistics\ndecoding (OSD) for post-processing to list and recover both qubits and\nsyndromes. Numerical results demonstrate that our proposed algorithm\nefficiently recovers erroneous syndromes and significantly improves the\ndecoding performance of surface codes with erroneous syndromes compared to\nminimum-weight perfect matching (MWPM), BP and original BP-OSD algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum error-correcting codes (QECCs) are necessary for fault-tolerant\nquantum computation. Surface codes are a class of topological QECCs that have\nattracted significant attention due to their exceptional error-correcting\ncapabilities and easy implementation. In the decoding process of surface codes,\nthe syndromes are crucial for error correction, however, they are not always\ncorrectly measured. Most of the existing decoding algorithms for surface codes\nneed extra measurements to correct syndromes with errors, which implies a\npotential increase in inference complexity and decoding latency. In this paper,\nwe propose a high-performance list decoding algorithm for surface codes with\nerroneous syndromes, where syndrome soft information is incorporated in the\ndecoding, allowing qubits and syndrome to be recovered without needing extra\nmeasurements. Precisely, we first use belief propagation (BP) decoding for\npre-processing with syndrome soft information, followed by ordered statistics\ndecoding (OSD) for post-processing to list and recover both qubits and\nsyndromes. Numerical results demonstrate that our proposed algorithm\nefficiently recovers erroneous syndromes and significantly improves the\ndecoding performance of surface codes with erroneous syndromes compared to\nminimum-weight perfect matching (MWPM), BP and original BP-OSD algorithms."
                },
                "authors": [
                    {
                        "name": "Jifan Liang"
                    },
                    {
                        "name": "Qianfan Wang"
                    },
                    {
                        "name": "Lvzhou Li"
                    },
                    {
                        "name": "Xiao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Ma"
                },
                "author": "Xiao Ma",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01251v3",
                "updated": "2024-11-08T06:07:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    7,
                    51,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-02T16:23:44Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    16,
                    23,
                    44,
                    5,
                    62,
                    0
                ],
                "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization\n  via Probe Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization\n  via Probe Sampling"
                },
                "summary": "Safety of Large Language Models (LLMs) has become a critical issue given\ntheir rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be\neffective in constructing adversarial prompts to break the aligned LLMs, but\noptimization of GCG is time-consuming. To reduce the time cost of GCG and\nenable more comprehensive studies of LLM safety, in this work, we study a new\nalgorithm called $\\texttt{Probe sampling}$. At the core of the algorithm is a\nmechanism that dynamically determines how similar a smaller draft model's\npredictions are to the target model's predictions for prompt candidates. When\nthe target model is similar to the draft model, we rely heavily on the draft\nmodel to filter out a large number of potential prompt candidates. Probe\nsampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to\nequal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe\nsampling is also able to accelerate other prompt optimization techniques and\nadversarial methods, leading to acceleration of $1.8\\times$ for AutoPrompt,\n$2.4\\times$ for APE and $2.4\\times$ for AutoDAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety of Large Language Models (LLMs) has become a critical issue given\ntheir rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be\neffective in constructing adversarial prompts to break the aligned LLMs, but\noptimization of GCG is time-consuming. To reduce the time cost of GCG and\nenable more comprehensive studies of LLM safety, in this work, we study a new\nalgorithm called $\\texttt{Probe sampling}$. At the core of the algorithm is a\nmechanism that dynamically determines how similar a smaller draft model's\npredictions are to the target model's predictions for prompt candidates. When\nthe target model is similar to the draft model, we rely heavily on the draft\nmodel to filter out a large number of potential prompt candidates. Probe\nsampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to\nequal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe\nsampling is also able to accelerate other prompt optimization techniques and\nadversarial methods, leading to acceleration of $1.8\\times$ for AutoPrompt,\n$2.4\\times$ for APE and $2.4\\times$ for AutoDAN."
                },
                "authors": [
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Wenyue Zheng"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xuan Long Do"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05348v1",
                "updated": "2024-11-08T06:04:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:04:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PySC2: Starcraft II learning environment for Large Language Models"
                },
                "summary": "This paper introduces a new environment LLM-PySC2 (the Large Language Model\nStarCraft II Learning Environment), a platform derived from DeepMind's\nStarCraft II Learning Environment that serves to develop Large Language Models\n(LLMs) based decision-making methodologies. This environment is the first to\noffer the complete StarCraft II action space, multi-modal observation\ninterfaces, and a structured game knowledge database, which are seamlessly\nconnected with various LLMs to facilitate the research of LLMs-based\ndecision-making. To further support multi-agent research, we developed an LLM\ncollaborative framework that supports multi-agent concurrent queries and\nmulti-agent communication. In our experiments, the LLM-PySC2 environment is\nadapted to be compatible with the StarCraft Multi-Agent Challenge (SMAC) task\ngroup and provided eight new scenarios focused on macro-decision abilities. We\nevaluated nine mainstream LLMs in the experiments, and results show that\nsufficient parameters are necessary for LLMs to make decisions, but improving\nreasoning ability does not directly lead to better decision-making outcomes.\nOur findings further indicate the importance of enabling large models to learn\nautonomously in the deployment environment through parameter training or\ntrain-free learning techniques. Ultimately, we expect that the LLM-PySC2\nenvironment can promote research on learning methods for LLMs, helping\nLLM-based methods better adapt to task scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new environment LLM-PySC2 (the Large Language Model\nStarCraft II Learning Environment), a platform derived from DeepMind's\nStarCraft II Learning Environment that serves to develop Large Language Models\n(LLMs) based decision-making methodologies. This environment is the first to\noffer the complete StarCraft II action space, multi-modal observation\ninterfaces, and a structured game knowledge database, which are seamlessly\nconnected with various LLMs to facilitate the research of LLMs-based\ndecision-making. To further support multi-agent research, we developed an LLM\ncollaborative framework that supports multi-agent concurrent queries and\nmulti-agent communication. In our experiments, the LLM-PySC2 environment is\nadapted to be compatible with the StarCraft Multi-Agent Challenge (SMAC) task\ngroup and provided eight new scenarios focused on macro-decision abilities. We\nevaluated nine mainstream LLMs in the experiments, and results show that\nsufficient parameters are necessary for LLMs to make decisions, but improving\nreasoning ability does not directly lead to better decision-making outcomes.\nOur findings further indicate the importance of enabling large models to learn\nautonomously in the deployment environment through parameter training or\ntrain-free learning techniques. Ultimately, we expect that the LLM-PySC2\nenvironment can promote research on learning methods for LLMs, helping\nLLM-based methods better adapt to task scenarios."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Chang Lu"
                    },
                    {
                        "name": "Xiaojie Xu"
                    },
                    {
                        "name": "Xiangbei Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yunzheng Guo"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Xian Guo"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xuebo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuebo Zhang"
                },
                "author": "Xuebo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05345v1",
                "updated": "2024-11-08T05:54:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    54,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:54:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    54,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Robustness of LLMs to Adversarial Typographical Errors"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Esther Gan"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Liying Cheng"
                    },
                    {
                        "name": "Yancan Mao"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05338v1",
                "updated": "2024-11-08T05:28:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    28,
                    22,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:28:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    28,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers"
                },
                "summary": "Scientific literature is typically dense, requiring significant background\nknowledge and deep comprehension for effective engagement. We introduce SciDQA,\na new dataset for reading comprehension that challenges LLMs for a deep\nunderstanding of scientific articles, consisting of 2,937 QA pairs. Unlike\nother scientific QA datasets, SciDQA sources questions from peer reviews by\ndomain experts and answers by paper authors, ensuring a thorough examination of\nthe literature. We enhance the dataset's quality through a process that\ncarefully filters out lower quality questions, decontextualizes the content,\ntracks the source document across different versions, and incorporates a\nbibliography for multi-document question-answering. Questions in SciDQA\nnecessitate reasoning across figures, tables, equations, appendices, and\nsupplementary materials, and require multi-document reasoning. We evaluate\nseveral open-source and proprietary LLMs across various configurations to\nexplore their capabilities in generating relevant and factual responses. Our\ncomprehensive evaluation, based on metrics for surface-level similarity and LLM\njudgements, highlights notable performance discrepancies. SciDQA represents a\nrigorously curated, naturally derived scientific QA dataset, designed to\nfacilitate research on complex scientific text understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature is typically dense, requiring significant background\nknowledge and deep comprehension for effective engagement. We introduce SciDQA,\na new dataset for reading comprehension that challenges LLMs for a deep\nunderstanding of scientific articles, consisting of 2,937 QA pairs. Unlike\nother scientific QA datasets, SciDQA sources questions from peer reviews by\ndomain experts and answers by paper authors, ensuring a thorough examination of\nthe literature. We enhance the dataset's quality through a process that\ncarefully filters out lower quality questions, decontextualizes the content,\ntracks the source document across different versions, and incorporates a\nbibliography for multi-document question-answering. Questions in SciDQA\nnecessitate reasoning across figures, tables, equations, appendices, and\nsupplementary materials, and require multi-document reasoning. We evaluate\nseveral open-source and proprietary LLMs across various configurations to\nexplore their capabilities in generating relevant and factual responses. Our\ncomprehensive evaluation, based on metrics for surface-level similarity and LLM\njudgements, highlights notable performance discrepancies. SciDQA represents a\nrigorously curated, naturally derived scientific QA dataset, designed to\nfacilitate research on complex scientific text understanding."
                },
                "authors": [
                    {
                        "name": "Shruti Singh"
                    },
                    {
                        "name": "Nandan Sarkar"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "18 pages, Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16964v2",
                "updated": "2024-11-08T05:19:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    19,
                    48,
                    4,
                    313,
                    0
                ],
                "published": "2024-05-27T08:57:04Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    8,
                    57,
                    4,
                    0,
                    148,
                    0
                ],
                "title": "Exploring the LLM Journey from Cognition to Expression with Linear\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the LLM Journey from Cognition to Expression with Linear\n  Representations"
                },
                "summary": "This paper presents an in-depth examination of the evolution and interplay of\ncognitive and expressive capabilities in large language models (LLMs), with a\nspecific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese\nand English) LLM series. We define and explore the model's cognitive and\nexpressive capabilities through linear representations across three critical\nphases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning\nfrom Human Feedback (RLHF). Cognitive capability is defined as the quantity and\nquality of information conveyed by the neuron output vectors within the\nnetwork, similar to the neural signal processing in human cognition. Expressive\ncapability is defined as the model's capability to produce word-level output.\nOur findings unveil a sequential development pattern, where cognitive abilities\nare largely established during Pretraining, whereas expressive abilities\npredominantly advance during SFT and RLHF. Statistical analyses confirm a\nsignificant correlation between the two capabilities, suggesting that cognitive\ncapacity may limit expressive potential. The paper also explores the\ntheoretical underpinnings of these divergent developmental trajectories and\ntheir connection to the LLMs' architectural design. Moreover, we evaluate\nvarious optimization-independent strategies, such as few-shot learning and\nrepeated sampling, which bridge the gap between cognitive and expressive\ncapabilities. This research reveals the potential connection between the hidden\nspace and the output space, contributing valuable insights into the\ninterpretability and controllability of their training processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth examination of the evolution and interplay of\ncognitive and expressive capabilities in large language models (LLMs), with a\nspecific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese\nand English) LLM series. We define and explore the model's cognitive and\nexpressive capabilities through linear representations across three critical\nphases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning\nfrom Human Feedback (RLHF). Cognitive capability is defined as the quantity and\nquality of information conveyed by the neuron output vectors within the\nnetwork, similar to the neural signal processing in human cognition. Expressive\ncapability is defined as the model's capability to produce word-level output.\nOur findings unveil a sequential development pattern, where cognitive abilities\nare largely established during Pretraining, whereas expressive abilities\npredominantly advance during SFT and RLHF. Statistical analyses confirm a\nsignificant correlation between the two capabilities, suggesting that cognitive\ncapacity may limit expressive potential. The paper also explores the\ntheoretical underpinnings of these divergent developmental trajectories and\ntheir connection to the LLMs' architectural design. Moreover, we evaluate\nvarious optimization-independent strategies, such as few-shot learning and\nrepeated sampling, which bridge the gap between cognitive and expressive\ncapabilities. This research reveals the potential connection between the hidden\nspace and the output space, contributing valuable insights into the\ninterpretability and controllability of their training processes."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yipin Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yan"
                },
                "author": "Dong Yan",
                "arxiv_comment": "Published in ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05331v1",
                "updated": "2024-11-08T05:12:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    12,
                    16,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:12:16Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    12,
                    16,
                    4,
                    313,
                    0
                ],
                "title": "Discovering Latent Structural Causal Models from Spatio-Temporal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Latent Structural Causal Models from Spatio-Temporal Data"
                },
                "summary": "Many important phenomena in scientific fields such as climate, neuroscience,\nand epidemiology are naturally represented as spatiotemporal gridded data with\ncomplex interactions. For example, in climate science, researchers aim to\nuncover how large-scale events, such as the North Atlantic Oscillation (NAO)\nand the Antarctic Oscillation (AAO), influence other global processes.\nInferring causal relationships from these data is a challenging problem\ncompounded by the high dimensionality of such data and the correlations between\nspatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),\na novel framework based on variational inference, designed to explicitly model\nlatent time-series and their causal relationships from spatially confined modes\nin the data. Our method uses an end-to-end training process that maximizes an\nevidence-lower bound (ELBO) for the data likelihood. Theoretically, we show\nthat, under some conditions, the latent variables are identifiable up to\ntransformation by an invertible matrix. Empirically, we show that SPACY\noutperforms state-of-the-art baselines on synthetic data, remains scalable for\nlarge grids, and identifies key known phenomena from real-world climate data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many important phenomena in scientific fields such as climate, neuroscience,\nand epidemiology are naturally represented as spatiotemporal gridded data with\ncomplex interactions. For example, in climate science, researchers aim to\nuncover how large-scale events, such as the North Atlantic Oscillation (NAO)\nand the Antarctic Oscillation (AAO), influence other global processes.\nInferring causal relationships from these data is a challenging problem\ncompounded by the high dimensionality of such data and the correlations between\nspatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),\na novel framework based on variational inference, designed to explicitly model\nlatent time-series and their causal relationships from spatially confined modes\nin the data. Our method uses an end-to-end training process that maximizes an\nevidence-lower bound (ELBO) for the data likelihood. Theoretically, we show\nthat, under some conditions, the latent variables are identifiable up to\ntransformation by an invertible matrix. Empirically, we show that SPACY\noutperforms state-of-the-art baselines on synthetic data, remains scalable for\nlarge grids, and identifies key known phenomena from real-world climate data."
                },
                "authors": [
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Sumanth Varambally"
                    },
                    {
                        "name": "Duncan Watson-Parris"
                    },
                    {
                        "name": "Yi-An Ma"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15721v2",
                "updated": "2024-11-08T05:08:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    8,
                    43,
                    4,
                    313,
                    0
                ],
                "published": "2024-02-24T05:14:52Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    5,
                    14,
                    52,
                    5,
                    55,
                    0
                ],
                "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models"
                },
                "summary": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data."
                },
                "authors": [
                    {
                        "name": "Chaoya Jiang"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "arxiv_comment": "Accepted by ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05328v1",
                "updated": "2024-11-08T05:04:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    4,
                    55,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:04:55Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    4,
                    55,
                    4,
                    313,
                    0
                ],
                "title": "Content Quality vs. Attention Allocation: An LLM-Based Case Study in\n  Peer-to-peer Mental Health Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Quality vs. Attention Allocation: An LLM-Based Case Study in\n  Peer-to-peer Mental Health Networks"
                },
                "summary": "With the rise of social media and peer-to-peer networks, users increasingly\nrely on crowdsourced responses for information and assistance. However, the\nmechanisms used to rank and promote responses often prioritize and end up\nbiasing in favor of timeliness over quality, which may result in suboptimal\nsupport for help-seekers. We analyze millions of responses to mental\nhealth-related posts, utilizing large language models (LLMs) to assess the\nmulti-dimensional quality of content, including relevance, empathy, and\ncultural alignment, among other aspects. Our findings reveal a mismatch between\ncontent quality and attention allocation: earlier responses - despite being\nrelatively lower in quality - receive disproportionately high fractions of\nupvotes and visibility due to platform ranking algorithms. We demonstrate that\nthe quality of the top-ranked responses could be improved by up to 39 percent,\nand even the simplest re-ranking strategy could significantly improve the\nquality of top responses, highlighting the need for more nuanced ranking\nmechanisms that prioritize both timeliness and content quality, especially\nemotional engagement in online mental health communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of social media and peer-to-peer networks, users increasingly\nrely on crowdsourced responses for information and assistance. However, the\nmechanisms used to rank and promote responses often prioritize and end up\nbiasing in favor of timeliness over quality, which may result in suboptimal\nsupport for help-seekers. We analyze millions of responses to mental\nhealth-related posts, utilizing large language models (LLMs) to assess the\nmulti-dimensional quality of content, including relevance, empathy, and\ncultural alignment, among other aspects. Our findings reveal a mismatch between\ncontent quality and attention allocation: earlier responses - despite being\nrelatively lower in quality - receive disproportionately high fractions of\nupvotes and visibility due to platform ranking algorithms. We demonstrate that\nthe quality of the top-ranked responses could be improved by up to 39 percent,\nand even the simplest re-ranking strategy could significantly improve the\nquality of top responses, highlighting the need for more nuanced ranking\nmechanisms that prioritize both timeliness and content quality, especially\nemotional engagement in online mental health communities."
                },
                "authors": [
                    {
                        "name": "Teng Ye"
                    },
                    {
                        "name": "Hanson Yan"
                    },
                    {
                        "name": "Xuhuan Huang"
                    },
                    {
                        "name": "Connor Grogan"
                    },
                    {
                        "name": "Walter Yuan"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Matthew O. Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew O. Jackson"
                },
                "author": "Matthew O. Jackson",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91D30, 94A16",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05316v1",
                "updated": "2024-11-08T04:15:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    15,
                    8,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T04:15:08Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    15,
                    8,
                    4,
                    313,
                    0
                ],
                "title": "Exploring the Alignment Landscape: LLMs and Geometric Deep Models in\n  Protein Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Alignment Landscape: LLMs and Geometric Deep Models in\n  Protein Representation"
                },
                "summary": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Bingbing Duan"
                    },
                    {
                        "name": "Kai Guo"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10108v3",
                "updated": "2024-11-08T03:58:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    3,
                    58,
                    0,
                    4,
                    313,
                    0
                ],
                "published": "2023-10-16T06:41:16Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    6,
                    41,
                    16,
                    0,
                    289,
                    0
                ],
                "title": "On Generative Agents in Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Generative Agents in Recommendation"
                },
                "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a user\nsimulator in recommendation, leveraging LLM-empowered generative agents\nequipped with user profile, memory, and actions modules specifically tailored\nfor the recommender system. In particular, these agents' profile modules are\ninitialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),\ncapturing users' unique tastes and social traits; memory modules log both\nfactual and emotional memories and are integrated with an emotion-driven\nreflection mechanism; action modules support a wide variety of behaviors,\nspanning both taste-driven and emotion-driven actions. Each agent interacts\nwith personalized recommender models in a page-by-page manner, relying on a\npre-implemented collaborative filtering-based recommendation algorithm. We\ndelve into both the capabilities and limitations of Agent4Rec, aiming to\nexplore an essential research question: ``To what extent can LLM-empowered\ngenerative agents faithfully simulate the behavior of real, autonomous humans\nin recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec\nhighlight both the alignment and deviation between agents and user-personalized\npreferences. Beyond mere performance comparison, we explore insightful\nexperiments, such as emulating the filter bubble effect and discovering the\nunderlying causal relationships in recommendation tasks. Our codes are\navailable at https://github.com/LehengTHU/Agent4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a user\nsimulator in recommendation, leveraging LLM-empowered generative agents\nequipped with user profile, memory, and actions modules specifically tailored\nfor the recommender system. In particular, these agents' profile modules are\ninitialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),\ncapturing users' unique tastes and social traits; memory modules log both\nfactual and emotional memories and are integrated with an emotion-driven\nreflection mechanism; action modules support a wide variety of behaviors,\nspanning both taste-driven and emotion-driven actions. Each agent interacts\nwith personalized recommender models in a page-by-page manner, relying on a\npre-implemented collaborative filtering-based recommendation algorithm. We\ndelve into both the capabilities and limitations of Agent4Rec, aiming to\nexplore an essential research question: ``To what extent can LLM-empowered\ngenerative agents faithfully simulate the behavior of real, autonomous humans\nin recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec\nhighlight both the alignment and deviation between agents and user-personalized\npreferences. Beyond mere performance comparison, we explore insightful\nexperiments, such as emulating the filter bubble effect and discovering the\nunderlying causal relationships in recommendation tasks. Our codes are\navailable at https://github.com/LehengTHU/Agent4Rec."
                },
                "authors": [
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "SIGIR 2024 perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00369v3",
                "updated": "2024-11-08T03:09:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    3,
                    9,
                    37,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-01T05:14:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    14,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset"
                },
                "summary": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics."
                },
                "authors": [
                    {
                        "name": "Anish Pahilajani"
                    },
                    {
                        "name": "Devasha Trivedi"
                    },
                    {
                        "name": "Jincen Shuai"
                    },
                    {
                        "name": "Khin S. Yone"
                    },
                    {
                        "name": "Samyak Rajesh Jain"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "15 pages, 24 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05289v1",
                "updated": "2024-11-08T02:47:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    47,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:47:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    47,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}."
                },
                "authors": [
                    {
                        "name": "Ryan Sun"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05285v1",
                "updated": "2024-11-08T02:31:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    31,
                    3,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:31:03Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    31,
                    3,
                    4,
                    313,
                    0
                ],
                "title": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model\n  based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model\n  based Agents"
                },
                "summary": "The ever-improving quality of LLMs has fueled the growth of a diverse range\nof downstream tasks, leading to an increased demand for AI automation and a\nburgeoning interest in developing foundation model (FM)-based autonomous\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\na wider range of stakeholders, including agent users, agentic system developers\nand deployers, and AI model developers. These systems also integrate multiple\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\ncapabilities, and observability features. In this case, obtaining reliable\noutputs and answers from these agents remains challenging, necessitating a\ndependable execution process and end-to-end observability solutions. To build\nreliable AI agents and LLM applications, it is essential to shift towards\ndesigning AgentOps platforms that ensure observability and traceability across\nthe entire development-to-production life-cycle. To this end, we conducted a\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\nBased on this review, we provide an overview of the essential features of\nAgentOps and propose a comprehensive overview of observability data/traceable\nartifacts across the agent production life-cycle. Our findings provide a\nsystematic overview of the current AgentOps landscape, emphasizing the critical\nrole of observability/traceability in enhancing the reliability of autonomous\nagent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-improving quality of LLMs has fueled the growth of a diverse range\nof downstream tasks, leading to an increased demand for AI automation and a\nburgeoning interest in developing foundation model (FM)-based autonomous\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\na wider range of stakeholders, including agent users, agentic system developers\nand deployers, and AI model developers. These systems also integrate multiple\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\ncapabilities, and observability features. In this case, obtaining reliable\noutputs and answers from these agents remains challenging, necessitating a\ndependable execution process and end-to-end observability solutions. To build\nreliable AI agents and LLM applications, it is essential to shift towards\ndesigning AgentOps platforms that ensure observability and traceability across\nthe entire development-to-production life-cycle. To this end, we conducted a\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\nBased on this review, we provide an overview of the essential features of\nAgentOps and propose a comprehensive overview of observability data/traceable\nartifacts across the agent production life-cycle. Our findings provide a\nsystematic overview of the current AgentOps landscape, emphasizing the critical\nrole of observability/traceability in enhancing the reliability of autonomous\nagent systems."
                },
                "authors": [
                    {
                        "name": "Liming Dong"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; D.2.9; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.05787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v1",
                "updated": "2024-11-08T18:57:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "Recycled Attention: Efficient inference for long-context language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recycled Attention: Efficient inference for long-context language models"
                },
                "summary": "Generating long sequences of tokens given a long-context input imposes a\nheavy computational burden for large language models (LLMs). One of the\ncomputational bottleneck comes from computing attention over a long sequence of\ninput at each generation step. In this paper, we propose Recycled Attention, an\ninference-time method which alternates between full context attention and\nattention over a subset of input tokens. When performing partial attention, we\nrecycle the attention pattern of a previous token that has performed full\nattention and attend only to the top K most attended tokens, reducing the cost\nof data movement and attention computation. Compared to previously proposed\ninference-time acceleration method which attends only to local context or\ntokens with high accumulative attention scores, our approach flexibly chooses\ntokens that are relevant to the current decoding step. We evaluate our methods\non RULER, a suite of tasks designed to comprehensively evaluate long-context\nabilities, and long-context language modeling tasks. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to baselines which only consider\nlocal context while improving the performance by 2x. We further explore two\nideas to improve performance-efficiency trade-offs: (1) dynamically decide when\nto perform recycled or full attention step based on the query similarities and\n(2) continued pre-training the model with Recycled Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input imposes a\nheavy computational burden for large language models (LLMs). One of the\ncomputational bottleneck comes from computing attention over a long sequence of\ninput at each generation step. In this paper, we propose Recycled Attention, an\ninference-time method which alternates between full context attention and\nattention over a subset of input tokens. When performing partial attention, we\nrecycle the attention pattern of a previous token that has performed full\nattention and attend only to the top K most attended tokens, reducing the cost\nof data movement and attention computation. Compared to previously proposed\ninference-time acceleration method which attends only to local context or\ntokens with high accumulative attention scores, our approach flexibly chooses\ntokens that are relevant to the current decoding step. We evaluate our methods\non RULER, a suite of tasks designed to comprehensively evaluate long-context\nabilities, and long-context language modeling tasks. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to baselines which only consider\nlocal context while improving the performance by 2x. We further explore two\nideas to improve performance-efficiency trade-offs: (1) dynamically decide when\nto perform recycled or full attention step based on the query similarities and\n(2) continued pre-training the model with Recycled Attention."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08627v2",
                "updated": "2024-11-08T18:56:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    56,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-04-12T17:41:05Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    17,
                    41,
                    5,
                    4,
                    103,
                    0
                ],
                "title": "Is ChatGPT Transforming Academics' Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT Transforming Academics' Writing Style?"
                },
                "summary": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style."
                },
                "authors": [
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09539v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09539v3",
                "updated": "2024-11-08T18:56:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    56,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-14T16:27:49Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    16,
                    27,
                    49,
                    3,
                    74,
                    0
                ],
                "title": "Logits of API-Protected LLMs Leak Proprietary Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logits of API-Protected LLMs Leak Proprietary Information"
                },
                "summary": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability."
                },
                "authors": [
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09539v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09539v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05781v1",
                "updated": "2024-11-08T18:48:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    48,
                    57,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:48:57Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    48,
                    57,
                    4,
                    313,
                    0
                ],
                "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Language Models to Disambiguate Lexical Choices in Translation"
                },
                "summary": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4."
                },
                "authors": [
                    {
                        "name": "Josh Barua"
                    },
                    {
                        "name": "Sanjay Subramanian"
                    },
                    {
                        "name": "Kayo Yin"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05778v1",
                "updated": "2024-11-08T18:45:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    45,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:45:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    45,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture"
                },
                "summary": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%."
                },
                "authors": [
                    {
                        "name": "Colin Doyle"
                    }
                ],
                "author_detail": {
                    "name": "Colin Doyle"
                },
                "author": "Colin Doyle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05777v1",
                "updated": "2024-11-08T18:43:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:43:15Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding"
                },
                "summary": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results."
                },
                "authors": [
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Ondrej Sotolar"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Sotolar"
                },
                "author": "Ondrej Sotolar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05775v1",
                "updated": "2024-11-08T18:36:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    36,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:36:33Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    36,
                    33,
                    4,
                    313,
                    0
                ],
                "title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?"
                },
                "summary": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media."
                },
                "authors": [
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Shaina Raza"
                    }
                ],
                "author_detail": {
                    "name": "Shaina Raza"
                },
                "author": "Shaina Raza",
                "arxiv_comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04264v4",
                "updated": "2024-11-08T18:35:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    35,
                    49,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-17T17:01:45Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    17,
                    1,
                    45,
                    6,
                    77,
                    0
                ],
                "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs"
                },
                "summary": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT."
                },
                "authors": [
                    {
                        "name": "Lihui Liu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Eunice Chan"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05007v2",
                "updated": "2024-11-08T18:32:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    32,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T18:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    58,
                    3,
                    312,
                    0
                ],
                "title": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models"
                },
                "summary": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced."
                },
                "authors": [
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Quantization Library: https://github.com/mit-han-lab/deepcompressor\n  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:\n  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:\n  https://hanlab.mit.edu/blog/svdquant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05764v1",
                "updated": "2024-11-08T18:26:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    26,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    26,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents"
                },
                "summary": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents."
                },
                "authors": [
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yitao Long"
                    },
                    {
                        "name": "Yuru Jiang"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Weiyuan Chen"
                    },
                    {
                        "name": "Hongjun Liu"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05762v1",
                "updated": "2024-11-08T18:25:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    25,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:25:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    25,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024"
                },
                "summary": "Separating disinformation from fact on the web has long challenged both the\nsearch and the reasoning powers of humans. We show that the reasoning power of\nlarge language models (LLMs) and the retrieval power of modern search engines\ncan be combined to automate this process and explainably verify claims. We\nintegrate LLMs and search under a multi-hop evidence pursuit strategy. This\nstrategy generates an initial question based on an input claim using a sequence\nto sequence model, searches and formulates an answer to the question, and\niteratively generates follow-up questions to pursue the evidence that is\nmissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)\nshared task. Compared to a strategy of generating all the questions at once,\nour method obtains .045 higher label accuracy and .155 higher AVeriTeC score\n(evaluating the adequacy of the evidence). Through ablations, we show the\nimportance of various design choices, such as the question generation method,\nmedium-sized context, reasoning with one document at a time, adding metadata,\nparaphrasing, reducing the problem to two classes, and reconsidering the final\nverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and\n.477 AVeriTeC score on the test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating disinformation from fact on the web has long challenged both the\nsearch and the reasoning powers of humans. We show that the reasoning power of\nlarge language models (LLMs) and the retrieval power of modern search engines\ncan be combined to automate this process and explainably verify claims. We\nintegrate LLMs and search under a multi-hop evidence pursuit strategy. This\nstrategy generates an initial question based on an input claim using a sequence\nto sequence model, searches and formulates an answer to the question, and\niteratively generates follow-up questions to pursue the evidence that is\nmissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)\nshared task. Compared to a strategy of generating all the questions at once,\nour method obtains .045 higher label accuracy and .155 higher AVeriTeC score\n(evaluating the adequacy of the evidence). Through ablations, we show the\nimportance of various design choices, such as the question generation method,\nmedium-sized context, reasoning with one document at a time, adding metadata,\nparaphrasing, reducing the problem to two classes, and reconsidering the final\nverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and\n.477 AVeriTeC score on the test set."
                },
                "authors": [
                    {
                        "name": "Christopher Malon"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Malon"
                },
                "author": "Christopher Malon",
                "arxiv_comment": "To appear in the Seventh FEVER Workshop at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10162v2",
                "updated": "2024-11-08T18:22:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    22,
                    46,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-19T17:16:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    16,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Physics-Aware Combinatorial Assembly Sequence Planning using Data-free\n  Action Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Aware Combinatorial Assembly Sequence Planning using Data-free\n  Action Masking"
                },
                "summary": "Combinatorial assembly uses standardized unit primitives to build objects\nthat satisfy user specifications. This paper studies assembly sequence planning\n(ASP) for physical combinatorial assembly. Given the shape of the desired\nobject, the goal is to find a sequence of actions for placing unit primitives\nto build the target object. In particular, we aim to ensure the planned\nassembly sequence is physically executable. However, ASP for combinatorial\nassembly is particularly challenging due to its combinatorial nature. To\naddress the challenge, we employ deep reinforcement learning to learn a\nconstruction policy for placing unit primitives sequentially to build the\ndesired object. Specifically, we design an online physics-aware action mask\nthat filters out invalid actions, which effectively guides policy learning and\nensures violation-free deployment. In the end, we apply the proposed method to\nLego assembly with more than 250 3D structures. The experiment results\ndemonstrate that the proposed method plans physically valid assembly sequences\nto build all structures, achieving a $100\\%$ success rate, whereas the best\ncomparable baseline fails more than $40$ structures. Our implementation is\navailable at\n\\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial assembly uses standardized unit primitives to build objects\nthat satisfy user specifications. This paper studies assembly sequence planning\n(ASP) for physical combinatorial assembly. Given the shape of the desired\nobject, the goal is to find a sequence of actions for placing unit primitives\nto build the target object. In particular, we aim to ensure the planned\nassembly sequence is physically executable. However, ASP for combinatorial\nassembly is particularly challenging due to its combinatorial nature. To\naddress the challenge, we employ deep reinforcement learning to learn a\nconstruction policy for placing unit primitives sequentially to build the\ndesired object. Specifically, we design an online physics-aware action mask\nthat filters out invalid actions, which effectively guides policy learning and\nensures violation-free deployment. In the end, we apply the proposed method to\nLego assembly with more than 250 3D structures. The experiment results\ndemonstrate that the proposed method plans physically valid assembly sequences\nto build all structures, achieving a $100\\%$ success rate, whereas the best\ncomparable baseline fails more than $40$ structures. Our implementation is\navailable at\n\\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Alan Chen"
                    },
                    {
                        "name": "Weiye Zhao"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05748v1",
                "updated": "2024-11-08T18:08:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    8,
                    49,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T18:08:49Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    8,
                    49,
                    4,
                    313,
                    0
                ],
                "title": "Multi-Dimensional Reconfigurable, Physically Composable Hybrid\n  Diffractive Optical Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Reconfigurable, Physically Composable Hybrid\n  Diffractive Optical Neural Network"
                },
                "summary": "Diffractive optical neural networks (DONNs), leveraging free-space light wave\npropagation for ultra-parallel, high-efficiency computing, have emerged as\npromising artificial intelligence (AI) accelerators. However, their inherent\nlack of reconfigurability due to fixed optical structures post-fabrication\nhinders practical deployment in the face of dynamic AI workloads and evolving\napplications. To overcome this challenge, we introduce, for the first time, a\nmulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a\nphysically composable architecture that unlocks a new degree of freedom and\nunprecedented versatility in DONNs. By leveraging full-system learnability,\nMDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially\nexpanded functionality and superior task adaptability through the\ndifferentiable learning of system variables. Furthermore, MDR-HDONN adopts a\nhybrid optical/photonic design, combining the reconfigurability of integrated\nphotonics with the ultra-parallelism of free-space diffractive systems.\nExtensive evaluations demonstrate that MDR-HDONN has digital-comparable\naccuracy on various task adaptations with 74x faster speed and 194x lower\nenergy. Compared to prior DONNs, MDR-HDONN shows exponentially larger\nfunctional space with 5x faster training speed, paving the way for a new\nparadigm of versatile, composable, hybrid optical/photonic AI computing. We\nwill open-source our codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffractive optical neural networks (DONNs), leveraging free-space light wave\npropagation for ultra-parallel, high-efficiency computing, have emerged as\npromising artificial intelligence (AI) accelerators. However, their inherent\nlack of reconfigurability due to fixed optical structures post-fabrication\nhinders practical deployment in the face of dynamic AI workloads and evolving\napplications. To overcome this challenge, we introduce, for the first time, a\nmulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a\nphysically composable architecture that unlocks a new degree of freedom and\nunprecedented versatility in DONNs. By leveraging full-system learnability,\nMDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially\nexpanded functionality and superior task adaptability through the\ndifferentiable learning of system variables. Furthermore, MDR-HDONN adopts a\nhybrid optical/photonic design, combining the reconfigurability of integrated\nphotonics with the ultra-parallelism of free-space diffractive systems.\nExtensive evaluations demonstrate that MDR-HDONN has digital-comparable\naccuracy on various task adaptations with 74x faster speed and 194x lower\nenergy. Compared to prior DONNs, MDR-HDONN shows exponentially larger\nfunctional space with 5x faster training speed, paving the way for a new\nparadigm of versatile, composable, hybrid optical/photonic AI computing. We\nwill open-source our codes."
                },
                "authors": [
                    {
                        "name": "Ziang Yin"
                    },
                    {
                        "name": "Yu Yao"
                    },
                    {
                        "name": "Jeff Zhang"
                    },
                    {
                        "name": "Jiaqi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Gu"
                },
                "author": "Jiaqi Gu",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05732v1",
                "updated": "2024-11-08T17:44:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    44,
                    40,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T17:44:40Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    44,
                    40,
                    4,
                    313,
                    0
                ],
                "title": "Foundations for the psychological safety of human and autonomous\n  vehicles interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations for the psychological safety of human and autonomous\n  vehicles interaction"
                },
                "summary": "This paper addresses the critical issue of psychological safety in the design\nand operation of autonomous vehicles, which are increasingly integrated with\nartificial intelligence technologies. While traditional safety standards focus\nprimarily on physical safety, this paper emphasizes the psychological\nimplications that arise from human interactions with autonomous vehicles,\nhighlighting the importance of trust and perceived risk as significant factors\ninfluencing user acceptance. Through a review of existing safety techniques,\nthe paper defines psychological safety in the context of autonomous vehicles,\nproposes a risk model to identify and assess psychological risks, and adopts a\nsystem-theoretic analysis method. The paper illustrates the potential\npsychological hazards using a scenario involving a family's experience with an\nautonomous vehicle, aiming to systematically evaluate situations that could\nlead to psychological harm. By establishing a framework that incorporates\npsychological safety alongside physical safety, the paper contributes to the\nbroader discourse on the safe deployment of autonomous vehicle and aims to\nguide future developments in user-cantered design and regulatory practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical issue of psychological safety in the design\nand operation of autonomous vehicles, which are increasingly integrated with\nartificial intelligence technologies. While traditional safety standards focus\nprimarily on physical safety, this paper emphasizes the psychological\nimplications that arise from human interactions with autonomous vehicles,\nhighlighting the importance of trust and perceived risk as significant factors\ninfluencing user acceptance. Through a review of existing safety techniques,\nthe paper defines psychological safety in the context of autonomous vehicles,\nproposes a risk model to identify and assess psychological risks, and adopts a\nsystem-theoretic analysis method. The paper illustrates the potential\npsychological hazards using a scenario involving a family's experience with an\nautonomous vehicle, aiming to systematically evaluate situations that could\nlead to psychological harm. By establishing a framework that incorporates\npsychological safety alongside physical safety, the paper contributes to the\nbroader discourse on the safe deployment of autonomous vehicle and aims to\nguide future developments in user-cantered design and regulatory practices."
                },
                "authors": [
                    {
                        "name": "Yandika Sirgabsou"
                    },
                    {
                        "name": "Benjamin Hardin"
                    },
                    {
                        "name": "Franois Leblanc"
                    },
                    {
                        "name": "Efi Raili"
                    },
                    {
                        "name": "Pericle Salvini"
                    },
                    {
                        "name": "David Jackson"
                    },
                    {
                        "name": "Marina Jirotka"
                    },
                    {
                        "name": "Lars Kunze"
                    }
                ],
                "author_detail": {
                    "name": "Lars Kunze"
                },
                "author": "Lars Kunze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05718v1",
                "updated": "2024-11-08T17:20:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    20,
                    47,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T17:20:47Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    20,
                    47,
                    4,
                    313,
                    0
                ],
                "title": "A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust,\n  Reliable, and Safe Learning Techniques for Real-world Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust,\n  Reliable, and Safe Learning Techniques for Real-world Robotics"
                },
                "summary": "Machine learning methods have a groundbreaking impact in many application\ndomains, but their application on real robotic platforms is still limited.\nDespite the many challenges associated with combining machine learning\ntechnology with robotics, robot learning remains one of the most promising\ndirections for enhancing the capabilities of robots. When deploying\nlearning-based approaches on real robots, extra effort is required to address\nthe challenges posed by various real-world factors. To investigate the key\nfactors influencing real-world deployment and to encourage original solutions\nfrom different researchers, we organized the Robot Air Hockey Challenge at the\nNeurIPS 2023 conference. We selected the air hockey task as a benchmark,\nencompassing low-level robotics problems and high-level tactics. Different from\nother machine learning-centric benchmarks, participants need to tackle\npractical challenges in robotics, such as the sim-to-real gap, low-level\ncontrol issues, safety problems, real-time requirements, and the limited\navailability of real-world data. Furthermore, we focus on a dynamic\nenvironment, removing the typical assumption of quasi-static motions of other\nreal-world benchmarks. The competition's results show that solutions combining\nlearning-based approaches with prior knowledge outperform those relying solely\non data when real-world deployment is challenging. Our ablation study reveals\nwhich real-world factors may be overlooked when building a learning-based\nsolution. The successful real-world air hockey deployment of best-performing\nagents sets the foundation for future competitions and follow-up research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning methods have a groundbreaking impact in many application\ndomains, but their application on real robotic platforms is still limited.\nDespite the many challenges associated with combining machine learning\ntechnology with robotics, robot learning remains one of the most promising\ndirections for enhancing the capabilities of robots. When deploying\nlearning-based approaches on real robots, extra effort is required to address\nthe challenges posed by various real-world factors. To investigate the key\nfactors influencing real-world deployment and to encourage original solutions\nfrom different researchers, we organized the Robot Air Hockey Challenge at the\nNeurIPS 2023 conference. We selected the air hockey task as a benchmark,\nencompassing low-level robotics problems and high-level tactics. Different from\nother machine learning-centric benchmarks, participants need to tackle\npractical challenges in robotics, such as the sim-to-real gap, low-level\ncontrol issues, safety problems, real-time requirements, and the limited\navailability of real-world data. Furthermore, we focus on a dynamic\nenvironment, removing the typical assumption of quasi-static motions of other\nreal-world benchmarks. The competition's results show that solutions combining\nlearning-based approaches with prior knowledge outperform those relying solely\non data when real-world deployment is challenging. Our ablation study reveals\nwhich real-world factors may be overlooked when building a learning-based\nsolution. The successful real-world air hockey deployment of best-performing\nagents sets the foundation for future competitions and follow-up research\ndirections."
                },
                "authors": [
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Jonas Gnster"
                    },
                    {
                        "name": "Niklas Funk"
                    },
                    {
                        "name": "Simon Grger"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Julius Jankowski"
                    },
                    {
                        "name": "Ante Mari"
                    },
                    {
                        "name": "Sylvain Calinon"
                    },
                    {
                        "name": "Andrej Orsula"
                    },
                    {
                        "name": "Miguel Olivares-Mendez"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    },
                    {
                        "name": "Gerhard Neumann"
                    },
                    {
                        "name": "Amarildo Likmeta Amirhossein Zhalehmehrabi"
                    },
                    {
                        "name": "Thomas Bonenfant"
                    },
                    {
                        "name": "Marcello Restelli"
                    },
                    {
                        "name": "Davide Tateo"
                    },
                    {
                        "name": "Ziyuan Liu"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "arxiv_comment": "Accept at NeurIPS 2024 Dataset and Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04242v2",
                "updated": "2024-11-08T17:16:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    16,
                    29,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-06T20:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    20,
                    11,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "Multimodal Structure-Aware Quantum Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Structure-Aware Quantum Data Processing"
                },
                "summary": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured."
                },
                "authors": [
                    {
                        "name": "Hala Hawashin"
                    },
                    {
                        "name": "Mehrnoosh Sadrzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrnoosh Sadrzadeh"
                },
                "author": "Mehrnoosh Sadrzadeh",
                "arxiv_comment": "10 Pages, 16 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05699v1",
                "updated": "2024-11-08T16:53:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    53,
                    12,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:53:12Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    53,
                    12,
                    4,
                    313,
                    0
                ],
                "title": "Renewable Energy Powered and Open RAN-based Architecture for 5G Fixed\n  Wireless Access Provisioning in Rural Areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Renewable Energy Powered and Open RAN-based Architecture for 5G Fixed\n  Wireless Access Provisioning in Rural Areas"
                },
                "summary": "Due to the high costs of optical fiber deployment in Low-Density and Rural\nAreas (LDRAs), 5G Fixed Wireless Access (5G FWA) recently emerged as an\naffordable solution. A widely adopted deployment scenario of 5G FWA includes\nedge cloud that supports computing services and Radio Access Network (RAN)\nfunctions. Such edge cloud requires network and energy resources for 5G FWA.\nThis paper proposes renewable energy powered and Open RAN-based architecture\nfor 5G FWA serving LDRAs using three-level closed-loops. Open RAN is a new 5G\nRAN architecture allowing Open Central Unit and Open Distributed Unit to be\ndistributed in virtualized environment. The first closed-loop distributes radio\nresources to Open RAN instances and slices at the edge cloud. The second\nclosed-loop allocates radio resources to houses. We design a new energy model\nthat leverages renewable energy. We jointly optimize radio and energy resource\nallocation in closed-loop 3. We formulate ultra-small and small-time scale\noptimization problems that link closed-loops to maximize communication utility\nwhile minimizing energy costs. We propose reinforcement learning and successive\nconvex approximation to solve the formulated problems. Then, we use solution\ndata and continual learning to improve resource allocation on a large\ntimescale. Our proposal satisfies 97.14% slice delay budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high costs of optical fiber deployment in Low-Density and Rural\nAreas (LDRAs), 5G Fixed Wireless Access (5G FWA) recently emerged as an\naffordable solution. A widely adopted deployment scenario of 5G FWA includes\nedge cloud that supports computing services and Radio Access Network (RAN)\nfunctions. Such edge cloud requires network and energy resources for 5G FWA.\nThis paper proposes renewable energy powered and Open RAN-based architecture\nfor 5G FWA serving LDRAs using three-level closed-loops. Open RAN is a new 5G\nRAN architecture allowing Open Central Unit and Open Distributed Unit to be\ndistributed in virtualized environment. The first closed-loop distributes radio\nresources to Open RAN instances and slices at the edge cloud. The second\nclosed-loop allocates radio resources to houses. We design a new energy model\nthat leverages renewable energy. We jointly optimize radio and energy resource\nallocation in closed-loop 3. We formulate ultra-small and small-time scale\noptimization problems that link closed-loops to maximize communication utility\nwhile minimizing energy costs. We propose reinforcement learning and successive\nconvex approximation to solve the formulated problems. Then, we use solution\ndata and continual learning to improve resource allocation on a large\ntimescale. Our proposal satisfies 97.14% slice delay budget."
                },
                "authors": [
                    {
                        "name": "Anselme Ndikumana"
                    },
                    {
                        "name": "Kim Khoa Nguyen"
                    },
                    {
                        "name": "Mohamed Cheriet"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Cheriet"
                },
                "author": "Mohamed Cheriet",
                "arxiv_doi": "10.1109/TGCN.2024.3431989",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TGCN.2024.3431989",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Green Communications and Networking ( Volume:\n  8, Issue: 3, September 2024)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02791v2",
                "updated": "2024-11-08T16:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    50,
                    24,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-04T21:29:56Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    21,
                    29,
                    56,
                    1,
                    156,
                    0
                ],
                "title": "Language Models can Infer Action Semantics for Symbolic Planners from\n  Environment Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Infer Action Semantics for Symbolic Planners from\n  Environment Feedback"
                },
                "summary": "Symbolic planners can discover a sequence of actions from initial to goal\nstates given expert-defined, domain-specific logical action semantics. Large\nLanguage Models (LLMs) can directly generate such sequences, but limitations in\nreasoning and state-tracking often result in plans that are insufficient or\nunexecutable. We propose Predicting Semantics of Actions with Language Models\n(PSALM), which automatically learns action semantics by leveraging the\nstrengths of both symbolic planners and LLMs. PSALM repeatedly proposes and\nexecutes plans, using the LLM to partially generate plans and to infer\ndomain-specific action semantics based on execution outcomes. PSALM maintains a\nbelief over possible action semantics that is iteratively updated until a goal\nstate is reached. Experiments on 7 environments show that when learning just\nfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to\n100%, and explores the environment more efficiently than prior work to infer\nground truth domain action semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic planners can discover a sequence of actions from initial to goal\nstates given expert-defined, domain-specific logical action semantics. Large\nLanguage Models (LLMs) can directly generate such sequences, but limitations in\nreasoning and state-tracking often result in plans that are insufficient or\nunexecutable. We propose Predicting Semantics of Actions with Language Models\n(PSALM), which automatically learns action semantics by leveraging the\nstrengths of both symbolic planners and LLMs. PSALM repeatedly proposes and\nexecutes plans, using the LLM to partially generate plans and to infer\ndomain-specific action semantics based on execution outcomes. PSALM maintains a\nbelief over possible action semantics that is iteratively updated until a goal\nstate is reached. Experiments on 7 environments show that when learning just\nfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to\n100%, and explores the environment more efficiently than prior work to infer\nground truth domain action semantics."
                },
                "authors": [
                    {
                        "name": "Wang Zhu"
                    },
                    {
                        "name": "Ishika Singh"
                    },
                    {
                        "name": "Robin Jia"
                    },
                    {
                        "name": "Jesse Thomason"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Thomason"
                },
                "author": "Jesse Thomason",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v2",
                "updated": "2024-11-08T16:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    50,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v3",
                "updated": "2024-11-08T16:42:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    42,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4"
                },
                "summary": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00177v2",
                "updated": "2024-11-08T16:42:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    42,
                    18,
                    4,
                    313,
                    0
                ],
                "published": "2024-10-31T19:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    48,
                    12,
                    3,
                    305,
                    0
                ],
                "title": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction"
                },
                "summary": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction."
                },
                "authors": [
                    {
                        "name": "Andre Niyongabo Rubungo"
                    },
                    {
                        "name": "Kangming Li"
                    },
                    {
                        "name": "Jason Hattrick-Simpers"
                    },
                    {
                        "name": "Adji Bousso Dieng"
                    }
                ],
                "author_detail": {
                    "name": "Adji Bousso Dieng"
                },
                "author": "Adji Bousso Dieng",
                "arxiv_comment": "Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can\n  be found at: https://github.com/vertaix/LLM4Mat-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05679v1",
                "updated": "2024-11-08T16:29:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:29:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text"
                },
                "summary": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning."
                },
                "authors": [
                    {
                        "name": "Xiulong Liu"
                    },
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Eli Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shlizerman"
                },
                "author": "Eli Shlizerman",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05665v1",
                "updated": "2024-11-08T16:07:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    7,
                    47,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:07:47Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    7,
                    47,
                    4,
                    313,
                    0
                ],
                "title": "Unmasking the Limits of Large Language Models: A Systematic Evaluation\n  of Masked Text Processing Ability through MskQA and MskCal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Limits of Large Language Models: A Systematic Evaluation\n  of Masked Text Processing Ability through MskQA and MskCal"
                },
                "summary": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities."
                },
                "authors": [
                    {
                        "name": "Fuka Matsuzaki"
                    },
                    {
                        "name": "Haru-Tada Sato"
                    }
                ],
                "author_detail": {
                    "name": "Haru-Tada Sato"
                },
                "author": "Haru-Tada Sato",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v2",
                "updated": "2024-11-08T16:06:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    6,
                    9,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Building Very Large Knowledge Bases from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Building Very Large Knowledge Bases from Language Models"
                },
                "summary": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at http://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at http://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "11 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05664v1",
                "updated": "2024-11-08T16:05:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    5,
                    54,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T16:05:54Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    5,
                    54,
                    4,
                    313,
                    0
                ],
                "title": "Digital Twin Backed Closed-Loops for Energy-Aware and Open RAN-based\n  Fixed Wireless Access Serving Rural Areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin Backed Closed-Loops for Energy-Aware and Open RAN-based\n  Fixed Wireless Access Serving Rural Areas"
                },
                "summary": "Internet access in rural areas should be improved to support digital\ninclusion and 5G services. Due to the high deployment costs of fiber optics in\nthese areas, Fixed Wireless Access (FWA) has become a preferable alternative.\nAdditionally, the Open Radio Access Network (O-RAN) can facilitate the\ninteroperability of FWA elements, allowing some FWA functions to be deployed at\nthe edge cloud. However, deploying edge clouds in rural areas can increase\nnetwork and energy costs. To address these challenges, we propose a closed-loop\nsystem assisted by a Digital Twin (DT) to automate energy-aware O-RAN based FWA\nresource management in rural areas. We consider the FWA and edge cloud as the\nPhysical Twin (PT) and design a closed-loop that distributes radio resources to\nedge cloud instances for scheduling. We develop another closed-loop for\nintra-slice resource allocation to houses. We design an energy model that\nintegrates radio resource allocation and formulate ultra-small and\nsmall-timescale optimizations for the PT to maximize slice requirement\nsatisfaction while minimizing energy costs. We then design a reinforcement\nlearning approach and successive convex approximation to address the formulated\nproblems. We present a DT that replicates the PT by incorporating solution\nexperiences into future states. The results show that our approach efficiently\nuses radio and energy resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet access in rural areas should be improved to support digital\ninclusion and 5G services. Due to the high deployment costs of fiber optics in\nthese areas, Fixed Wireless Access (FWA) has become a preferable alternative.\nAdditionally, the Open Radio Access Network (O-RAN) can facilitate the\ninteroperability of FWA elements, allowing some FWA functions to be deployed at\nthe edge cloud. However, deploying edge clouds in rural areas can increase\nnetwork and energy costs. To address these challenges, we propose a closed-loop\nsystem assisted by a Digital Twin (DT) to automate energy-aware O-RAN based FWA\nresource management in rural areas. We consider the FWA and edge cloud as the\nPhysical Twin (PT) and design a closed-loop that distributes radio resources to\nedge cloud instances for scheduling. We develop another closed-loop for\nintra-slice resource allocation to houses. We design an energy model that\nintegrates radio resource allocation and formulate ultra-small and\nsmall-timescale optimizations for the PT to maximize slice requirement\nsatisfaction while minimizing energy costs. We then design a reinforcement\nlearning approach and successive convex approximation to address the formulated\nproblems. We present a DT that replicates the PT by incorporating solution\nexperiences into future states. The results show that our approach efficiently\nuses radio and energy resources."
                },
                "authors": [
                    {
                        "name": "Anselme Ndikumana"
                    },
                    {
                        "name": "Kim Khoa Nguyen"
                    },
                    {
                        "name": "Mohamed Cheriet"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Cheriet"
                },
                "author": "Mohamed Cheriet",
                "arxiv_doi": "10.1109/TMC.2024.3482985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMC.2024.3482985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Mobile Computing 01 (2024): 1-15",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05653v1",
                "updated": "2024-11-08T15:49:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    49,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:49:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    49,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "The influence of persona and conversational task on social interactions\n  with a LLM-controlled embodied conversational agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of persona and conversational task on social interactions\n  with a LLM-controlled embodied conversational agent"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment."
                },
                "authors": [
                    {
                        "name": "Leon O. H. Kroczek"
                    },
                    {
                        "name": "Alexander May"
                    },
                    {
                        "name": "Selina Hettenkofer"
                    },
                    {
                        "name": "Andreas Ruider"
                    },
                    {
                        "name": "Bernd Ludwig"
                    },
                    {
                        "name": "Andreas Mhlberger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Mhlberger"
                },
                "author": "Andreas Mhlberger",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05651v1",
                "updated": "2024-11-08T15:46:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    46,
                    10,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:46:10Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    46,
                    10,
                    4,
                    313,
                    0
                ],
                "title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning\n  and Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning\n  and Execution"
                },
                "summary": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks\nbased on observations and execute tasks by creating visualizations and\ninteractive exploration to gain insights. This process demands skills in\nprogramming, data processing, and visualization tools, highlighting the need\nfor a more intelligent, streamlined VA approach. Large language models (LLMs)\nhave recently been developed as agents to handle various tasks with dynamic\nplanning and tool-using capabilities, offering the potential to enhance the\nefficiency and versatility of VA. We propose LightVA, a lightweight VA\nframework that supports task decomposition, data analysis, and interactive\nexploration through human-agent collaboration. Our method is designed to help\nusers progressively translate high-level analytical goals into low-level tasks,\nproducing visualizations and deriving insights. Specifically, we introduce an\nLLM agent-based task planning and execution strategy, employing a recursive\nprocess involving a planner, executor, and controller. The planner is\nresponsible for recommending and decomposing tasks, the executor handles task\nexecution, including data analysis, visualization generation and multi-view\ncomposition, and the controller coordinates the interaction between the planner\nand executor. Building on the framework, we develop a system with a hybrid user\ninterface that includes a task flow diagram for monitoring and managing the\ntask planning process, a visualization panel for interactive data exploration,\nand a chat view for guiding the model through natural language instructions. We\nexamine the effectiveness of our method through a usage scenario and an expert\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks\nbased on observations and execute tasks by creating visualizations and\ninteractive exploration to gain insights. This process demands skills in\nprogramming, data processing, and visualization tools, highlighting the need\nfor a more intelligent, streamlined VA approach. Large language models (LLMs)\nhave recently been developed as agents to handle various tasks with dynamic\nplanning and tool-using capabilities, offering the potential to enhance the\nefficiency and versatility of VA. We propose LightVA, a lightweight VA\nframework that supports task decomposition, data analysis, and interactive\nexploration through human-agent collaboration. Our method is designed to help\nusers progressively translate high-level analytical goals into low-level tasks,\nproducing visualizations and deriving insights. Specifically, we introduce an\nLLM agent-based task planning and execution strategy, employing a recursive\nprocess involving a planner, executor, and controller. The planner is\nresponsible for recommending and decomposing tasks, the executor handles task\nexecution, including data analysis, visualization generation and multi-view\ncomposition, and the controller coordinates the interaction between the planner\nand executor. Building on the framework, we develop a system with a hybrid user\ninterface that includes a task flow diagram for monitoring and managing the\ntask planning process, a visualization panel for interactive data exploration,\nand a chat view for guiding the model through natural language instructions. We\nexamine the effectiveness of our method through a usage scenario and an expert\nstudy."
                },
                "authors": [
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Linbin Xiang"
                    },
                    {
                        "name": "Xiaowen Zhang"
                    },
                    {
                        "name": "Zifei Guo"
                    },
                    {
                        "name": "Cagatay Turkay"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03503v2",
                "updated": "2024-11-08T15:43:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    43,
                    55,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-05T20:33:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    33,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "TwiNet: Connecting Real World Networks to their Digital Twins Through a\n  Live Bidirectional Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwiNet: Connecting Real World Networks to their Digital Twins Through a\n  Live Bidirectional Link"
                },
                "summary": "The wireless spectrum's increasing complexity poses challenges and\nopportunities, highlighting the necessity for real-time solutions and robust\ndata processing capabilities. Digital Twin (DT), virtual replicas of physical\nsystems, integrate real-time data to mirror their real-world counterparts,\nenabling precise monitoring and optimization. Incorporating DTs into wireless\ncommunication enhances predictive maintenance, resource allocation, and\ntroubleshooting, thus bolstering network reliability. Our paper introduces\nTwiNet, enabling bidirectional, near-realtime links between real-world wireless\nspectrum scenarios and DT replicas. Utilizing the protocol, MQTT, we can\nachieve data transfer times with an average latency of 14 ms, suitable for\nreal-time communication. This is confirmed by monitoring real-world traffic and\nmirroring it in real-time within the DT's wireless environment. We evaluate\nTwiNet's performance in two use cases: (i) assessing risky traffic\nconfigurations of UEs in a Safe Adaptive Data Rate (SADR) system, improving\nnetwork performance by approximately 15% compared to original network\nselections; and (ii) deploying new CNNs in response to jammed pilots, achieving\nup to 97% accuracy training on artificial data and deploying a new model in as\nlow as 2 minutes to counter persistent adversaries. TwiNet enables swift\ndeployment and adaptation of DTs, addressing crucial challenges in modern\nwireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wireless spectrum's increasing complexity poses challenges and\nopportunities, highlighting the necessity for real-time solutions and robust\ndata processing capabilities. Digital Twin (DT), virtual replicas of physical\nsystems, integrate real-time data to mirror their real-world counterparts,\nenabling precise monitoring and optimization. Incorporating DTs into wireless\ncommunication enhances predictive maintenance, resource allocation, and\ntroubleshooting, thus bolstering network reliability. Our paper introduces\nTwiNet, enabling bidirectional, near-realtime links between real-world wireless\nspectrum scenarios and DT replicas. Utilizing the protocol, MQTT, we can\nachieve data transfer times with an average latency of 14 ms, suitable for\nreal-time communication. This is confirmed by monitoring real-world traffic and\nmirroring it in real-time within the DT's wireless environment. We evaluate\nTwiNet's performance in two use cases: (i) assessing risky traffic\nconfigurations of UEs in a Safe Adaptive Data Rate (SADR) system, improving\nnetwork performance by approximately 15% compared to original network\nselections; and (ii) deploying new CNNs in response to jammed pilots, achieving\nup to 97% accuracy training on artificial data and deploying a new model in as\nlow as 2 minutes to counter persistent adversaries. TwiNet enables swift\ndeployment and adaptation of DTs, addressing crucial challenges in modern\nwireless communication systems."
                },
                "authors": [
                    {
                        "name": "Clifton Paul Robinson"
                    },
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Pedram Johari"
                    },
                    {
                        "name": "Francesca Cuomo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "6 pages, 7 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05641v1",
                "updated": "2024-11-08T15:35:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    35,
                    43,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:35:43Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    35,
                    43,
                    4,
                    313,
                    0
                ],
                "title": "Evaluating Large Language Model Capability in Vietnamese Fact-Checking\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Model Capability in Vietnamese Fact-Checking\n  Data Generation"
                },
                "summary": "Large Language Models (LLMs), with gradually improving reading comprehension\nand reasoning capabilities, are being applied to a range of complex language\ntasks, including the automatic generation of language data for various\npurposes. However, research on applying LLMs for automatic data generation in\nlow-resource languages like Vietnamese is still underdeveloped and lacks\ncomprehensive evaluation. In this paper, we explore the use of LLMs for\nautomatic data generation for the Vietnamese fact-checking task, which faces\nsignificant data limitations. Specifically, we focus on fact-checking data\nwhere claims are synthesized from multiple evidence sentences to assess the\ninformation synthesis capabilities of LLMs. We develop an automatic data\nconstruction process using simple prompt techniques on LLMs and explore several\nmethods to improve the quality of the generated data. To evaluate the quality\nof the data generated by LLMs, we conduct both manual quality assessments and\nperformance evaluations using language models. Experimental results and manual\nevaluations illustrate that while the quality of the generated data has\nsignificantly improved through fine-tuning techniques, LLMs still cannot match\nthe data quality produced by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with gradually improving reading comprehension\nand reasoning capabilities, are being applied to a range of complex language\ntasks, including the automatic generation of language data for various\npurposes. However, research on applying LLMs for automatic data generation in\nlow-resource languages like Vietnamese is still underdeveloped and lacks\ncomprehensive evaluation. In this paper, we explore the use of LLMs for\nautomatic data generation for the Vietnamese fact-checking task, which faces\nsignificant data limitations. Specifically, we focus on fact-checking data\nwhere claims are synthesized from multiple evidence sentences to assess the\ninformation synthesis capabilities of LLMs. We develop an automatic data\nconstruction process using simple prompt techniques on LLMs and explore several\nmethods to improve the quality of the generated data. To evaluate the quality\nof the data generated by LLMs, we conduct both manual quality assessments and\nperformance evaluations using language models. Experimental results and manual\nevaluations illustrate that while the quality of the generated data has\nsignificantly improved through fine-tuning techniques, LLMs still cannot match\nthe data quality produced by humans."
                },
                "authors": [
                    {
                        "name": "Long Truong To"
                    },
                    {
                        "name": "Hung Tuan Le"
                    },
                    {
                        "name": "Dat Van-Thanh Nguyen"
                    },
                    {
                        "name": "Manh Trong Nguyen"
                    },
                    {
                        "name": "Tri Thien Nguyen"
                    },
                    {
                        "name": "Tin Van Huynh"
                    },
                    {
                        "name": "Kiet Van Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Kiet Van Nguyen"
                },
                "author": "Kiet Van Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05639v1",
                "updated": "2024-11-08T15:34:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    34,
                    8,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T15:34:08Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    15,
                    34,
                    8,
                    4,
                    313,
                    0
                ],
                "title": "Assessing Open-Source Large Language Models on Argumentation Mining\n  Subtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Open-Source Large Language Models on Argumentation Mining\n  Subtasks"
                },
                "summary": "We explore the capability of four open-sourcelarge language models (LLMs) in\nargumentation mining (AM). We conduct experiments on three different corpora;\npersuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based\non two argumentation mining sub-tasks: (i) argumentative discourse units\nclassifications (ADUC), and (ii) argumentative relation classification (ARC).\nThis work aims to assess the argumentation capability of open-source LLMs,\nincluding Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot\nand few-shot scenarios. Our analysis contributes to further assessing\ncomputational argumentation with open-source LLMs in future research efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the capability of four open-sourcelarge language models (LLMs) in\nargumentation mining (AM). We conduct experiments on three different corpora;\npersuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based\non two argumentation mining sub-tasks: (i) argumentative discourse units\nclassifications (ADUC), and (ii) argumentative relation classification (ARC).\nThis work aims to assess the argumentation capability of open-source LLMs,\nincluding Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot\nand few-shot scenarios. Our analysis contributes to further assessing\ncomputational argumentation with open-source LLMs in future research efforts."
                },
                "authors": [
                    {
                        "name": "Mohammad Yeghaneh Abkenar"
                    },
                    {
                        "name": "Weixing Wang"
                    },
                    {
                        "name": "Hendrik Graupner"
                    },
                    {
                        "name": "Manfred Stede"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Stede"
                },
                "author": "Manfred Stede",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05609v1",
                "updated": "2024-11-08T14:52:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:52:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis"
                },
                "summary": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and a Large Language\nModel (LLM) to generate disease diagnoses based on the predicted concepts. We\nvalidate our approach on three skin lesion datasets, demonstrating that it\noutperforms traditional CBMs and state-of-the-art explainable methods, all\nwithout requiring any training and utilizing only a few annotated examples. The\ncode is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and a Large Language\nModel (LLM) to generate disease diagnoses based on the predicted concepts. We\nvalidate our approach on three skin lesion datasets, demonstrating that it\noutperforms traditional CBMs and state-of-the-art explainable methods, all\nwithout requiring any training and utilizing only a few annotated examples. The\ncode is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis."
                },
                "authors": [
                    {
                        "name": "Cristiano Patrcio"
                    },
                    {
                        "name": "Lus F. Teixeira"
                    },
                    {
                        "name": "Joo C. Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joo C. Neves"
                },
                "author": "Joo C. Neves",
                "arxiv_comment": "Preprint submitted for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v3",
                "updated": "2024-11-08T14:46:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    46,
                    40,
                    4,
                    313,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05593v1",
                "updated": "2024-11-08T14:26:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    26,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:26:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    26,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "Evaluating and Adapting Large Language Models to Represent Folktales in\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Adapting Large Language Models to Represent Folktales in\n  Low-Resource Languages"
                },
                "summary": "Folktales are a rich resource of knowledge about the society and culture of a\ncivilisation. Digital folklore research aims to use automated techniques to\nbetter understand these folktales, and it relies on abstract representations of\nthe textual data. Although a number of large language models (LLMs) claim to be\nable to represent low-resource langauges such as Irish and Gaelic, we present\ntwo classification tasks to explore how useful these representations are, and\nthree adaptations to improve the performance of these models. We find that\nadapting the models to work with longer sequences, and continuing pre-training\non the domain of folktales improves classification performance, although these\nfindings are tempered by the impressive performance of a baseline SVM with\nnon-contextual features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Folktales are a rich resource of knowledge about the society and culture of a\ncivilisation. Digital folklore research aims to use automated techniques to\nbetter understand these folktales, and it relies on abstract representations of\nthe textual data. Although a number of large language models (LLMs) claim to be\nable to represent low-resource langauges such as Irish and Gaelic, we present\ntwo classification tasks to explore how useful these representations are, and\nthree adaptations to improve the performance of these models. We find that\nadapting the models to work with longer sequences, and continuing pre-training\non the domain of folktales improves classification performance, although these\nfindings are tempered by the impressive performance of a baseline SVM with\nnon-contextual features."
                },
                "authors": [
                    {
                        "name": "JA Meaney"
                    },
                    {
                        "name": "Beatrice Alex"
                    },
                    {
                        "name": "William Lamb"
                    }
                ],
                "author_detail": {
                    "name": "William Lamb"
                },
                "author": "William Lamb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02355v3",
                "updated": "2024-11-08T14:17:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    17,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-05-03T02:48:55Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    2,
                    48,
                    55,
                    4,
                    124,
                    0
                ],
                "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation"
                },
                "summary": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/."
                },
                "authors": [
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Jizheng Chen"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Lingyue Fu"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05577v1",
                "updated": "2024-11-08T14:07:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    7,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T14:07:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    7,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "Exploring Relationships Between Cryptocurrency News Outlets and\n  Influencers' Twitter Activity and Market Prices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Relationships Between Cryptocurrency News Outlets and\n  Influencers' Twitter Activity and Market Prices"
                },
                "summary": "Academics increasingly acknowledge the predictive power of social media for a\nwide variety of events and, more specifically, for financial markets. Anecdotal\nand empirical findings show that cryptocurrencies are among the financial\nassets that have been affected by news and influencers' activities on Twitter.\nHowever, the extent to which Twitter crypto influencer's posts about trading\nsignals and their effect on market prices is mostly unexplored. In this paper,\nwe use LLMs to uncover buy and not-buy signals from influencers and news\noutlets' Twitter posts and use a VAR analysis with Granger Causality tests and\ncross-correlation analysis to understand how these trading signals are\ntemporally correlated with the top nine major cryptocurrencies' prices.\nOverall, the results show a mixed pattern across cryptocurrencies and temporal\nperiods. However, we found that for the top three cryptocurrencies with the\nhighest presence within news and influencer posts, their aggregated\nLLM-detected trading signal over the preceding 24 hours granger-causes\nfluctuations in their market prices, exhibiting a lag of at least 6 hours. In\naddition, the results reveal fundamental differences in how influencers and\nnews outlets cover cryptocurrencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academics increasingly acknowledge the predictive power of social media for a\nwide variety of events and, more specifically, for financial markets. Anecdotal\nand empirical findings show that cryptocurrencies are among the financial\nassets that have been affected by news and influencers' activities on Twitter.\nHowever, the extent to which Twitter crypto influencer's posts about trading\nsignals and their effect on market prices is mostly unexplored. In this paper,\nwe use LLMs to uncover buy and not-buy signals from influencers and news\noutlets' Twitter posts and use a VAR analysis with Granger Causality tests and\ncross-correlation analysis to understand how these trading signals are\ntemporally correlated with the top nine major cryptocurrencies' prices.\nOverall, the results show a mixed pattern across cryptocurrencies and temporal\nperiods. However, we found that for the top three cryptocurrencies with the\nhighest presence within news and influencer posts, their aggregated\nLLM-detected trading signal over the preceding 24 hours granger-causes\nfluctuations in their market prices, exhibiting a lag of at least 6 hours. In\naddition, the results reveal fundamental differences in how influencers and\nnews outlets cover cryptocurrencies."
                },
                "authors": [
                    {
                        "name": "Meysam Alizadeh"
                    },
                    {
                        "name": "Yasaman Asgari"
                    },
                    {
                        "name": "Zeynab Samei"
                    },
                    {
                        "name": "Sara Yari"
                    },
                    {
                        "name": "Shirin Dehghani"
                    },
                    {
                        "name": "Mael Kubli"
                    },
                    {
                        "name": "Darya Zare"
                    },
                    {
                        "name": "Juan Diego Bermeo"
                    },
                    {
                        "name": "Veronika Batzdorfer"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Gilardi"
                },
                "author": "Fabrizio Gilardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15267v2",
                "updated": "2024-11-08T14:02:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    2,
                    13,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-21T16:03:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    16,
                    3,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Evaluating Diversity in Automatic Poetry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Diversity in Automatic Poetry Generation"
                },
                "summary": "Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation -- can humans distinguish between automatic and\nhuman generated poetry -- we evaluate the diversity of automatically generated\npoetry (with a focus on quatrains), by comparing distributions of generated\npoetry to distributions of human poetry along structural, lexical, semantic and\nstylistic dimensions, assessing different model types (word vs.\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\nunconditioned). We find that current automatic poetry systems are considerably\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\nare semantically too uniform and even do not match the length distribution of\nhuman poetry. Our experiments reveal, however, that style-conditioning and\ncharacter-level modeling clearly increases diversity across virtually all\ndimensions we explore. Our identified limitations may serve as the basis for\nmore genuinely diverse future poetry generation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation -- can humans distinguish between automatic and\nhuman generated poetry -- we evaluate the diversity of automatically generated\npoetry (with a focus on quatrains), by comparing distributions of generated\npoetry to distributions of human poetry along structural, lexical, semantic and\nstylistic dimensions, assessing different model types (word vs.\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\nunconditioned). We find that current automatic poetry systems are considerably\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\nare semantically too uniform and even do not match the length distribution of\nhuman poetry. Our experiments reveal, however, that style-conditioning and\ncharacter-level modeling clearly increases diversity across virtually all\ndimensions we explore. Our identified limitations may serve as the basis for\nmore genuinely diverse future poetry generation models."
                },
                "authors": [
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Hannes Grner"
                    },
                    {
                        "name": "Sina Zarrie"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "EMNLP 2024 main; camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05572v1",
                "updated": "2024-11-08T13:51:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    51,
                    37,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:51:37Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    51,
                    37,
                    4,
                    313,
                    0
                ],
                "title": "Why These Documents? Explainable Generative Retrieval with Hierarchical\n  Category Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why These Documents? Explainable Generative Retrieval with Hierarchical\n  Category Paths"
                },
                "summary": "Generative retrieval has recently emerged as a new alternative of traditional\ninformation retrieval approaches. However, existing generative retrieval\nmethods directly decode docid when a query is given, making it impossible to\nprovide users with explanations as an answer for \"Why this document is\nretrieved?\". To address this limitation, we propose Hierarchical Category\nPath-Enhanced Generative Retrieval(HyPE), which enhances explainability by\ngenerating hierarchical category paths step-by-step before decoding docid. HyPE\nleverages hierarchical category paths as explanation, progressing from broad to\nspecific semantic categories. This approach enables diverse explanations for\nthe same document depending on the query by using shared category paths between\nthe query and the document, and provides reasonable explanation by reflecting\nthe document's semantic structure through a coarse-to-fine manner. HyPE\nconstructs category paths with external high-quality semantic hierarchy,\nleverages LLM to select appropriate candidate paths for each document, and\noptimizes the generative retrieval model with path-augmented dataset. During\ninference, HyPE utilizes path-aware reranking strategy to aggregate diverse\ntopic information, allowing the most relevant documents to be prioritized in\nthe final ranked list of docids. Our extensive experiments demonstrate that\nHyPE not only offers a high level of explainability but also improves the\nretrieval performance in the document retrieval task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval has recently emerged as a new alternative of traditional\ninformation retrieval approaches. However, existing generative retrieval\nmethods directly decode docid when a query is given, making it impossible to\nprovide users with explanations as an answer for \"Why this document is\nretrieved?\". To address this limitation, we propose Hierarchical Category\nPath-Enhanced Generative Retrieval(HyPE), which enhances explainability by\ngenerating hierarchical category paths step-by-step before decoding docid. HyPE\nleverages hierarchical category paths as explanation, progressing from broad to\nspecific semantic categories. This approach enables diverse explanations for\nthe same document depending on the query by using shared category paths between\nthe query and the document, and provides reasonable explanation by reflecting\nthe document's semantic structure through a coarse-to-fine manner. HyPE\nconstructs category paths with external high-quality semantic hierarchy,\nleverages LLM to select appropriate candidate paths for each document, and\noptimizes the generative retrieval model with path-augmented dataset. During\ninference, HyPE utilizes path-aware reranking strategy to aggregate diverse\ntopic information, allowing the most relevant documents to be prioritized in\nthe final ranked list of docids. Our extensive experiments demonstrate that\nHyPE not only offers a high level of explainability but also improves the\nretrieval performance in the document retrieval task."
                },
                "authors": [
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Susik Yoon"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05547v1",
                "updated": "2024-11-08T13:09:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    9,
                    14,
                    4,
                    313,
                    0
                ],
                "title": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance."
                },
                "authors": [
                    {
                        "name": "Geonmin Kim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Hancheol Park"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16254v2",
                "updated": "2024-11-08T12:54:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    54,
                    16,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-24T01:31:03Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    1,
                    31,
                    3,
                    0,
                    176,
                    0
                ],
                "title": "Confidence Regulation Neurons in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Regulation Neurons in Language Models"
                },
                "summary": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences."
                },
                "authors": [
                    {
                        "name": "Alessandro Stolfo"
                    },
                    {
                        "name": "Ben Wu"
                    },
                    {
                        "name": "Wes Gurnee"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Xingyi Song"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17044v2",
                "updated": "2024-11-08T12:44:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    44,
                    49,
                    4,
                    313,
                    0
                ],
                "published": "2024-09-25T15:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not"
                },
                "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM."
                },
                "authors": [
                    {
                        "name": "Francesco Verdini"
                    },
                    {
                        "name": "Pierfrancesco Melucci"
                    },
                    {
                        "name": "Stefano Perna"
                    },
                    {
                        "name": "Francesco Cariaggi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Szymon Mazurek"
                    },
                    {
                        "name": "Marek Kasztelnik"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Sbastien Bratires"
                    },
                    {
                        "name": "Paolo Merialdo"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05533v1",
                "updated": "2024-11-08T12:42:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    42,
                    45,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:42:45Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    42,
                    45,
                    4,
                    313,
                    0
                ],
                "title": "Analyzing Logs of Large-Scale Software Systems using Time Curves\n  Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Logs of Large-Scale Software Systems using Time Curves\n  Visualization"
                },
                "summary": "Logs are crucial for analyzing large-scale software systems, offering\ninsights into system health, performance, security threats, potential bugs,\netc. However, their chaotic nature$\\unicode{x2013}$characterized by sheer\nvolume, lack of standards, and variability$\\unicode{x2013}$makes manual\nanalysis complex. The use of clustering algorithms can assist by grouping logs\ninto a smaller set of templates, but lose the temporal and relational context\nin doing so. On the contrary, Large Language Models (LLMs) can provide\nmeaningful explanations but struggle with processing large collections\nefficiently. Moreover, representation techniques for both approaches are\ntypically limited to either plain text or traditional charting, especially when\ndealing with large-scale systems. In this paper, we combine clustering and LLM\nsummarization with event detection and Multidimensional Scaling through the use\nof Time Curves to produce a holistic pipeline that enables efficient and\nautomatic summarization of vast collections of software system logs. The core\nof our approach is the proposal of a semimetric distance that effectively\nmeasures similarity between events, thus enabling a meaningful representation.\nWe show that our method can explain the main events of logs collected from\ndifferent applications without prior knowledge. We also show how the approach\ncan be used to detect general trends as well as outliers in parallel and\ndistributed systems by overlapping multiple projections. As a result, we expect\na significant reduction of the time required to analyze and resolve system-wide\nissues, identify performance bottlenecks and security risks, debug\napplications, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs are crucial for analyzing large-scale software systems, offering\ninsights into system health, performance, security threats, potential bugs,\netc. However, their chaotic nature$\\unicode{x2013}$characterized by sheer\nvolume, lack of standards, and variability$\\unicode{x2013}$makes manual\nanalysis complex. The use of clustering algorithms can assist by grouping logs\ninto a smaller set of templates, but lose the temporal and relational context\nin doing so. On the contrary, Large Language Models (LLMs) can provide\nmeaningful explanations but struggle with processing large collections\nefficiently. Moreover, representation techniques for both approaches are\ntypically limited to either plain text or traditional charting, especially when\ndealing with large-scale systems. In this paper, we combine clustering and LLM\nsummarization with event detection and Multidimensional Scaling through the use\nof Time Curves to produce a holistic pipeline that enables efficient and\nautomatic summarization of vast collections of software system logs. The core\nof our approach is the proposal of a semimetric distance that effectively\nmeasures similarity between events, thus enabling a meaningful representation.\nWe show that our method can explain the main events of logs collected from\ndifferent applications without prior knowledge. We also show how the approach\ncan be used to detect general trends as well as outliers in parallel and\ndistributed systems by overlapping multiple projections. As a result, we expect\na significant reduction of the time required to analyze and resolve system-wide\nissues, identify performance bottlenecks and security risks, debug\napplications, etc."
                },
                "authors": [
                    {
                        "name": "Dmytro Borysenkov"
                    },
                    {
                        "name": "Adriano Vogel"
                    },
                    {
                        "name": "Sren Henning"
                    },
                    {
                        "name": "Esteban Perez-Wohlfeil"
                    }
                ],
                "author_detail": {
                    "name": "Esteban Perez-Wohlfeil"
                },
                "author": "Esteban Perez-Wohlfeil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05521v1",
                "updated": "2024-11-08T12:27:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:27:13Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark"
                },
                "summary": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases."
                },
                "authors": [
                    {
                        "name": "Sithursan Sivasubramaniam"
                    },
                    {
                        "name": "Cedric Osei-Akoto"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    },
                    {
                        "name": "Jonathan Fuerst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fuerst"
                },
                "author": "Jonathan Fuerst",
                "arxiv_comment": "NeurIPS 2024 Track Datasets and Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05508v1",
                "updated": "2024-11-08T12:08:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    8,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:08:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    8,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "An Early FIRST Reproduction and Improvements to Single-Token Decoding\n  for Fast Listwise Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Early FIRST Reproduction and Improvements to Single-Token Decoding\n  for Fast Listwise Reranking"
                },
                "summary": "Recent advances have demonstrated that large language models (LLMs) excel as\nlistwise rerankers, but their high computational demands remain a barrier to\nwidespread adoption. Further, the traditional language modeling (LM) objective\nis not ideally suited for reranking tasks. FIRST is a novel approach that\naddresses these challenges by integrating a learning-to-rank objective and\nleveraging the logits of only the first generated token, thereby significantly\nreducing inference latency compared to traditional LLM rerankers. In this\nstudy, we extend the evaluation of FIRST to the TREC Deep Learning datasets\n(DL19-22), validating its robustness across diverse domains. We investigate the\ninfluence of different first-stage retrievers on FIRST rerankers, observing\ndiminishing returns and patterns consistent with traditional LLM rerankers.\nThrough applying the FIRST objective to a broader range of backbone models, we\nachieve effectiveness surpassing the original implementation. Our experiments\nconfirm that fast reranking with single-token logits does not compromise\nout-of-domain reranking quality. To better quantify the computational savings\nin the original study, we measure and compare latency to find a 21%-42% gain\nacross various models and benchmarks. Moreover, while LM training implicitly\nimproves zero-shot single-token reranking, our experiments also raise questions\nabout whether LM pre-training may hinder subsequent fine-tuning with the FIRST\nobjective. These findings pave the way for more efficient and effective\nlistwise reranking in future applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have demonstrated that large language models (LLMs) excel as\nlistwise rerankers, but their high computational demands remain a barrier to\nwidespread adoption. Further, the traditional language modeling (LM) objective\nis not ideally suited for reranking tasks. FIRST is a novel approach that\naddresses these challenges by integrating a learning-to-rank objective and\nleveraging the logits of only the first generated token, thereby significantly\nreducing inference latency compared to traditional LLM rerankers. In this\nstudy, we extend the evaluation of FIRST to the TREC Deep Learning datasets\n(DL19-22), validating its robustness across diverse domains. We investigate the\ninfluence of different first-stage retrievers on FIRST rerankers, observing\ndiminishing returns and patterns consistent with traditional LLM rerankers.\nThrough applying the FIRST objective to a broader range of backbone models, we\nachieve effectiveness surpassing the original implementation. Our experiments\nconfirm that fast reranking with single-token logits does not compromise\nout-of-domain reranking quality. To better quantify the computational savings\nin the original study, we measure and compare latency to find a 21%-42% gain\nacross various models and benchmarks. Moreover, while LM training implicitly\nimproves zero-shot single-token reranking, our experiments also raise questions\nabout whether LM pre-training may hinder subsequent fine-tuning with the FIRST\nobjective. These findings pave the way for more efficient and effective\nlistwise reranking in future applications."
                },
                "authors": [
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05504v1",
                "updated": "2024-11-08T12:03:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    36,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:03:36Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    36,
                    4,
                    313,
                    0
                ],
                "title": "LBPE: Long-token-first Tokenization to Improve Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBPE: Long-token-first Tokenization to Improve Large Language Models"
                },
                "summary": "The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)\nfacilitates robust handling of subword units and avoids issues of\nout-of-vocabulary words. Despite its success, a critical challenge persists:\nlong tokens, rich in semantic information, have fewer occurrences in tokenized\ndatasets compared to short tokens, which can result in imbalanced learning\nissue across different tokens. To address that, we propose LBPE, which\nprioritizes long tokens during the encoding process. LBPE generates tokens\naccording to their reverse ranks of token length rather than their ranks in the\nvocabulary, granting longer tokens higher priority during the encoding process.\nConsequently, LBPE smooths the frequency differences between short and long\ntokens, and thus mitigates the learning imbalance. Extensive experiments across\ndiverse language modeling tasks demonstrate that LBPE consistently outperforms\nthe original BPE, well demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)\nfacilitates robust handling of subword units and avoids issues of\nout-of-vocabulary words. Despite its success, a critical challenge persists:\nlong tokens, rich in semantic information, have fewer occurrences in tokenized\ndatasets compared to short tokens, which can result in imbalanced learning\nissue across different tokens. To address that, we propose LBPE, which\nprioritizes long tokens during the encoding process. LBPE generates tokens\naccording to their reverse ranks of token length rather than their ranks in the\nvocabulary, granting longer tokens higher priority during the encoding process.\nConsequently, LBPE smooths the frequency differences between short and long\ntokens, and thus mitigates the learning imbalance. Extensive experiments across\ndiverse language modeling tasks demonstrate that LBPE consistently outperforms\nthe original BPE, well demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jianwei Niu"
                    },
                    {
                        "name": "Shasha Mo"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.17808",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05503v1",
                "updated": "2024-11-08T12:03:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    31,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T12:03:31Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    3,
                    31,
                    4,
                    313,
                    0
                ],
                "title": "KyrgyzNLP: Challenges, Progress, and Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KyrgyzNLP: Challenges, Progress, and Future"
                },
                "summary": "Large language models (LLMs) have excelled in numerous benchmarks, advancing\nAI applications in both linguistic and non-linguistic tasks. However, this has\nprimarily benefited well-resourced languages, leaving less-resourced ones\n(LRLs) at a disadvantage. In this paper, we highlight the current state of the\nNLP field in the specific LRL: kyrgyz tili.\n  Human evaluation, including annotated datasets created by native speakers,\nremains an irreplaceable component of reliable NLP performance, especially for\nLRLs where automatic evaluations can fall short. In recent assessments of the\nresources for Turkic languages, Kyrgyz is labeled with the status 'Scraping\nBy', a severely under-resourced language spoken by millions. This is concerning\ngiven the growing importance of the language, not only in Kyrgyzstan but also\namong diaspora communities where it holds no official status.\n  We review prior efforts in the field, noting that many of the publicly\navailable resources have only recently been developed, with few exceptions\nbeyond dictionaries (the processed data used for the analysis is presented at\nhttps://kyrgyznlp.github.io/). While recent papers have made some headway, much\nmore remains to be done. Despite interest and support from both business and\ngovernment sectors in the Kyrgyz Republic, the situation for Kyrgyz language\nresources remains challenging. We stress the importance of community-driven\nefforts to build these resources, ensuring the future advancement\nsustainability. We then share our view of the most pressing challenges in\nKyrgyz NLP. Finally, we propose a roadmap for future development in terms of\nresearch topics and language resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in numerous benchmarks, advancing\nAI applications in both linguistic and non-linguistic tasks. However, this has\nprimarily benefited well-resourced languages, leaving less-resourced ones\n(LRLs) at a disadvantage. In this paper, we highlight the current state of the\nNLP field in the specific LRL: kyrgyz tili.\n  Human evaluation, including annotated datasets created by native speakers,\nremains an irreplaceable component of reliable NLP performance, especially for\nLRLs where automatic evaluations can fall short. In recent assessments of the\nresources for Turkic languages, Kyrgyz is labeled with the status 'Scraping\nBy', a severely under-resourced language spoken by millions. This is concerning\ngiven the growing importance of the language, not only in Kyrgyzstan but also\namong diaspora communities where it holds no official status.\n  We review prior efforts in the field, noting that many of the publicly\navailable resources have only recently been developed, with few exceptions\nbeyond dictionaries (the processed data used for the analysis is presented at\nhttps://kyrgyznlp.github.io/). While recent papers have made some headway, much\nmore remains to be done. Despite interest and support from both business and\ngovernment sectors in the Kyrgyz Republic, the situation for Kyrgyz language\nresources remains challenging. We stress the importance of community-driven\nefforts to build these resources, ensuring the future advancement\nsustainability. We then share our view of the most pressing challenges in\nKyrgyz NLP. Finally, we propose a roadmap for future development in terms of\nresearch topics and language resources."
                },
                "authors": [
                    {
                        "name": "Anton Alekseev"
                    },
                    {
                        "name": "Timur Turatali"
                    }
                ],
                "author_detail": {
                    "name": "Timur Turatali"
                },
                "author": "Timur Turatali",
                "arxiv_comment": "Keynote talk at the 12th International Conference on Analysis of\n  Images, Social Networks and Texts (AIST-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05479v1",
                "updated": "2024-11-08T11:09:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    9,
                    45,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T11:09:45Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    9,
                    45,
                    4,
                    313,
                    0
                ],
                "title": "EUREKHA: Enhancing User Representation for Key Hackers Identification in\n  Underground Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EUREKHA: Enhancing User Representation for Key Hackers Identification in\n  Underground Forums"
                },
                "summary": "Underground forums serve as hubs for cybercriminal activities, offering a\nspace for anonymity and evasion of conventional online oversight. In these\nhidden communities, malicious actors collaborate to exchange illicit knowledge,\ntools, and tactics, driving a range of cyber threats from hacking techniques to\nthe sale of stolen data, malware, and zero-day exploits. Identifying the key\ninstigators (i.e., key hackers), behind these operations is essential but\nremains a complex challenge. This paper presents a novel method called EUREKHA\n(Enhancing User Representation for Key Hacker Identification in Underground\nForums), designed to identify these key hackers by modeling each user as a\ntextual sequence. This sequence is processed through a large language model\n(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.\nThese extracted features are then fed into a Graph Neural Network (GNN) to\nmodel user structural relationships, significantly improving identification\naccuracy. Furthermore, we employ BERTopic (Bidirectional Encoder\nRepresentations from Transformers Topic Modeling) to extract personalized\ntopics from user-generated content, enabling multiple textual representations\nper user and optimizing the selection of the most representative sequence. Our\nstudy demonstrates that fine-tuned LLMs outperform state-of-the-art methods in\nidentifying key hackers. Additionally, when combined with GNNs, our model\nachieves significant improvements, resulting in approximately 6% and 10%\nincreases in accuracy and F1-score, respectively, over existing methods.\nEUREKHA was tested on the Hack-Forums dataset, and we provide open-source\naccess to our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underground forums serve as hubs for cybercriminal activities, offering a\nspace for anonymity and evasion of conventional online oversight. In these\nhidden communities, malicious actors collaborate to exchange illicit knowledge,\ntools, and tactics, driving a range of cyber threats from hacking techniques to\nthe sale of stolen data, malware, and zero-day exploits. Identifying the key\ninstigators (i.e., key hackers), behind these operations is essential but\nremains a complex challenge. This paper presents a novel method called EUREKHA\n(Enhancing User Representation for Key Hacker Identification in Underground\nForums), designed to identify these key hackers by modeling each user as a\ntextual sequence. This sequence is processed through a large language model\n(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.\nThese extracted features are then fed into a Graph Neural Network (GNN) to\nmodel user structural relationships, significantly improving identification\naccuracy. Furthermore, we employ BERTopic (Bidirectional Encoder\nRepresentations from Transformers Topic Modeling) to extract personalized\ntopics from user-generated content, enabling multiple textual representations\nper user and optimizing the selection of the most representative sequence. Our\nstudy demonstrates that fine-tuned LLMs outperform state-of-the-art methods in\nidentifying key hackers. Additionally, when combined with GNNs, our model\nachieves significant improvements, resulting in approximately 6% and 10%\nincreases in accuracy and F1-score, respectively, over existing methods.\nEUREKHA was tested on the Hack-Forums dataset, and we provide open-source\naccess to our code."
                },
                "authors": [
                    {
                        "name": "Abdoul Nasser Hassane Amadou"
                    },
                    {
                        "name": "Anas Motii"
                    },
                    {
                        "name": "Saida Elouardi"
                    },
                    {
                        "name": "EL Houcine Bergou"
                    }
                ],
                "author_detail": {
                    "name": "EL Houcine Bergou"
                },
                "author": "EL Houcine Bergou",
                "arxiv_comment": "Accepted at IEEE Trustcom 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05474v1",
                "updated": "2024-11-08T11:00:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T11:00:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction"
                },
                "summary": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution."
                },
                "authors": [
                    {
                        "name": "miland Garrab"
                    },
                    {
                        "name": "Pierre Teixeira"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stphane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Doncieux"
                },
                "author": "Stphane Doncieux",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v3",
                "updated": "2024-11-08T10:30:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    10,
                    30,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05451v1",
                "updated": "2024-11-08T09:58:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    58,
                    2,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:58:02Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    58,
                    2,
                    4,
                    313,
                    0
                ],
                "title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM."
                },
                "authors": [
                    {
                        "name": "Shengda Fan"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Yuepeng Fu"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Shuyan Zhang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05442v1",
                "updated": "2024-11-08T09:40:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    40,
                    53,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:40:53Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    40,
                    53,
                    4,
                    313,
                    0
                ],
                "title": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\n  Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\n  Delivery"
                },
                "summary": "In the rapidly evolving landscape of cyber security, intelligent chatbots are\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\nLanguage Processing empower these chatbots to handle user inquiries and deliver\nthreat intelligence. This helps cyber security knowledge readily available to\nboth professionals and the public. Traditional rule-based chatbots often lack\nflexibility and struggle to adapt to user interactions. In contrast, Large\nLanguage Model-based chatbots offer contextually relevant information across\nmultiple domains and adapt to evolving conversational contexts. In this work,\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\ncutting-edge technologies like Large Language Models and Langchain alongside a\nRetrieval-Augmented Generation model to deliver superior capabilities. This\nchatbot gathers information from diverse data sources to create a comprehensive\nknowledge base covering known vulnerabilities, recent cyber attacks, and\nemerging threats. It delivers tailored responses, serving as a primary hub for\ncyber security insights. By providing instant access to relevant information\nand resources, this IntellBot enhances threat intelligence, incident response,\nand overall security posture, saving time and empowering users with knowledge\nof cyber security best practices. Moreover, we analyzed the performance of our\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\nevaluate the RAG model, and all evaluation metrics consistently produced scores\nabove 0.77, highlighting the efficacy of our system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of cyber security, intelligent chatbots are\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\nLanguage Processing empower these chatbots to handle user inquiries and deliver\nthreat intelligence. This helps cyber security knowledge readily available to\nboth professionals and the public. Traditional rule-based chatbots often lack\nflexibility and struggle to adapt to user interactions. In contrast, Large\nLanguage Model-based chatbots offer contextually relevant information across\nmultiple domains and adapt to evolving conversational contexts. In this work,\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\ncutting-edge technologies like Large Language Models and Langchain alongside a\nRetrieval-Augmented Generation model to deliver superior capabilities. This\nchatbot gathers information from diverse data sources to create a comprehensive\nknowledge base covering known vulnerabilities, recent cyber attacks, and\nemerging threats. It delivers tailored responses, serving as a primary hub for\ncyber security insights. By providing instant access to relevant information\nand resources, this IntellBot enhances threat intelligence, incident response,\nand overall security posture, saving time and empowering users with knowledge\nof cyber security best practices. Moreover, we analyzed the performance of our\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\nevaluate the RAG model, and all evaluation metrics consistently produced scores\nabove 0.77, highlighting the efficacy of our system."
                },
                "authors": [
                    {
                        "name": "Dincy R. Arikkat"
                    },
                    {
                        "name": "Abhinav M."
                    },
                    {
                        "name": "Navya Binu"
                    },
                    {
                        "name": "Parvathi M."
                    },
                    {
                        "name": "Navya Biju"
                    },
                    {
                        "name": "K. S. Arunima"
                    },
                    {
                        "name": "Vinod P."
                    },
                    {
                        "name": "Rafidha Rehiman K. A."
                    },
                    {
                        "name": "Mauro Conti"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Conti"
                },
                "author": "Mauro Conti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05423v1",
                "updated": "2024-11-08T09:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    15,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T09:15:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    15,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "VISTA: Visual Integrated System for Tailored Automation in Math Problem\n  Generation Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA: Visual Integrated System for Tailored Automation in Math Problem\n  Generation Using LLM"
                },
                "summary": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education."
                },
                "authors": [
                    {
                        "name": "Jeongwoo Lee"
                    },
                    {
                        "name": "Kwangsuk Park"
                    },
                    {
                        "name": "Jihyeon Park"
                    }
                ],
                "author_detail": {
                    "name": "Jihyeon Park"
                },
                "author": "Jihyeon Park",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03488v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03488v4",
                "updated": "2024-11-08T09:02:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    9,
                    2,
                    24,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-05T17:50:03Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    50,
                    3,
                    2,
                    157,
                    0
                ],
                "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training"
                },
                "summary": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03488v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03488v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05409v1",
                "updated": "2024-11-08T08:59:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    59,
                    40,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:59:40Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    59,
                    40,
                    4,
                    313,
                    0
                ],
                "title": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights"
                },
                "summary": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges."
                },
                "authors": [
                    {
                        "name": "Abigail Yongping Huang"
                    },
                    {
                        "name": "Ashwin Nair"
                    },
                    {
                        "name": "Zhen Rong Goh"
                    },
                    {
                        "name": "Tianrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Liu"
                },
                "author": "Tianrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07230v2",
                "updated": "2024-11-08T08:55:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    55,
                    0,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-12T00:58:19Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    0,
                    58,
                    19,
                    1,
                    72,
                    0
                ],
                "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences"
                },
                "summary": "Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique. We release the preference pairs used in\nalignment at:\nhttps://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique. We release the preference pairs used in\nalignment at:\nhttps://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences"
                },
                "authors": [
                    {
                        "name": "Pulkit Pattnaik"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "arxiv_comment": "Published at EMNLP 2024 as long (findings) conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05407v1",
                "updated": "2024-11-08T08:52:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    52,
                    59,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:52:59Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    52,
                    59,
                    4,
                    313,
                    0
                ],
                "title": "Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning"
                },
                "summary": "Despite the strong performance of large language models (LLMs) in tasks like\nmathematical reasoning, their practical use is limited by high computational\ndemands and proprietary restrictions. Chain-of-thought (CoT) and\nprogram-of-thought (PoT) fine-tuning are common methods to transfer LLM\nknowledge to small language models (SLMs). However, CoT often leads to\ncalculation errors in SLMs, while PoT has shown more promise. While most\nPoT-based approaches focus on direct problem-to-code conversion or extracting\nonly the key information from questions and then providing code solution for\nit, this work emphasizes filling the gaps in the question to clearly illustrate\nthe solution path, which can be challenging for an SLM to understand when such\ninformation is not explicitly provided. Therefore, this paper introduces\nGap-Filling Prompting (GFP), a novel two-step prompting strategy designed to\nenhance the problem-solving process for SLMs. The first step identifies these\ngaps and provides hints for filling them, while the second step adds the hints\nto the question to generate a final code solution. Experimental results on two\nbenchmark datasets demonstrate that GFP significantly improves the mathematical\nreasoning abilities of SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the strong performance of large language models (LLMs) in tasks like\nmathematical reasoning, their practical use is limited by high computational\ndemands and proprietary restrictions. Chain-of-thought (CoT) and\nprogram-of-thought (PoT) fine-tuning are common methods to transfer LLM\nknowledge to small language models (SLMs). However, CoT often leads to\ncalculation errors in SLMs, while PoT has shown more promise. While most\nPoT-based approaches focus on direct problem-to-code conversion or extracting\nonly the key information from questions and then providing code solution for\nit, this work emphasizes filling the gaps in the question to clearly illustrate\nthe solution path, which can be challenging for an SLM to understand when such\ninformation is not explicitly provided. Therefore, this paper introduces\nGap-Filling Prompting (GFP), a novel two-step prompting strategy designed to\nenhance the problem-solving process for SLMs. The first step identifies these\ngaps and provides hints for filling them, while the second step adds the hints\nto the question to generate a final code solution. Experimental results on two\nbenchmark datasets demonstrate that GFP significantly improves the mathematical\nreasoning abilities of SLMs."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghiasvand Mohammadkhani"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Ghiasvand Mohammadkhani"
                },
                "author": "Mohammad Ghiasvand Mohammadkhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05403v1",
                "updated": "2024-11-08T08:41:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    41,
                    17,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T08:41:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    41,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "Benchmarking Distributional Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Distributional Alignment of Large Language Models"
                },
                "summary": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions."
                },
                "authors": [
                    {
                        "name": "Nicole Meister"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02540v2",
                "updated": "2024-11-08T08:29:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    8,
                    29,
                    10,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-04T19:21:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    21,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphXAIN: Narratives to Explain Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose interpretability challenges, especially\nfor non-expert users. Existing GNN explanation methods often yield technical\noutputs such as subgraphs and feature importance scores, which are not easily\nunderstood. Building on recent insights from social science and other\nExplainable AI (XAI) methods, we propose GraphXAIN, a natural language\nnarrative that explains individual predictions made by GNNs. We present a\nmodel-agnostic and explainer-agnostic XAI approach that complements graph\nexplainers by generating GraphXAINs, using Large Language Models (LLMs) and\nintegrating graph data, individual predictions from GNNs, explanatory\nsubgraphs, and feature importances. We define XAI Narratives and XAI\nDescriptions, highlighting their distinctions and emphasizing the importance of\nnarrative principles in effective explanations. By incorporating natural\nlanguage narratives, our approach supports graph practitioners and non-expert\nusers, aligning with social science research on explainability and enhancing\nuser understanding and trust in complex GNN models. We demonstrate GraphXAIN's\ncapabilities on a real-world graph dataset, illustrating how its generated\nnarratives can aid understanding compared to traditional graph explainer\noutputs or other descriptive explanation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose interpretability challenges, especially\nfor non-expert users. Existing GNN explanation methods often yield technical\noutputs such as subgraphs and feature importance scores, which are not easily\nunderstood. Building on recent insights from social science and other\nExplainable AI (XAI) methods, we propose GraphXAIN, a natural language\nnarrative that explains individual predictions made by GNNs. We present a\nmodel-agnostic and explainer-agnostic XAI approach that complements graph\nexplainers by generating GraphXAINs, using Large Language Models (LLMs) and\nintegrating graph data, individual predictions from GNNs, explanatory\nsubgraphs, and feature importances. We define XAI Narratives and XAI\nDescriptions, highlighting their distinctions and emphasizing the importance of\nnarrative principles in effective explanations. By incorporating natural\nlanguage narratives, our approach supports graph practitioners and non-expert\nusers, aligning with social science research on explainability and enhancing\nuser understanding and trust in complex GNN models. We demonstrate GraphXAIN's\ncapabilities on a real-world graph dataset, illustrating how its generated\nnarratives can aid understanding compared to traditional graph explainer\noutputs or other descriptive explanation methods."
                },
                "authors": [
                    {
                        "name": "Mateusz Cedro"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04358v2",
                "updated": "2024-11-08T07:31:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    31,
                    26,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-07T01:31:48Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    1,
                    31,
                    48,
                    3,
                    312,
                    0
                ],
                "title": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation"
                },
                "summary": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Vaibhav Seth"
                    },
                    {
                        "name": "Arinjay Pathak"
                    },
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Sriram Gopalakrishnan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "48 pages, 10 figures, 10 tables, Code:\n  https://github.com/LCS2-IIITD/MonteCLoRA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05375v1",
                "updated": "2024-11-08T07:05:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    5,
                    6,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T07:05:06Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    7,
                    5,
                    6,
                    4,
                    313,
                    0
                ],
                "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking"
                },
                "summary": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches."
                },
                "authors": [
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Michael Schlichtkrull"
                    },
                    {
                        "name": "Andreas Vlachos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vlachos"
                },
                "author": "Andreas Vlachos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17263v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17263v5",
                "updated": "2024-11-08T06:57:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    57,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-01-30T18:56:08Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    18,
                    56,
                    8,
                    1,
                    30,
                    0
                ],
                "title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks"
                },
                "summary": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo"
                },
                "authors": [
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "NeurIPS 2024 Spotlight; code available at\n  https://github.com/lapisrocks/rpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17263v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17263v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09254v2",
                "updated": "2024-11-08T06:47:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    47,
                    4,
                    4,
                    313,
                    0
                ],
                "published": "2023-07-18T13:36:24Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    13,
                    36,
                    24,
                    1,
                    199,
                    0
                ],
                "title": "Selective Generation for Controllable Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Generation for Controllable Language Models"
                },
                "summary": "Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation."
                },
                "authors": [
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Kyungmin Kim"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Sangdon Park"
                    }
                ],
                "author_detail": {
                    "name": "Sangdon Park"
                },
                "author": "Sangdon Park",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05357v1",
                "updated": "2024-11-08T06:28:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    28,
                    2,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:28:02Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    28,
                    2,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Visual Classification using Comparative Descriptors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Classification using Comparative Descriptors"
                },
                "summary": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences."
                },
                "authors": [
                    {
                        "name": "Hankyeol Lee"
                    },
                    {
                        "name": "Gawon Seo"
                    },
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    },
                    {
                        "name": "Jiyoung Jung"
                    }
                ],
                "author_detail": {
                    "name": "Jiyoung Jung"
                },
                "author": "Jiyoung Jung",
                "arxiv_comment": "Accepted to WACV 2025. Main paper with 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05349v1",
                "updated": "2024-11-08T06:12:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    12,
                    56,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:12:56Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    12,
                    56,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\n  Cluster Diagnosis System and Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\n  Cluster Diagnosis System and Evaluation Framework"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods."
                },
                "authors": [
                    {
                        "name": "Honghao Shi"
                    },
                    {
                        "name": "Longkai Cheng"
                    },
                    {
                        "name": "Wenli Wu"
                    },
                    {
                        "name": "Yuhang Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Shaokai Nie"
                    },
                    {
                        "name": "Weixv Wang"
                    },
                    {
                        "name": "Xuebin Min"
                    },
                    {
                        "name": "Chunlei Men"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01251v3",
                "updated": "2024-11-08T06:07:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    7,
                    51,
                    4,
                    313,
                    0
                ],
                "published": "2024-03-02T16:23:44Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    16,
                    23,
                    44,
                    5,
                    62,
                    0
                ],
                "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization\n  via Probe Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization\n  via Probe Sampling"
                },
                "summary": "Safety of Large Language Models (LLMs) has become a critical issue given\ntheir rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be\neffective in constructing adversarial prompts to break the aligned LLMs, but\noptimization of GCG is time-consuming. To reduce the time cost of GCG and\nenable more comprehensive studies of LLM safety, in this work, we study a new\nalgorithm called $\\texttt{Probe sampling}$. At the core of the algorithm is a\nmechanism that dynamically determines how similar a smaller draft model's\npredictions are to the target model's predictions for prompt candidates. When\nthe target model is similar to the draft model, we rely heavily on the draft\nmodel to filter out a large number of potential prompt candidates. Probe\nsampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to\nequal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe\nsampling is also able to accelerate other prompt optimization techniques and\nadversarial methods, leading to acceleration of $1.8\\times$ for AutoPrompt,\n$2.4\\times$ for APE and $2.4\\times$ for AutoDAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety of Large Language Models (LLMs) has become a critical issue given\ntheir rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be\neffective in constructing adversarial prompts to break the aligned LLMs, but\noptimization of GCG is time-consuming. To reduce the time cost of GCG and\nenable more comprehensive studies of LLM safety, in this work, we study a new\nalgorithm called $\\texttt{Probe sampling}$. At the core of the algorithm is a\nmechanism that dynamically determines how similar a smaller draft model's\npredictions are to the target model's predictions for prompt candidates. When\nthe target model is similar to the draft model, we rely heavily on the draft\nmodel to filter out a large number of potential prompt candidates. Probe\nsampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to\nequal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe\nsampling is also able to accelerate other prompt optimization techniques and\nadversarial methods, leading to acceleration of $1.8\\times$ for AutoPrompt,\n$2.4\\times$ for APE and $2.4\\times$ for AutoDAN."
                },
                "authors": [
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Wenyue Zheng"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Xuan Long Do"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05348v1",
                "updated": "2024-11-08T06:04:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T06:04:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PySC2: Starcraft II learning environment for Large Language Models"
                },
                "summary": "This paper introduces a new environment LLM-PySC2 (the Large Language Model\nStarCraft II Learning Environment), a platform derived from DeepMind's\nStarCraft II Learning Environment that serves to develop Large Language Models\n(LLMs) based decision-making methodologies. This environment is the first to\noffer the complete StarCraft II action space, multi-modal observation\ninterfaces, and a structured game knowledge database, which are seamlessly\nconnected with various LLMs to facilitate the research of LLMs-based\ndecision-making. To further support multi-agent research, we developed an LLM\ncollaborative framework that supports multi-agent concurrent queries and\nmulti-agent communication. In our experiments, the LLM-PySC2 environment is\nadapted to be compatible with the StarCraft Multi-Agent Challenge (SMAC) task\ngroup and provided eight new scenarios focused on macro-decision abilities. We\nevaluated nine mainstream LLMs in the experiments, and results show that\nsufficient parameters are necessary for LLMs to make decisions, but improving\nreasoning ability does not directly lead to better decision-making outcomes.\nOur findings further indicate the importance of enabling large models to learn\nautonomously in the deployment environment through parameter training or\ntrain-free learning techniques. Ultimately, we expect that the LLM-PySC2\nenvironment can promote research on learning methods for LLMs, helping\nLLM-based methods better adapt to task scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new environment LLM-PySC2 (the Large Language Model\nStarCraft II Learning Environment), a platform derived from DeepMind's\nStarCraft II Learning Environment that serves to develop Large Language Models\n(LLMs) based decision-making methodologies. This environment is the first to\noffer the complete StarCraft II action space, multi-modal observation\ninterfaces, and a structured game knowledge database, which are seamlessly\nconnected with various LLMs to facilitate the research of LLMs-based\ndecision-making. To further support multi-agent research, we developed an LLM\ncollaborative framework that supports multi-agent concurrent queries and\nmulti-agent communication. In our experiments, the LLM-PySC2 environment is\nadapted to be compatible with the StarCraft Multi-Agent Challenge (SMAC) task\ngroup and provided eight new scenarios focused on macro-decision abilities. We\nevaluated nine mainstream LLMs in the experiments, and results show that\nsufficient parameters are necessary for LLMs to make decisions, but improving\nreasoning ability does not directly lead to better decision-making outcomes.\nOur findings further indicate the importance of enabling large models to learn\nautonomously in the deployment environment through parameter training or\ntrain-free learning techniques. Ultimately, we expect that the LLM-PySC2\nenvironment can promote research on learning methods for LLMs, helping\nLLM-based methods better adapt to task scenarios."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Chang Lu"
                    },
                    {
                        "name": "Xiaojie Xu"
                    },
                    {
                        "name": "Xiangbei Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yunzheng Guo"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Xian Guo"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xuebo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuebo Zhang"
                },
                "author": "Xuebo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05345v1",
                "updated": "2024-11-08T05:54:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    54,
                    5,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:54:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    54,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Robustness of LLMs to Adversarial Typographical Errors"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Esther Gan"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Liying Cheng"
                    },
                    {
                        "name": "Yancan Mao"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05338v1",
                "updated": "2024-11-08T05:28:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    28,
                    22,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:28:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    28,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers"
                },
                "summary": "Scientific literature is typically dense, requiring significant background\nknowledge and deep comprehension for effective engagement. We introduce SciDQA,\na new dataset for reading comprehension that challenges LLMs for a deep\nunderstanding of scientific articles, consisting of 2,937 QA pairs. Unlike\nother scientific QA datasets, SciDQA sources questions from peer reviews by\ndomain experts and answers by paper authors, ensuring a thorough examination of\nthe literature. We enhance the dataset's quality through a process that\ncarefully filters out lower quality questions, decontextualizes the content,\ntracks the source document across different versions, and incorporates a\nbibliography for multi-document question-answering. Questions in SciDQA\nnecessitate reasoning across figures, tables, equations, appendices, and\nsupplementary materials, and require multi-document reasoning. We evaluate\nseveral open-source and proprietary LLMs across various configurations to\nexplore their capabilities in generating relevant and factual responses. Our\ncomprehensive evaluation, based on metrics for surface-level similarity and LLM\njudgements, highlights notable performance discrepancies. SciDQA represents a\nrigorously curated, naturally derived scientific QA dataset, designed to\nfacilitate research on complex scientific text understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature is typically dense, requiring significant background\nknowledge and deep comprehension for effective engagement. We introduce SciDQA,\na new dataset for reading comprehension that challenges LLMs for a deep\nunderstanding of scientific articles, consisting of 2,937 QA pairs. Unlike\nother scientific QA datasets, SciDQA sources questions from peer reviews by\ndomain experts and answers by paper authors, ensuring a thorough examination of\nthe literature. We enhance the dataset's quality through a process that\ncarefully filters out lower quality questions, decontextualizes the content,\ntracks the source document across different versions, and incorporates a\nbibliography for multi-document question-answering. Questions in SciDQA\nnecessitate reasoning across figures, tables, equations, appendices, and\nsupplementary materials, and require multi-document reasoning. We evaluate\nseveral open-source and proprietary LLMs across various configurations to\nexplore their capabilities in generating relevant and factual responses. Our\ncomprehensive evaluation, based on metrics for surface-level similarity and LLM\njudgements, highlights notable performance discrepancies. SciDQA represents a\nrigorously curated, naturally derived scientific QA dataset, designed to\nfacilitate research on complex scientific text understanding."
                },
                "authors": [
                    {
                        "name": "Shruti Singh"
                    },
                    {
                        "name": "Nandan Sarkar"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "18 pages, Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16964v2",
                "updated": "2024-11-08T05:19:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    19,
                    48,
                    4,
                    313,
                    0
                ],
                "published": "2024-05-27T08:57:04Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    8,
                    57,
                    4,
                    0,
                    148,
                    0
                ],
                "title": "Exploring the LLM Journey from Cognition to Expression with Linear\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the LLM Journey from Cognition to Expression with Linear\n  Representations"
                },
                "summary": "This paper presents an in-depth examination of the evolution and interplay of\ncognitive and expressive capabilities in large language models (LLMs), with a\nspecific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese\nand English) LLM series. We define and explore the model's cognitive and\nexpressive capabilities through linear representations across three critical\nphases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning\nfrom Human Feedback (RLHF). Cognitive capability is defined as the quantity and\nquality of information conveyed by the neuron output vectors within the\nnetwork, similar to the neural signal processing in human cognition. Expressive\ncapability is defined as the model's capability to produce word-level output.\nOur findings unveil a sequential development pattern, where cognitive abilities\nare largely established during Pretraining, whereas expressive abilities\npredominantly advance during SFT and RLHF. Statistical analyses confirm a\nsignificant correlation between the two capabilities, suggesting that cognitive\ncapacity may limit expressive potential. The paper also explores the\ntheoretical underpinnings of these divergent developmental trajectories and\ntheir connection to the LLMs' architectural design. Moreover, we evaluate\nvarious optimization-independent strategies, such as few-shot learning and\nrepeated sampling, which bridge the gap between cognitive and expressive\ncapabilities. This research reveals the potential connection between the hidden\nspace and the output space, contributing valuable insights into the\ninterpretability and controllability of their training processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth examination of the evolution and interplay of\ncognitive and expressive capabilities in large language models (LLMs), with a\nspecific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese\nand English) LLM series. We define and explore the model's cognitive and\nexpressive capabilities through linear representations across three critical\nphases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning\nfrom Human Feedback (RLHF). Cognitive capability is defined as the quantity and\nquality of information conveyed by the neuron output vectors within the\nnetwork, similar to the neural signal processing in human cognition. Expressive\ncapability is defined as the model's capability to produce word-level output.\nOur findings unveil a sequential development pattern, where cognitive abilities\nare largely established during Pretraining, whereas expressive abilities\npredominantly advance during SFT and RLHF. Statistical analyses confirm a\nsignificant correlation between the two capabilities, suggesting that cognitive\ncapacity may limit expressive potential. The paper also explores the\ntheoretical underpinnings of these divergent developmental trajectories and\ntheir connection to the LLMs' architectural design. Moreover, we evaluate\nvarious optimization-independent strategies, such as few-shot learning and\nrepeated sampling, which bridge the gap between cognitive and expressive\ncapabilities. This research reveals the potential connection between the hidden\nspace and the output space, contributing valuable insights into the\ninterpretability and controllability of their training processes."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yipin Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yan"
                },
                "author": "Dong Yan",
                "arxiv_comment": "Published in ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15721v2",
                "updated": "2024-11-08T05:08:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    8,
                    43,
                    4,
                    313,
                    0
                ],
                "published": "2024-02-24T05:14:52Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    5,
                    14,
                    52,
                    5,
                    55,
                    0
                ],
                "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation\n  Framework for Large Vision Language Models"
                },
                "summary": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data."
                },
                "authors": [
                    {
                        "name": "Chaoya Jiang"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Mengfan Dong"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "arxiv_comment": "Accepted by ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05328v1",
                "updated": "2024-11-08T05:04:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    4,
                    55,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T05:04:55Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    5,
                    4,
                    55,
                    4,
                    313,
                    0
                ],
                "title": "Content Quality vs. Attention Allocation: An LLM-Based Case Study in\n  Peer-to-peer Mental Health Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Quality vs. Attention Allocation: An LLM-Based Case Study in\n  Peer-to-peer Mental Health Networks"
                },
                "summary": "With the rise of social media and peer-to-peer networks, users increasingly\nrely on crowdsourced responses for information and assistance. However, the\nmechanisms used to rank and promote responses often prioritize and end up\nbiasing in favor of timeliness over quality, which may result in suboptimal\nsupport for help-seekers. We analyze millions of responses to mental\nhealth-related posts, utilizing large language models (LLMs) to assess the\nmulti-dimensional quality of content, including relevance, empathy, and\ncultural alignment, among other aspects. Our findings reveal a mismatch between\ncontent quality and attention allocation: earlier responses - despite being\nrelatively lower in quality - receive disproportionately high fractions of\nupvotes and visibility due to platform ranking algorithms. We demonstrate that\nthe quality of the top-ranked responses could be improved by up to 39 percent,\nand even the simplest re-ranking strategy could significantly improve the\nquality of top responses, highlighting the need for more nuanced ranking\nmechanisms that prioritize both timeliness and content quality, especially\nemotional engagement in online mental health communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of social media and peer-to-peer networks, users increasingly\nrely on crowdsourced responses for information and assistance. However, the\nmechanisms used to rank and promote responses often prioritize and end up\nbiasing in favor of timeliness over quality, which may result in suboptimal\nsupport for help-seekers. We analyze millions of responses to mental\nhealth-related posts, utilizing large language models (LLMs) to assess the\nmulti-dimensional quality of content, including relevance, empathy, and\ncultural alignment, among other aspects. Our findings reveal a mismatch between\ncontent quality and attention allocation: earlier responses - despite being\nrelatively lower in quality - receive disproportionately high fractions of\nupvotes and visibility due to platform ranking algorithms. We demonstrate that\nthe quality of the top-ranked responses could be improved by up to 39 percent,\nand even the simplest re-ranking strategy could significantly improve the\nquality of top responses, highlighting the need for more nuanced ranking\nmechanisms that prioritize both timeliness and content quality, especially\nemotional engagement in online mental health communities."
                },
                "authors": [
                    {
                        "name": "Teng Ye"
                    },
                    {
                        "name": "Hanson Yan"
                    },
                    {
                        "name": "Xuhuan Huang"
                    },
                    {
                        "name": "Connor Grogan"
                    },
                    {
                        "name": "Walter Yuan"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Matthew O. Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew O. Jackson"
                },
                "author": "Matthew O. Jackson",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91D30, 94A16",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05323v1",
                "updated": "2024-11-08T04:35:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    35,
                    14,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T04:35:14Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    35,
                    14,
                    4,
                    313,
                    0
                ],
                "title": "TraDE: Network and Traffic-aware Adaptive Scheduling for Microservices\n  Under Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraDE: Network and Traffic-aware Adaptive Scheduling for Microservices\n  Under Dynamics"
                },
                "summary": "The transition from monolithic architecture to microservices has enhanced\nflexibility in application design and its scalable execution. This approach\noften involves using a computing cluster managed by a container orchestration\nplatform, which supports the deployment of microservices. However, this shift\nintroduces significant challenges, particularly in the efficient scheduling of\ncontainerized services. These challenges are compounded by unpredictable\nscenarios such as dynamic incoming workloads with various execution traffic and\nvariable communication delays among cluster nodes. Existing works often\noverlook the real-time traffic impacts of dynamic requests on running\nmicroservices, as well as the varied communication delays across cluster nodes.\nConsequently, even optimally deployed microservices could suffer from\nsignificant performance degradation over time. To address these issues, we\nintroduce a network and traffic-aware adaptive scheduling framework, TraDE.\nThis framework can adaptively redeploy microservice containers to maintain\ndesired performance amid changing traffic and network conditions within the\nhosting cluster. We have implemented TraDE as an extension to the Kubernetes\nplatform. Additionally, we deployed realistic microservice applications in a\nreal compute cluster and conducted extensive experiments to assess our\nframework's performance in various scenarios. The results demonstrate the\neffectiveness of TraDE in rescheduling running microservices to enhance\nend-to-end performance while maintaining a high goodput ratio. Compared with\nthe existing method NetMARKS, TraDE outperforms it by reducing the average\nresponse time of the application by up to 48.3\\%, and improving the throughput\nby up to 1.4x while maintaining a goodput ratio of 95.36\\% and showing robust\nadaptive capability under sustained workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from monolithic architecture to microservices has enhanced\nflexibility in application design and its scalable execution. This approach\noften involves using a computing cluster managed by a container orchestration\nplatform, which supports the deployment of microservices. However, this shift\nintroduces significant challenges, particularly in the efficient scheduling of\ncontainerized services. These challenges are compounded by unpredictable\nscenarios such as dynamic incoming workloads with various execution traffic and\nvariable communication delays among cluster nodes. Existing works often\noverlook the real-time traffic impacts of dynamic requests on running\nmicroservices, as well as the varied communication delays across cluster nodes.\nConsequently, even optimally deployed microservices could suffer from\nsignificant performance degradation over time. To address these issues, we\nintroduce a network and traffic-aware adaptive scheduling framework, TraDE.\nThis framework can adaptively redeploy microservice containers to maintain\ndesired performance amid changing traffic and network conditions within the\nhosting cluster. We have implemented TraDE as an extension to the Kubernetes\nplatform. Additionally, we deployed realistic microservice applications in a\nreal compute cluster and conducted extensive experiments to assess our\nframework's performance in various scenarios. The results demonstrate the\neffectiveness of TraDE in rescheduling running microservices to enhance\nend-to-end performance while maintaining a high goodput ratio. Compared with\nthe existing method NetMARKS, TraDE outperforms it by reducing the average\nresponse time of the application by up to 48.3\\%, and improving the throughput\nby up to 1.4x while maintaining a goodput ratio of 95.36\\% and showing robust\nadaptive capability under sustained workloads."
                },
                "authors": [
                    {
                        "name": "Ming Chen"
                    },
                    {
                        "name": "Muhammed Tawfiqul Islam"
                    },
                    {
                        "name": "Maria Rodriguez Read"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05316v1",
                "updated": "2024-11-08T04:15:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    15,
                    8,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T04:15:08Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    4,
                    15,
                    8,
                    4,
                    313,
                    0
                ],
                "title": "Exploring the Alignment Landscape: LLMs and Geometric Deep Models in\n  Protein Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Alignment Landscape: LLMs and Geometric Deep Models in\n  Protein Representation"
                },
                "summary": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Bingbing Duan"
                    },
                    {
                        "name": "Kai Guo"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10108v3",
                "updated": "2024-11-08T03:58:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    3,
                    58,
                    0,
                    4,
                    313,
                    0
                ],
                "published": "2023-10-16T06:41:16Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    6,
                    41,
                    16,
                    0,
                    289,
                    0
                ],
                "title": "On Generative Agents in Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Generative Agents in Recommendation"
                },
                "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a user\nsimulator in recommendation, leveraging LLM-empowered generative agents\nequipped with user profile, memory, and actions modules specifically tailored\nfor the recommender system. In particular, these agents' profile modules are\ninitialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),\ncapturing users' unique tastes and social traits; memory modules log both\nfactual and emotional memories and are integrated with an emotion-driven\nreflection mechanism; action modules support a wide variety of behaviors,\nspanning both taste-driven and emotion-driven actions. Each agent interacts\nwith personalized recommender models in a page-by-page manner, relying on a\npre-implemented collaborative filtering-based recommendation algorithm. We\ndelve into both the capabilities and limitations of Agent4Rec, aiming to\nexplore an essential research question: ``To what extent can LLM-empowered\ngenerative agents faithfully simulate the behavior of real, autonomous humans\nin recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec\nhighlight both the alignment and deviation between agents and user-personalized\npreferences. Beyond mere performance comparison, we explore insightful\nexperiments, such as emulating the filter bubble effect and discovering the\nunderlying causal relationships in recommendation tasks. Our codes are\navailable at https://github.com/LehengTHU/Agent4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a user\nsimulator in recommendation, leveraging LLM-empowered generative agents\nequipped with user profile, memory, and actions modules specifically tailored\nfor the recommender system. In particular, these agents' profile modules are\ninitialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),\ncapturing users' unique tastes and social traits; memory modules log both\nfactual and emotional memories and are integrated with an emotion-driven\nreflection mechanism; action modules support a wide variety of behaviors,\nspanning both taste-driven and emotion-driven actions. Each agent interacts\nwith personalized recommender models in a page-by-page manner, relying on a\npre-implemented collaborative filtering-based recommendation algorithm. We\ndelve into both the capabilities and limitations of Agent4Rec, aiming to\nexplore an essential research question: ``To what extent can LLM-empowered\ngenerative agents faithfully simulate the behavior of real, autonomous humans\nin recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec\nhighlight both the alignment and deviation between agents and user-personalized\npreferences. Beyond mere performance comparison, we explore insightful\nexperiments, such as emulating the filter bubble effect and discovering the\nunderlying causal relationships in recommendation tasks. Our codes are\navailable at https://github.com/LehengTHU/Agent4Rec."
                },
                "authors": [
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "SIGIR 2024 perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00369v3",
                "updated": "2024-11-08T03:09:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    3,
                    9,
                    37,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-01T05:14:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    14,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset"
                },
                "summary": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics."
                },
                "authors": [
                    {
                        "name": "Anish Pahilajani"
                    },
                    {
                        "name": "Devasha Trivedi"
                    },
                    {
                        "name": "Jincen Shuai"
                    },
                    {
                        "name": "Khin S. Yone"
                    },
                    {
                        "name": "Samyak Rajesh Jain"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "15 pages, 24 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05289v1",
                "updated": "2024-11-08T02:47:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    47,
                    7,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:47:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    47,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}."
                },
                "authors": [
                    {
                        "name": "Ryan Sun"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05285v1",
                "updated": "2024-11-08T02:31:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    31,
                    3,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:31:03Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    31,
                    3,
                    4,
                    313,
                    0
                ],
                "title": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model\n  based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model\n  based Agents"
                },
                "summary": "The ever-improving quality of LLMs has fueled the growth of a diverse range\nof downstream tasks, leading to an increased demand for AI automation and a\nburgeoning interest in developing foundation model (FM)-based autonomous\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\na wider range of stakeholders, including agent users, agentic system developers\nand deployers, and AI model developers. These systems also integrate multiple\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\ncapabilities, and observability features. In this case, obtaining reliable\noutputs and answers from these agents remains challenging, necessitating a\ndependable execution process and end-to-end observability solutions. To build\nreliable AI agents and LLM applications, it is essential to shift towards\ndesigning AgentOps platforms that ensure observability and traceability across\nthe entire development-to-production life-cycle. To this end, we conducted a\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\nBased on this review, we provide an overview of the essential features of\nAgentOps and propose a comprehensive overview of observability data/traceable\nartifacts across the agent production life-cycle. Our findings provide a\nsystematic overview of the current AgentOps landscape, emphasizing the critical\nrole of observability/traceability in enhancing the reliability of autonomous\nagent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-improving quality of LLMs has fueled the growth of a diverse range\nof downstream tasks, leading to an increased demand for AI automation and a\nburgeoning interest in developing foundation model (FM)-based autonomous\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\na wider range of stakeholders, including agent users, agentic system developers\nand deployers, and AI model developers. These systems also integrate multiple\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\ncapabilities, and observability features. In this case, obtaining reliable\noutputs and answers from these agents remains challenging, necessitating a\ndependable execution process and end-to-end observability solutions. To build\nreliable AI agents and LLM applications, it is essential to shift towards\ndesigning AgentOps platforms that ensure observability and traceability across\nthe entire development-to-production life-cycle. To this end, we conducted a\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\nBased on this review, we provide an overview of the essential features of\nAgentOps and propose a comprehensive overview of observability data/traceable\nartifacts across the agent production life-cycle. Our findings provide a\nsystematic overview of the current AgentOps landscape, emphasizing the critical\nrole of observability/traceability in enhancing the reliability of autonomous\nagent systems."
                },
                "authors": [
                    {
                        "name": "Liming Dong"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; D.2.9; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05281v1",
                "updated": "2024-11-08T02:24:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    24,
                    29,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:24:29Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    24,
                    29,
                    4,
                    313,
                    0
                ],
                "title": "Fox-1 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fox-1 Technical Report"
                },
                "summary": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community."
                },
                "authors": [
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "Base model is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B and the instruction-tuned\n  version is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16235v2",
                "updated": "2024-11-08T02:17:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    17,
                    22,
                    4,
                    313,
                    0
                ],
                "published": "2024-06-23T22:53:47Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    22,
                    53,
                    47,
                    6,
                    175,
                    0
                ],
                "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages"
                },
                "summary": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning."
                },
                "authors": [
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Stephen H. Bach"
                    }
                ],
                "author_detail": {
                    "name": "Stephen H. Bach"
                },
                "author": "Stephen H. Bach",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05270v1",
                "updated": "2024-11-08T02:06:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    6,
                    41,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:06:41Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    6,
                    41,
                    4,
                    313,
                    0
                ],
                "title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination\n  Detection Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination\n  Detection Systems"
                },
                "summary": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Seth Rosen"
                    },
                    {
                        "name": "Vishnu Vettrivel"
                    }
                ],
                "author_detail": {
                    "name": "Vishnu Vettrivel"
                },
                "author": "Vishnu Vettrivel",
                "arxiv_comment": "18 pags, 13 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18822v2",
                "updated": "2024-11-08T01:49:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    1,
                    49,
                    58,
                    4,
                    313,
                    0
                ],
                "published": "2024-05-29T07:03:31Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    7,
                    3,
                    31,
                    2,
                    150,
                    0
                ],
                "title": "Toxicity Detection for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity Detection for Free"
                },
                "summary": "Current LLMs are generally aligned to follow safety requirements and tend to\nrefuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be\novercautious and refuse benign examples. In addition, state-of-the-art toxicity\ndetectors have low TPRs at low FPR, incurring high costs in real-world\napplications where toxic examples are rare. In this paper, we introduce\nModeration Using LLM Introspection (MULI), which detects toxic prompts using\nthe information extracted directly from LLMs themselves. We found we can\ndistinguish between benign and toxic prompts from the distribution of the first\nresponse token's logits. Using this idea, we build a robust detector of toxic\nprompts using a sparse logistic regression model on the first response token\nlogits. Our scheme outperforms SOTA detectors under multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLMs are generally aligned to follow safety requirements and tend to\nrefuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be\novercautious and refuse benign examples. In addition, state-of-the-art toxicity\ndetectors have low TPRs at low FPR, incurring high costs in real-world\napplications where toxic examples are rare. In this paper, we introduce\nModeration Using LLM Introspection (MULI), which detects toxic prompts using\nthe information extracted directly from LLMs themselves. We found we can\ndistinguish between benign and toxic prompts from the distribution of the first\nresponse token's logits. Using this idea, we build a robust detector of toxic\nprompts using a sparse logistic regression model on the first response token\nlogits. Our scheme outperforms SOTA detectors under multiple metrics."
                },
                "authors": [
                    {
                        "name": "Zhanhao Hu"
                    },
                    {
                        "name": "Julien Piet"
                    },
                    {
                        "name": "Geng Zhao"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "arxiv_comment": "Accepted by Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05254v1",
                "updated": "2024-11-08T00:58:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    0,
                    58,
                    12,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T00:58:12Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    0,
                    58,
                    12,
                    4,
                    313,
                    0
                ],
                "title": "Hierarchical Visual Feature Aggregation for OCR-Free Document\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Visual Feature Aggregation for OCR-Free Document\n  Understanding"
                },
                "summary": "We present a novel OCR-free document understanding framework based on\npretrained Multimodal Large Language Models (MLLMs). Our approach employs\nmulti-scale visual features to effectively handle various font sizes within\ndocument images. To address the increasing costs of considering the multi-scale\nvisual inputs for MLLMs, we propose the Hierarchical Visual Feature Aggregation\n(HVFA) module, designed to reduce the number of input tokens to LLMs.\nLeveraging a feature pyramid with cross-attentive pooling, our approach\neffectively manages the trade-off between information loss and efficiency\nwithout being affected by varying document image sizes. Furthermore, we\nintroduce a novel instruction tuning task, which facilitates the model's\ntext-reading capability by learning to predict the relative positions of input\ntext, eventually minimizing the risk of truncated text caused by the limited\ncapacity of LLMs. Comprehensive experiments validate the effectiveness of our\napproach, demonstrating superior performance in various document understanding\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel OCR-free document understanding framework based on\npretrained Multimodal Large Language Models (MLLMs). Our approach employs\nmulti-scale visual features to effectively handle various font sizes within\ndocument images. To address the increasing costs of considering the multi-scale\nvisual inputs for MLLMs, we propose the Hierarchical Visual Feature Aggregation\n(HVFA) module, designed to reduce the number of input tokens to LLMs.\nLeveraging a feature pyramid with cross-attentive pooling, our approach\neffectively manages the trade-off between information loss and efficiency\nwithout being affected by varying document image sizes. Furthermore, we\nintroduce a novel instruction tuning task, which facilitates the model's\ntext-reading capability by learning to predict the relative positions of input\ntext, eventually minimizing the risk of truncated text caused by the limited\ncapacity of LLMs. Comprehensive experiments validate the effectiveness of our\napproach, demonstrating superior performance in various document understanding\ntasks."
                },
                "authors": [
                    {
                        "name": "Jaeyoo Park"
                    },
                    {
                        "name": "Jin Young Choi"
                    },
                    {
                        "name": "Jeonghyung Park"
                    },
                    {
                        "name": "Bohyung Han"
                    }
                ],
                "author_detail": {
                    "name": "Bohyung Han"
                },
                "author": "Bohyung Han",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05253v1",
                "updated": "2024-11-08T00:46:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    0,
                    46,
                    24,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T00:46:24Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    0,
                    46,
                    24,
                    4,
                    313,
                    0
                ],
                "title": "What talking you?: Translating Code-Mixed Messaging Texts to English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What talking you?: Translating Code-Mixed Messaging Texts to English"
                },
                "summary": "Translation of code-mixed texts to formal English allow a wider audience to\nunderstand these code-mixed languages, and facilitate downstream analysis\napplications such as sentiment analysis. In this work, we look at translating\nSinglish, which is colloquial Singaporean English, to formal standard English.\nSinglish is formed through the code-mixing of multiple Asian languages and\ndialects. We analysed the presence of other Asian languages and variants which\ncan facilitate translation. Our dataset is short message texts, written as\ninformal communication between Singlish speakers. We use a multi-step prompting\nscheme on five Large Language Models (LLMs) for language detection and\ntranslation. Our analysis show that LLMs do not perform well in this task, and\nwe describe the challenges involved in translation of code-mixed languages. We\nalso release our dataset in this link https://github.com/luoqichan/singlish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation of code-mixed texts to formal English allow a wider audience to\nunderstand these code-mixed languages, and facilitate downstream analysis\napplications such as sentiment analysis. In this work, we look at translating\nSinglish, which is colloquial Singaporean English, to formal standard English.\nSinglish is formed through the code-mixing of multiple Asian languages and\ndialects. We analysed the presence of other Asian languages and variants which\ncan facilitate translation. Our dataset is short message texts, written as\ninformal communication between Singlish speakers. We use a multi-step prompting\nscheme on five Large Language Models (LLMs) for language detection and\ntranslation. Our analysis show that LLMs do not perform well in this task, and\nwe describe the challenges involved in translation of code-mixed languages. We\nalso release our dataset in this link https://github.com/luoqichan/singlish."
                },
                "authors": [
                    {
                        "name": "Lynnette Hui Xian Ng"
                    },
                    {
                        "name": "Luo Qi Chan"
                    }
                ],
                "author_detail": {
                    "name": "Luo Qi Chan"
                },
                "author": "Luo Qi Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00980v2",
                "updated": "2024-11-08T00:36:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    0,
                    36,
                    10,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-01T19:11:54Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    19,
                    11,
                    54,
                    4,
                    306,
                    0
                ],
                "title": "Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An\n  Evaluation Using TORGO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An\n  Evaluation Using TORGO"
                },
                "summary": "Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS)\nfrequently face challenges with articulation, leading to dysarthria and\nresulting in atypical speech patterns. In healthcare settings, communication\nbreakdowns reduce the quality of care. While building an augmentative and\nalternative communication (AAC) tool to enable fluid communication we found\nthat state-of-the-art (SOTA) automatic speech recognition (ASR) technology like\nWhisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack\nof training data. Our work looks to leverage SOTA ASR followed by domain\nspecific error-correction. English dysarthric ASR performance is often\nevaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this\ndataset where phrases overlap between training and test speakers. Our work\nproposes an algorithm to break this prompt-overlap. After reducing\nprompt-overlap, results with SOTA ASR models produce extremely high word error\nrates for speakers with mild and severe dysarthria. Furthermore, to improve\nASR, our work looks at the impact of n-gram language models and large-language\nmodel (LLM) based multi-modal generative error-correction algorithms like\nWhispering-LLaMA for a second pass ASR. Our work highlights how much more needs\nto be done to improve ASR for atypical speakers to enable equitable healthcare\naccess both in-person and in e-health settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS)\nfrequently face challenges with articulation, leading to dysarthria and\nresulting in atypical speech patterns. In healthcare settings, communication\nbreakdowns reduce the quality of care. While building an augmentative and\nalternative communication (AAC) tool to enable fluid communication we found\nthat state-of-the-art (SOTA) automatic speech recognition (ASR) technology like\nWhisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack\nof training data. Our work looks to leverage SOTA ASR followed by domain\nspecific error-correction. English dysarthric ASR performance is often\nevaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this\ndataset where phrases overlap between training and test speakers. Our work\nproposes an algorithm to break this prompt-overlap. After reducing\nprompt-overlap, results with SOTA ASR models produce extremely high word error\nrates for speakers with mild and severe dysarthria. Furthermore, to improve\nASR, our work looks at the impact of n-gram language models and large-language\nmodel (LLM) based multi-modal generative error-correction algorithms like\nWhispering-LLaMA for a second pass ASR. Our work highlights how much more needs\nto be done to improve ASR for atypical speakers to enable equitable healthcare\naccess both in-person and in e-health settings."
                },
                "authors": [
                    {
                        "name": "Macarious Hui"
                    },
                    {
                        "name": "Jinda Zhang"
                    },
                    {
                        "name": "Aanchan Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Aanchan Mohan"
                },
                "author": "Aanchan Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05239v1",
                "updated": "2024-11-07T23:28:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    23,
                    28,
                    23,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T23:28:23Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    23,
                    28,
                    23,
                    3,
                    312,
                    0
                ],
                "title": "ZipNN: Lossless Compression for AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipNN: Lossless Compression for AI Models"
                },
                "summary": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face."
                },
                "authors": [
                    {
                        "name": "Moshik Hershcovitch"
                    },
                    {
                        "name": "Andrew Wood"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Guy Girmonsky"
                    },
                    {
                        "name": "Roy Leibovitz"
                    },
                    {
                        "name": "Ilias Ennmouri"
                    },
                    {
                        "name": "Michal Malka"
                    },
                    {
                        "name": "Peter Chin"
                    },
                    {
                        "name": "Swaminathan Sundararaman"
                    },
                    {
                        "name": "Danny Harnik"
                    }
                ],
                "author_detail": {
                    "name": "Danny Harnik"
                },
                "author": "Danny Harnik",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2404.15198",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21338v2",
                "updated": "2024-11-07T23:11:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    23,
                    11,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-28T00:40:55Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    0,
                    40,
                    55,
                    0,
                    302,
                    0
                ],
                "title": "FinTeamExperts: Role Specialized MOEs For Financial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinTeamExperts: Role Specialized MOEs For Financial Analysis"
                },
                "summary": "Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading\na significant leap in AI, as they can generalize knowledge from their training\nto new tasks without fine-tuning. However, their application in the financial\ndomain remains relatively limited. The financial field is inherently complex,\nrequiring a deep understanding across various perspectives, from macro, micro\neconomic trend to quantitative analysis. Motivated by this complexity, a\nmixture of expert LLMs tailored to specific financial domains could offer a\nmore comprehensive understanding for intricate financial tasks. In this paper,\nwe present the FinTeamExperts, a role-specialized LLM framework structured as a\nMixture of Experts (MOEs) for financial analysis. The framework simulates a\ncollaborative team setting by training each model to specialize in distinct\nroles: Macro Analysts, Micro analysts, and Quantitative Analysts. This\nrole-specific specialization enhances the model's ability to integrate their\ndomain-specific expertise. We achieve this by training three 8-billion\nparameter models on different corpus, each dedicated to excelling in specific\nfinance-related roles. We then instruct-tune FinTeamExperts on downstream tasks\nto align with practical financial tasks. The experimental results show that\nFinTeamExperts outperform all models of the same size and larger on three out\nof four datasets. On the fourth dataset, which presents a more complex task,\nFinTeamExperts still surpass all models of the same size. This highlights the\nsuccess of our role-based specialization approach and the continued training\napproach for FinTeamExperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading\na significant leap in AI, as they can generalize knowledge from their training\nto new tasks without fine-tuning. However, their application in the financial\ndomain remains relatively limited. The financial field is inherently complex,\nrequiring a deep understanding across various perspectives, from macro, micro\neconomic trend to quantitative analysis. Motivated by this complexity, a\nmixture of expert LLMs tailored to specific financial domains could offer a\nmore comprehensive understanding for intricate financial tasks. In this paper,\nwe present the FinTeamExperts, a role-specialized LLM framework structured as a\nMixture of Experts (MOEs) for financial analysis. The framework simulates a\ncollaborative team setting by training each model to specialize in distinct\nroles: Macro Analysts, Micro analysts, and Quantitative Analysts. This\nrole-specific specialization enhances the model's ability to integrate their\ndomain-specific expertise. We achieve this by training three 8-billion\nparameter models on different corpus, each dedicated to excelling in specific\nfinance-related roles. We then instruct-tune FinTeamExperts on downstream tasks\nto align with practical financial tasks. The experimental results show that\nFinTeamExperts outperform all models of the same size and larger on three out\nof four datasets. On the fourth dataset, which presents a more complex task,\nFinTeamExperts still surpass all models of the same size. This highlights the\nsuccess of our role-based specialization approach and the continued training\napproach for FinTeamExperts."
                },
                "authors": [
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Prayag Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Prayag Tiwari"
                },
                "author": "Prayag Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05232v1",
                "updated": "2024-11-07T22:57:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    57,
                    2,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T22:57:02Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    57,
                    2,
                    3,
                    312,
                    0
                ],
                "title": "Abstract2Appendix: Academic Reviews Enhance LLM Long-Context\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract2Appendix: Academic Reviews Enhance LLM Long-Context\n  Capabilities"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across various\ntasks, yet their ability to handle long-context reading remains challenging.\nThis study explores the effectiveness of leveraging high-quality academic peer\nreview data for fine-tuning LLMs to enhance their long-context capabilities. We\ncompare the Direct Preference Optimization (DPO) method with the Supervised\nFine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.\nOur experiments show that the fine-tuned model achieves a 4.04-point\nimprovement over phi-3 and a 2.6\\% increase on the Qasper benchmark using only\n2000 samples. Despite facing limitations in data scale and processing costs,\nthis study underscores the potential of DPO and high-quality data in advancing\nLLM performance.\n  Additionally, the zero-shot benchmark results indicate that aggregated\nhigh-quality human reviews are overwhelmingly preferred over LLM-generated\nresponses, even for the most capable models like GPT-4o. This suggests that\nhigh-quality human reviews are extremely rich in information, reasoning, and\nlong-context retrieval, capabilities that even the most advanced models have\nnot fully captured. These findings highlight the high utility of leveraging\nhuman reviews to further advance the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across various\ntasks, yet their ability to handle long-context reading remains challenging.\nThis study explores the effectiveness of leveraging high-quality academic peer\nreview data for fine-tuning LLMs to enhance their long-context capabilities. We\ncompare the Direct Preference Optimization (DPO) method with the Supervised\nFine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.\nOur experiments show that the fine-tuned model achieves a 4.04-point\nimprovement over phi-3 and a 2.6\\% increase on the Qasper benchmark using only\n2000 samples. Despite facing limitations in data scale and processing costs,\nthis study underscores the potential of DPO and high-quality data in advancing\nLLM performance.\n  Additionally, the zero-shot benchmark results indicate that aggregated\nhigh-quality human reviews are overwhelmingly preferred over LLM-generated\nresponses, even for the most capable models like GPT-4o. This suggests that\nhigh-quality human reviews are extremely rich in information, reasoning, and\nlong-context retrieval, capabilities that even the most advanced models have\nnot fully captured. These findings highlight the high utility of leveraging\nhuman reviews to further advance the field."
                },
                "authors": [
                    {
                        "name": "Shengzhi Li"
                    },
                    {
                        "name": "Kittipat Kampa"
                    },
                    {
                        "name": "Rongyu Lin"
                    },
                    {
                        "name": "Bohang Li"
                    },
                    {
                        "name": "Shichao Pei"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Pei"
                },
                "author": "Shichao Pei",
                "arxiv_comment": "We share our latest dataset on\n  https://github.com/findalexli/Abstract2Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05212v1",
                "updated": "2024-11-07T22:17:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    17,
                    53,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T22:17:53Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    17,
                    53,
                    3,
                    312,
                    0
                ],
                "title": "RT-Grasp: Reasoning Tuning Robotic Grasping via Multi-modal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Grasp: Reasoning Tuning Robotic Grasping via Multi-modal Large\n  Language Model"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have showcased their\nremarkable reasoning capabilities, making them influential across various\nfields. However, in robotics, their use has primarily been limited to\nmanipulation planning tasks due to their inherent textual output. This paper\naddresses this limitation by investigating the potential of adopting the\nreasoning ability of LLMs for generating numerical predictions in robotics\ntasks, specifically for robotic grasping. We propose Reasoning Tuning, a novel\nmethod that integrates a reasoning phase before prediction during training,\nleveraging the extensive prior knowledge and advanced reasoning abilities of\nLLMs. This approach enables LLMs, notably with multi-modal capabilities, to\ngenerate accurate numerical outputs like grasp poses that are context-aware and\nadaptable through conversations. Additionally, we present the Reasoning Tuning\nVLM Grasp dataset, carefully curated to facilitate the adaptation of LLMs to\nrobotic grasping. Extensive validation on both grasping datasets and real-world\nexperiments underscores the adaptability of multi-modal LLMs for numerical\nprediction tasks in robotics. This not only expands their applicability but\nalso bridges the gap between text-based planning and direct robot control,\nthereby maximizing the potential of LLMs in robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have showcased their\nremarkable reasoning capabilities, making them influential across various\nfields. However, in robotics, their use has primarily been limited to\nmanipulation planning tasks due to their inherent textual output. This paper\naddresses this limitation by investigating the potential of adopting the\nreasoning ability of LLMs for generating numerical predictions in robotics\ntasks, specifically for robotic grasping. We propose Reasoning Tuning, a novel\nmethod that integrates a reasoning phase before prediction during training,\nleveraging the extensive prior knowledge and advanced reasoning abilities of\nLLMs. This approach enables LLMs, notably with multi-modal capabilities, to\ngenerate accurate numerical outputs like grasp poses that are context-aware and\nadaptable through conversations. Additionally, we present the Reasoning Tuning\nVLM Grasp dataset, carefully curated to facilitate the adaptation of LLMs to\nrobotic grasping. Extensive validation on both grasping datasets and real-world\nexperiments underscores the adaptability of multi-modal LLMs for numerical\nprediction tasks in robotics. This not only expands their applicability but\nalso bridges the gap between text-based planning and direct robot control,\nthereby maximizing the potential of LLMs in robotics."
                },
                "authors": [
                    {
                        "name": "Jinxuan Xu"
                    },
                    {
                        "name": "Shiyu Jin"
                    },
                    {
                        "name": "Yutian Lei"
                    },
                    {
                        "name": "Yuqian Zhang"
                    },
                    {
                        "name": "Liangjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liangjun Zhang"
                },
                "author": "Liangjun Zhang",
                "arxiv_comment": "Accepted to IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05209v1",
                "updated": "2024-11-07T22:15:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    15,
                    17,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T22:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    15,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "Alopex: A Computational Framework for Enabling On-Device Function Calls\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alopex: A Computational Framework for Enabling On-Device Function Calls\n  with LLMs"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has led to their\nincreased integration into mobile devices for personalized assistance, which\nenables LLMs to call external API functions to enhance their performance.\nHowever, challenges such as data scarcity, ineffective question formatting, and\ncatastrophic forgetting hinder the development of on-device LLM agents. To\ntackle these issues, we propose Alopex, a framework that enables precise\non-device function calls using the Fox LLM. Alopex introduces a logic-based\nmethod for generating high-quality training data and a novel\n``description-question-output'' format for fine-tuning, reducing risks of\nfunction information leakage. Additionally, a data mixing strategy is used to\nmitigate catastrophic forgetting, combining function call data with textbook\ndatasets to enhance performance in various tasks. Experimental results show\nthat Alopex improves function call accuracy and significantly reduces\ncatastrophic forgetting, providing a robust solution for integrating function\ncall capabilities into LLMs without manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has led to their\nincreased integration into mobile devices for personalized assistance, which\nenables LLMs to call external API functions to enhance their performance.\nHowever, challenges such as data scarcity, ineffective question formatting, and\ncatastrophic forgetting hinder the development of on-device LLM agents. To\ntackle these issues, we propose Alopex, a framework that enables precise\non-device function calls using the Fox LLM. Alopex introduces a logic-based\nmethod for generating high-quality training data and a novel\n``description-question-output'' format for fine-tuning, reducing risks of\nfunction information leakage. Additionally, a data mixing strategy is used to\nmitigate catastrophic forgetting, combining function call data with textbook\ndatasets to enhance performance in various tasks. Experimental results show\nthat Alopex improves function call accuracy and significantly reduces\ncatastrophic forgetting, providing a robust solution for integrating function\ncall capabilities into LLMs without manual intervention."
                },
                "authors": [
                    {
                        "name": "Yide Ran"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Shanshan Han"
                    },
                    {
                        "name": "Han Jin"
                    },
                    {
                        "name": "Alay Dilipbhai Shah"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05200v1",
                "updated": "2024-11-07T22:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    1,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T22:01:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    22,
                    1,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "Toward Cultural Interpretability: A Linguistic Anthropological Framework\n  for Describing and Evaluating Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Cultural Interpretability: A Linguistic Anthropological Framework\n  for Describing and Evaluating Large Language Models (LLMs)"
                },
                "summary": "This article proposes a new integration of linguistic anthropology and\nmachine learning (ML) around convergent interests in both the underpinnings of\nlanguage and making language technologies more socially responsible. While\nlinguistic anthropology focuses on interpreting the cultural basis for human\nlanguage use, the ML field of interpretability is concerned with uncovering the\npatterns that Large Language Models (LLMs) learn from human verbal behavior.\nThrough the analysis of a conversation between a human user and an LLM-powered\nchatbot, we demonstrate the theoretical feasibility of a new, conjoint field of\ninquiry, cultural interpretability (CI). By focusing attention on the\ncommunicative competence involved in the way human users and AI chatbots\nco-produce meaning in the articulatory interface of human-computer interaction,\nCI emphasizes how the dynamic relationship between language and culture makes\ncontextually sensitive, open-ended conversation possible. We suggest that, by\nexamining how LLMs internally \"represent\" relationships between language and\nculture, CI can: (1) provide insight into long-standing linguistic\nanthropological questions about the patterning of those relationships; and (2)\naid model developers and interface designers in improving value alignment\nbetween language models and stylistically diverse speakers and culturally\ndiverse speech communities. Our discussion proposes three critical research\naxes: relativity, variation, and indexicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article proposes a new integration of linguistic anthropology and\nmachine learning (ML) around convergent interests in both the underpinnings of\nlanguage and making language technologies more socially responsible. While\nlinguistic anthropology focuses on interpreting the cultural basis for human\nlanguage use, the ML field of interpretability is concerned with uncovering the\npatterns that Large Language Models (LLMs) learn from human verbal behavior.\nThrough the analysis of a conversation between a human user and an LLM-powered\nchatbot, we demonstrate the theoretical feasibility of a new, conjoint field of\ninquiry, cultural interpretability (CI). By focusing attention on the\ncommunicative competence involved in the way human users and AI chatbots\nco-produce meaning in the articulatory interface of human-computer interaction,\nCI emphasizes how the dynamic relationship between language and culture makes\ncontextually sensitive, open-ended conversation possible. We suggest that, by\nexamining how LLMs internally \"represent\" relationships between language and\nculture, CI can: (1) provide insight into long-standing linguistic\nanthropological questions about the patterning of those relationships; and (2)\naid model developers and interface designers in improving value alignment\nbetween language models and stylistically diverse speakers and culturally\ndiverse speech communities. Our discussion proposes three critical research\naxes: relativity, variation, and indexicality."
                },
                "authors": [
                    {
                        "name": "Graham M. Jones"
                    },
                    {
                        "name": "Shai Satran"
                    },
                    {
                        "name": "Arvind Satyanarayan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Satyanarayan"
                },
                "author": "Arvind Satyanarayan",
                "arxiv_comment": "Accepted for publication in Big Data & Society, November 2, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05199v1",
                "updated": "2024-11-07T21:51:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    51,
                    7,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:51:07Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    51,
                    7,
                    3,
                    312,
                    0
                ],
                "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced code generation but\noften require substantial resources and tend to over-generalize, limiting their\nefficiency for specific tasks. Fine-tuning smaller, open-source LLMs presents a\nviable alternative; however, it typically lags behind cutting-edge models due\nto supervised fine-tuning's reliance solely on correct code examples, which\nrestricts the model's ability to learn from its own mistakes and adapt to\ndiverse programming challenges. To bridge this gap, we introduce CodeLutra, a\nnovel framework that enhances low-performing LLMs by leveraging both successful\nand failed code generation attempts. Unlike conventional fine-tuning, CodeLutra\nemploys an iterative preference learning mechanism to compare correct and\nincorrect solutions as well as maximize the likelihood of correct codes.\nThrough continuous iterative refinement, CodeLutra enables smaller LLMs to\nmatch or surpass GPT-4's performance in various code generation tasks without\nrelying on vast external datasets or larger auxiliary models. On a challenging\ndata analysis task, using just 500 samples improved Llama-3-8B's accuracy from\n28.2% to 48.6%, approaching GPT-4's performance. These results highlight\nCodeLutra's potential to close the gap between open-source and closed-source\nmodels, making it a promising approach in the field of code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced code generation but\noften require substantial resources and tend to over-generalize, limiting their\nefficiency for specific tasks. Fine-tuning smaller, open-source LLMs presents a\nviable alternative; however, it typically lags behind cutting-edge models due\nto supervised fine-tuning's reliance solely on correct code examples, which\nrestricts the model's ability to learn from its own mistakes and adapt to\ndiverse programming challenges. To bridge this gap, we introduce CodeLutra, a\nnovel framework that enhances low-performing LLMs by leveraging both successful\nand failed code generation attempts. Unlike conventional fine-tuning, CodeLutra\nemploys an iterative preference learning mechanism to compare correct and\nincorrect solutions as well as maximize the likelihood of correct codes.\nThrough continuous iterative refinement, CodeLutra enables smaller LLMs to\nmatch or surpass GPT-4's performance in various code generation tasks without\nrelying on vast external datasets or larger auxiliary models. On a challenging\ndata analysis task, using just 500 samples improved Llama-3-8B's accuracy from\n28.2% to 48.6%, approaching GPT-4's performance. These results highlight\nCodeLutra's potential to close the gap between open-source and closed-source\nmodels, making it a promising approach in the field of code generation."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Tung Mai"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Saayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Saayan Mitra"
                },
                "author": "Saayan Mitra",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05197v1",
                "updated": "2024-11-07T21:44:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    44,
                    4,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:44:04Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    44,
                    4,
                    3,
                    312,
                    0
                ],
                "title": "Hardware and Software Platform Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware and Software Platform Inference"
                },
                "summary": "It is now a common business practice to buy access to large language model\n(LLM) inference rather than self-host, because of significant upfront hardware\ninfrastructure and energy costs. However, as a buyer, there is no mechanism to\nverify the authenticity of the advertised service including the serving\nhardware platform, e.g. that it is actually being served using an NVIDIA H100.\nFurthermore, there are reports suggesting that model providers may deliver\nmodels that differ slightly from the advertised ones, often to make them run on\nless expensive hardware. That way, a client pays premium for a capable model\naccess on more expensive hardware, yet ends up being served by a (potentially\nless capable) cheaper model on cheaper hardware. In this paper we introduce\n\\textit{\\textbf{hardware and software platform inference (HSPI)}} -- a method\nfor identifying the underlying \\GPU{} architecture and software stack of a\n(black-box) machine learning model solely based on its input-output behavior.\nOur method leverages the inherent differences of various \\GPU{} architectures\nand compilers to distinguish between different \\GPU{} types and software\nstacks. By analyzing the numerical patterns in the model's outputs, we propose\na classification framework capable of accurately identifying the \\GPU{} used\nfor model inference as well as the underlying software configuration. Our\nfindings demonstrate the feasibility of inferring \\GPU{} type from black-box\nmodels. We evaluate HSPI against models served on different real hardware and\nfind that in a white-box setting we can distinguish between different \\GPU{}s\nwith between $83.9\\%$ and $100\\%$ accuracy. Even in a black-box setting we are\nable to achieve results that are up to three times higher than random guess\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is now a common business practice to buy access to large language model\n(LLM) inference rather than self-host, because of significant upfront hardware\ninfrastructure and energy costs. However, as a buyer, there is no mechanism to\nverify the authenticity of the advertised service including the serving\nhardware platform, e.g. that it is actually being served using an NVIDIA H100.\nFurthermore, there are reports suggesting that model providers may deliver\nmodels that differ slightly from the advertised ones, often to make them run on\nless expensive hardware. That way, a client pays premium for a capable model\naccess on more expensive hardware, yet ends up being served by a (potentially\nless capable) cheaper model on cheaper hardware. In this paper we introduce\n\\textit{\\textbf{hardware and software platform inference (HSPI)}} -- a method\nfor identifying the underlying \\GPU{} architecture and software stack of a\n(black-box) machine learning model solely based on its input-output behavior.\nOur method leverages the inherent differences of various \\GPU{} architectures\nand compilers to distinguish between different \\GPU{} types and software\nstacks. By analyzing the numerical patterns in the model's outputs, we propose\na classification framework capable of accurately identifying the \\GPU{} used\nfor model inference as well as the underlying software configuration. Our\nfindings demonstrate the feasibility of inferring \\GPU{} type from black-box\nmodels. We evaluate HSPI against models served on different real hardware and\nfind that in a white-box setting we can distinguish between different \\GPU{}s\nwith between $83.9\\%$ and $100\\%$ accuracy. Even in a black-box setting we are\nable to achieve results that are up to three times higher than random guess\naccuracy."
                },
                "authors": [
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Hanna Foerster"
                    },
                    {
                        "name": "Robert D. Mullins"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Ilia Shumailov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Shumailov"
                },
                "author": "Ilia Shumailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05194v1",
                "updated": "2024-11-07T21:37:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    37,
                    51,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:37:51Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    37,
                    51,
                    3,
                    312,
                    0
                ],
                "title": "Interactive Dialogue Agents via Reinforcement Learning on Hindsight\n  Regenerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Dialogue Agents via Reinforcement Learning on Hindsight\n  Regenerations"
                },
                "summary": "Recent progress on large language models (LLMs) has enabled dialogue agents\nto generate highly naturalistic and plausible text. However, current LLM\nlanguage generation focuses on responding accurately to questions and requests\nwith a single effective response. In reality, many real dialogues are\ninteractive, meaning an agent's utterances will influence their conversational\npartner, elicit information, or change their opinion. Accounting for how an\nagent can effectively steer a conversation is a crucial ability in many\ndialogue tasks, from healthcare to preference elicitation. Existing methods for\nfine-tuning dialogue agents to accomplish such tasks would rely on curating\nsome amount of expert data. However, doing so often requires understanding the\nunderlying cognitive processes of the conversational partner, which is a skill\nneither humans nor LLMs trained on human data can reliably do. Our key insight\nis that while LLMs may not be adept at identifying effective strategies for\nsteering conversations a priori, or in the middle of an ongoing conversation,\nthey can do so post-hoc, or in hindsight, after seeing how their conversational\npartner responds. We use this fact to rewrite and augment existing suboptimal\ndata, and train via offline reinforcement learning (RL) an agent that\noutperforms both prompting and learning from unaltered human demonstrations. We\napply our approach to two domains that require understanding human mental\nstate, intelligent interaction, and persuasion: mental health support, and\nsoliciting charitable donations. Our results in a user study with real humans\nshow that our approach greatly outperforms existing state-of-the-art dialogue\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress on large language models (LLMs) has enabled dialogue agents\nto generate highly naturalistic and plausible text. However, current LLM\nlanguage generation focuses on responding accurately to questions and requests\nwith a single effective response. In reality, many real dialogues are\ninteractive, meaning an agent's utterances will influence their conversational\npartner, elicit information, or change their opinion. Accounting for how an\nagent can effectively steer a conversation is a crucial ability in many\ndialogue tasks, from healthcare to preference elicitation. Existing methods for\nfine-tuning dialogue agents to accomplish such tasks would rely on curating\nsome amount of expert data. However, doing so often requires understanding the\nunderlying cognitive processes of the conversational partner, which is a skill\nneither humans nor LLMs trained on human data can reliably do. Our key insight\nis that while LLMs may not be adept at identifying effective strategies for\nsteering conversations a priori, or in the middle of an ongoing conversation,\nthey can do so post-hoc, or in hindsight, after seeing how their conversational\npartner responds. We use this fact to rewrite and augment existing suboptimal\ndata, and train via offline reinforcement learning (RL) an agent that\noutperforms both prompting and learning from unaltered human demonstrations. We\napply our approach to two domains that require understanding human mental\nstate, intelligent interaction, and persuasion: mental health support, and\nsoliciting charitable donations. Our results in a user study with real humans\nshow that our approach greatly outperforms existing state-of-the-art dialogue\nagents."
                },
                "authors": [
                    {
                        "name": "Joey Hong"
                    },
                    {
                        "name": "Jessica Lin"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05193v1",
                "updated": "2024-11-07T21:36:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    36,
                    52,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:36:52Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    36,
                    52,
                    3,
                    312,
                    0
                ],
                "title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning"
                },
                "summary": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images."
                },
                "authors": [
                    {
                        "name": "Joey Hong"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05192v1",
                "updated": "2024-11-07T21:34:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    34,
                    5,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:34:05Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    34,
                    5,
                    3,
                    312,
                    0
                ],
                "title": "Explaining Mixtures of Sources in News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Mixtures of Sources in News Articles"
                },
                "summary": "Human writers plan, then write. For large language models (LLMs) to play a\nrole in longer-form article generation, we must understand the planning steps\nhumans make before writing. We explore one kind of planning, source-selection\nin news, as a case-study for evaluating plans in long-form generation. We ask:\nwhy do specific stories call for specific kinds of sources? We imagine a\ngenerative process for story writing where a source-selection schema is first\nselected by a journalist, and then sources are chosen based on categories in\nthat schema. Learning the article's plan means predicting the schema initially\nchosen by the journalist. Working with professional journalists, we adapt five\nexisting schemata and introduce three new ones to describe journalistic plans\nfor the inclusion of sources in documents. Then, inspired by Bayesian\nlatent-variable modeling, we develop metrics to select the most likely plan, or\nschema, underlying a story, which we use to compare schemata. We find that two\nschemata: stance and social affiliation best explain source plans in most\ndocuments. However, other schemata like textual entailment explain source plans\nin factually rich topics like \"Science\". Finally, we find we can predict the\nmost suitable schema given just the article's headline with reasonable\naccuracy. We see this as an important case-study for human planning, and\nprovides a framework and approach for evaluating other kinds of plans. We\nrelease a corpora, NewsSources, with annotations for 4M articles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human writers plan, then write. For large language models (LLMs) to play a\nrole in longer-form article generation, we must understand the planning steps\nhumans make before writing. We explore one kind of planning, source-selection\nin news, as a case-study for evaluating plans in long-form generation. We ask:\nwhy do specific stories call for specific kinds of sources? We imagine a\ngenerative process for story writing where a source-selection schema is first\nselected by a journalist, and then sources are chosen based on categories in\nthat schema. Learning the article's plan means predicting the schema initially\nchosen by the journalist. Working with professional journalists, we adapt five\nexisting schemata and introduce three new ones to describe journalistic plans\nfor the inclusion of sources in documents. Then, inspired by Bayesian\nlatent-variable modeling, we develop metrics to select the most likely plan, or\nschema, underlying a story, which we use to compare schemata. We find that two\nschemata: stance and social affiliation best explain source plans in most\ndocuments. However, other schemata like textual entailment explain source plans\nin factually rich topics like \"Science\". Finally, we find we can predict the\nmost suitable schema given just the article's headline with reasonable\naccuracy. We see this as an important case-study for human planning, and\nprovides a framework and approach for evaluating other kinds of plans. We\nrelease a corpora, NewsSources, with annotations for 4M articles."
                },
                "authors": [
                    {
                        "name": "Alexander Spangher"
                    },
                    {
                        "name": "James Youn"
                    },
                    {
                        "name": "Matt DeButts"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Jonathan May"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan May"
                },
                "author": "Jonathan May",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18097v2",
                "updated": "2024-11-07T21:23:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    23,
                    15,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-08T11:28:06Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    6,
                    1,
                    282,
                    0
                ],
                "title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Document\n  Re-Ranking of Long-Tail Queries in a Search Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RRADistill: Distilling LLMs' Passage Ranking Ability for Document\n  Re-Ranking of Long-Tail Queries in a Search Engine"
                },
                "summary": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries."
                },
                "authors": [
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Youngjune Lee"
                    },
                    {
                        "name": "Gyu-Hwung Cho"
                    },
                    {
                        "name": "Haeyu Jeong"
                    },
                    {
                        "name": "Jungmin Kong"
                    },
                    {
                        "name": "Saehun Kim"
                    },
                    {
                        "name": "Keunchan Park"
                    },
                    {
                        "name": "Jaeho Choi"
                    },
                    {
                        "name": "Sarah Cho"
                    },
                    {
                        "name": "Inchang Jeong"
                    },
                    {
                        "name": "Gyohee Nam"
                    },
                    {
                        "name": "Sunghoon Han"
                    },
                    {
                        "name": "Wonil Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wonil Yang"
                },
                "author": "Wonil Yang",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21337v2",
                "updated": "2024-11-07T21:20:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    20,
                    31,
                    3,
                    312,
                    0
                ],
                "published": "2024-10-28T00:36:21Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    0,
                    36,
                    21,
                    0,
                    302,
                    0
                ],
                "title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection\n  Attacks Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection\n  Attacks Detection"
                },
                "summary": "Large language models (LLMs) are becoming a popular tool as they have\nsignificantly advanced in their capability to tackle a wide range of\nlanguage-based tasks. However, LLMs applications are highly vulnerable to\nprompt injection attacks, which poses a critical problem. These attacks target\nLLMs applications through using carefully designed input prompts to divert the\nmodel from adhering to original instruction, thereby it could execute\nunintended actions. These manipulations pose serious security threats which\npotentially results in data leaks, biased outputs, or harmful responses. This\nproject explores the security vulnerabilities in relation to prompt injection\nattacks. To detect whether a prompt is vulnerable or not, we follows two\napproaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a\nthorough analysis and comparison of the classification performance. Firstly, we\nuse pre-trained XLM-RoBERTa model to detect prompt injections using test\ndataset without any fine-tuning and evaluate it by zero-shot classification.\nThen, this proposed work will apply supervised fine-tuning to this pre-trained\nLLM using a task-specific labeled dataset from deepset in huggingface, and this\nfine-tuned model achieves impressive results with 99.13\\% accuracy, 100\\%\nprecision, 98.33\\% recall and 99.15\\% F1-score thorough rigorous\nexperimentation and evaluation. We observe that our approach is highly\nefficient in detecting prompt injection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming a popular tool as they have\nsignificantly advanced in their capability to tackle a wide range of\nlanguage-based tasks. However, LLMs applications are highly vulnerable to\nprompt injection attacks, which poses a critical problem. These attacks target\nLLMs applications through using carefully designed input prompts to divert the\nmodel from adhering to original instruction, thereby it could execute\nunintended actions. These manipulations pose serious security threats which\npotentially results in data leaks, biased outputs, or harmful responses. This\nproject explores the security vulnerabilities in relation to prompt injection\nattacks. To detect whether a prompt is vulnerable or not, we follows two\napproaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a\nthorough analysis and comparison of the classification performance. Firstly, we\nuse pre-trained XLM-RoBERTa model to detect prompt injections using test\ndataset without any fine-tuning and evaluate it by zero-shot classification.\nThen, this proposed work will apply supervised fine-tuning to this pre-trained\nLLM using a task-specific labeled dataset from deepset in huggingface, and this\nfine-tuned model achieves impressive results with 99.13\\% accuracy, 100\\%\nprecision, 98.33\\% recall and 99.15\\% F1-score thorough rigorous\nexperimentation and evaluation. We observe that our approach is highly\nefficient in detecting prompt injection attacks."
                },
                "authors": [
                    {
                        "name": "Md Abdur Rahman"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Alfredo Cuzzocrea"
                    },
                    {
                        "name": "Sheikh Iqbal Ahamed"
                    }
                ],
                "author_detail": {
                    "name": "Sheikh Iqbal Ahamed"
                },
                "author": "Sheikh Iqbal Ahamed",
                "arxiv_comment": "I am requesting the withdrawal of my paper due to critical issues\n  identified in the methodology/results that may impact its accuracy and\n  reliability. I also plan to make substantial revisions that go beyond minor\n  corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05185v1",
                "updated": "2024-11-07T21:10:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    10,
                    39,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T21:10:39Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    10,
                    39,
                    3,
                    312,
                    0
                ],
                "title": "PentestAgent: Incorporating LLM Agents to Automated Penetration Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentestAgent: Incorporating LLM Agents to Automated Penetration Testing"
                },
                "summary": "Penetration testing is a critical technique for identifying security\nvulnerabilities, traditionally performed manually by skilled security\nspecialists. This complex process involves gathering information about the\ntarget system, identifying entry points, exploiting the system, and reporting\nfindings. Despite its effectiveness, manual penetration testing is\ntime-consuming and expensive, often requiring significant expertise and\nresources that many organizations cannot afford. While automated penetration\ntesting methods have been proposed, they often fall short in real-world\napplications due to limitations in flexibility, adaptability, and\nimplementation.\n  Recent advancements in large language models (LLMs) offer new opportunities\nfor enhancing penetration testing through increased intelligence and\nautomation. However, current LLM-based approaches still face significant\nchallenges, including limited penetration testing knowledge and a lack of\ncomprehensive automation capabilities. To address these gaps, we propose\nPentestAgent, a novel LLM-based automated penetration testing framework that\nleverages the power of LLMs and various LLM-based techniques like Retrieval\nAugmented Generation (RAG) to enhance penetration testing knowledge and\nautomate various tasks. Our framework leverages multi-agent collaboration to\nautomate intelligence gathering, vulnerability analysis, and exploitation\nstages, reducing manual intervention. We evaluate PentestAgent using a\ncomprehensive benchmark, demonstrating superior performance in task completion\nand overall efficiency. This work significantly advances the practical\napplicability of automated penetration testing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing is a critical technique for identifying security\nvulnerabilities, traditionally performed manually by skilled security\nspecialists. This complex process involves gathering information about the\ntarget system, identifying entry points, exploiting the system, and reporting\nfindings. Despite its effectiveness, manual penetration testing is\ntime-consuming and expensive, often requiring significant expertise and\nresources that many organizations cannot afford. While automated penetration\ntesting methods have been proposed, they often fall short in real-world\napplications due to limitations in flexibility, adaptability, and\nimplementation.\n  Recent advancements in large language models (LLMs) offer new opportunities\nfor enhancing penetration testing through increased intelligence and\nautomation. However, current LLM-based approaches still face significant\nchallenges, including limited penetration testing knowledge and a lack of\ncomprehensive automation capabilities. To address these gaps, we propose\nPentestAgent, a novel LLM-based automated penetration testing framework that\nleverages the power of LLMs and various LLM-based techniques like Retrieval\nAugmented Generation (RAG) to enhance penetration testing knowledge and\nautomate various tasks. Our framework leverages multi-agent collaboration to\nautomate intelligence gathering, vulnerability analysis, and exploitation\nstages, reducing manual intervention. We evaluate PentestAgent using a\ncomprehensive benchmark, demonstrating superior performance in task completion\nand overall efficiency. This work significantly advances the practical\napplicability of automated penetration testing systems."
                },
                "authors": [
                    {
                        "name": "Xiangmin Shen"
                    },
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Zhenyuan Li"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Wencheng Zhao"
                    },
                    {
                        "name": "Dawei Sun"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Wei Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ruan"
                },
                "author": "Wei Ruan",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]